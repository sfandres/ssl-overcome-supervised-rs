IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 09:23:51,540	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 09:23:51,540	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 09:23:53,965	SUCC scripts.py:747 -- --------------------
2024-01-07 09:23:53,965	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 09:23:53,965	SUCC scripts.py:749 -- --------------------
2024-01-07 09:23:53,966	INFO scripts.py:751 -- Next steps
2024-01-07 09:23:53,966	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 09:23:53,966	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 09:23:53,966	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 09:23:53,966	INFO scripts.py:773 -- import ray
2024-01-07 09:23:53,966	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 09:23:53,966	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 09:23:53,966	INFO scripts.py:791 --   ray status
2024-01-07 09:23:53,966	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 09:23:53,966	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 09:23:53,966	INFO scripts.py:810 --   ray stop
2024-01-07 09:23:53,967	INFO scripts.py:891 -- --block
2024-01-07 09:23:53,967	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 09:23:53,967	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              14364972840096048950
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7f59228ab160>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          BarlowTwins
task_name:           multilabel
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          5
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         random
seed:                42
dropout:             None
transfer_learning:   FT
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1


Model resnet18 with pretrained weights using BarlowTwins SSL
Model loaded from snapshot_BarlowTwins_resnet18_bd=False_iw=random.pt
Model name:        BarlowTwins
Backbone name:     resnet18
Hidden layer dim.: 256
Output layer dim.: 128
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Fine-tuning adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 09:24:37,803	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 09:24:37,823	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 09:24:55,996	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-07 09:24:56 (running for 00:00:17.95)
Memory usage on this node: 13.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |
| train_32e5a_00001 | PENDING  |                    | 0.001  |       0.99 |         0      |
| train_32e5a_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_32e5a_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34042)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=34042)[0m Configuration completed!
[2m[36m(func pid=34042)[0m New optimizer parameters:
[2m[36m(func pid=34042)[0m SGD (
[2m[36m(func pid=34042)[0m Parameter Group 0
[2m[36m(func pid=34042)[0m     dampening: 0
[2m[36m(func pid=34042)[0m     differentiable: False
[2m[36m(func pid=34042)[0m     foreach: None
[2m[36m(func pid=34042)[0m     lr: 0.0001
[2m[36m(func pid=34042)[0m     maximize: False
[2m[36m(func pid=34042)[0m     momentum: 0.99
[2m[36m(func pid=34042)[0m     nesterov: False
[2m[36m(func pid=34042)[0m     weight_decay: 0
[2m[36m(func pid=34042)[0m )
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8949 | Steps: 2 | Val loss: 0.7091 | Batch size: 32 | lr: 0.0001 | Duration: 5.23s
[2m[36m(func pid=34042)[0m rmse: 0.18269839882850647
[2m[36m(func pid=34042)[0m mae:  0.1344444304704666
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
== Status ==
Current time: 2024-01-07 09:25:07 (running for 00:00:28.12)
Memory usage on this node: 15.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |
| train_32e5a_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_32e5a_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34415)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=34415)[0m Configuration completed!
[2m[36m(func pid=34415)[0m New optimizer parameters:
[2m[36m(func pid=34415)[0m SGD (
[2m[36m(func pid=34415)[0m Parameter Group 0
[2m[36m(func pid=34415)[0m     dampening: 0
[2m[36m(func pid=34415)[0m     differentiable: False
[2m[36m(func pid=34415)[0m     foreach: None
[2m[36m(func pid=34415)[0m     lr: 0.001
[2m[36m(func pid=34415)[0m     maximize: False
[2m[36m(func pid=34415)[0m     momentum: 0.99
[2m[36m(func pid=34415)[0m     nesterov: False
[2m[36m(func pid=34415)[0m     weight_decay: 0
[2m[36m(func pid=34415)[0m )
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8978 | Steps: 2 | Val loss: 0.7073 | Batch size: 32 | lr: 0.001 | Duration: 4.63s
[2m[36m(func pid=34415)[0m rmse: 0.18267856538295746
[2m[36m(func pid=34415)[0m mae:  0.13440752029418945
[2m[36m(func pid=34415)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
== Status ==
Current time: 2024-01-07 09:25:15 (running for 00:00:36.70)
Memory usage on this node: 17.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |
| train_32e5a_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34838)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34838)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=34838)[0m Configuration completed!
[2m[36m(func pid=34838)[0m New optimizer parameters:
[2m[36m(func pid=34838)[0m SGD (
[2m[36m(func pid=34838)[0m Parameter Group 0
[2m[36m(func pid=34838)[0m     dampening: 0
[2m[36m(func pid=34838)[0m     differentiable: False
[2m[36m(func pid=34838)[0m     foreach: None
[2m[36m(func pid=34838)[0m     lr: 0.01
[2m[36m(func pid=34838)[0m     maximize: False
[2m[36m(func pid=34838)[0m     momentum: 0.99
[2m[36m(func pid=34838)[0m     nesterov: False
[2m[36m(func pid=34838)[0m     weight_decay: 0
[2m[36m(func pid=34838)[0m )
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8896 | Steps: 2 | Val loss: 0.6845 | Batch size: 32 | lr: 0.01 | Duration: 4.68s
[2m[36m(func pid=34838)[0m rmse: 0.18171049654483795
[2m[36m(func pid=34838)[0m mae:  0.13357847929000854
[2m[36m(func pid=34838)[0m rmse_per_class: [0.113, 0.266, 0.11, 0.339, 0.109, 0.191, 0.29, 0.143, 0.144, 0.112]
== Status ==
Current time: 2024-01-07 09:25:23 (running for 00:00:44.35)
Memory usage on this node: 20.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 09:25:30 (running for 00:00:51.78)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |        |        |                      |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |        |        |                      |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |   0.89 |  0.182 |                    1 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |        |        |                      |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=35262)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=35262)[0m Configuration completed!
[2m[36m(func pid=35262)[0m New optimizer parameters:
[2m[36m(func pid=35262)[0m SGD (
[2m[36m(func pid=35262)[0m Parameter Group 0
[2m[36m(func pid=35262)[0m     dampening: 0
[2m[36m(func pid=35262)[0m     differentiable: False
[2m[36m(func pid=35262)[0m     foreach: None
[2m[36m(func pid=35262)[0m     lr: 0.1
[2m[36m(func pid=35262)[0m     maximize: False
[2m[36m(func pid=35262)[0m     momentum: 0.99
[2m[36m(func pid=35262)[0m     nesterov: False
[2m[36m(func pid=35262)[0m     weight_decay: 0
[2m[36m(func pid=35262)[0m )
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8915 | Steps: 2 | Val loss: 0.6985 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8922 | Steps: 2 | Val loss: 0.7047 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8377 | Steps: 2 | Val loss: 0.6266 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8353 | Steps: 2 | Val loss: 0.4977 | Batch size: 32 | lr: 0.1 | Duration: 4.76s
== Status ==
Current time: 2024-01-07 09:25:35 (running for 00:00:56.82)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.895 |  0.183 |                    1 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.898 |  0.183 |                    1 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.89  |  0.182 |                    1 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |        |        |                      |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.18246980011463165
[2m[36m(func pid=34415)[0m mae:  0.13428457081317902
[2m[36m(func pid=34415)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.143, 0.113]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.1824539452791214
[2m[36m(func pid=34042)[0m mae:  0.13430489599704742
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1810193955898285
[2m[36m(func pid=34838)[0m mae:  0.1327868103981018
[2m[36m(func pid=34838)[0m rmse_per_class: [0.111, 0.266, 0.11, 0.338, 0.111, 0.191, 0.287, 0.142, 0.143, 0.112]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.18018095195293427
[2m[36m(func pid=35262)[0m mae:  0.1318250149488449
[2m[36m(func pid=35262)[0m rmse_per_class: [0.109, 0.265, 0.116, 0.337, 0.105, 0.19, 0.285, 0.142, 0.144, 0.109]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8822 | Steps: 2 | Val loss: 0.6866 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8948 | Steps: 2 | Val loss: 0.7017 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7471 | Steps: 2 | Val loss: 0.5475 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5227 | Steps: 2 | Val loss: 0.3324 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 09:25:41 (running for 00:01:02.13)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.892 |  0.182 |                    2 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.882 |  0.182 |                    3 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.838 |  0.181 |                    2 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.835 |  0.18  |                    1 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.1820816993713379
[2m[36m(func pid=34415)[0m mae:  0.13398970663547516
[2m[36m(func pid=34415)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.112, 0.19, 0.293, 0.143, 0.143, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.18213501572608948
[2m[36m(func pid=34042)[0m mae:  0.13406425714492798
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.266, 0.105, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.18010464310646057
[2m[36m(func pid=34838)[0m mae:  0.13188093900680542
[2m[36m(func pid=34838)[0m rmse_per_class: [0.111, 0.265, 0.108, 0.336, 0.11, 0.191, 0.285, 0.142, 0.142, 0.112]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.17583782970905304
[2m[36m(func pid=35262)[0m mae:  0.12832605838775635
[2m[36m(func pid=35262)[0m rmse_per_class: [0.113, 0.266, 0.105, 0.331, 0.088, 0.19, 0.275, 0.139, 0.144, 0.107]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8655 | Steps: 2 | Val loss: 0.6718 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8951 | Steps: 2 | Val loss: 0.6984 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6427 | Steps: 2 | Val loss: 0.4658 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4033 | Steps: 2 | Val loss: 0.3211 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 09:25:46 (running for 00:01:07.39)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.895 |  0.182 |                    3 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.865 |  0.182 |                    4 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.747 |  0.18  |                    3 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.523 |  0.176 |                    2 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.18159747123718262
[2m[36m(func pid=34415)[0m mae:  0.13360509276390076
[2m[36m(func pid=34415)[0m rmse_per_class: [0.117, 0.266, 0.104, 0.339, 0.112, 0.19, 0.293, 0.142, 0.143, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.18173673748970032
[2m[36m(func pid=34042)[0m mae:  0.13375124335289001
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.339, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.17881546914577484
[2m[36m(func pid=34838)[0m mae:  0.1307525634765625
[2m[36m(func pid=34838)[0m rmse_per_class: [0.112, 0.265, 0.104, 0.335, 0.106, 0.191, 0.282, 0.141, 0.142, 0.111]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.16901114583015442
[2m[36m(func pid=35262)[0m mae:  0.12265465408563614
[2m[36m(func pid=35262)[0m rmse_per_class: [0.117, 0.266, 0.085, 0.319, 0.074, 0.194, 0.258, 0.132, 0.143, 0.104]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8454 | Steps: 2 | Val loss: 0.6551 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8903 | Steps: 2 | Val loss: 0.6957 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5379 | Steps: 2 | Val loss: 0.3966 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4717 | Steps: 2 | Val loss: 0.3766 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=34415)[0m rmse: 0.181075319647789
[2m[36m(func pid=34415)[0m mae:  0.13316655158996582
[2m[36m(func pid=34415)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.111, 0.19, 0.292, 0.141, 0.143, 0.111]
[2m[36m(func pid=34415)[0m 
== Status ==
Current time: 2024-01-07 09:25:51 (running for 00:01:12.70)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.89  |  0.181 |                    5 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.845 |  0.181 |                    5 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.643 |  0.179 |                    4 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.403 |  0.169 |                    3 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.1813364028930664
[2m[36m(func pid=34042)[0m mae:  0.13342206180095673
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.112, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1772620975971222
[2m[36m(func pid=34838)[0m mae:  0.12944339215755463
[2m[36m(func pid=34838)[0m rmse_per_class: [0.114, 0.264, 0.101, 0.332, 0.1, 0.19, 0.278, 0.14, 0.143, 0.111]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.16229018568992615
[2m[36m(func pid=35262)[0m mae:  0.11630459874868393
[2m[36m(func pid=35262)[0m rmse_per_class: [0.121, 0.259, 0.067, 0.308, 0.06, 0.195, 0.241, 0.131, 0.141, 0.098]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8221 | Steps: 2 | Val loss: 0.6377 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8908 | Steps: 2 | Val loss: 0.6934 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4649 | Steps: 2 | Val loss: 0.3479 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5458 | Steps: 2 | Val loss: 0.4264 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=34415)[0m rmse: 0.18067188560962677
[2m[36m(func pid=34415)[0m mae:  0.13280005753040314
[2m[36m(func pid=34415)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.337, 0.11, 0.19, 0.291, 0.141, 0.143, 0.111]
[2m[36m(func pid=34415)[0m 
== Status ==
Current time: 2024-01-07 09:25:56 (running for 00:01:17.76)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.891 |  0.181 |                    6 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.822 |  0.181 |                    6 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.538 |  0.177 |                    5 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.472 |  0.162 |                    4 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.1809806078672409
[2m[36m(func pid=34042)[0m mae:  0.13311806321144104
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.111, 0.19, 0.293, 0.141, 0.142, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.17563574016094208
[2m[36m(func pid=34838)[0m mae:  0.12807230651378632
[2m[36m(func pid=34838)[0m rmse_per_class: [0.115, 0.263, 0.097, 0.329, 0.094, 0.19, 0.275, 0.138, 0.143, 0.11]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.15561792254447937
[2m[36m(func pid=35262)[0m mae:  0.10853207111358643
[2m[36m(func pid=35262)[0m rmse_per_class: [0.12, 0.252, 0.052, 0.297, 0.055, 0.193, 0.228, 0.129, 0.14, 0.091]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7958 | Steps: 2 | Val loss: 0.6164 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8848 | Steps: 2 | Val loss: 0.6905 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4138 | Steps: 2 | Val loss: 0.3194 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5764 | Steps: 2 | Val loss: 0.4618 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 09:26:01 (running for 00:01:22.81)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.891 |  0.181 |                    6 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.796 |  0.18  |                    7 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.465 |  0.176 |                    6 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.546 |  0.156 |                    5 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.18024662137031555
[2m[36m(func pid=34415)[0m mae:  0.1323992908000946
[2m[36m(func pid=34415)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.108, 0.19, 0.29, 0.141, 0.143, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.18068714439868927
[2m[36m(func pid=34042)[0m mae:  0.1328611671924591
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.264, 0.103, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1739494502544403
[2m[36m(func pid=34838)[0m mae:  0.12661615014076233
[2m[36m(func pid=34838)[0m rmse_per_class: [0.117, 0.264, 0.094, 0.326, 0.088, 0.19, 0.271, 0.137, 0.143, 0.109]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.1522052139043808
[2m[36m(func pid=35262)[0m mae:  0.1006699949502945
[2m[36m(func pid=35262)[0m rmse_per_class: [0.104, 0.252, 0.047, 0.286, 0.055, 0.199, 0.235, 0.126, 0.133, 0.085]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7637 | Steps: 2 | Val loss: 0.5939 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8838 | Steps: 2 | Val loss: 0.6880 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3971 | Steps: 2 | Val loss: 0.3090 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5557 | Steps: 2 | Val loss: 0.4678 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 09:26:07 (running for 00:01:28.12)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.884 |  0.18  |                    8 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.796 |  0.18  |                    7 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.414 |  0.174 |                    7 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.576 |  0.152 |                    6 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.1804639995098114
[2m[36m(func pid=34042)[0m mae:  0.13265417516231537
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17983940243721008
[2m[36m(func pid=34415)[0m mae:  0.13199780881404877
[2m[36m(func pid=34415)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.106, 0.19, 0.289, 0.141, 0.143, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1720980852842331
[2m[36m(func pid=34838)[0m mae:  0.12497419118881226
[2m[36m(func pid=34838)[0m rmse_per_class: [0.118, 0.264, 0.091, 0.322, 0.082, 0.191, 0.266, 0.135, 0.143, 0.109]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.16048716008663177
[2m[36m(func pid=35262)[0m mae:  0.10180250555276871
[2m[36m(func pid=35262)[0m rmse_per_class: [0.092, 0.253, 0.047, 0.303, 0.056, 0.207, 0.31, 0.123, 0.131, 0.083]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8796 | Steps: 2 | Val loss: 0.6851 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7351 | Steps: 2 | Val loss: 0.5708 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3982 | Steps: 2 | Val loss: 0.3132 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.5044 | Steps: 2 | Val loss: 0.4539 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 09:26:12 (running for 00:01:33.23)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.88  |  0.18  |                    9 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.764 |  0.18  |                    8 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.397 |  0.172 |                    8 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.556 |  0.16  |                    7 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.18023869395256042
[2m[36m(func pid=34042)[0m mae:  0.13245601952075958
[2m[36m(func pid=34042)[0m rmse_per_class: [0.115, 0.263, 0.102, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17960616946220398
[2m[36m(func pid=34415)[0m mae:  0.13173195719718933
[2m[36m(func pid=34415)[0m rmse_per_class: [0.116, 0.264, 0.101, 0.336, 0.106, 0.189, 0.287, 0.141, 0.143, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16987934708595276
[2m[36m(func pid=34838)[0m mae:  0.12300882488489151
[2m[36m(func pid=34838)[0m rmse_per_class: [0.118, 0.263, 0.086, 0.317, 0.076, 0.191, 0.261, 0.134, 0.143, 0.109]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.17960330843925476
[2m[36m(func pid=35262)[0m mae:  0.11332354694604874
[2m[36m(func pid=35262)[0m rmse_per_class: [0.088, 0.257, 0.047, 0.345, 0.056, 0.217, 0.449, 0.121, 0.134, 0.083]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8736 | Steps: 2 | Val loss: 0.6817 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7015 | Steps: 2 | Val loss: 0.5455 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4187 | Steps: 2 | Val loss: 0.3263 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4328 | Steps: 2 | Val loss: 0.4730 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=34042)[0m rmse: 0.1800527274608612
[2m[36m(func pid=34042)[0m mae:  0.1322776973247528
[2m[36m(func pid=34042)[0m rmse_per_class: [0.115, 0.263, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.109]
== Status ==
Current time: 2024-01-07 09:26:17 (running for 00:01:38.40)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.874 |   0.18 |                   10 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.735 |   0.18 |                    9 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.398 |   0.17 |                    9 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.504 |   0.18 |                    8 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.1792479157447815
[2m[36m(func pid=34415)[0m mae:  0.13137702643871307
[2m[36m(func pid=34415)[0m rmse_per_class: [0.117, 0.263, 0.1, 0.336, 0.105, 0.19, 0.286, 0.141, 0.143, 0.113]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16700080037117004
[2m[36m(func pid=34838)[0m mae:  0.12049535661935806
[2m[36m(func pid=34838)[0m rmse_per_class: [0.118, 0.261, 0.08, 0.313, 0.07, 0.191, 0.253, 0.133, 0.142, 0.108]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.19527335464954376
[2m[36m(func pid=35262)[0m mae:  0.1236894354224205
[2m[36m(func pid=35262)[0m rmse_per_class: [0.092, 0.265, 0.048, 0.369, 0.056, 0.222, 0.556, 0.126, 0.136, 0.082]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8689 | Steps: 2 | Val loss: 0.6782 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6704 | Steps: 2 | Val loss: 0.5187 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4422 | Steps: 2 | Val loss: 0.3461 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3826 | Steps: 2 | Val loss: 0.5794 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 09:26:22 (running for 00:01:43.71)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.869 |  0.18  |                   11 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.701 |  0.179 |                   10 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.419 |  0.167 |                   10 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.433 |  0.195 |                    9 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.1798866242170334
[2m[36m(func pid=34042)[0m mae:  0.1321292370557785
[2m[36m(func pid=34042)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17887946963310242
[2m[36m(func pid=34415)[0m mae:  0.1310196965932846
[2m[36m(func pid=34415)[0m rmse_per_class: [0.117, 0.263, 0.099, 0.335, 0.104, 0.19, 0.286, 0.141, 0.142, 0.113]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1643771231174469
[2m[36m(func pid=34838)[0m mae:  0.11813479661941528
[2m[36m(func pid=34838)[0m rmse_per_class: [0.118, 0.26, 0.075, 0.309, 0.065, 0.192, 0.245, 0.131, 0.142, 0.106]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.2081555426120758
[2m[36m(func pid=35262)[0m mae:  0.13184623420238495
[2m[36m(func pid=35262)[0m rmse_per_class: [0.104, 0.271, 0.049, 0.38, 0.056, 0.226, 0.619, 0.156, 0.138, 0.082]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8662 | Steps: 2 | Val loss: 0.6746 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4722 | Steps: 2 | Val loss: 0.3686 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.6367 | Steps: 2 | Val loss: 0.4919 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3391 | Steps: 2 | Val loss: 0.7210 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 09:26:27 (running for 00:01:48.93)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.866 |  0.18  |                   12 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.67  |  0.179 |                   11 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.442 |  0.164 |                   11 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.383 |  0.208 |                   10 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17978335916996002
[2m[36m(func pid=34042)[0m mae:  0.13203418254852295
[2m[36m(func pid=34042)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16127999126911163
[2m[36m(func pid=34838)[0m mae:  0.11523289978504181
[2m[36m(func pid=34838)[0m rmse_per_class: [0.117, 0.257, 0.068, 0.306, 0.061, 0.192, 0.237, 0.131, 0.142, 0.103]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17859478294849396
[2m[36m(func pid=34415)[0m mae:  0.13075920939445496
[2m[36m(func pid=34415)[0m rmse_per_class: [0.117, 0.263, 0.098, 0.335, 0.102, 0.19, 0.285, 0.141, 0.142, 0.113]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.21215887367725372
[2m[36m(func pid=35262)[0m mae:  0.13327786326408386
[2m[36m(func pid=35262)[0m rmse_per_class: [0.109, 0.284, 0.049, 0.382, 0.056, 0.225, 0.621, 0.172, 0.139, 0.083]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8583 | Steps: 2 | Val loss: 0.6706 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.6039 | Steps: 2 | Val loss: 0.4660 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4930 | Steps: 2 | Val loss: 0.3913 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3415 | Steps: 2 | Val loss: 0.9072 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 09:26:32 (running for 00:01:54.02)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.858 |  0.18  |                   13 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.637 |  0.179 |                   12 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.472 |  0.161 |                   12 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.339 |  0.212 |                   11 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17970798909664154
[2m[36m(func pid=34042)[0m mae:  0.13196676969528198
[2m[36m(func pid=34042)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1582622528076172
[2m[36m(func pid=34838)[0m mae:  0.11225327104330063
[2m[36m(func pid=34838)[0m rmse_per_class: [0.116, 0.254, 0.061, 0.302, 0.058, 0.192, 0.228, 0.13, 0.141, 0.1]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.2127574384212494
[2m[36m(func pid=35262)[0m mae:  0.1330891251564026
[2m[36m(func pid=35262)[0m rmse_per_class: [0.11, 0.278, 0.049, 0.38, 0.056, 0.224, 0.61, 0.196, 0.14, 0.086]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17820152640342712
[2m[36m(func pid=34415)[0m mae:  0.13039708137512207
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.262, 0.097, 0.334, 0.101, 0.19, 0.284, 0.141, 0.142, 0.113]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8509 | Steps: 2 | Val loss: 0.6655 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3474 | Steps: 2 | Val loss: 0.8689 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5741 | Steps: 2 | Val loss: 0.4415 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5159 | Steps: 2 | Val loss: 0.4130 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 09:26:38 (running for 00:01:59.42)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.851 |  0.18  |                   14 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.604 |  0.178 |                   13 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.493 |  0.158 |                   13 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.342 |  0.213 |                   12 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17958658933639526
[2m[36m(func pid=34042)[0m mae:  0.13185885548591614
[2m[36m(func pid=34042)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.20876345038414001
[2m[36m(func pid=35262)[0m mae:  0.12992307543754578
[2m[36m(func pid=35262)[0m rmse_per_class: [0.11, 0.286, 0.049, 0.377, 0.056, 0.194, 0.578, 0.201, 0.142, 0.094]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17783787846565247
[2m[36m(func pid=34415)[0m mae:  0.1300768405199051
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.262, 0.096, 0.333, 0.099, 0.19, 0.283, 0.141, 0.142, 0.113]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1555553674697876
[2m[36m(func pid=34838)[0m mae:  0.10942234098911285
[2m[36m(func pid=34838)[0m rmse_per_class: [0.114, 0.25, 0.054, 0.3, 0.056, 0.192, 0.223, 0.13, 0.14, 0.096]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8483 | Steps: 2 | Val loss: 0.6604 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3545 | Steps: 2 | Val loss: 1.0706 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5458 | Steps: 2 | Val loss: 0.4190 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5288 | Steps: 2 | Val loss: 0.4314 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 09:26:43 (running for 00:02:04.65)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.848 |  0.18  |                   15 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.574 |  0.178 |                   14 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.516 |  0.156 |                   14 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.347 |  0.209 |                   13 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17950166761875153
[2m[36m(func pid=34042)[0m mae:  0.13177905976772308
[2m[36m(func pid=34042)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.20759618282318115
[2m[36m(func pid=35262)[0m mae:  0.13082121312618256
[2m[36m(func pid=35262)[0m rmse_per_class: [0.111, 0.294, 0.049, 0.374, 0.056, 0.187, 0.415, 0.323, 0.142, 0.124]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15393871068954468
[2m[36m(func pid=34838)[0m mae:  0.10755244642496109
[2m[36m(func pid=34838)[0m rmse_per_class: [0.113, 0.247, 0.049, 0.299, 0.055, 0.192, 0.223, 0.129, 0.14, 0.093]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17735877633094788
[2m[36m(func pid=34415)[0m mae:  0.12964364886283875
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.262, 0.095, 0.332, 0.098, 0.19, 0.283, 0.141, 0.142, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8410 | Steps: 2 | Val loss: 0.6557 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3514 | Steps: 2 | Val loss: 0.4363 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5172 | Steps: 2 | Val loss: 0.3982 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5407 | Steps: 2 | Val loss: 0.4454 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 09:26:48 (running for 00:02:09.92)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.848 |  0.18  |                   15 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.546 |  0.177 |                   15 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.529 |  0.154 |                   15 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.351 |  0.194 |                   15 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.1794746071100235
[2m[36m(func pid=34042)[0m mae:  0.13175883889198303
[2m[36m(func pid=34042)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.1939590871334076
[2m[36m(func pid=35262)[0m mae:  0.11383672058582306
[2m[36m(func pid=35262)[0m rmse_per_class: [0.324, 0.288, 0.051, 0.294, 0.056, 0.186, 0.256, 0.201, 0.137, 0.146]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1524670124053955
[2m[36m(func pid=34838)[0m mae:  0.10550590604543686
[2m[36m(func pid=34838)[0m rmse_per_class: [0.11, 0.245, 0.044, 0.296, 0.054, 0.192, 0.225, 0.129, 0.139, 0.091]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.1769731193780899
[2m[36m(func pid=34415)[0m mae:  0.12931343913078308
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.262, 0.095, 0.331, 0.096, 0.191, 0.282, 0.141, 0.142, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8351 | Steps: 2 | Val loss: 0.6507 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3172 | Steps: 2 | Val loss: 0.5749 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4948 | Steps: 2 | Val loss: 0.3801 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5336 | Steps: 2 | Val loss: 0.4546 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 09:26:54 (running for 00:02:15.05)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.835 |  0.179 |                   17 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.517 |  0.177 |                   16 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.541 |  0.152 |                   16 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.351 |  0.194 |                   15 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17945453524589539
[2m[36m(func pid=34042)[0m mae:  0.13173319399356842
[2m[36m(func pid=34042)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17668390274047852
[2m[36m(func pid=34415)[0m mae:  0.12905465066432953
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.262, 0.095, 0.331, 0.095, 0.191, 0.281, 0.141, 0.142, 0.113]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.23646993935108185
[2m[36m(func pid=35262)[0m mae:  0.14725583791732788
[2m[36m(func pid=35262)[0m rmse_per_class: [0.399, 0.295, 0.109, 0.364, 0.056, 0.196, 0.29, 0.187, 0.267, 0.202]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1510189324617386
[2m[36m(func pid=34838)[0m mae:  0.10338398069143295
[2m[36m(func pid=34838)[0m rmse_per_class: [0.106, 0.243, 0.04, 0.292, 0.054, 0.192, 0.23, 0.128, 0.137, 0.088]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8276 | Steps: 2 | Val loss: 0.6450 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4742 | Steps: 2 | Val loss: 0.3634 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3131 | Steps: 2 | Val loss: 0.6679 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.5271 | Steps: 2 | Val loss: 0.4597 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 09:26:59 (running for 00:02:20.16)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.828 |  0.179 |                   18 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.495 |  0.177 |                   17 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.534 |  0.151 |                   17 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.317 |  0.236 |                   16 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17938591539859772
[2m[36m(func pid=34042)[0m mae:  0.1316724270582199
[2m[36m(func pid=34042)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17619839310646057
[2m[36m(func pid=34415)[0m mae:  0.1286381632089615
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.261, 0.094, 0.33, 0.093, 0.191, 0.28, 0.14, 0.143, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.24524912238121033
[2m[36m(func pid=35262)[0m mae:  0.15662728250026703
[2m[36m(func pid=35262)[0m rmse_per_class: [0.188, 0.296, 0.11, 0.383, 0.056, 0.183, 0.309, 0.187, 0.489, 0.252]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15030141174793243
[2m[36m(func pid=34838)[0m mae:  0.10170366615056992
[2m[36m(func pid=34838)[0m rmse_per_class: [0.102, 0.241, 0.037, 0.29, 0.054, 0.193, 0.236, 0.128, 0.136, 0.086]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8204 | Steps: 2 | Val loss: 0.6390 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4547 | Steps: 2 | Val loss: 0.3493 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3189 | Steps: 2 | Val loss: 0.5386 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.5265 | Steps: 2 | Val loss: 0.4622 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:27:04 (running for 00:02:25.39)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.82  |  0.179 |                   19 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.474 |  0.176 |                   18 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.527 |  0.15  |                   18 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.313 |  0.245 |                   17 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17927642166614532
[2m[36m(func pid=34042)[0m mae:  0.1315743625164032
[2m[36m(func pid=34042)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.14, 0.142, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17579515278339386
[2m[36m(func pid=34415)[0m mae:  0.12827423214912415
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.261, 0.093, 0.329, 0.092, 0.191, 0.279, 0.14, 0.143, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.2353765070438385
[2m[36m(func pid=35262)[0m mae:  0.14951938390731812
[2m[36m(func pid=35262)[0m rmse_per_class: [0.166, 0.293, 0.111, 0.378, 0.056, 0.171, 0.294, 0.168, 0.316, 0.4]
[2m[36m(func pid=34838)[0m rmse: 0.1512409746646881
[2m[36m(func pid=34838)[0m mae:  0.10104302316904068
[2m[36m(func pid=34838)[0m rmse_per_class: [0.098, 0.24, 0.036, 0.291, 0.054, 0.195, 0.25, 0.128, 0.135, 0.085]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8134 | Steps: 2 | Val loss: 0.6325 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4378 | Steps: 2 | Val loss: 0.3376 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3330 | Steps: 2 | Val loss: 0.4533 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5109 | Steps: 2 | Val loss: 0.4642 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=34042)[0m rmse: 0.17926454544067383
[2m[36m(func pid=34042)[0m mae:  0.13156679272651672
[2m[36m(func pid=34042)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=34042)[0m 
== Status ==
Current time: 2024-01-07 09:27:09 (running for 00:02:30.78)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.813 |  0.179 |                   20 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.455 |  0.176 |                   19 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.527 |  0.151 |                   19 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.319 |  0.235 |                   18 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.17528177797794342
[2m[36m(func pid=34415)[0m mae:  0.1278320997953415
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.261, 0.093, 0.328, 0.09, 0.191, 0.278, 0.139, 0.143, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.21749000251293182
[2m[36m(func pid=35262)[0m mae:  0.13985806703567505
[2m[36m(func pid=35262)[0m rmse_per_class: [0.162, 0.287, 0.096, 0.359, 0.056, 0.167, 0.282, 0.151, 0.187, 0.429]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15373595058918
[2m[36m(func pid=34838)[0m mae:  0.10160690546035767
[2m[36m(func pid=34838)[0m rmse_per_class: [0.095, 0.241, 0.036, 0.297, 0.055, 0.197, 0.271, 0.128, 0.134, 0.084]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8059 | Steps: 2 | Val loss: 0.6261 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.2810 | Steps: 2 | Val loss: 0.4169 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4259 | Steps: 2 | Val loss: 0.3287 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5022 | Steps: 2 | Val loss: 0.4606 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 09:27:14 (running for 00:02:36.02)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.806 |  0.179 |                   21 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.438 |  0.175 |                   20 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.511 |  0.154 |                   20 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.333 |  0.217 |                   19 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.1792462170124054
[2m[36m(func pid=34042)[0m mae:  0.13154242932796478
[2m[36m(func pid=34042)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.14, 0.142, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.21157780289649963
[2m[36m(func pid=35262)[0m mae:  0.13505634665489197
[2m[36m(func pid=35262)[0m rmse_per_class: [0.17, 0.282, 0.112, 0.349, 0.056, 0.176, 0.279, 0.158, 0.159, 0.375]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17493422329425812
[2m[36m(func pid=34415)[0m mae:  0.12753964960575104
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.261, 0.092, 0.327, 0.088, 0.191, 0.277, 0.139, 0.143, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15715806186199188
[2m[36m(func pid=34838)[0m mae:  0.10288208723068237
[2m[36m(func pid=34838)[0m rmse_per_class: [0.091, 0.242, 0.035, 0.307, 0.055, 0.198, 0.298, 0.128, 0.134, 0.084]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.7990 | Steps: 2 | Val loss: 0.6204 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3147 | Steps: 2 | Val loss: 0.3938 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4165 | Steps: 2 | Val loss: 0.3214 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4871 | Steps: 2 | Val loss: 0.4513 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 09:27:20 (running for 00:02:41.18)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.806 |  0.179 |                   21 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.426 |  0.175 |                   21 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.502 |  0.157 |                   21 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.315 |  0.209 |                   21 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=35262)[0m rmse: 0.20864303410053253
[2m[36m(func pid=35262)[0m mae:  0.1316433846950531
[2m[36m(func pid=35262)[0m rmse_per_class: [0.18, 0.275, 0.164, 0.333, 0.055, 0.199, 0.284, 0.158, 0.147, 0.291]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.17921604216098785
[2m[36m(func pid=34042)[0m mae:  0.13150498270988464
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17434635758399963
[2m[36m(func pid=34415)[0m mae:  0.1270413100719452
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.261, 0.091, 0.326, 0.086, 0.191, 0.276, 0.139, 0.143, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16024407744407654
[2m[36m(func pid=34838)[0m mae:  0.10413850843906403
[2m[36m(func pid=34838)[0m rmse_per_class: [0.089, 0.243, 0.033, 0.316, 0.055, 0.199, 0.323, 0.126, 0.134, 0.083]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3019 | Steps: 2 | Val loss: 0.4087 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.7876 | Steps: 2 | Val loss: 0.6141 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4090 | Steps: 2 | Val loss: 0.3161 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4665 | Steps: 2 | Val loss: 0.4355 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=35262)[0m rmse: 0.21699635684490204
[2m[36m(func pid=35262)[0m mae:  0.134700208902359
[2m[36m(func pid=35262)[0m rmse_per_class: [0.208, 0.266, 0.254, 0.32, 0.053, 0.187, 0.305, 0.153, 0.147, 0.278]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:27:25 (running for 00:02:46.42)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.788 |  0.179 |                   23 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.417 |  0.174 |                   22 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.487 |  0.16  |                   22 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.302 |  0.217 |                   22 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17919549345970154
[2m[36m(func pid=34042)[0m mae:  0.1314873844385147
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17381510138511658
[2m[36m(func pid=34415)[0m mae:  0.12659482657909393
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.261, 0.09, 0.326, 0.085, 0.191, 0.274, 0.139, 0.143, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16243383288383484
[2m[36m(func pid=34838)[0m mae:  0.10501394420862198
[2m[36m(func pid=34838)[0m rmse_per_class: [0.087, 0.244, 0.031, 0.323, 0.055, 0.2, 0.343, 0.125, 0.134, 0.083]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2767 | Steps: 2 | Val loss: 0.4603 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7826 | Steps: 2 | Val loss: 0.6081 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4007 | Steps: 2 | Val loss: 0.3128 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4364 | Steps: 2 | Val loss: 0.4135 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=35262)[0m rmse: 0.234409362077713
[2m[36m(func pid=35262)[0m mae:  0.14496812224388123
[2m[36m(func pid=35262)[0m rmse_per_class: [0.238, 0.262, 0.274, 0.339, 0.05, 0.179, 0.319, 0.149, 0.149, 0.385]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:27:30 (running for 00:02:51.68)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.788 |  0.179 |                   23 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.409 |  0.174 |                   23 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.436 |  0.163 |                   24 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.277 |  0.234 |                   23 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34838)[0m rmse: 0.1631220579147339
[2m[36m(func pid=34838)[0m mae:  0.10515675693750381
[2m[36m(func pid=34838)[0m rmse_per_class: [0.086, 0.244, 0.03, 0.327, 0.055, 0.2, 0.351, 0.123, 0.134, 0.082]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.17922399938106537
[2m[36m(func pid=34042)[0m mae:  0.13150359690189362
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17347964644432068
[2m[36m(func pid=34415)[0m mae:  0.12632311880588531
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.261, 0.09, 0.324, 0.083, 0.191, 0.273, 0.138, 0.144, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2851 | Steps: 2 | Val loss: 0.4935 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7693 | Steps: 2 | Val loss: 0.6015 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4268 | Steps: 2 | Val loss: 0.3894 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3984 | Steps: 2 | Val loss: 0.3105 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=35262)[0m rmse: 0.23841354250907898
[2m[36m(func pid=35262)[0m mae:  0.14906001091003418
[2m[36m(func pid=35262)[0m rmse_per_class: [0.261, 0.258, 0.169, 0.349, 0.051, 0.198, 0.32, 0.142, 0.154, 0.482]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:27:35 (running for 00:02:56.89)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.783 |  0.179 |                   24 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.401 |  0.173 |                   24 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.427 |  0.163 |                   25 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.285 |  0.238 |                   24 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17914578318595886
[2m[36m(func pid=34042)[0m mae:  0.13142630457878113
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.142, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16251906752586365
[2m[36m(func pid=34838)[0m mae:  0.10470360517501831
[2m[36m(func pid=34838)[0m rmse_per_class: [0.085, 0.244, 0.028, 0.327, 0.055, 0.199, 0.349, 0.121, 0.134, 0.082]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17291110754013062
[2m[36m(func pid=34415)[0m mae:  0.12584158778190613
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.261, 0.089, 0.323, 0.082, 0.191, 0.272, 0.138, 0.144, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2897 | Steps: 2 | Val loss: 0.4927 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7640 | Steps: 2 | Val loss: 0.5946 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4302 | Steps: 2 | Val loss: 0.3675 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3950 | Steps: 2 | Val loss: 0.3100 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=35262)[0m rmse: 0.23700866103172302
[2m[36m(func pid=35262)[0m mae:  0.14893683791160583
[2m[36m(func pid=35262)[0m rmse_per_class: [0.265, 0.253, 0.112, 0.351, 0.054, 0.21, 0.316, 0.142, 0.173, 0.495]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:27:40 (running for 00:03:02.01)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.764 |  0.179 |                   26 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.398 |  0.173 |                   25 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.427 |  0.163 |                   25 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.29  |  0.237 |                   25 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17909769713878632
[2m[36m(func pid=34042)[0m mae:  0.13138103485107422
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.141, 0.143, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1622508466243744
[2m[36m(func pid=34838)[0m mae:  0.10448051989078522
[2m[36m(func pid=34838)[0m rmse_per_class: [0.085, 0.245, 0.027, 0.328, 0.055, 0.199, 0.345, 0.12, 0.134, 0.082]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.1724461019039154
[2m[36m(func pid=34415)[0m mae:  0.12544980645179749
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.261, 0.088, 0.322, 0.08, 0.191, 0.27, 0.138, 0.144, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2834 | Steps: 2 | Val loss: 0.4532 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7555 | Steps: 2 | Val loss: 0.5877 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3855 | Steps: 2 | Val loss: 0.3491 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3948 | Steps: 2 | Val loss: 0.3107 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=35262)[0m rmse: 0.22657731175422668
[2m[36m(func pid=35262)[0m mae:  0.13833071291446686
[2m[36m(func pid=35262)[0m rmse_per_class: [0.228, 0.254, 0.122, 0.343, 0.056, 0.204, 0.298, 0.145, 0.176, 0.439]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:27:46 (running for 00:03:07.12)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.756 |  0.179 |                   27 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.395 |  0.172 |                   26 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.43  |  0.162 |                   26 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.283 |  0.227 |                   26 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.1790362447500229
[2m[36m(func pid=34042)[0m mae:  0.13132300972938538
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.141, 0.143, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16157567501068115
[2m[36m(func pid=34838)[0m mae:  0.10409829765558243
[2m[36m(func pid=34838)[0m rmse_per_class: [0.086, 0.246, 0.027, 0.33, 0.055, 0.197, 0.34, 0.119, 0.134, 0.081]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.17188522219657898
[2m[36m(func pid=34415)[0m mae:  0.12497761100530624
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.261, 0.087, 0.321, 0.078, 0.191, 0.269, 0.138, 0.143, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2781 | Steps: 2 | Val loss: 0.4436 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.7441 | Steps: 2 | Val loss: 0.5808 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3972 | Steps: 2 | Val loss: 0.3122 | Batch size: 32 | lr: 0.001 | Duration: 2.68s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3760 | Steps: 2 | Val loss: 0.3337 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=35262)[0m rmse: 0.215796560049057
[2m[36m(func pid=35262)[0m mae:  0.12978768348693848
[2m[36m(func pid=35262)[0m rmse_per_class: [0.2, 0.262, 0.055, 0.31, 0.059, 0.201, 0.304, 0.149, 0.175, 0.444]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:27:51 (running for 00:03:12.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.744 |  0.179 |                   28 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.395 |  0.172 |                   27 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.386 |  0.162 |                   27 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.278 |  0.216 |                   27 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17906515300273895
[2m[36m(func pid=34042)[0m mae:  0.13133616745471954
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.141, 0.143, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.1712498515844345
[2m[36m(func pid=34415)[0m mae:  0.12444858253002167
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.261, 0.085, 0.32, 0.077, 0.191, 0.268, 0.138, 0.143, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16071723401546478
[2m[36m(func pid=34838)[0m mae:  0.10360269248485565
[2m[36m(func pid=34838)[0m rmse_per_class: [0.087, 0.247, 0.027, 0.33, 0.055, 0.195, 0.332, 0.118, 0.134, 0.081]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2848 | Steps: 2 | Val loss: 0.5664 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7360 | Steps: 2 | Val loss: 0.5745 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3985 | Steps: 2 | Val loss: 0.3147 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3549 | Steps: 2 | Val loss: 0.3222 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=35262)[0m rmse: 0.22803516685962677
[2m[36m(func pid=35262)[0m mae:  0.13639071583747864
[2m[36m(func pid=35262)[0m rmse_per_class: [0.165, 0.304, 0.049, 0.301, 0.06, 0.215, 0.332, 0.158, 0.202, 0.495]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:27:56 (running for 00:03:17.51)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.736 |  0.179 |                   29 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.397 |  0.171 |                   28 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.376 |  0.161 |                   28 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.285 |  0.228 |                   28 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=34042)[0m rmse: 0.1790579855442047

[2m[36m(func pid=34042)[0m mae:  0.13130983710289001
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.141, 0.143, 0.108]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.1707088202238083
[2m[36m(func pid=34415)[0m mae:  0.12397955358028412
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.26, 0.085, 0.32, 0.075, 0.191, 0.266, 0.138, 0.143, 0.112]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1603751927614212
[2m[36m(func pid=34838)[0m mae:  0.10331080108880997
[2m[36m(func pid=34838)[0m rmse_per_class: [0.087, 0.248, 0.027, 0.331, 0.055, 0.193, 0.329, 0.118, 0.134, 0.081]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2727 | Steps: 2 | Val loss: 0.6884 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.7282 | Steps: 2 | Val loss: 0.5681 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4016 | Steps: 2 | Val loss: 0.3175 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3428 | Steps: 2 | Val loss: 0.3162 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=35262)[0m rmse: 0.23420366644859314
[2m[36m(func pid=35262)[0m mae:  0.14018996059894562
[2m[36m(func pid=35262)[0m rmse_per_class: [0.156, 0.304, 0.049, 0.318, 0.06, 0.222, 0.339, 0.16, 0.209, 0.525]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:28:01 (running for 00:03:22.57)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.728 |  0.179 |                   30 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.399 |  0.171 |                   29 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.355 |  0.16  |                   29 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.273 |  0.234 |                   29 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17907465994358063
[2m[36m(func pid=34042)[0m mae:  0.1313195675611496
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.106, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.1700206845998764
[2m[36m(func pid=34415)[0m mae:  0.123411163687706
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.26, 0.083, 0.318, 0.074, 0.191, 0.264, 0.138, 0.143, 0.111]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16091306507587433
[2m[36m(func pid=34838)[0m mae:  0.10334423929452896
[2m[36m(func pid=34838)[0m rmse_per_class: [0.087, 0.249, 0.027, 0.333, 0.055, 0.192, 0.333, 0.118, 0.134, 0.081]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2875 | Steps: 2 | Val loss: 0.6895 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.7189 | Steps: 2 | Val loss: 0.5616 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3312 | Steps: 2 | Val loss: 0.3125 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4007 | Steps: 2 | Val loss: 0.3205 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=35262)[0m rmse: 0.23343577980995178
[2m[36m(func pid=35262)[0m mae:  0.13790841400623322
[2m[36m(func pid=35262)[0m rmse_per_class: [0.158, 0.295, 0.049, 0.334, 0.067, 0.221, 0.334, 0.159, 0.204, 0.513]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:28:06 (running for 00:03:27.58)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.719 |  0.179 |                   31 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.402 |  0.17  |                   30 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.343 |  0.161 |                   30 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.288 |  0.233 |                   30 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.1790475696325302
[2m[36m(func pid=34042)[0m mae:  0.13128527998924255
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.106, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16114547848701477
[2m[36m(func pid=34838)[0m mae:  0.10318674892187119
[2m[36m(func pid=34838)[0m rmse_per_class: [0.087, 0.25, 0.028, 0.334, 0.055, 0.19, 0.335, 0.119, 0.134, 0.081]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.16915404796600342
[2m[36m(func pid=34415)[0m mae:  0.12268640846014023
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.259, 0.081, 0.317, 0.072, 0.19, 0.262, 0.137, 0.143, 0.111]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2742 | Steps: 2 | Val loss: 0.5538 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7118 | Steps: 2 | Val loss: 0.5542 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4040 | Steps: 2 | Val loss: 0.3242 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3302 | Steps: 2 | Val loss: 0.3095 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=35262)[0m rmse: 0.2165399044752121
[2m[36m(func pid=35262)[0m mae:  0.12439849227666855
[2m[36m(func pid=35262)[0m rmse_per_class: [0.155, 0.281, 0.05, 0.34, 0.075, 0.209, 0.298, 0.172, 0.206, 0.379]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:28:11 (running for 00:03:32.71)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.712 |  0.179 |                   32 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.401 |  0.169 |                   31 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.331 |  0.161 |                   31 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.274 |  0.217 |                   31 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.179015651345253
[2m[36m(func pid=34042)[0m mae:  0.1312592774629593
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.106, 0.19, 0.29, 0.141, 0.143, 0.109]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.16832807660102844
[2m[36m(func pid=34415)[0m mae:  0.1219739094376564
[2m[36m(func pid=34415)[0m rmse_per_class: [0.118, 0.259, 0.08, 0.316, 0.071, 0.19, 0.26, 0.137, 0.143, 0.11]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1607307642698288
[2m[36m(func pid=34838)[0m mae:  0.10271026194095612
[2m[36m(func pid=34838)[0m rmse_per_class: [0.087, 0.25, 0.028, 0.332, 0.055, 0.189, 0.332, 0.12, 0.134, 0.081]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2619 | Steps: 2 | Val loss: 0.4308 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7024 | Steps: 2 | Val loss: 0.5472 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4094 | Steps: 2 | Val loss: 0.3282 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3390 | Steps: 2 | Val loss: 0.3094 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=35262)[0m rmse: 0.20018652081489563
[2m[36m(func pid=35262)[0m mae:  0.11293832212686539
[2m[36m(func pid=35262)[0m rmse_per_class: [0.138, 0.268, 0.05, 0.325, 0.081, 0.198, 0.263, 0.207, 0.202, 0.272]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:28:16 (running for 00:03:38.01)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.712 |  0.179 |                   32 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.404 |  0.168 |                   32 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.339 |  0.161 |                   33 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.262 |  0.2   |                   32 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17895297706127167
[2m[36m(func pid=34042)[0m mae:  0.13120117783546448
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.105, 0.19, 0.29, 0.141, 0.143, 0.109]
[2m[36m(func pid=34415)[0m rmse: 0.1675155758857727
[2m[36m(func pid=34415)[0m mae:  0.12129048258066177
[2m[36m(func pid=34415)[0m rmse_per_class: [0.117, 0.258, 0.078, 0.315, 0.069, 0.19, 0.258, 0.137, 0.143, 0.11]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16073361039161682
[2m[36m(func pid=34838)[0m mae:  0.1024271622300148
[2m[36m(func pid=34838)[0m rmse_per_class: [0.086, 0.25, 0.028, 0.33, 0.055, 0.187, 0.332, 0.124, 0.133, 0.081]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2595 | Steps: 2 | Val loss: 0.3646 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.6910 | Steps: 2 | Val loss: 0.5400 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4157 | Steps: 2 | Val loss: 0.3326 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3293 | Steps: 2 | Val loss: 0.3106 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=35262)[0m rmse: 0.18988069891929626
[2m[36m(func pid=35262)[0m mae:  0.10827915370464325
[2m[36m(func pid=35262)[0m rmse_per_class: [0.126, 0.264, 0.049, 0.305, 0.082, 0.189, 0.257, 0.219, 0.204, 0.203]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:28:22 (running for 00:03:43.24)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.691 |  0.179 |                   34 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.409 |  0.168 |                   33 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.339 |  0.161 |                   33 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.259 |  0.19  |                   33 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.1788932830095291
[2m[36m(func pid=34042)[0m mae:  0.13115175068378448
[2m[36m(func pid=34042)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.105, 0.19, 0.29, 0.141, 0.143, 0.109]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.1666237860918045
[2m[36m(func pid=34415)[0m mae:  0.12052067369222641
[2m[36m(func pid=34415)[0m rmse_per_class: [0.117, 0.258, 0.077, 0.314, 0.068, 0.19, 0.256, 0.137, 0.143, 0.109]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.160659521818161
[2m[36m(func pid=34838)[0m mae:  0.10226847976446152
[2m[36m(func pid=34838)[0m rmse_per_class: [0.085, 0.249, 0.028, 0.33, 0.055, 0.185, 0.335, 0.125, 0.133, 0.081]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2556 | Steps: 2 | Val loss: 0.3499 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.6858 | Steps: 2 | Val loss: 0.5329 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4179 | Steps: 2 | Val loss: 0.3371 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3183 | Steps: 2 | Val loss: 0.3080 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=35262)[0m rmse: 0.18772003054618835
[2m[36m(func pid=35262)[0m mae:  0.10959991067647934
[2m[36m(func pid=35262)[0m rmse_per_class: [0.145, 0.268, 0.033, 0.312, 0.083, 0.185, 0.254, 0.219, 0.207, 0.17]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:28:27 (running for 00:03:48.37)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.686 |  0.179 |                   35 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.416 |  0.167 |                   34 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.329 |  0.161 |                   34 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.256 |  0.188 |                   34 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17888501286506653
[2m[36m(func pid=34042)[0m mae:  0.13113687932491302
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.26, 0.099, 0.336, 0.104, 0.19, 0.289, 0.141, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.16585512459278107
[2m[36m(func pid=34415)[0m mae:  0.11989222466945648
[2m[36m(func pid=34415)[0m rmse_per_class: [0.117, 0.257, 0.075, 0.313, 0.066, 0.189, 0.254, 0.136, 0.143, 0.109]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15922674536705017
[2m[36m(func pid=34838)[0m mae:  0.10145139694213867
[2m[36m(func pid=34838)[0m rmse_per_class: [0.083, 0.248, 0.028, 0.328, 0.055, 0.182, 0.328, 0.126, 0.133, 0.082]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2871 | Steps: 2 | Val loss: 0.3600 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6745 | Steps: 2 | Val loss: 0.5266 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4250 | Steps: 2 | Val loss: 0.3414 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3209 | Steps: 2 | Val loss: 0.3033 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=35262)[0m rmse: 0.1943615823984146
[2m[36m(func pid=35262)[0m mae:  0.11368997395038605
[2m[36m(func pid=35262)[0m rmse_per_class: [0.151, 0.279, 0.031, 0.328, 0.084, 0.19, 0.256, 0.234, 0.208, 0.184]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:28:32 (running for 00:03:53.64)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.675 |  0.179 |                   36 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.418 |  0.166 |                   35 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.318 |  0.159 |                   35 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.287 |  0.194 |                   35 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17887966334819794
[2m[36m(func pid=34042)[0m mae:  0.13110654056072235
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.26, 0.099, 0.335, 0.104, 0.19, 0.289, 0.141, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.16513511538505554
[2m[36m(func pid=34415)[0m mae:  0.11924858391284943
[2m[36m(func pid=34415)[0m rmse_per_class: [0.117, 0.256, 0.073, 0.312, 0.065, 0.189, 0.252, 0.136, 0.143, 0.108]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15720036625862122
[2m[36m(func pid=34838)[0m mae:  0.10044065862894058
[2m[36m(func pid=34838)[0m rmse_per_class: [0.082, 0.246, 0.028, 0.325, 0.055, 0.177, 0.318, 0.124, 0.132, 0.082]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2704 | Steps: 2 | Val loss: 0.3804 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6642 | Steps: 2 | Val loss: 0.5198 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4254 | Steps: 2 | Val loss: 0.3455 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3267 | Steps: 2 | Val loss: 0.3002 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=35262)[0m rmse: 0.20170588791370392
[2m[36m(func pid=35262)[0m mae:  0.11731322109699249
[2m[36m(func pid=35262)[0m rmse_per_class: [0.145, 0.284, 0.028, 0.334, 0.085, 0.2, 0.269, 0.261, 0.211, 0.199]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:28:37 (running for 00:03:58.98)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.664 |  0.179 |                   37 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.425 |  0.165 |                   36 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.321 |  0.157 |                   36 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.27  |  0.202 |                   36 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17891642451286316
[2m[36m(func pid=34042)[0m mae:  0.13112486898899078
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.261, 0.098, 0.335, 0.104, 0.19, 0.289, 0.141, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15587209165096283
[2m[36m(func pid=34838)[0m mae:  0.0997985452413559
[2m[36m(func pid=34838)[0m rmse_per_class: [0.082, 0.245, 0.028, 0.324, 0.055, 0.172, 0.315, 0.121, 0.132, 0.083]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.16420038044452667
[2m[36m(func pid=34415)[0m mae:  0.11842519044876099
[2m[36m(func pid=34415)[0m rmse_per_class: [0.116, 0.255, 0.071, 0.311, 0.064, 0.189, 0.249, 0.135, 0.143, 0.107]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2763 | Steps: 2 | Val loss: 0.3848 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6588 | Steps: 2 | Val loss: 0.5131 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3219 | Steps: 2 | Val loss: 0.2939 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=35262)[0m rmse: 0.20118467509746552
[2m[36m(func pid=35262)[0m mae:  0.1170450821518898
[2m[36m(func pid=35262)[0m rmse_per_class: [0.15, 0.287, 0.037, 0.354, 0.086, 0.2, 0.267, 0.215, 0.214, 0.201]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4342 | Steps: 2 | Val loss: 0.3496 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 09:28:43 (running for 00:04:04.27)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.664 |  0.179 |                   37 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.425 |  0.164 |                   37 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.322 |  0.154 |                   38 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.276 |  0.201 |                   37 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34838)[0m rmse: 0.1535746157169342
[2m[36m(func pid=34838)[0m mae:  0.0987163633108139
[2m[36m(func pid=34838)[0m rmse_per_class: [0.081, 0.244, 0.029, 0.319, 0.055, 0.167, 0.302, 0.122, 0.133, 0.084]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.1788962185382843
[2m[36m(func pid=34042)[0m mae:  0.13111117482185364
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.261, 0.098, 0.335, 0.104, 0.19, 0.289, 0.141, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.16328580677509308
[2m[36m(func pid=34415)[0m mae:  0.11763665825128555
[2m[36m(func pid=34415)[0m rmse_per_class: [0.115, 0.254, 0.069, 0.311, 0.063, 0.189, 0.247, 0.135, 0.143, 0.107]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2612 | Steps: 2 | Val loss: 0.3886 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6494 | Steps: 2 | Val loss: 0.5061 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=35262)[0m rmse: 0.2038329541683197
[2m[36m(func pid=35262)[0m mae:  0.11845667660236359
[2m[36m(func pid=35262)[0m rmse_per_class: [0.157, 0.315, 0.057, 0.381, 0.084, 0.195, 0.256, 0.14, 0.245, 0.207]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3199 | Steps: 2 | Val loss: 0.2890 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4359 | Steps: 2 | Val loss: 0.3544 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 09:28:48 (running for 00:04:09.50)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.649 |  0.179 |                   39 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.434 |  0.163 |                   38 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.322 |  0.154 |                   38 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.261 |  0.204 |                   38 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.1788025200366974
[2m[36m(func pid=34042)[0m mae:  0.13101698458194733
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.261, 0.098, 0.335, 0.103, 0.19, 0.289, 0.141, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.1624327301979065
[2m[36m(func pid=34415)[0m mae:  0.11687877029180527
[2m[36m(func pid=34415)[0m rmse_per_class: [0.115, 0.253, 0.067, 0.309, 0.062, 0.188, 0.246, 0.135, 0.143, 0.106]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15207378566265106
[2m[36m(func pid=34838)[0m mae:  0.0982271358370781
[2m[36m(func pid=34838)[0m rmse_per_class: [0.081, 0.244, 0.035, 0.316, 0.055, 0.162, 0.289, 0.121, 0.133, 0.084]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2613 | Steps: 2 | Val loss: 0.3776 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=35262)[0m rmse: 0.19613924622535706
[2m[36m(func pid=35262)[0m mae:  0.1133325919508934
[2m[36m(func pid=35262)[0m rmse_per_class: [0.158, 0.332, 0.067, 0.377, 0.081, 0.183, 0.235, 0.123, 0.203, 0.202]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4420 | Steps: 2 | Val loss: 0.3588 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6393 | Steps: 2 | Val loss: 0.4995 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3238 | Steps: 2 | Val loss: 0.2875 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=34838)[0m rmse: 0.15271112322807312
[2m[36m(func pid=34838)[0m mae:  0.09864021837711334
[2m[36m(func pid=34838)[0m rmse_per_class: [0.081, 0.245, 0.046, 0.315, 0.055, 0.159, 0.279, 0.126, 0.134, 0.087]
[2m[36m(func pid=34838)[0m 
== Status ==
Current time: 2024-01-07 09:28:53 (running for 00:04:14.84)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.649 |  0.179 |                   39 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.436 |  0.162 |                   39 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.324 |  0.153 |                   40 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.261 |  0.196 |                   39 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17871281504631042
[2m[36m(func pid=34042)[0m mae:  0.13092289865016937
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.261, 0.098, 0.335, 0.103, 0.19, 0.288, 0.141, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.1616838127374649
[2m[36m(func pid=34415)[0m mae:  0.11619739234447479
[2m[36m(func pid=34415)[0m rmse_per_class: [0.114, 0.253, 0.066, 0.309, 0.061, 0.188, 0.244, 0.134, 0.142, 0.105]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3047 | Steps: 2 | Val loss: 0.3570 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=35262)[0m rmse: 0.18020188808441162
[2m[36m(func pid=35262)[0m mae:  0.10318398475646973
[2m[36m(func pid=35262)[0m rmse_per_class: [0.153, 0.318, 0.052, 0.344, 0.084, 0.162, 0.219, 0.123, 0.156, 0.191]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3090 | Steps: 2 | Val loss: 0.2877 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6311 | Steps: 2 | Val loss: 0.4926 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4421 | Steps: 2 | Val loss: 0.3621 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=34042)[0m rmse: 0.17855553328990936
[2m[36m(func pid=34042)[0m mae:  0.13076891005039215
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.335, 0.103, 0.19, 0.288, 0.141, 0.143, 0.111]
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2802 | Steps: 2 | Val loss: 0.3736 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 09:28:59 (running for 00:04:20.21)
Memory usage on this node: 24.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.631 |  0.179 |                   41 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.442 |  0.162 |                   40 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.324 |  0.153 |                   40 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.305 |  0.18  |                   40 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.16099697351455688
[2m[36m(func pid=34415)[0m mae:  0.11560861766338348
[2m[36m(func pid=34415)[0m rmse_per_class: [0.114, 0.252, 0.064, 0.308, 0.06, 0.188, 0.242, 0.134, 0.142, 0.105]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15346954762935638
[2m[36m(func pid=34838)[0m mae:  0.09970887750387192
[2m[36m(func pid=34838)[0m rmse_per_class: [0.082, 0.246, 0.049, 0.314, 0.056, 0.159, 0.268, 0.134, 0.135, 0.09]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.18357393145561218
[2m[36m(func pid=35262)[0m mae:  0.10348570346832275
[2m[36m(func pid=35262)[0m rmse_per_class: [0.146, 0.294, 0.043, 0.323, 0.074, 0.194, 0.214, 0.148, 0.16, 0.24]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4475 | Steps: 2 | Val loss: 0.3658 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3119 | Steps: 2 | Val loss: 0.2898 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6233 | Steps: 2 | Val loss: 0.4864 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=34415)[0m rmse: 0.16005390882492065
[2m[36m(func pid=34415)[0m mae:  0.11470886319875717
[2m[36m(func pid=34415)[0m rmse_per_class: [0.114, 0.251, 0.063, 0.307, 0.06, 0.187, 0.24, 0.133, 0.142, 0.104]
[2m[36m(func pid=34415)[0m 
== Status ==
Current time: 2024-01-07 09:29:04 (running for 00:04:25.37)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.631 |  0.179 |                   41 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.447 |  0.16  |                   42 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.309 |  0.153 |                   41 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.28  |  0.184 |                   41 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2596 | Steps: 2 | Val loss: 0.3939 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=34042)[0m rmse: 0.17847324907779694
[2m[36m(func pid=34042)[0m mae:  0.13069310784339905
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.334, 0.103, 0.19, 0.288, 0.141, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15516845881938934
[2m[36m(func pid=34838)[0m mae:  0.10120614618062973
[2m[36m(func pid=34838)[0m rmse_per_class: [0.084, 0.248, 0.049, 0.315, 0.056, 0.162, 0.262, 0.146, 0.136, 0.096]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.18779243528842926
[2m[36m(func pid=35262)[0m mae:  0.10654793679714203
[2m[36m(func pid=35262)[0m rmse_per_class: [0.12, 0.273, 0.039, 0.307, 0.074, 0.206, 0.229, 0.194, 0.155, 0.281]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4486 | Steps: 2 | Val loss: 0.3682 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6143 | Steps: 2 | Val loss: 0.4795 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3188 | Steps: 2 | Val loss: 0.2907 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 09:29:09 (running for 00:04:30.64)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.623 |  0.178 |                   42 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.449 |  0.159 |                   43 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.312 |  0.155 |                   42 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.26  |  0.188 |                   42 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.15918579697608948
[2m[36m(func pid=34415)[0m mae:  0.11396358162164688
[2m[36m(func pid=34415)[0m rmse_per_class: [0.113, 0.25, 0.061, 0.306, 0.059, 0.187, 0.239, 0.133, 0.142, 0.103]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.17844490706920624
[2m[36m(func pid=34042)[0m mae:  0.13066722452640533
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.102, 0.19, 0.288, 0.141, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2811 | Steps: 2 | Val loss: 0.4267 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=34838)[0m rmse: 0.1563860923051834
[2m[36m(func pid=34838)[0m mae:  0.10231129080057144
[2m[36m(func pid=34838)[0m rmse_per_class: [0.084, 0.248, 0.049, 0.314, 0.056, 0.165, 0.255, 0.153, 0.137, 0.101]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.19495739042758942
[2m[36m(func pid=35262)[0m mae:  0.11241847276687622
[2m[36m(func pid=35262)[0m rmse_per_class: [0.107, 0.269, 0.035, 0.303, 0.077, 0.21, 0.259, 0.213, 0.146, 0.33]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4494 | Steps: 2 | Val loss: 0.3713 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.6092 | Steps: 2 | Val loss: 0.4728 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3161 | Steps: 2 | Val loss: 0.2904 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2743 | Steps: 2 | Val loss: 0.4518 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 09:29:15 (running for 00:04:36.08)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.614 |  0.178 |                   43 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.449 |  0.158 |                   44 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.319 |  0.156 |                   43 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.281 |  0.195 |                   43 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.15837273001670837
[2m[36m(func pid=34415)[0m mae:  0.11321042478084564
[2m[36m(func pid=34415)[0m rmse_per_class: [0.112, 0.249, 0.059, 0.305, 0.058, 0.187, 0.237, 0.133, 0.142, 0.102]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.17838558554649353
[2m[36m(func pid=34042)[0m mae:  0.13060389459133148
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.261, 0.097, 0.334, 0.102, 0.19, 0.287, 0.141, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15668323636054993
[2m[36m(func pid=34838)[0m mae:  0.10300364345312119
[2m[36m(func pid=34838)[0m rmse_per_class: [0.084, 0.248, 0.049, 0.314, 0.056, 0.169, 0.252, 0.153, 0.139, 0.104]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.20075294375419617
[2m[36m(func pid=35262)[0m mae:  0.11769895255565643
[2m[36m(func pid=35262)[0m rmse_per_class: [0.104, 0.266, 0.039, 0.298, 0.074, 0.211, 0.279, 0.211, 0.141, 0.384]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4511 | Steps: 2 | Val loss: 0.3748 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5972 | Steps: 2 | Val loss: 0.4668 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3150 | Steps: 2 | Val loss: 0.2894 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2777 | Steps: 2 | Val loss: 0.4460 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 09:29:20 (running for 00:04:41.25)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.609 |  0.178 |                   44 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.451 |  0.158 |                   45 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.316 |  0.157 |                   44 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.274 |  0.201 |                   44 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.15765443444252014
[2m[36m(func pid=34415)[0m mae:  0.11252881586551666
[2m[36m(func pid=34415)[0m rmse_per_class: [0.111, 0.248, 0.057, 0.304, 0.058, 0.186, 0.236, 0.132, 0.142, 0.101]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.17831872403621674
[2m[36m(func pid=34042)[0m mae:  0.13053134083747864
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.261, 0.097, 0.334, 0.102, 0.19, 0.287, 0.141, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15605676174163818
[2m[36m(func pid=34838)[0m mae:  0.10313812643289566
[2m[36m(func pid=34838)[0m rmse_per_class: [0.085, 0.247, 0.049, 0.314, 0.056, 0.172, 0.25, 0.146, 0.14, 0.102]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.2008705884218216
[2m[36m(func pid=35262)[0m mae:  0.11758967489004135
[2m[36m(func pid=35262)[0m rmse_per_class: [0.11, 0.262, 0.038, 0.299, 0.076, 0.209, 0.284, 0.207, 0.141, 0.383]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4526 | Steps: 2 | Val loss: 0.3773 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5910 | Steps: 2 | Val loss: 0.4606 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3273 | Steps: 2 | Val loss: 0.2893 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 09:29:25 (running for 00:04:46.26)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.597 |  0.178 |                   45 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.453 |  0.157 |                   46 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.315 |  0.156 |                   45 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.278 |  0.201 |                   45 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.1570078730583191
[2m[36m(func pid=34415)[0m mae:  0.11193998903036118
[2m[36m(func pid=34415)[0m rmse_per_class: [0.111, 0.248, 0.056, 0.303, 0.057, 0.186, 0.235, 0.132, 0.142, 0.1]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2571 | Steps: 2 | Val loss: 0.4170 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=34042)[0m rmse: 0.17819835245609283
[2m[36m(func pid=34042)[0m mae:  0.1304125338792801
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.261, 0.097, 0.333, 0.102, 0.19, 0.287, 0.141, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1554933488368988
[2m[36m(func pid=34838)[0m mae:  0.10336419194936752
[2m[36m(func pid=34838)[0m rmse_per_class: [0.087, 0.247, 0.046, 0.316, 0.056, 0.174, 0.248, 0.138, 0.143, 0.101]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.19790497422218323
[2m[36m(func pid=35262)[0m mae:  0.11584298312664032
[2m[36m(func pid=35262)[0m rmse_per_class: [0.118, 0.26, 0.038, 0.299, 0.075, 0.204, 0.28, 0.2, 0.142, 0.362]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4533 | Steps: 2 | Val loss: 0.3795 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5835 | Steps: 2 | Val loss: 0.4542 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3078 | Steps: 2 | Val loss: 0.2891 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2761 | Steps: 2 | Val loss: 0.3484 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 09:29:30 (running for 00:04:51.66)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.591 |  0.178 |                   46 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.453 |  0.156 |                   47 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.327 |  0.155 |                   46 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.257 |  0.198 |                   46 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.15627972781658173
[2m[36m(func pid=34415)[0m mae:  0.11122512817382812
[2m[36m(func pid=34415)[0m rmse_per_class: [0.11, 0.247, 0.054, 0.303, 0.057, 0.185, 0.234, 0.132, 0.141, 0.099]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15443167090415955
[2m[36m(func pid=34838)[0m mae:  0.10327879339456558
[2m[36m(func pid=34838)[0m rmse_per_class: [0.089, 0.247, 0.04, 0.316, 0.056, 0.174, 0.248, 0.129, 0.144, 0.1]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.17813685536384583
[2m[36m(func pid=34042)[0m mae:  0.13034646213054657
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.333, 0.102, 0.19, 0.286, 0.141, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.1801929771900177
[2m[36m(func pid=35262)[0m mae:  0.10342981666326523
[2m[36m(func pid=35262)[0m rmse_per_class: [0.125, 0.259, 0.039, 0.301, 0.075, 0.188, 0.247, 0.195, 0.148, 0.225]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4521 | Steps: 2 | Val loss: 0.3811 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3045 | Steps: 2 | Val loss: 0.2885 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5731 | Steps: 2 | Val loss: 0.4476 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2618 | Steps: 2 | Val loss: 0.3303 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 09:29:35 (running for 00:04:56.84)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.583 |  0.178 |                   47 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.453 |  0.156 |                   47 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.304 |  0.154 |                   48 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.276 |  0.18  |                   47 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34838)[0m rmse: 0.15381070971488953
[2m[36m(func pid=34838)[0m mae:  0.10300973802804947
[2m[36m(func pid=34838)[0m rmse_per_class: [0.092, 0.245, 0.034, 0.316, 0.055, 0.175, 0.248, 0.122, 0.145, 0.104]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.15556517243385315
[2m[36m(func pid=34415)[0m mae:  0.1105225682258606
[2m[36m(func pid=34415)[0m rmse_per_class: [0.11, 0.246, 0.053, 0.301, 0.056, 0.185, 0.233, 0.131, 0.141, 0.098]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.17795702815055847
[2m[36m(func pid=34042)[0m mae:  0.13019105792045593
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.261, 0.096, 0.333, 0.102, 0.19, 0.286, 0.141, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.1761937439441681
[2m[36m(func pid=35262)[0m mae:  0.10117252916097641
[2m[36m(func pid=35262)[0m rmse_per_class: [0.144, 0.26, 0.042, 0.314, 0.079, 0.177, 0.24, 0.15, 0.144, 0.213]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4531 | Steps: 2 | Val loss: 0.3823 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3174 | Steps: 2 | Val loss: 0.2891 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5671 | Steps: 2 | Val loss: 0.4413 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2681 | Steps: 2 | Val loss: 0.3348 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 09:29:41 (running for 00:05:02.06)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.573 |  0.178 |                   48 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.453 |  0.155 |                   49 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.304 |  0.154 |                   48 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.262 |  0.176 |                   48 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.15483640134334564
[2m[36m(func pid=34415)[0m mae:  0.10978511720895767
[2m[36m(func pid=34415)[0m rmse_per_class: [0.109, 0.246, 0.052, 0.3, 0.056, 0.185, 0.231, 0.131, 0.14, 0.098]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1547146737575531
[2m[36m(func pid=34838)[0m mae:  0.10334847867488861
[2m[36m(func pid=34838)[0m rmse_per_class: [0.096, 0.245, 0.035, 0.317, 0.055, 0.172, 0.249, 0.12, 0.147, 0.111]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.1778203248977661
[2m[36m(func pid=34042)[0m mae:  0.1300741732120514
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.101, 0.191, 0.286, 0.14, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.17936912178993225
[2m[36m(func pid=35262)[0m mae:  0.1043490394949913
[2m[36m(func pid=35262)[0m rmse_per_class: [0.171, 0.259, 0.046, 0.334, 0.08, 0.176, 0.249, 0.119, 0.142, 0.216]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4601 | Steps: 2 | Val loss: 0.3827 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5603 | Steps: 2 | Val loss: 0.4352 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2965 | Steps: 2 | Val loss: 0.2904 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2722 | Steps: 2 | Val loss: 0.3406 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 09:29:46 (running for 00:05:07.36)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.56  |  0.178 |                   50 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.453 |  0.155 |                   49 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.317 |  0.155 |                   49 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.268 |  0.179 |                   49 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17773142457008362
[2m[36m(func pid=34042)[0m mae:  0.1300055980682373
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.101, 0.191, 0.286, 0.14, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.15411540865898132
[2m[36m(func pid=34415)[0m mae:  0.10902849584817886
[2m[36m(func pid=34415)[0m rmse_per_class: [0.108, 0.245, 0.05, 0.299, 0.056, 0.184, 0.23, 0.131, 0.14, 0.097]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.156245619058609
[2m[36m(func pid=34838)[0m mae:  0.10407944023609161
[2m[36m(func pid=34838)[0m rmse_per_class: [0.1, 0.246, 0.036, 0.319, 0.055, 0.171, 0.251, 0.121, 0.148, 0.116]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.18188655376434326
[2m[36m(func pid=35262)[0m mae:  0.10615166276693344
[2m[36m(func pid=35262)[0m rmse_per_class: [0.189, 0.265, 0.065, 0.344, 0.082, 0.171, 0.247, 0.126, 0.143, 0.185]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5527 | Steps: 2 | Val loss: 0.4293 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4620 | Steps: 2 | Val loss: 0.3826 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2921 | Steps: 2 | Val loss: 0.2901 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2609 | Steps: 2 | Val loss: 0.3502 | Batch size: 32 | lr: 0.1 | Duration: 2.57s
== Status ==
Current time: 2024-01-07 09:29:51 (running for 00:05:12.41)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.553 |  0.178 |                   51 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.46  |  0.154 |                   50 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.297 |  0.156 |                   50 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.272 |  0.182 |                   50 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17768585681915283
[2m[36m(func pid=34042)[0m mae:  0.12997236847877502
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.1, 0.191, 0.285, 0.14, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.15330305695533752
[2m[36m(func pid=34415)[0m mae:  0.1081867665052414
[2m[36m(func pid=34415)[0m rmse_per_class: [0.107, 0.244, 0.049, 0.298, 0.055, 0.184, 0.229, 0.131, 0.14, 0.096]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15679213404655457
[2m[36m(func pid=34838)[0m mae:  0.10402107238769531
[2m[36m(func pid=34838)[0m rmse_per_class: [0.099, 0.245, 0.036, 0.318, 0.055, 0.17, 0.253, 0.122, 0.149, 0.122]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.1875503957271576
[2m[36m(func pid=35262)[0m mae:  0.10981683433055878
[2m[36m(func pid=35262)[0m rmse_per_class: [0.187, 0.278, 0.088, 0.344, 0.078, 0.175, 0.248, 0.146, 0.142, 0.19]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5455 | Steps: 2 | Val loss: 0.4238 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4516 | Steps: 2 | Val loss: 0.3823 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2932 | Steps: 2 | Val loss: 0.2904 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2648 | Steps: 2 | Val loss: 0.3543 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 09:29:56 (running for 00:05:17.50)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.546 |  0.178 |                   52 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.462 |  0.153 |                   51 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.292 |  0.157 |                   51 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.261 |  0.188 |                   51 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17759506404399872
[2m[36m(func pid=34042)[0m mae:  0.12989361584186554
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.1, 0.191, 0.285, 0.14, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.15281429886817932
[2m[36m(func pid=34415)[0m mae:  0.10770497471094131
[2m[36m(func pid=34415)[0m rmse_per_class: [0.106, 0.244, 0.048, 0.297, 0.055, 0.184, 0.229, 0.13, 0.14, 0.095]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1577746868133545
[2m[36m(func pid=34838)[0m mae:  0.10403458774089813
[2m[36m(func pid=34838)[0m rmse_per_class: [0.098, 0.246, 0.038, 0.316, 0.055, 0.166, 0.254, 0.127, 0.149, 0.129]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.19120359420776367
[2m[36m(func pid=35262)[0m mae:  0.1118951290845871
[2m[36m(func pid=35262)[0m rmse_per_class: [0.186, 0.284, 0.099, 0.341, 0.071, 0.184, 0.249, 0.152, 0.14, 0.206]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5399 | Steps: 2 | Val loss: 0.4179 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4524 | Steps: 2 | Val loss: 0.3823 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2995 | Steps: 2 | Val loss: 0.2916 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2689 | Steps: 2 | Val loss: 0.3415 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 09:30:01 (running for 00:05:22.69)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.54  |  0.177 |                   53 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.452 |  0.153 |                   52 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.293 |  0.158 |                   52 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.265 |  0.191 |                   52 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.1774405986070633
[2m[36m(func pid=34042)[0m mae:  0.12976227700710297
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.332, 0.099, 0.191, 0.285, 0.14, 0.144, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15940141677856445
[2m[36m(func pid=34838)[0m mae:  0.10442566871643066
[2m[36m(func pid=34838)[0m rmse_per_class: [0.097, 0.247, 0.043, 0.315, 0.054, 0.164, 0.255, 0.132, 0.151, 0.136]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.1523357778787613
[2m[36m(func pid=34415)[0m mae:  0.10714066028594971
[2m[36m(func pid=34415)[0m rmse_per_class: [0.105, 0.244, 0.047, 0.296, 0.055, 0.183, 0.229, 0.13, 0.139, 0.094]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.18660502135753632
[2m[36m(func pid=35262)[0m mae:  0.1078079342842102
[2m[36m(func pid=35262)[0m rmse_per_class: [0.2, 0.279, 0.116, 0.334, 0.068, 0.176, 0.237, 0.146, 0.141, 0.169]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5337 | Steps: 2 | Val loss: 0.4123 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2856 | Steps: 2 | Val loss: 0.2915 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4489 | Steps: 2 | Val loss: 0.3818 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2555 | Steps: 2 | Val loss: 0.3367 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=34042)[0m rmse: 0.17728403210639954
[2m[36m(func pid=34042)[0m mae:  0.129617378115654
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.332, 0.099, 0.191, 0.284, 0.14, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
== Status ==
Current time: 2024-01-07 09:30:06 (running for 00:05:27.73)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.534 |  0.177 |                   54 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.452 |  0.152 |                   53 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.299 |  0.159 |                   53 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.269 |  0.187 |                   53 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34838)[0m rmse: 0.16035372018814087
[2m[36m(func pid=34838)[0m mae:  0.1041044145822525
[2m[36m(func pid=34838)[0m rmse_per_class: [0.094, 0.248, 0.046, 0.312, 0.054, 0.161, 0.255, 0.14, 0.152, 0.142]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.15187019109725952
[2m[36m(func pid=34415)[0m mae:  0.1065349131822586
[2m[36m(func pid=34415)[0m rmse_per_class: [0.104, 0.243, 0.046, 0.297, 0.055, 0.183, 0.229, 0.129, 0.139, 0.094]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.18603113293647766
[2m[36m(func pid=35262)[0m mae:  0.10688243806362152
[2m[36m(func pid=35262)[0m rmse_per_class: [0.214, 0.275, 0.126, 0.332, 0.068, 0.17, 0.233, 0.138, 0.147, 0.156]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5275 | Steps: 2 | Val loss: 0.4066 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4450 | Steps: 2 | Val loss: 0.3808 | Batch size: 32 | lr: 0.001 | Duration: 2.63s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2808 | Steps: 2 | Val loss: 0.2921 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2549 | Steps: 2 | Val loss: 0.3348 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:30:12 (running for 00:05:33.13)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.528 |  0.177 |                   55 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.449 |  0.152 |                   54 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.286 |  0.16  |                   54 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.255 |  0.186 |                   54 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17710430920124054
[2m[36m(func pid=34042)[0m mae:  0.1294633448123932
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.332, 0.098, 0.191, 0.284, 0.14, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.1513739675283432
[2m[36m(func pid=34415)[0m mae:  0.10593611001968384
[2m[36m(func pid=34415)[0m rmse_per_class: [0.104, 0.242, 0.045, 0.296, 0.055, 0.183, 0.229, 0.129, 0.139, 0.093]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1613706648349762
[2m[36m(func pid=34838)[0m mae:  0.10419459640979767
[2m[36m(func pid=34838)[0m rmse_per_class: [0.095, 0.249, 0.049, 0.31, 0.053, 0.158, 0.253, 0.145, 0.153, 0.148]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.18634197115898132
[2m[36m(func pid=35262)[0m mae:  0.1071586161851883
[2m[36m(func pid=35262)[0m rmse_per_class: [0.228, 0.274, 0.129, 0.335, 0.068, 0.165, 0.231, 0.137, 0.145, 0.152]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5224 | Steps: 2 | Val loss: 0.4020 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4525 | Steps: 2 | Val loss: 0.3798 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2844 | Steps: 2 | Val loss: 0.2912 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2698 | Steps: 2 | Val loss: 0.3381 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 09:30:17 (running for 00:05:38.26)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.522 |  0.177 |                   56 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.445 |  0.151 |                   55 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.281 |  0.161 |                   55 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.255 |  0.186 |                   55 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17701813578605652
[2m[36m(func pid=34042)[0m mae:  0.1293816715478897
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.095, 0.332, 0.098, 0.191, 0.283, 0.14, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.15101847052574158
[2m[36m(func pid=34415)[0m mae:  0.10544337332248688
[2m[36m(func pid=34415)[0m rmse_per_class: [0.103, 0.242, 0.044, 0.296, 0.055, 0.183, 0.229, 0.129, 0.138, 0.092]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16158679127693176
[2m[36m(func pid=34838)[0m mae:  0.10376808792352676
[2m[36m(func pid=34838)[0m rmse_per_class: [0.096, 0.25, 0.051, 0.305, 0.053, 0.156, 0.25, 0.15, 0.155, 0.151]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.18716754019260406
[2m[36m(func pid=35262)[0m mae:  0.10775092989206314
[2m[36m(func pid=35262)[0m rmse_per_class: [0.252, 0.272, 0.115, 0.341, 0.067, 0.159, 0.231, 0.139, 0.14, 0.155]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5155 | Steps: 2 | Val loss: 0.3974 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4411 | Steps: 2 | Val loss: 0.3794 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2824 | Steps: 2 | Val loss: 0.2906 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2536 | Steps: 2 | Val loss: 0.3384 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:30:22 (running for 00:05:43.59)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.515 |  0.177 |                   57 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.452 |  0.151 |                   56 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.284 |  0.162 |                   56 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.27  |  0.187 |                   56 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.1769428253173828
[2m[36m(func pid=34042)[0m mae:  0.12932908535003662
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.095, 0.332, 0.098, 0.191, 0.283, 0.14, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.1506391167640686
[2m[36m(func pid=34415)[0m mae:  0.1048700213432312
[2m[36m(func pid=34415)[0m rmse_per_class: [0.101, 0.242, 0.043, 0.296, 0.054, 0.182, 0.23, 0.128, 0.138, 0.091]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16178177297115326
[2m[36m(func pid=34838)[0m mae:  0.10356505960226059
[2m[36m(func pid=34838)[0m rmse_per_class: [0.098, 0.25, 0.052, 0.301, 0.052, 0.155, 0.247, 0.153, 0.158, 0.152]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.18565109372138977
[2m[36m(func pid=35262)[0m mae:  0.10779273509979248
[2m[36m(func pid=35262)[0m rmse_per_class: [0.286, 0.268, 0.072, 0.341, 0.066, 0.159, 0.231, 0.136, 0.14, 0.157]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5097 | Steps: 2 | Val loss: 0.3926 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4415 | Steps: 2 | Val loss: 0.3775 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2815 | Steps: 2 | Val loss: 0.2917 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2675 | Steps: 2 | Val loss: 0.3479 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=34042)[0m rmse: 0.17688117921352386
[2m[36m(func pid=34042)[0m mae:  0.12927240133285522
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.094, 0.332, 0.097, 0.191, 0.283, 0.14, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
== Status ==
Current time: 2024-01-07 09:30:27 (running for 00:05:48.81)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.51  |  0.177 |                   58 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.441 |  0.151 |                   57 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.282 |  0.162 |                   57 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.254 |  0.186 |                   57 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.15028738975524902
[2m[36m(func pid=34415)[0m mae:  0.10437144339084625
[2m[36m(func pid=34415)[0m rmse_per_class: [0.101, 0.242, 0.042, 0.296, 0.054, 0.182, 0.23, 0.127, 0.138, 0.09]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.162894606590271
[2m[36m(func pid=34838)[0m mae:  0.10407336056232452
[2m[36m(func pid=34838)[0m rmse_per_class: [0.101, 0.249, 0.052, 0.299, 0.051, 0.157, 0.245, 0.154, 0.166, 0.156]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.18945744633674622
[2m[36m(func pid=35262)[0m mae:  0.10998634994029999
[2m[36m(func pid=35262)[0m rmse_per_class: [0.3, 0.266, 0.062, 0.348, 0.067, 0.168, 0.235, 0.13, 0.141, 0.177]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4471 | Steps: 2 | Val loss: 0.3758 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2694 | Steps: 2 | Val loss: 0.2920 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5023 | Steps: 2 | Val loss: 0.3878 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2540 | Steps: 2 | Val loss: 0.3555 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 09:30:33 (running for 00:05:54.07)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.51  |  0.177 |                   58 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.441 |  0.15  |                   58 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.281 |  0.163 |                   58 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.254 |  0.193 |                   59 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=35262)[0m rmse: 0.1933293640613556
[2m[36m(func pid=35262)[0m mae:  0.1133497953414917
[2m[36m(func pid=35262)[0m rmse_per_class: [0.287, 0.263, 0.056, 0.352, 0.068, 0.186, 0.243, 0.136, 0.144, 0.198]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1632964313030243
[2m[36m(func pid=34838)[0m mae:  0.10417859256267548
[2m[36m(func pid=34838)[0m rmse_per_class: [0.105, 0.247, 0.05, 0.296, 0.05, 0.161, 0.243, 0.151, 0.174, 0.155]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.1767931878566742
[2m[36m(func pid=34042)[0m mae:  0.12919305264949799
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.094, 0.331, 0.097, 0.191, 0.283, 0.14, 0.143, 0.111]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.14992110431194305
[2m[36m(func pid=34415)[0m mae:  0.1037677749991417
[2m[36m(func pid=34415)[0m rmse_per_class: [0.1, 0.241, 0.041, 0.295, 0.054, 0.182, 0.231, 0.127, 0.137, 0.09]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2664 | Steps: 2 | Val loss: 0.3632 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4985 | Steps: 2 | Val loss: 0.3830 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4324 | Steps: 2 | Val loss: 0.3742 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2681 | Steps: 2 | Val loss: 0.2914 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 09:30:38 (running for 00:05:59.26)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.502 |  0.177 |                   59 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.447 |  0.15  |                   59 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.269 |  0.163 |                   59 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.254 |  0.193 |                   59 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.14980977773666382
[2m[36m(func pid=34415)[0m mae:  0.10342997312545776
[2m[36m(func pid=34415)[0m rmse_per_class: [0.099, 0.241, 0.04, 0.296, 0.054, 0.182, 0.233, 0.126, 0.137, 0.089]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16263753175735474
[2m[36m(func pid=34838)[0m mae:  0.10375654697418213
[2m[36m(func pid=34838)[0m rmse_per_class: [0.108, 0.246, 0.048, 0.293, 0.049, 0.164, 0.241, 0.145, 0.18, 0.152]
[2m[36m(func pid=34042)[0m rmse: 0.1765735149383545
[2m[36m(func pid=34042)[0m mae:  0.12901143729686737
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.094, 0.331, 0.096, 0.191, 0.282, 0.14, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.1966598778963089
[2m[36m(func pid=35262)[0m mae:  0.11600692570209503
[2m[36m(func pid=35262)[0m rmse_per_class: [0.247, 0.257, 0.061, 0.352, 0.072, 0.198, 0.252, 0.146, 0.15, 0.231]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4314 | Steps: 2 | Val loss: 0.3729 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2608 | Steps: 2 | Val loss: 0.2893 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2479 | Steps: 2 | Val loss: 0.3577 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4911 | Steps: 2 | Val loss: 0.3786 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 09:30:43 (running for 00:06:04.51)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.499 |  0.177 |                   60 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.432 |  0.15  |                   60 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.268 |  0.163 |                   60 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.248 |  0.196 |                   61 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.14981865882873535
[2m[36m(func pid=34415)[0m mae:  0.10323651880025864
[2m[36m(func pid=34415)[0m rmse_per_class: [0.098, 0.241, 0.04, 0.296, 0.054, 0.182, 0.236, 0.125, 0.137, 0.089]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.19611766934394836
[2m[36m(func pid=35262)[0m mae:  0.11558268219232559
[2m[36m(func pid=35262)[0m rmse_per_class: [0.196, 0.256, 0.08, 0.344, 0.076, 0.197, 0.253, 0.153, 0.158, 0.249]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.17640943825244904
[2m[36m(func pid=34042)[0m mae:  0.12887433171272278
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.094, 0.331, 0.096, 0.191, 0.282, 0.14, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16111889481544495
[2m[36m(func pid=34838)[0m mae:  0.10279084742069244
[2m[36m(func pid=34838)[0m rmse_per_class: [0.114, 0.244, 0.045, 0.289, 0.05, 0.166, 0.238, 0.138, 0.182, 0.146]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2467 | Steps: 2 | Val loss: 0.3477 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4905 | Steps: 2 | Val loss: 0.3746 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4297 | Steps: 2 | Val loss: 0.3693 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2704 | Steps: 2 | Val loss: 0.2875 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 09:30:48 (running for 00:06:09.62)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.491 |  0.176 |                   61 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.431 |  0.15  |                   61 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.261 |  0.161 |                   61 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.248 |  0.196 |                   61 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=35262)[0m rmse: 0.19230610132217407
[2m[36m(func pid=35262)[0m mae:  0.11206908524036407
[2m[36m(func pid=35262)[0m rmse_per_class: [0.147, 0.256, 0.093, 0.331, 0.077, 0.191, 0.248, 0.156, 0.174, 0.249]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.17623281478881836
[2m[36m(func pid=34042)[0m mae:  0.12872068583965302
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.093, 0.331, 0.095, 0.191, 0.282, 0.139, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.14975817501544952
[2m[36m(func pid=34415)[0m mae:  0.10292781889438629
[2m[36m(func pid=34415)[0m rmse_per_class: [0.097, 0.241, 0.039, 0.296, 0.054, 0.183, 0.238, 0.125, 0.137, 0.089]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1597440391778946
[2m[36m(func pid=34838)[0m mae:  0.10146681219339371
[2m[36m(func pid=34838)[0m rmse_per_class: [0.117, 0.244, 0.042, 0.285, 0.053, 0.166, 0.233, 0.134, 0.182, 0.141]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2671 | Steps: 2 | Val loss: 0.3423 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4189 | Steps: 2 | Val loss: 0.3671 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4815 | Steps: 2 | Val loss: 0.3702 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2692 | Steps: 2 | Val loss: 0.2837 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 09:30:53 (running for 00:06:14.83)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.491 |  0.176 |                   62 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.43  |  0.15  |                   62 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.27  |  0.16  |                   62 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.267 |  0.19  |                   63 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=35262)[0m rmse: 0.18988551199436188
[2m[36m(func pid=35262)[0m mae:  0.10923357307910919
[2m[36m(func pid=35262)[0m rmse_per_class: [0.114, 0.259, 0.095, 0.315, 0.079, 0.185, 0.246, 0.167, 0.183, 0.257]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.14972831308841705
[2m[36m(func pid=34415)[0m mae:  0.102788046002388
[2m[36m(func pid=34415)[0m rmse_per_class: [0.096, 0.241, 0.038, 0.296, 0.054, 0.183, 0.24, 0.124, 0.137, 0.088]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.1761351078748703
[2m[36m(func pid=34042)[0m mae:  0.1286303848028183
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.093, 0.33, 0.095, 0.191, 0.281, 0.139, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15695886313915253
[2m[36m(func pid=34838)[0m mae:  0.09932059049606323
[2m[36m(func pid=34838)[0m rmse_per_class: [0.118, 0.243, 0.038, 0.281, 0.058, 0.164, 0.228, 0.13, 0.176, 0.133]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2582 | Steps: 2 | Val loss: 0.3334 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4213 | Steps: 2 | Val loss: 0.3648 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4794 | Steps: 2 | Val loss: 0.3667 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2575 | Steps: 2 | Val loss: 0.2848 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 09:30:58 (running for 00:06:19.99)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.482 |  0.176 |                   63 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.419 |  0.15  |                   63 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.269 |  0.157 |                   63 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.258 |  0.185 |                   64 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=35262)[0m rmse: 0.18471430242061615
[2m[36m(func pid=35262)[0m mae:  0.10466592013835907
[2m[36m(func pid=35262)[0m rmse_per_class: [0.098, 0.262, 0.086, 0.3, 0.081, 0.172, 0.24, 0.172, 0.185, 0.251]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.1499338448047638
[2m[36m(func pid=34415)[0m mae:  0.10270203649997711
[2m[36m(func pid=34415)[0m rmse_per_class: [0.095, 0.241, 0.038, 0.297, 0.054, 0.183, 0.243, 0.124, 0.136, 0.088]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1576344072818756
[2m[36m(func pid=34838)[0m mae:  0.09938398003578186
[2m[36m(func pid=34838)[0m rmse_per_class: [0.128, 0.244, 0.033, 0.287, 0.069, 0.164, 0.223, 0.126, 0.174, 0.128]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.17604482173919678
[2m[36m(func pid=34042)[0m mae:  0.12855049967765808
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.093, 0.33, 0.095, 0.191, 0.281, 0.139, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2646 | Steps: 2 | Val loss: 0.3334 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4721 | Steps: 2 | Val loss: 0.3633 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4194 | Steps: 2 | Val loss: 0.3618 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2555 | Steps: 2 | Val loss: 0.2860 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 09:31:04 (running for 00:06:25.09)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.479 |  0.176 |                   64 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.421 |  0.15  |                   64 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.257 |  0.158 |                   64 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.265 |  0.185 |                   65 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=35262)[0m rmse: 0.1845267117023468
[2m[36m(func pid=35262)[0m mae:  0.10308708995580673
[2m[36m(func pid=35262)[0m rmse_per_class: [0.095, 0.263, 0.093, 0.297, 0.082, 0.166, 0.239, 0.178, 0.18, 0.253]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.1759389340877533
[2m[36m(func pid=34042)[0m mae:  0.1284467726945877
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.093, 0.33, 0.095, 0.191, 0.281, 0.139, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1581086963415146
[2m[36m(func pid=34838)[0m mae:  0.09918975085020065
[2m[36m(func pid=34838)[0m rmse_per_class: [0.135, 0.246, 0.029, 0.293, 0.079, 0.163, 0.218, 0.123, 0.171, 0.124]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.14981940388679504
[2m[36m(func pid=34415)[0m mae:  0.10252588987350464
[2m[36m(func pid=34415)[0m rmse_per_class: [0.095, 0.24, 0.037, 0.297, 0.054, 0.183, 0.245, 0.124, 0.136, 0.087]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2509 | Steps: 2 | Val loss: 0.3317 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4667 | Steps: 2 | Val loss: 0.3600 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4082 | Steps: 2 | Val loss: 0.3580 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2570 | Steps: 2 | Val loss: 0.2887 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 09:31:09 (running for 00:06:30.27)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.472 |  0.176 |                   65 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.419 |  0.15  |                   65 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.255 |  0.158 |                   65 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.251 |  0.183 |                   66 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=35262)[0m rmse: 0.18256735801696777
[2m[36m(func pid=35262)[0m mae:  0.10228478908538818
[2m[36m(func pid=35262)[0m rmse_per_class: [0.093, 0.261, 0.086, 0.295, 0.08, 0.166, 0.24, 0.183, 0.173, 0.248]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.17586569488048553
[2m[36m(func pid=34042)[0m mae:  0.12843063473701477
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.26, 0.093, 0.33, 0.094, 0.191, 0.281, 0.139, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.15972135961055756
[2m[36m(func pid=34838)[0m mae:  0.09953053295612335
[2m[36m(func pid=34838)[0m rmse_per_class: [0.144, 0.248, 0.027, 0.299, 0.089, 0.164, 0.215, 0.121, 0.169, 0.121]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.149658203125
[2m[36m(func pid=34415)[0m mae:  0.10231731086969376
[2m[36m(func pid=34415)[0m rmse_per_class: [0.094, 0.24, 0.037, 0.297, 0.054, 0.182, 0.246, 0.123, 0.136, 0.087]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2517 | Steps: 2 | Val loss: 0.3317 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4655 | Steps: 2 | Val loss: 0.3566 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4069 | Steps: 2 | Val loss: 0.3548 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2608 | Steps: 2 | Val loss: 0.2916 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:31:14 (running for 00:06:35.30)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.467 |  0.176 |                   66 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.408 |  0.15  |                   66 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.257 |  0.16  |                   66 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.182 |                   67 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=35262)[0m rmse: 0.18208171427249908
[2m[36m(func pid=35262)[0m mae:  0.10275596380233765
[2m[36m(func pid=35262)[0m rmse_per_class: [0.092, 0.263, 0.084, 0.292, 0.075, 0.171, 0.245, 0.195, 0.162, 0.243]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.175760880112648
[2m[36m(func pid=34042)[0m mae:  0.1283230483531952
[2m[36m(func pid=34042)[0m rmse_per_class: [0.117, 0.26, 0.093, 0.33, 0.094, 0.191, 0.281, 0.139, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.14939574897289276
[2m[36m(func pid=34415)[0m mae:  0.10202548652887344
[2m[36m(func pid=34415)[0m rmse_per_class: [0.093, 0.24, 0.036, 0.297, 0.054, 0.182, 0.247, 0.123, 0.136, 0.087]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1612989604473114
[2m[36m(func pid=34838)[0m mae:  0.0996001735329628
[2m[36m(func pid=34838)[0m rmse_per_class: [0.148, 0.251, 0.026, 0.306, 0.101, 0.164, 0.212, 0.119, 0.166, 0.12]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2534 | Steps: 2 | Val loss: 0.3297 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4597 | Steps: 2 | Val loss: 0.3535 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.3979 | Steps: 2 | Val loss: 0.3506 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2644 | Steps: 2 | Val loss: 0.2949 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=35262)[0m rmse: 0.1814819574356079
[2m[36m(func pid=35262)[0m mae:  0.10284405946731567
[2m[36m(func pid=35262)[0m rmse_per_class: [0.095, 0.265, 0.079, 0.289, 0.073, 0.177, 0.247, 0.198, 0.156, 0.235]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:31:19 (running for 00:06:40.33)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.465 |  0.176 |                   67 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.407 |  0.149 |                   67 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.261 |  0.161 |                   67 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.253 |  0.181 |                   68 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17563529312610626
[2m[36m(func pid=34042)[0m mae:  0.12822525203227997
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.093, 0.329, 0.093, 0.191, 0.28, 0.139, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.14900843799114227
[2m[36m(func pid=34415)[0m mae:  0.10176906734704971
[2m[36m(func pid=34415)[0m rmse_per_class: [0.092, 0.24, 0.035, 0.296, 0.054, 0.182, 0.246, 0.122, 0.136, 0.087]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1631893515586853
[2m[36m(func pid=34838)[0m mae:  0.10023166984319687
[2m[36m(func pid=34838)[0m rmse_per_class: [0.156, 0.255, 0.026, 0.314, 0.112, 0.163, 0.212, 0.114, 0.159, 0.121]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2469 | Steps: 2 | Val loss: 0.3279 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4562 | Steps: 2 | Val loss: 0.3505 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3975 | Steps: 2 | Val loss: 0.3458 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2675 | Steps: 2 | Val loss: 0.2976 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=35262)[0m rmse: 0.18340131640434265
[2m[36m(func pid=35262)[0m mae:  0.1039903536438942
[2m[36m(func pid=35262)[0m rmse_per_class: [0.106, 0.268, 0.088, 0.288, 0.072, 0.183, 0.247, 0.201, 0.153, 0.228]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:31:25 (running for 00:06:46.32)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.456 |  0.176 |                   69 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.398 |  0.149 |                   68 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.264 |  0.163 |                   68 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.247 |  0.183 |                   69 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17552617192268372
[2m[36m(func pid=34042)[0m mae:  0.12811993062496185
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.093, 0.329, 0.093, 0.191, 0.28, 0.139, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.14849424362182617
[2m[36m(func pid=34415)[0m mae:  0.10146476328372955
[2m[36m(func pid=34415)[0m rmse_per_class: [0.092, 0.24, 0.034, 0.296, 0.054, 0.181, 0.244, 0.122, 0.136, 0.086]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16461534798145294
[2m[36m(func pid=34838)[0m mae:  0.100372314453125
[2m[36m(func pid=34838)[0m rmse_per_class: [0.157, 0.259, 0.027, 0.318, 0.121, 0.163, 0.212, 0.111, 0.158, 0.12]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2456 | Steps: 2 | Val loss: 0.3289 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4525 | Steps: 2 | Val loss: 0.3473 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3956 | Steps: 2 | Val loss: 0.3416 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2536 | Steps: 2 | Val loss: 0.3003 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=35262)[0m rmse: 0.18504220247268677
[2m[36m(func pid=35262)[0m mae:  0.10542573779821396
[2m[36m(func pid=35262)[0m rmse_per_class: [0.127, 0.274, 0.082, 0.289, 0.071, 0.19, 0.245, 0.192, 0.151, 0.231]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:31:30 (running for 00:06:51.49)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.453 |  0.175 |                   70 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.398 |  0.148 |                   69 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.268 |  0.165 |                   69 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.246 |  0.185 |                   70 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17532561719417572
[2m[36m(func pid=34042)[0m mae:  0.12793895602226257
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.092, 0.329, 0.093, 0.191, 0.28, 0.139, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.14778226613998413
[2m[36m(func pid=34415)[0m mae:  0.10097255557775497
[2m[36m(func pid=34415)[0m rmse_per_class: [0.091, 0.24, 0.033, 0.295, 0.054, 0.18, 0.242, 0.121, 0.136, 0.086]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16630123555660248
[2m[36m(func pid=34838)[0m mae:  0.1007598489522934
[2m[36m(func pid=34838)[0m rmse_per_class: [0.161, 0.262, 0.03, 0.322, 0.127, 0.161, 0.21, 0.109, 0.157, 0.123]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2487 | Steps: 2 | Val loss: 0.3294 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3971 | Steps: 2 | Val loss: 0.3364 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4470 | Steps: 2 | Val loss: 0.3446 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2483 | Steps: 2 | Val loss: 0.3031 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=35262)[0m rmse: 0.18499699234962463
[2m[36m(func pid=35262)[0m mae:  0.10618338733911514
[2m[36m(func pid=35262)[0m rmse_per_class: [0.151, 0.275, 0.067, 0.292, 0.071, 0.193, 0.241, 0.178, 0.15, 0.232]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:31:35 (running for 00:06:56.67)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.453 |  0.175 |                   70 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.397 |  0.147 |                   71 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.166 |                   70 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.249 |  0.185 |                   71 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.14723947644233704
[2m[36m(func pid=34415)[0m mae:  0.10053753852844238
[2m[36m(func pid=34415)[0m rmse_per_class: [0.09, 0.24, 0.033, 0.294, 0.054, 0.18, 0.24, 0.12, 0.136, 0.085]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.1752198040485382
[2m[36m(func pid=34042)[0m mae:  0.12783962488174438
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.092, 0.328, 0.092, 0.191, 0.279, 0.139, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1679038107395172
[2m[36m(func pid=34838)[0m mae:  0.10103484243154526
[2m[36m(func pid=34838)[0m rmse_per_class: [0.159, 0.266, 0.033, 0.325, 0.134, 0.161, 0.209, 0.108, 0.155, 0.13]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2616 | Steps: 2 | Val loss: 0.3272 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3863 | Steps: 2 | Val loss: 0.3323 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4438 | Steps: 2 | Val loss: 0.3424 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2575 | Steps: 2 | Val loss: 0.3060 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=35262)[0m rmse: 0.18408037722110748
[2m[36m(func pid=35262)[0m mae:  0.10629534721374512
[2m[36m(func pid=35262)[0m rmse_per_class: [0.183, 0.268, 0.061, 0.294, 0.071, 0.19, 0.237, 0.16, 0.152, 0.225]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:31:40 (running for 00:07:01.82)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.447 |  0.175 |                   71 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.386 |  0.147 |                   72 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.248 |  0.168 |                   71 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.262 |  0.184 |                   72 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.14690366387367249
[2m[36m(func pid=34415)[0m mae:  0.10021503269672394
[2m[36m(func pid=34415)[0m rmse_per_class: [0.089, 0.24, 0.032, 0.294, 0.054, 0.18, 0.239, 0.12, 0.136, 0.086]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.17524969577789307
[2m[36m(func pid=34042)[0m mae:  0.12786313891410828
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.092, 0.328, 0.092, 0.191, 0.279, 0.139, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16894686222076416
[2m[36m(func pid=34838)[0m mae:  0.10116174072027206
[2m[36m(func pid=34838)[0m rmse_per_class: [0.156, 0.27, 0.036, 0.328, 0.136, 0.16, 0.207, 0.107, 0.152, 0.136]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2508 | Steps: 2 | Val loss: 0.3272 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3859 | Steps: 2 | Val loss: 0.3282 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4399 | Steps: 2 | Val loss: 0.3399 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2562 | Steps: 2 | Val loss: 0.3083 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=35262)[0m rmse: 0.18316441774368286
[2m[36m(func pid=35262)[0m mae:  0.10620798915624619
[2m[36m(func pid=35262)[0m rmse_per_class: [0.2, 0.262, 0.054, 0.295, 0.071, 0.189, 0.232, 0.149, 0.156, 0.224]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:31:45 (running for 00:07:06.97)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.444 |  0.175 |                   72 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.386 |  0.147 |                   73 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.257 |  0.169 |                   72 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.251 |  0.183 |                   73 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.1465052217245102
[2m[36m(func pid=34415)[0m mae:  0.09987636655569077
[2m[36m(func pid=34415)[0m rmse_per_class: [0.088, 0.24, 0.031, 0.294, 0.054, 0.18, 0.238, 0.119, 0.136, 0.085]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34042)[0m rmse: 0.17518097162246704
[2m[36m(func pid=34042)[0m mae:  0.12779811024665833
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.092, 0.328, 0.091, 0.191, 0.279, 0.139, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.1698518693447113
[2m[36m(func pid=34838)[0m mae:  0.10107642412185669
[2m[36m(func pid=34838)[0m rmse_per_class: [0.156, 0.271, 0.038, 0.329, 0.134, 0.16, 0.206, 0.106, 0.152, 0.146]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2507 | Steps: 2 | Val loss: 0.3220 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3802 | Steps: 2 | Val loss: 0.3243 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4398 | Steps: 2 | Val loss: 0.3378 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2511 | Steps: 2 | Val loss: 0.3095 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=35262)[0m rmse: 0.17787979543209076
[2m[36m(func pid=35262)[0m mae:  0.10308544337749481
[2m[36m(func pid=35262)[0m rmse_per_class: [0.187, 0.26, 0.05, 0.292, 0.069, 0.184, 0.225, 0.138, 0.159, 0.216]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.1461249738931656
[2m[36m(func pid=34415)[0m mae:  0.09962069988250732
[2m[36m(func pid=34415)[0m rmse_per_class: [0.088, 0.239, 0.031, 0.293, 0.054, 0.179, 0.237, 0.119, 0.136, 0.085]
[2m[36m(func pid=34415)[0m 
== Status ==
Current time: 2024-01-07 09:31:51 (running for 00:07:12.20)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | RUNNING  | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.44  |  0.175 |                   73 |
| train_32e5a_00001 | RUNNING  | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.38  |  0.146 |                   74 |
| train_32e5a_00002 | RUNNING  | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.256 |  0.17  |                   73 |
| train_32e5a_00003 | RUNNING  | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.251 |  0.178 |                   74 |
| train_32e5a_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34042)[0m rmse: 0.17511600255966187
[2m[36m(func pid=34042)[0m mae:  0.1277465671300888
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.092, 0.328, 0.091, 0.191, 0.279, 0.139, 0.143, 0.11]
[2m[36m(func pid=34042)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.16950096189975739
[2m[36m(func pid=34838)[0m mae:  0.10036797821521759
[2m[36m(func pid=34838)[0m rmse_per_class: [0.15, 0.273, 0.039, 0.33, 0.133, 0.158, 0.204, 0.106, 0.151, 0.151]
[2m[36m(func pid=34838)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2540 | Steps: 2 | Val loss: 0.3189 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
[2m[36m(func pid=34042)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4358 | Steps: 2 | Val loss: 0.3355 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3632 | Steps: 2 | Val loss: 0.3203 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=35262)[0m rmse: 0.1723969280719757
[2m[36m(func pid=35262)[0m mae:  0.09948672354221344
[2m[36m(func pid=35262)[0m rmse_per_class: [0.16, 0.26, 0.049, 0.289, 0.067, 0.179, 0.223, 0.129, 0.159, 0.21]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34838)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2541 | Steps: 2 | Val loss: 0.3122 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=34042)[0m rmse: 0.17496611177921295
[2m[36m(func pid=34042)[0m mae:  0.1276237815618515
[2m[36m(func pid=34042)[0m rmse_per_class: [0.118, 0.26, 0.092, 0.327, 0.09, 0.191, 0.279, 0.139, 0.143, 0.11]
== Status ==
Current time: 2024-01-07 09:31:56 (running for 00:07:17.57)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: -0.17275000363588333
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 3 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.38  |  0.146 |                   74 |
| train_32e5a_00002 | RUNNING    | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.251 |  0.17  |                   74 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.254 |  0.172 |                   75 |
| train_32e5a_00004 | PENDING    |                    | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.14576615393161774
[2m[36m(func pid=34415)[0m mae:  0.09940163046121597
[2m[36m(func pid=34415)[0m rmse_per_class: [0.087, 0.239, 0.03, 0.293, 0.054, 0.179, 0.236, 0.118, 0.136, 0.085]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=34838)[0m rmse: 0.17039230465888977
[2m[36m(func pid=34838)[0m mae:  0.10060540586709976
[2m[36m(func pid=34838)[0m rmse_per_class: [0.152, 0.276, 0.039, 0.332, 0.132, 0.157, 0.204, 0.105, 0.151, 0.156]
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2554 | Steps: 2 | Val loss: 0.3175 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.3648 | Steps: 2 | Val loss: 0.3170 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=35262)[0m rmse: 0.1686875820159912
[2m[36m(func pid=35262)[0m mae:  0.09693309664726257
[2m[36m(func pid=35262)[0m rmse_per_class: [0.137, 0.259, 0.048, 0.288, 0.066, 0.175, 0.224, 0.124, 0.16, 0.208]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.14555734395980835
[2m[36m(func pid=34415)[0m mae:  0.09923551976680756
[2m[36m(func pid=34415)[0m rmse_per_class: [0.087, 0.24, 0.03, 0.293, 0.054, 0.178, 0.236, 0.118, 0.136, 0.085]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2595 | Steps: 2 | Val loss: 0.3164 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=35262)[0m rmse: 0.16664716601371765
[2m[36m(func pid=35262)[0m mae:  0.09569624811410904
[2m[36m(func pid=35262)[0m rmse_per_class: [0.119, 0.257, 0.045, 0.286, 0.064, 0.173, 0.223, 0.128, 0.16, 0.211]
[2m[36m(func pid=51709)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51709)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=51709)[0m Configuration completed!
[2m[36m(func pid=51709)[0m New optimizer parameters:
[2m[36m(func pid=51709)[0m SGD (
[2m[36m(func pid=51709)[0m Parameter Group 0
[2m[36m(func pid=51709)[0m     dampening: 0
[2m[36m(func pid=51709)[0m     differentiable: False
[2m[36m(func pid=51709)[0m     foreach: None
[2m[36m(func pid=51709)[0m     lr: 0.0001
[2m[36m(func pid=51709)[0m     maximize: False
[2m[36m(func pid=51709)[0m     momentum: 0.9
[2m[36m(func pid=51709)[0m     nesterov: False
[2m[36m(func pid=51709)[0m     weight_decay: 0
[2m[36m(func pid=51709)[0m )
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.3713 | Steps: 2 | Val loss: 0.3131 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 09:32:01 (running for 00:07:22.74)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.365 |  0.146 |                   76 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.255 |  0.169 |                   76 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=51774)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=51774)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=51774)[0m Configuration completed!
[2m[36m(func pid=51774)[0m New optimizer parameters:
[2m[36m(func pid=51774)[0m SGD (
[2m[36m(func pid=51774)[0m Parameter Group 0
[2m[36m(func pid=51774)[0m     dampening: 0
[2m[36m(func pid=51774)[0m     differentiable: False
[2m[36m(func pid=51774)[0m     foreach: None
[2m[36m(func pid=51774)[0m     lr: 0.001
[2m[36m(func pid=51774)[0m     maximize: False
[2m[36m(func pid=51774)[0m     momentum: 0.9
[2m[36m(func pid=51774)[0m     nesterov: False
[2m[36m(func pid=51774)[0m     weight_decay: 0
[2m[36m(func pid=51774)[0m )
[2m[36m(func pid=51774)[0m 
== Status ==
Current time: 2024-01-07 09:32:06 (running for 00:07:27.84)
Memory usage on this node: 23.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.371 |  0.145 |                   77 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.26  |  0.167 |                   77 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |        |        |                      |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.14534039795398712
[2m[36m(func pid=34415)[0m mae:  0.09892734885215759
[2m[36m(func pid=34415)[0m rmse_per_class: [0.086, 0.239, 0.029, 0.293, 0.054, 0.178, 0.236, 0.118, 0.136, 0.085]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8976 | Steps: 2 | Val loss: 0.7089 | Batch size: 32 | lr: 0.0001 | Duration: 4.71s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2568 | Steps: 2 | Val loss: 0.3155 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.3524 | Steps: 2 | Val loss: 0.3097 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8982 | Steps: 2 | Val loss: 0.7066 | Batch size: 32 | lr: 0.001 | Duration: 4.46s
[2m[36m(func pid=35262)[0m rmse: 0.16580964624881744
[2m[36m(func pid=35262)[0m mae:  0.09516189247369766
[2m[36m(func pid=35262)[0m rmse_per_class: [0.107, 0.257, 0.045, 0.288, 0.065, 0.172, 0.222, 0.129, 0.159, 0.215]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.182662695646286
[2m[36m(func pid=51709)[0m mae:  0.13440732657909393
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=51709)[0m 
== Status ==
Current time: 2024-01-07 09:32:11 (running for 00:07:32.96)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.352 |  0.145 |                   78 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.257 |  0.166 |                   78 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.898 |  0.183 |                    1 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |        |        |                      |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.14510764181613922
[2m[36m(func pid=34415)[0m mae:  0.09875483810901642
[2m[36m(func pid=34415)[0m rmse_per_class: [0.085, 0.239, 0.029, 0.293, 0.054, 0.178, 0.236, 0.117, 0.136, 0.085]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.18264588713645935
[2m[36m(func pid=51774)[0m mae:  0.13439390063285828
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.11, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2503 | Steps: 2 | Val loss: 0.3153 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.3544 | Steps: 2 | Val loss: 0.3064 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8973 | Steps: 2 | Val loss: 0.7047 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8912 | Steps: 2 | Val loss: 0.6983 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=35262)[0m rmse: 0.16784991323947906
[2m[36m(func pid=35262)[0m mae:  0.09597451984882355
[2m[36m(func pid=35262)[0m rmse_per_class: [0.1, 0.259, 0.055, 0.289, 0.064, 0.173, 0.22, 0.141, 0.158, 0.218]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:32:16 (running for 00:07:37.98)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.354 |  0.145 |                   79 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.25  |  0.168 |                   79 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.898 |  0.183 |                    1 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.898 |  0.183 |                    1 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.14504119753837585
[2m[36m(func pid=34415)[0m mae:  0.09866753220558167
[2m[36m(func pid=34415)[0m rmse_per_class: [0.085, 0.239, 0.028, 0.294, 0.054, 0.178, 0.236, 0.117, 0.136, 0.084]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.18247917294502258
[2m[36m(func pid=51709)[0m mae:  0.13431206345558167
[2m[36m(func pid=51709)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.112]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.18246182799339294
[2m[36m(func pid=51774)[0m mae:  0.1342986524105072
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.112]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2525 | Steps: 2 | Val loss: 0.3156 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.3537 | Steps: 2 | Val loss: 0.3031 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8964 | Steps: 2 | Val loss: 0.7012 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8809 | Steps: 2 | Val loss: 0.6880 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=35262)[0m rmse: 0.17093855142593384
[2m[36m(func pid=35262)[0m mae:  0.09789478033781052
[2m[36m(func pid=35262)[0m rmse_per_class: [0.099, 0.263, 0.065, 0.294, 0.065, 0.173, 0.221, 0.154, 0.156, 0.22]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:32:22 (running for 00:07:43.40)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.354 |  0.145 |                   80 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.171 |                   80 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.897 |  0.182 |                    2 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.891 |  0.182 |                    2 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.14499172568321228
[2m[36m(func pid=34415)[0m mae:  0.09855683147907257
[2m[36m(func pid=34415)[0m rmse_per_class: [0.085, 0.239, 0.028, 0.294, 0.054, 0.177, 0.236, 0.116, 0.136, 0.084]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.18217475712299347
[2m[36m(func pid=51709)[0m mae:  0.1340981423854828
[2m[36m(func pid=51709)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.18208815157413483
[2m[36m(func pid=51774)[0m mae:  0.1340111941099167
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2412 | Steps: 2 | Val loss: 0.3153 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.3454 | Steps: 2 | Val loss: 0.3002 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8948 | Steps: 2 | Val loss: 0.6987 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=35262)[0m rmse: 0.17220410704612732
[2m[36m(func pid=35262)[0m mae:  0.09903614223003387
[2m[36m(func pid=35262)[0m rmse_per_class: [0.101, 0.266, 0.065, 0.299, 0.066, 0.171, 0.222, 0.168, 0.153, 0.21]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8689 | Steps: 2 | Val loss: 0.6760 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 09:32:27 (running for 00:07:48.64)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.345 |  0.145 |                   81 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.241 |  0.172 |                   81 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.896 |  0.182 |                    3 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.881 |  0.182 |                    3 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=34415)[0m rmse: 0.14482229948043823
[2m[36m(func pid=34415)[0m mae:  0.09850598871707916
[2m[36m(func pid=34415)[0m rmse_per_class: [0.085, 0.239, 0.028, 0.294, 0.054, 0.177, 0.235, 0.116, 0.136, 0.084]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.18177476525306702
[2m[36m(func pid=51709)[0m mae:  0.13378114998340607
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.338, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2560 | Steps: 2 | Val loss: 0.3156 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=51774)[0m rmse: 0.18162912130355835
[2m[36m(func pid=51774)[0m mae:  0.13363544642925262
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.338, 0.112, 0.19, 0.293, 0.142, 0.143, 0.111]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.3405 | Steps: 2 | Val loss: 0.2975 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8923 | Steps: 2 | Val loss: 0.6964 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=35262)[0m rmse: 0.1724981814622879
[2m[36m(func pid=35262)[0m mae:  0.099581778049469
[2m[36m(func pid=35262)[0m rmse_per_class: [0.104, 0.272, 0.059, 0.303, 0.067, 0.168, 0.222, 0.179, 0.151, 0.201]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8557 | Steps: 2 | Val loss: 0.6638 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=34415)[0m rmse: 0.1447291076183319
[2m[36m(func pid=34415)[0m mae:  0.09846103936433792
[2m[36m(func pid=34415)[0m rmse_per_class: [0.085, 0.239, 0.028, 0.295, 0.054, 0.176, 0.235, 0.116, 0.136, 0.084]
[2m[36m(func pid=34415)[0m 
== Status ==
Current time: 2024-01-07 09:32:32 (running for 00:07:53.89)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.34  |  0.145 |                   82 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.256 |  0.172 |                   82 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.895 |  0.182 |                    4 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.869 |  0.182 |                    4 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.1814216673374176
[2m[36m(func pid=51709)[0m mae:  0.1334841549396515
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.112, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2465 | Steps: 2 | Val loss: 0.3160 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=51774)[0m rmse: 0.18118660151958466
[2m[36m(func pid=51774)[0m mae:  0.13326266407966614
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.111, 0.19, 0.293, 0.141, 0.143, 0.111]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.3364 | Steps: 2 | Val loss: 0.2946 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8954 | Steps: 2 | Val loss: 0.6952 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=35262)[0m rmse: 0.1730748862028122
[2m[36m(func pid=35262)[0m mae:  0.10001726448535919
[2m[36m(func pid=35262)[0m rmse_per_class: [0.115, 0.277, 0.052, 0.307, 0.068, 0.163, 0.222, 0.183, 0.149, 0.195]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8416 | Steps: 2 | Val loss: 0.6512 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=34415)[0m rmse: 0.14462411403656006
[2m[36m(func pid=34415)[0m mae:  0.09831835329532623
[2m[36m(func pid=34415)[0m rmse_per_class: [0.085, 0.239, 0.028, 0.295, 0.054, 0.176, 0.235, 0.115, 0.136, 0.084]
[2m[36m(func pid=34415)[0m 
== Status ==
Current time: 2024-01-07 09:32:38 (running for 00:07:59.50)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.336 |  0.145 |                   83 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.247 |  0.173 |                   83 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.895 |  0.181 |                    6 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.856 |  0.181 |                    5 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.1810939460992813
[2m[36m(func pid=51709)[0m mae:  0.13320818543434143
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.112, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2427 | Steps: 2 | Val loss: 0.3152 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=51774)[0m rmse: 0.18071427941322327
[2m[36m(func pid=51774)[0m mae:  0.13286033272743225
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.11, 0.19, 0.292, 0.141, 0.143, 0.111]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.3335 | Steps: 2 | Val loss: 0.2914 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8906 | Steps: 2 | Val loss: 0.6935 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=35262)[0m rmse: 0.1729896068572998
[2m[36m(func pid=35262)[0m mae:  0.10031137615442276
[2m[36m(func pid=35262)[0m rmse_per_class: [0.125, 0.282, 0.043, 0.31, 0.069, 0.159, 0.222, 0.182, 0.148, 0.189]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8230 | Steps: 2 | Val loss: 0.6377 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=34415)[0m rmse: 0.1443391591310501
[2m[36m(func pid=34415)[0m mae:  0.09806052595376968
[2m[36m(func pid=34415)[0m rmse_per_class: [0.084, 0.239, 0.027, 0.295, 0.054, 0.175, 0.235, 0.115, 0.136, 0.084]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.18074649572372437
[2m[36m(func pid=51709)[0m mae:  0.13289977610111237
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.264, 0.103, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=51709)[0m 
== Status ==
Current time: 2024-01-07 09:32:43 (running for 00:08:04.94)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.333 |  0.144 |                   84 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.243 |  0.173 |                   84 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.891 |  0.181 |                    7 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.842 |  0.181 |                    6 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2454 | Steps: 2 | Val loss: 0.3165 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
[2m[36m(func pid=51774)[0m rmse: 0.18032428622245789
[2m[36m(func pid=51774)[0m mae:  0.132511168718338
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.109, 0.19, 0.291, 0.141, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.3284 | Steps: 2 | Val loss: 0.2886 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=35262)[0m rmse: 0.17441856861114502
[2m[36m(func pid=35262)[0m mae:  0.1013299822807312
[2m[36m(func pid=35262)[0m rmse_per_class: [0.135, 0.282, 0.038, 0.312, 0.07, 0.156, 0.225, 0.189, 0.147, 0.19]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8866 | Steps: 2 | Val loss: 0.6925 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8061 | Steps: 2 | Val loss: 0.6245 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=34415)[0m rmse: 0.14419616758823395
[2m[36m(func pid=34415)[0m mae:  0.09803295135498047
[2m[36m(func pid=34415)[0m rmse_per_class: [0.085, 0.239, 0.027, 0.295, 0.054, 0.175, 0.233, 0.115, 0.136, 0.083]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2593 | Steps: 2 | Val loss: 0.3165 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
== Status ==
Current time: 2024-01-07 09:32:49 (running for 00:08:10.51)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.328 |  0.144 |                   85 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.245 |  0.174 |                   85 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.887 |  0.18  |                    8 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.823 |  0.18  |                    7 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.18049846589565277
[2m[36m(func pid=51709)[0m mae:  0.13267503678798676
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.263, 0.103, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.1800207793712616
[2m[36m(func pid=51774)[0m mae:  0.1322399079799652
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.263, 0.102, 0.337, 0.109, 0.19, 0.291, 0.14, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3364 | Steps: 2 | Val loss: 0.2860 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=35262)[0m rmse: 0.1745884269475937
[2m[36m(func pid=35262)[0m mae:  0.10107197612524033
[2m[36m(func pid=35262)[0m rmse_per_class: [0.144, 0.279, 0.036, 0.31, 0.072, 0.155, 0.227, 0.183, 0.145, 0.195]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8860 | Steps: 2 | Val loss: 0.6910 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7899 | Steps: 2 | Val loss: 0.6117 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=34415)[0m rmse: 0.14382407069206238
[2m[36m(func pid=34415)[0m mae:  0.09793607145547867
[2m[36m(func pid=34415)[0m rmse_per_class: [0.085, 0.238, 0.026, 0.295, 0.054, 0.174, 0.231, 0.114, 0.136, 0.084]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2528 | Steps: 2 | Val loss: 0.3099 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=51709)[0m rmse: 0.18028807640075684
[2m[36m(func pid=51709)[0m mae:  0.13249675929546356
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.263, 0.102, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=51709)[0m 
== Status ==
Current time: 2024-01-07 09:32:54 (running for 00:08:15.99)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.336 |  0.144 |                   86 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.259 |  0.175 |                   86 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.886 |  0.18  |                    9 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.806 |  0.18  |                    8 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.1798216551542282
[2m[36m(func pid=51774)[0m mae:  0.13204436004161835
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.263, 0.101, 0.337, 0.108, 0.19, 0.29, 0.141, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.3230 | Steps: 2 | Val loss: 0.2840 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=35262)[0m rmse: 0.1712745726108551
[2m[36m(func pid=35262)[0m mae:  0.09911106526851654
[2m[36m(func pid=35262)[0m rmse_per_class: [0.145, 0.272, 0.034, 0.301, 0.073, 0.155, 0.226, 0.172, 0.145, 0.19]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8848 | Steps: 2 | Val loss: 0.6893 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7689 | Steps: 2 | Val loss: 0.5993 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=34415)[0m rmse: 0.14380179345607758
[2m[36m(func pid=34415)[0m mae:  0.09796122461557388
[2m[36m(func pid=34415)[0m rmse_per_class: [0.085, 0.238, 0.026, 0.295, 0.054, 0.174, 0.231, 0.114, 0.136, 0.084]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2399 | Steps: 2 | Val loss: 0.3054 | Batch size: 32 | lr: 0.1 | Duration: 2.63s
== Status ==
Current time: 2024-01-07 09:33:00 (running for 00:08:21.15)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.323 |  0.144 |                   87 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.253 |  0.171 |                   87 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.885 |  0.18  |                   10 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.79  |  0.18  |                    9 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.18013153970241547
[2m[36m(func pid=51709)[0m mae:  0.13235101103782654
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.262, 0.102, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17962387204170227
[2m[36m(func pid=51774)[0m mae:  0.13183711469173431
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.262, 0.101, 0.337, 0.107, 0.19, 0.29, 0.141, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3260 | Steps: 2 | Val loss: 0.2820 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=35262)[0m rmse: 0.16838322579860687
[2m[36m(func pid=35262)[0m mae:  0.0975966826081276
[2m[36m(func pid=35262)[0m rmse_per_class: [0.144, 0.264, 0.033, 0.291, 0.072, 0.155, 0.226, 0.165, 0.143, 0.191]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8805 | Steps: 2 | Val loss: 0.6882 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=34415)[0m rmse: 0.1436522752046585
[2m[36m(func pid=34415)[0m mae:  0.09776069223880768
[2m[36m(func pid=34415)[0m rmse_per_class: [0.085, 0.238, 0.026, 0.295, 0.054, 0.173, 0.232, 0.114, 0.136, 0.083]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7503 | Steps: 2 | Val loss: 0.5865 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2433 | Steps: 2 | Val loss: 0.3025 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 09:33:05 (running for 00:08:26.60)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.326 |  0.144 |                   88 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.24  |  0.168 |                   88 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.881 |  0.18  |                   11 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.769 |  0.18  |                   10 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17999914288520813
[2m[36m(func pid=51709)[0m mae:  0.13223280012607574
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17951412498950958
[2m[36m(func pid=51774)[0m mae:  0.13170115649700165
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.262, 0.101, 0.337, 0.106, 0.19, 0.289, 0.141, 0.143, 0.111]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.3245 | Steps: 2 | Val loss: 0.2798 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=35262)[0m rmse: 0.16583861410617828
[2m[36m(func pid=35262)[0m mae:  0.0961514338850975
[2m[36m(func pid=35262)[0m rmse_per_class: [0.143, 0.259, 0.03, 0.283, 0.07, 0.156, 0.224, 0.157, 0.143, 0.193]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8777 | Steps: 2 | Val loss: 0.6871 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=34415)[0m rmse: 0.14342908561229706
[2m[36m(func pid=34415)[0m mae:  0.09770740568637848
[2m[36m(func pid=34415)[0m rmse_per_class: [0.086, 0.238, 0.026, 0.295, 0.054, 0.172, 0.231, 0.114, 0.136, 0.083]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7316 | Steps: 2 | Val loss: 0.5732 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2410 | Steps: 2 | Val loss: 0.3026 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 09:33:10 (running for 00:08:31.83)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.325 |  0.143 |                   89 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.243 |  0.166 |                   89 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.878 |  0.18  |                   12 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.75  |  0.18  |                   11 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.1799212396144867
[2m[36m(func pid=51709)[0m mae:  0.13215965032577515
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17936500906944275
[2m[36m(func pid=51774)[0m mae:  0.13154135644435883
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.262, 0.1, 0.336, 0.105, 0.19, 0.288, 0.141, 0.143, 0.111]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3274 | Steps: 2 | Val loss: 0.2777 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=35262)[0m rmse: 0.16439805924892426
[2m[36m(func pid=35262)[0m mae:  0.09495275467634201
[2m[36m(func pid=35262)[0m rmse_per_class: [0.138, 0.256, 0.029, 0.279, 0.068, 0.157, 0.223, 0.15, 0.143, 0.201]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m rmse: 0.14318761229515076
[2m[36m(func pid=34415)[0m mae:  0.0974765494465828
[2m[36m(func pid=34415)[0m rmse_per_class: [0.085, 0.238, 0.026, 0.294, 0.054, 0.171, 0.231, 0.113, 0.136, 0.083]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8763 | Steps: 2 | Val loss: 0.6858 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7127 | Steps: 2 | Val loss: 0.5600 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2501 | Steps: 2 | Val loss: 0.3022 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 09:33:16 (running for 00:08:37.24)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.327 |  0.143 |                   90 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.241 |  0.164 |                   90 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.876 |  0.18  |                   13 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.732 |  0.179 |                   12 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17987102270126343
[2m[36m(func pid=51709)[0m mae:  0.13210216164588928
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17924094200134277
[2m[36m(func pid=51774)[0m mae:  0.13139691948890686
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.262, 0.1, 0.336, 0.105, 0.19, 0.288, 0.141, 0.143, 0.112]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.16344007849693298
[2m[36m(func pid=35262)[0m mae:  0.09380307048559189
[2m[36m(func pid=35262)[0m rmse_per_class: [0.132, 0.253, 0.031, 0.276, 0.067, 0.158, 0.221, 0.148, 0.144, 0.205]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.3155 | Steps: 2 | Val loss: 0.2759 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8721 | Steps: 2 | Val loss: 0.6844 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=34415)[0m rmse: 0.14290621876716614
[2m[36m(func pid=34415)[0m mae:  0.09729847311973572
[2m[36m(func pid=34415)[0m rmse_per_class: [0.085, 0.238, 0.026, 0.293, 0.054, 0.171, 0.23, 0.113, 0.136, 0.084]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.6979 | Steps: 2 | Val loss: 0.5469 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2543 | Steps: 2 | Val loss: 0.3024 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 09:33:21 (running for 00:08:42.39)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.315 |  0.143 |                   91 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.25  |  0.163 |                   91 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.872 |  0.18  |                   14 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.713 |  0.179 |                   13 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17980888485908508
[2m[36m(func pid=51709)[0m mae:  0.1320316046476364
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=35262)[0m rmse: 0.16353759169578552
[2m[36m(func pid=35262)[0m mae:  0.09320250898599625
[2m[36m(func pid=35262)[0m rmse_per_class: [0.128, 0.25, 0.036, 0.276, 0.066, 0.159, 0.22, 0.146, 0.144, 0.211]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17913410067558289
[2m[36m(func pid=51774)[0m mae:  0.1312844455242157
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.262, 0.099, 0.336, 0.104, 0.19, 0.287, 0.141, 0.143, 0.112]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.3145 | Steps: 2 | Val loss: 0.2742 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8727 | Steps: 2 | Val loss: 0.6828 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2509 | Steps: 2 | Val loss: 0.3027 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=34415)[0m rmse: 0.14266766607761383
[2m[36m(func pid=34415)[0m mae:  0.09713871777057648
[2m[36m(func pid=34415)[0m rmse_per_class: [0.085, 0.238, 0.026, 0.293, 0.054, 0.17, 0.229, 0.113, 0.135, 0.084]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.6828 | Steps: 2 | Val loss: 0.5343 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=35262)[0m rmse: 0.16359974443912506
[2m[36m(func pid=35262)[0m mae:  0.09243224561214447
[2m[36m(func pid=35262)[0m rmse_per_class: [0.122, 0.246, 0.042, 0.277, 0.069, 0.16, 0.218, 0.149, 0.143, 0.21]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:33:26 (running for 00:08:47.83)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.315 |  0.143 |                   92 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.251 |  0.164 |                   93 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.872 |  0.18  |                   14 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.698 |  0.179 |                   14 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.1797449290752411
[2m[36m(func pid=51709)[0m mae:  0.13198186457157135
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17893168330192566
[2m[36m(func pid=51774)[0m mae:  0.1310812532901764
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.262, 0.098, 0.336, 0.104, 0.19, 0.287, 0.141, 0.143, 0.112]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.3121 | Steps: 2 | Val loss: 0.2730 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2501 | Steps: 2 | Val loss: 0.2994 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8695 | Steps: 2 | Val loss: 0.6806 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=34415)[0m rmse: 0.14270654320716858
[2m[36m(func pid=34415)[0m mae:  0.09719973802566528
[2m[36m(func pid=34415)[0m rmse_per_class: [0.085, 0.238, 0.026, 0.295, 0.054, 0.17, 0.228, 0.113, 0.135, 0.083]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6663 | Steps: 2 | Val loss: 0.5214 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 09:33:31 (running for 00:08:52.96)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.312 |  0.143 |                   93 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.25  |  0.163 |                   94 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.873 |  0.18  |                   15 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.683 |  0.179 |                   15 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=35262)[0m rmse: 0.16286969184875488
[2m[36m(func pid=35262)[0m mae:  0.09183250367641449
[2m[36m(func pid=35262)[0m rmse_per_class: [0.12, 0.241, 0.049, 0.279, 0.07, 0.161, 0.217, 0.146, 0.142, 0.204]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17968308925628662
[2m[36m(func pid=51709)[0m mae:  0.13193561136722565
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17879073321819305
[2m[36m(func pid=51774)[0m mae:  0.13094469904899597
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.261, 0.098, 0.335, 0.104, 0.19, 0.287, 0.141, 0.143, 0.112]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3105 | Steps: 2 | Val loss: 0.2719 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2485 | Steps: 2 | Val loss: 0.2998 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=34415)[0m rmse: 0.14274153113365173
[2m[36m(func pid=34415)[0m mae:  0.09728662669658661
[2m[36m(func pid=34415)[0m rmse_per_class: [0.086, 0.238, 0.026, 0.296, 0.054, 0.17, 0.227, 0.112, 0.135, 0.084]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8641 | Steps: 2 | Val loss: 0.6789 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6510 | Steps: 2 | Val loss: 0.5096 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 09:33:36 (running for 00:08:57.98)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.311 |  0.143 |                   94 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.249 |  0.163 |                   95 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.869 |  0.18  |                   16 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.666 |  0.179 |                   16 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=35262)[0m rmse: 0.16284683346748352
[2m[36m(func pid=35262)[0m mae:  0.09159065783023834
[2m[36m(func pid=35262)[0m rmse_per_class: [0.115, 0.24, 0.051, 0.28, 0.072, 0.162, 0.217, 0.147, 0.141, 0.205]
[2m[36m(func pid=35262)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17962828278541565
[2m[36m(func pid=51709)[0m mae:  0.13188236951828003
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.1786346137523651
[2m[36m(func pid=51774)[0m mae:  0.13079582154750824
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.261, 0.098, 0.335, 0.103, 0.19, 0.287, 0.141, 0.143, 0.112]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.3064 | Steps: 2 | Val loss: 0.2707 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2468 | Steps: 2 | Val loss: 0.3009 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=34415)[0m rmse: 0.14250987768173218
[2m[36m(func pid=34415)[0m mae:  0.09721960127353668
[2m[36m(func pid=34415)[0m rmse_per_class: [0.087, 0.239, 0.026, 0.295, 0.054, 0.169, 0.225, 0.112, 0.135, 0.084]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8644 | Steps: 2 | Val loss: 0.6773 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6361 | Steps: 2 | Val loss: 0.4987 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=35262)[0m rmse: 0.1634807586669922
[2m[36m(func pid=35262)[0m mae:  0.09201054275035858
[2m[36m(func pid=35262)[0m rmse_per_class: [0.116, 0.239, 0.052, 0.283, 0.072, 0.164, 0.218, 0.149, 0.142, 0.199]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:33:42 (running for 00:09:03.77)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.306 |  0.143 |                   95 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.247 |  0.163 |                   96 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.864 |  0.18  |                   18 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.651 |  0.179 |                   17 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17961296439170837
[2m[36m(func pid=51709)[0m mae:  0.1318725049495697
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.1785065233707428
[2m[36m(func pid=51774)[0m mae:  0.13067811727523804
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.335, 0.103, 0.19, 0.286, 0.141, 0.143, 0.112]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.3054 | Steps: 2 | Val loss: 0.2695 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2749 | Steps: 2 | Val loss: 0.3012 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=34415)[0m rmse: 0.14238204061985016
[2m[36m(func pid=34415)[0m mae:  0.09718478471040726
[2m[36m(func pid=34415)[0m rmse_per_class: [0.088, 0.239, 0.026, 0.295, 0.054, 0.169, 0.224, 0.112, 0.135, 0.084]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8640 | Steps: 2 | Val loss: 0.6755 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6242 | Steps: 2 | Val loss: 0.4872 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=35262)[0m rmse: 0.1637122631072998
[2m[36m(func pid=35262)[0m mae:  0.09249772876501083
[2m[36m(func pid=35262)[0m rmse_per_class: [0.119, 0.242, 0.051, 0.286, 0.073, 0.163, 0.218, 0.141, 0.145, 0.199]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:33:48 (running for 00:09:09.08)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.305 |  0.142 |                   96 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.275 |  0.164 |                   97 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.864 |  0.18  |                   18 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.624 |  0.178 |                   19 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.1782868653535843
[2m[36m(func pid=51774)[0m mae:  0.13049142062664032
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.102, 0.19, 0.286, 0.141, 0.143, 0.112]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17957410216331482
[2m[36m(func pid=51709)[0m mae:  0.13183392584323883
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.3064 | Steps: 2 | Val loss: 0.2688 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2441 | Steps: 2 | Val loss: 0.3009 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=34415)[0m rmse: 0.14229731261730194
[2m[36m(func pid=34415)[0m mae:  0.09723327308893204
[2m[36m(func pid=34415)[0m rmse_per_class: [0.088, 0.239, 0.025, 0.295, 0.054, 0.168, 0.224, 0.111, 0.135, 0.084]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6111 | Steps: 2 | Val loss: 0.4765 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8574 | Steps: 2 | Val loss: 0.6734 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=35262)[0m rmse: 0.16278497874736786
[2m[36m(func pid=35262)[0m mae:  0.09221664071083069
[2m[36m(func pid=35262)[0m rmse_per_class: [0.116, 0.243, 0.047, 0.287, 0.073, 0.162, 0.218, 0.139, 0.148, 0.195]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:33:53 (running for 00:09:14.31)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.306 |  0.142 |                   97 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.244 |  0.163 |                   98 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.864 |  0.18  |                   19 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.611 |  0.178 |                   20 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.17807093262672424
[2m[36m(func pid=51774)[0m mae:  0.1302947700023651
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.102, 0.19, 0.285, 0.141, 0.143, 0.112]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17958132922649384
[2m[36m(func pid=51709)[0m mae:  0.131828173995018
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.3042 | Steps: 2 | Val loss: 0.2681 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2505 | Steps: 2 | Val loss: 0.3015 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=34415)[0m rmse: 0.14231163263320923
[2m[36m(func pid=34415)[0m mae:  0.09731929004192352
[2m[36m(func pid=34415)[0m rmse_per_class: [0.088, 0.239, 0.025, 0.294, 0.054, 0.167, 0.224, 0.111, 0.135, 0.085]
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5966 | Steps: 2 | Val loss: 0.4659 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8562 | Steps: 2 | Val loss: 0.6713 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=35262)[0m rmse: 0.16251376271247864
[2m[36m(func pid=35262)[0m mae:  0.09211400896310806
[2m[36m(func pid=35262)[0m rmse_per_class: [0.117, 0.245, 0.043, 0.289, 0.076, 0.16, 0.218, 0.135, 0.151, 0.19]
[2m[36m(func pid=35262)[0m 
== Status ==
Current time: 2024-01-07 09:33:58 (running for 00:09:19.60)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.304 |  0.142 |                   98 |
| train_32e5a_00003 | RUNNING    | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.25  |  0.163 |                   99 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.857 |  0.18  |                   20 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.597 |  0.178 |                   21 |
| train_32e5a_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.17788934707641602
[2m[36m(func pid=51774)[0m mae:  0.1301203817129135
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.333, 0.101, 0.191, 0.285, 0.14, 0.143, 0.111]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.3059 | Steps: 2 | Val loss: 0.2676 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=51709)[0m rmse: 0.17955133318901062
[2m[36m(func pid=51709)[0m mae:  0.1318071186542511
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=35262)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2524 | Steps: 2 | Val loss: 0.3002 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=34415)[0m rmse: 0.14215314388275146
[2m[36m(func pid=34415)[0m mae:  0.09720095247030258
[2m[36m(func pid=34415)[0m rmse_per_class: [0.088, 0.239, 0.025, 0.295, 0.054, 0.168, 0.223, 0.111, 0.136, 0.084]
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5864 | Steps: 2 | Val loss: 0.4565 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=34415)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8569 | Steps: 2 | Val loss: 0.6694 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=35262)[0m rmse: 0.16173869371414185
[2m[36m(func pid=35262)[0m mae:  0.09194450825452805
[2m[36m(func pid=35262)[0m rmse_per_class: [0.112, 0.247, 0.042, 0.291, 0.081, 0.16, 0.215, 0.127, 0.156, 0.186]
[2m[36m(func pid=51774)[0m rmse: 0.17770931124687195
[2m[36m(func pid=51774)[0m mae:  0.12996824085712433
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.261, 0.096, 0.333, 0.101, 0.191, 0.284, 0.14, 0.143, 0.111]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=34415)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.3079 | Steps: 2 | Val loss: 0.2668 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=51709)[0m rmse: 0.17956148087978363
[2m[36m(func pid=51709)[0m mae:  0.1318170726299286
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=34415)[0m rmse: 0.14199195802211761
[2m[36m(func pid=34415)[0m mae:  0.09703686833381653
[2m[36m(func pid=34415)[0m rmse_per_class: [0.088, 0.239, 0.025, 0.294, 0.054, 0.167, 0.222, 0.111, 0.135, 0.084]
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5768 | Steps: 2 | Val loss: 0.4473 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=51774)[0m rmse: 0.17752496898174286
[2m[36m(func pid=51774)[0m mae:  0.12980645895004272
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.101, 0.191, 0.284, 0.14, 0.143, 0.111]
== Status ==
Current time: 2024-01-07 09:34:03 (running for 00:09:24.85)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.306 |  0.142 |                   99 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.856 |  0.18  |                   21 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.586 |  0.178 |                   22 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57346)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=57346)[0m Configuration completed!
[2m[36m(func pid=57346)[0m New optimizer parameters:
[2m[36m(func pid=57346)[0m SGD (
[2m[36m(func pid=57346)[0m Parameter Group 0
[2m[36m(func pid=57346)[0m     dampening: 0
[2m[36m(func pid=57346)[0m     differentiable: False
[2m[36m(func pid=57346)[0m     foreach: None
[2m[36m(func pid=57346)[0m     lr: 0.01
[2m[36m(func pid=57346)[0m     maximize: False
[2m[36m(func pid=57346)[0m     momentum: 0.9
[2m[36m(func pid=57346)[0m     nesterov: False
[2m[36m(func pid=57346)[0m     weight_decay: 0
[2m[36m(func pid=57346)[0m )
[2m[36m(func pid=57346)[0m 
== Status ==
Current time: 2024-01-07 09:34:10 (running for 00:09:31.77)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00001 | RUNNING    | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.306 |  0.142 |                   99 |
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.857 |  0.18  |                   22 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.586 |  0.178 |                   22 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8505 | Steps: 2 | Val loss: 0.6679 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5635 | Steps: 2 | Val loss: 0.4392 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8898 | Steps: 2 | Val loss: 0.6865 | Batch size: 32 | lr: 0.01 | Duration: 4.85s
[2m[36m(func pid=51709)[0m rmse: 0.17948216199874878
[2m[36m(func pid=51709)[0m mae:  0.13173708319664001
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.1774059236049652
[2m[36m(func pid=51774)[0m mae:  0.12972292304039001
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.1, 0.191, 0.284, 0.14, 0.143, 0.111]
[2m[36m(func pid=57346)[0m rmse: 0.18195240199565887
[2m[36m(func pid=57346)[0m mae:  0.13380339741706848
[2m[36m(func pid=57346)[0m rmse_per_class: [0.114, 0.266, 0.11, 0.339, 0.109, 0.191, 0.292, 0.143, 0.143, 0.113]
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.8480 | Steps: 2 | Val loss: 0.6661 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 09:34:16 (running for 00:09:37.37)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.85  |  0.179 |                   23 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.577 |  0.178 |                   23 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |        |        |                      |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57869)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=57869)[0m Configuration completed!
[2m[36m(func pid=57869)[0m New optimizer parameters:
[2m[36m(func pid=57869)[0m SGD (
[2m[36m(func pid=57869)[0m Parameter Group 0
[2m[36m(func pid=57869)[0m     dampening: 0
[2m[36m(func pid=57869)[0m     differentiable: False
[2m[36m(func pid=57869)[0m     foreach: None
[2m[36m(func pid=57869)[0m     lr: 0.1
[2m[36m(func pid=57869)[0m     maximize: False
[2m[36m(func pid=57869)[0m     momentum: 0.9
[2m[36m(func pid=57869)[0m     nesterov: False
[2m[36m(func pid=57869)[0m     weight_decay: 0
[2m[36m(func pid=57869)[0m )
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m 
== Status ==
Current time: 2024-01-07 09:34:21 (running for 00:09:42.78)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.848 |  0.18  |                   24 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.563 |  0.177 |                   24 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.89  |  0.182 |                    1 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |        |        |                      |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17952045798301697
[2m[36m(func pid=51709)[0m mae:  0.1317683309316635
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8417 | Steps: 2 | Val loss: 0.6332 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5560 | Steps: 2 | Val loss: 0.4318 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8383 | Steps: 2 | Val loss: 0.5311 | Batch size: 32 | lr: 0.1 | Duration: 4.38s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.8452 | Steps: 2 | Val loss: 0.6645 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=57346)[0m rmse: 0.1807505488395691
[2m[36m(func pid=57346)[0m mae:  0.13260787725448608
[2m[36m(func pid=57346)[0m rmse_per_class: [0.112, 0.266, 0.11, 0.338, 0.108, 0.191, 0.286, 0.141, 0.143, 0.112]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17724093794822693
[2m[36m(func pid=51774)[0m mae:  0.12958046793937683
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.1, 0.191, 0.283, 0.14, 0.143, 0.111]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.1810348629951477
[2m[36m(func pid=57869)[0m mae:  0.13282522559165955
[2m[36m(func pid=57869)[0m rmse_per_class: [0.108, 0.266, 0.12, 0.338, 0.098, 0.191, 0.29, 0.144, 0.143, 0.111]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:34:27 (running for 00:09:48.15)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.845 |  0.18  |                   25 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.556 |  0.177 |                   25 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.842 |  0.181 |                    2 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.838 |  0.181 |                    1 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17953819036483765
[2m[36m(func pid=51709)[0m mae:  0.13178089261054993
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5482 | Steps: 2 | Val loss: 0.4242 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7591 | Steps: 2 | Val loss: 0.5635 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5334 | Steps: 2 | Val loss: 0.3511 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.8407 | Steps: 2 | Val loss: 0.6626 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=51774)[0m rmse: 0.1771245002746582
[2m[36m(func pid=51774)[0m mae:  0.12947514653205872
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.332, 0.099, 0.191, 0.283, 0.14, 0.143, 0.111]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.17992547154426575
[2m[36m(func pid=57346)[0m mae:  0.1317378580570221
[2m[36m(func pid=57346)[0m rmse_per_class: [0.112, 0.265, 0.109, 0.337, 0.107, 0.191, 0.283, 0.141, 0.143, 0.111]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.17755171656608582
[2m[36m(func pid=57869)[0m mae:  0.13000082969665527
[2m[36m(func pid=57869)[0m rmse_per_class: [0.114, 0.265, 0.108, 0.334, 0.09, 0.192, 0.279, 0.139, 0.146, 0.11]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:34:32 (running for 00:09:53.68)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.841 |  0.18  |                   26 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.548 |  0.177 |                   26 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.759 |  0.18  |                    3 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.533 |  0.178 |                    2 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.1795298159122467
[2m[36m(func pid=51709)[0m mae:  0.1317714899778366
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5400 | Steps: 2 | Val loss: 0.4172 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6744 | Steps: 2 | Val loss: 0.4939 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.3993 | Steps: 2 | Val loss: 0.3142 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.8414 | Steps: 2 | Val loss: 0.6605 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=51774)[0m rmse: 0.1769610196352005
[2m[36m(func pid=51774)[0m mae:  0.12932920455932617
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.332, 0.099, 0.191, 0.283, 0.139, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.17917154729366302
[2m[36m(func pid=57346)[0m mae:  0.13103532791137695
[2m[36m(func pid=57346)[0m rmse_per_class: [0.112, 0.265, 0.106, 0.335, 0.106, 0.191, 0.281, 0.141, 0.143, 0.111]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.17260268330574036
[2m[36m(func pid=57869)[0m mae:  0.12617959082126617
[2m[36m(func pid=57869)[0m rmse_per_class: [0.116, 0.264, 0.089, 0.327, 0.077, 0.194, 0.268, 0.133, 0.15, 0.109]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:34:37 (running for 00:09:58.94)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.841 |  0.179 |                   27 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.54  |  0.177 |                   27 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.674 |  0.179 |                    4 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.399 |  0.173 |                    3 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17946550250053406
[2m[36m(func pid=51709)[0m mae:  0.131715327501297
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5311 | Steps: 2 | Val loss: 0.4103 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5844 | Steps: 2 | Val loss: 0.4373 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4202 | Steps: 2 | Val loss: 0.3251 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.8387 | Steps: 2 | Val loss: 0.6586 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=51774)[0m rmse: 0.17685459554195404
[2m[36m(func pid=51774)[0m mae:  0.12924076616764069
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.331, 0.099, 0.191, 0.283, 0.139, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.17815153300762177
[2m[36m(func pid=57346)[0m mae:  0.13015936315059662
[2m[36m(func pid=57346)[0m rmse_per_class: [0.112, 0.264, 0.103, 0.334, 0.103, 0.19, 0.28, 0.141, 0.143, 0.111]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.16832785308361053
[2m[36m(func pid=57869)[0m mae:  0.1228305920958519
[2m[36m(func pid=57869)[0m rmse_per_class: [0.123, 0.258, 0.074, 0.32, 0.065, 0.194, 0.261, 0.129, 0.157, 0.104]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:34:43 (running for 00:10:04.21)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.839 |  0.179 |                   28 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.531 |  0.177 |                   28 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.584 |  0.178 |                    5 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.42  |  0.168 |                    4 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17948704957962036
[2m[36m(func pid=51709)[0m mae:  0.13173779845237732
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5228 | Steps: 2 | Val loss: 0.4049 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5175 | Steps: 2 | Val loss: 0.3937 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4539 | Steps: 2 | Val loss: 0.3333 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.8379 | Steps: 2 | Val loss: 0.6563 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=51774)[0m rmse: 0.17685070633888245
[2m[36m(func pid=51774)[0m mae:  0.1292431354522705
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.331, 0.098, 0.191, 0.283, 0.139, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.17721059918403625
[2m[36m(func pid=57346)[0m mae:  0.12937696278095245
[2m[36m(func pid=57346)[0m rmse_per_class: [0.114, 0.264, 0.1, 0.332, 0.099, 0.19, 0.279, 0.14, 0.143, 0.111]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.16203688085079193
[2m[36m(func pid=57869)[0m mae:  0.11741448938846588
[2m[36m(func pid=57869)[0m rmse_per_class: [0.122, 0.252, 0.056, 0.305, 0.058, 0.194, 0.251, 0.128, 0.159, 0.096]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:34:48 (running for 00:10:09.66)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.838 |  0.179 |                   29 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.523 |  0.177 |                   29 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.517 |  0.177 |                    6 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.454 |  0.162 |                    5 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17946025729179382
[2m[36m(func pid=51709)[0m mae:  0.1317133605480194
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5168 | Steps: 2 | Val loss: 0.3997 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4664 | Steps: 2 | Val loss: 0.3614 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4420 | Steps: 2 | Val loss: 0.3353 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.8329 | Steps: 2 | Val loss: 0.6534 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=51774)[0m rmse: 0.17677226662635803
[2m[36m(func pid=51774)[0m mae:  0.12918467819690704
[2m[36m(func pid=51774)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.331, 0.098, 0.191, 0.283, 0.139, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.1762164980173111
[2m[36m(func pid=57346)[0m mae:  0.1285720020532608
[2m[36m(func pid=57346)[0m rmse_per_class: [0.116, 0.264, 0.098, 0.33, 0.095, 0.19, 0.277, 0.139, 0.143, 0.111]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.15260480344295502
[2m[36m(func pid=57869)[0m mae:  0.10809379816055298
[2m[36m(func pid=57869)[0m rmse_per_class: [0.11, 0.249, 0.039, 0.283, 0.055, 0.197, 0.231, 0.123, 0.15, 0.089]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:34:53 (running for 00:10:15.03)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.833 |  0.179 |                   30 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.517 |  0.177 |                   30 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.466 |  0.176 |                    7 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.442 |  0.153 |                    6 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17940887808799744
[2m[36m(func pid=51709)[0m mae:  0.13166490197181702
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5087 | Steps: 2 | Val loss: 0.3938 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4017 | Steps: 2 | Val loss: 0.3242 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4296 | Steps: 2 | Val loss: 0.3382 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.8336 | Steps: 2 | Val loss: 0.6518 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=51774)[0m rmse: 0.17656639218330383
[2m[36m(func pid=51774)[0m mae:  0.12901079654693604
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.331, 0.098, 0.191, 0.283, 0.139, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14946140348911285
[2m[36m(func pid=57869)[0m mae:  0.10402730852365494
[2m[36m(func pid=57869)[0m rmse_per_class: [0.107, 0.243, 0.031, 0.282, 0.055, 0.197, 0.226, 0.122, 0.148, 0.084]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.1751229614019394
[2m[36m(func pid=57346)[0m mae:  0.12765489518642426
[2m[36m(func pid=57346)[0m rmse_per_class: [0.116, 0.263, 0.096, 0.328, 0.092, 0.19, 0.275, 0.138, 0.143, 0.111]
[2m[36m(func pid=57346)[0m 
== Status ==
Current time: 2024-01-07 09:34:59 (running for 00:10:20.39)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.834 |  0.179 |                   31 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.509 |  0.177 |                   31 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.43  |  0.175 |                    8 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.402 |  0.149 |                    7 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17936088144779205
[2m[36m(func pid=51709)[0m mae:  0.13162122666835785
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5023 | Steps: 2 | Val loss: 0.3884 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3573 | Steps: 2 | Val loss: 0.3004 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4091 | Steps: 2 | Val loss: 0.3224 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.8283 | Steps: 2 | Val loss: 0.6502 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=51774)[0m rmse: 0.17637403309345245
[2m[36m(func pid=51774)[0m mae:  0.1288471519947052
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.33, 0.098, 0.191, 0.282, 0.139, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.1475677341222763
[2m[36m(func pid=57869)[0m mae:  0.10058178752660751
[2m[36m(func pid=57869)[0m rmse_per_class: [0.106, 0.237, 0.029, 0.291, 0.055, 0.195, 0.223, 0.116, 0.142, 0.081]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.1737552285194397
[2m[36m(func pid=57346)[0m mae:  0.12649127840995789
[2m[36m(func pid=57346)[0m rmse_per_class: [0.117, 0.263, 0.093, 0.326, 0.088, 0.19, 0.272, 0.137, 0.142, 0.111]
[2m[36m(func pid=57346)[0m 
== Status ==
Current time: 2024-01-07 09:35:04 (running for 00:10:25.73)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.828 |  0.179 |                   32 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.502 |  0.176 |                   32 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.409 |  0.174 |                    9 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.357 |  0.148 |                    8 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17935028672218323
[2m[36m(func pid=51709)[0m mae:  0.1316068023443222
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4956 | Steps: 2 | Val loss: 0.3836 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3194 | Steps: 2 | Val loss: 0.2819 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3960 | Steps: 2 | Val loss: 0.3123 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8274 | Steps: 2 | Val loss: 0.6482 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=51774)[0m rmse: 0.17624957859516144
[2m[36m(func pid=51774)[0m mae:  0.12873958051204681
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.33, 0.097, 0.191, 0.282, 0.139, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14918091893196106
[2m[36m(func pid=57869)[0m mae:  0.10097891092300415
[2m[36m(func pid=57869)[0m rmse_per_class: [0.112, 0.237, 0.033, 0.304, 0.055, 0.191, 0.221, 0.115, 0.142, 0.083]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.17220549285411835
[2m[36m(func pid=57346)[0m mae:  0.12518946826457977
[2m[36m(func pid=57346)[0m rmse_per_class: [0.117, 0.262, 0.09, 0.323, 0.084, 0.19, 0.269, 0.136, 0.142, 0.11]
[2m[36m(func pid=57346)[0m 
== Status ==
Current time: 2024-01-07 09:35:10 (running for 00:10:31.35)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.827 |  0.179 |                   33 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.496 |  0.176 |                   33 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.396 |  0.172 |                   10 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.319 |  0.149 |                    9 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17931246757507324
[2m[36m(func pid=51709)[0m mae:  0.13157185912132263
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4900 | Steps: 2 | Val loss: 0.3786 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3159 | Steps: 2 | Val loss: 0.2772 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3890 | Steps: 2 | Val loss: 0.3064 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8248 | Steps: 2 | Val loss: 0.6468 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=57869)[0m rmse: 0.15071307122707367
[2m[36m(func pid=57869)[0m mae:  0.10142283141613007
[2m[36m(func pid=57869)[0m rmse_per_class: [0.109, 0.247, 0.036, 0.311, 0.055, 0.185, 0.215, 0.116, 0.144, 0.088]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.1760958880186081
[2m[36m(func pid=51774)[0m mae:  0.12859904766082764
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.33, 0.097, 0.19, 0.282, 0.138, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.17055444419384003
[2m[36m(func pid=57346)[0m mae:  0.12380518019199371
[2m[36m(func pid=57346)[0m rmse_per_class: [0.116, 0.26, 0.086, 0.322, 0.081, 0.189, 0.265, 0.135, 0.142, 0.109]
[2m[36m(func pid=57346)[0m 
== Status ==
Current time: 2024-01-07 09:35:15 (running for 00:10:36.87)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.825 |  0.179 |                   34 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.49  |  0.176 |                   34 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.389 |  0.171 |                   11 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.316 |  0.151 |                   10 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17931953072547913
[2m[36m(func pid=51709)[0m mae:  0.13156981766223907
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4852 | Steps: 2 | Val loss: 0.3748 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3017 | Steps: 2 | Val loss: 0.2789 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3876 | Steps: 2 | Val loss: 0.3030 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=51774)[0m rmse: 0.1760096251964569
[2m[36m(func pid=51774)[0m mae:  0.12854240834712982
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.329, 0.097, 0.19, 0.282, 0.138, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.15137581527233124
[2m[36m(func pid=57869)[0m mae:  0.1025429219007492
[2m[36m(func pid=57869)[0m rmse_per_class: [0.104, 0.251, 0.026, 0.313, 0.055, 0.176, 0.223, 0.12, 0.145, 0.101]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8256 | Steps: 2 | Val loss: 0.6455 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=57346)[0m rmse: 0.1686459630727768
[2m[36m(func pid=57346)[0m mae:  0.12220615148544312
[2m[36m(func pid=57346)[0m rmse_per_class: [0.116, 0.258, 0.083, 0.32, 0.077, 0.189, 0.26, 0.135, 0.142, 0.107]
[2m[36m(func pid=57346)[0m 
== Status ==
Current time: 2024-01-07 09:35:21 (running for 00:10:42.30)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.826 |  0.179 |                   35 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.485 |  0.176 |                   35 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.388 |  0.169 |                   12 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.302 |  0.151 |                   11 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.1793149709701538
[2m[36m(func pid=51709)[0m mae:  0.13157282769680023
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4807 | Steps: 2 | Val loss: 0.3714 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3071 | Steps: 2 | Val loss: 0.2786 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3863 | Steps: 2 | Val loss: 0.3015 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=51774)[0m rmse: 0.1759011298418045
[2m[36m(func pid=51774)[0m mae:  0.12846221029758453
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.329, 0.096, 0.19, 0.282, 0.138, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.8192 | Steps: 2 | Val loss: 0.6438 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=57869)[0m rmse: 0.1516956388950348
[2m[36m(func pid=57869)[0m mae:  0.10225814580917358
[2m[36m(func pid=57869)[0m rmse_per_class: [0.09, 0.256, 0.03, 0.3, 0.054, 0.168, 0.233, 0.122, 0.141, 0.122]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.16700269281864166
[2m[36m(func pid=57346)[0m mae:  0.12089111655950546
[2m[36m(func pid=57346)[0m rmse_per_class: [0.115, 0.257, 0.079, 0.319, 0.074, 0.188, 0.257, 0.134, 0.142, 0.106]
[2m[36m(func pid=57346)[0m 
== Status ==
Current time: 2024-01-07 09:35:26 (running for 00:10:47.57)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.819 |  0.179 |                   36 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.481 |  0.176 |                   36 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.386 |  0.167 |                   13 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.307 |  0.152 |                   12 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.1793213188648224
[2m[36m(func pid=51709)[0m mae:  0.13157987594604492
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.2982 | Steps: 2 | Val loss: 0.2785 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4768 | Steps: 2 | Val loss: 0.3680 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3844 | Steps: 2 | Val loss: 0.3004 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=51774)[0m rmse: 0.17563900351524353
[2m[36m(func pid=51774)[0m mae:  0.12824535369873047
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.329, 0.096, 0.19, 0.281, 0.138, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.1530713140964508
[2m[36m(func pid=57869)[0m mae:  0.10240521281957626
[2m[36m(func pid=57869)[0m rmse_per_class: [0.087, 0.249, 0.039, 0.292, 0.054, 0.161, 0.242, 0.12, 0.139, 0.148]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.8153 | Steps: 2 | Val loss: 0.6422 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=57346)[0m rmse: 0.16520586609840393
[2m[36m(func pid=57346)[0m mae:  0.11945591121912003
[2m[36m(func pid=57346)[0m rmse_per_class: [0.114, 0.255, 0.075, 0.317, 0.071, 0.187, 0.255, 0.133, 0.142, 0.104]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.1793479174375534
[2m[36m(func pid=51709)[0m mae:  0.13159438967704773
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 09:35:32 (running for 00:10:53.14)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.815 |  0.179 |                   37 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.477 |  0.176 |                   37 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.384 |  0.165 |                   14 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.298 |  0.153 |                   13 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.2999 | Steps: 2 | Val loss: 0.2702 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4703 | Steps: 2 | Val loss: 0.3643 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3822 | Steps: 2 | Val loss: 0.2999 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=57869)[0m rmse: 0.14696122705936432
[2m[36m(func pid=57869)[0m mae:  0.0965939536690712
[2m[36m(func pid=57869)[0m rmse_per_class: [0.085, 0.248, 0.041, 0.282, 0.053, 0.153, 0.23, 0.12, 0.134, 0.124]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17552760243415833
[2m[36m(func pid=51774)[0m mae:  0.1281604915857315
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.26, 0.094, 0.329, 0.095, 0.19, 0.281, 0.138, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.8164 | Steps: 2 | Val loss: 0.6408 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=57346)[0m rmse: 0.1638132631778717
[2m[36m(func pid=57346)[0m mae:  0.11836328357458115
[2m[36m(func pid=57346)[0m rmse_per_class: [0.113, 0.253, 0.072, 0.315, 0.069, 0.185, 0.254, 0.132, 0.141, 0.103]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.2749 | Steps: 2 | Val loss: 0.2654 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
== Status ==
Current time: 2024-01-07 09:35:37 (running for 00:10:58.44)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.816 |  0.179 |                   38 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.47  |  0.176 |                   38 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.382 |  0.164 |                   15 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.3   |  0.147 |                   14 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.1793532818555832
[2m[36m(func pid=51709)[0m mae:  0.13159868121147156
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4698 | Steps: 2 | Val loss: 0.3611 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=57869)[0m rmse: 0.14351332187652588
[2m[36m(func pid=57869)[0m mae:  0.09280077368021011
[2m[36m(func pid=57869)[0m rmse_per_class: [0.081, 0.259, 0.041, 0.271, 0.053, 0.147, 0.223, 0.117, 0.13, 0.114]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3831 | Steps: 2 | Val loss: 0.2999 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8120 | Steps: 2 | Val loss: 0.6393 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=51774)[0m rmse: 0.17541570961475372
[2m[36m(func pid=51774)[0m mae:  0.1280737817287445
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.329, 0.095, 0.19, 0.281, 0.138, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.16292017698287964
[2m[36m(func pid=57346)[0m mae:  0.11770057678222656
[2m[36m(func pid=57346)[0m rmse_per_class: [0.113, 0.252, 0.069, 0.314, 0.068, 0.184, 0.255, 0.132, 0.142, 0.101]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.2728 | Steps: 2 | Val loss: 0.2658 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 09:35:42 (running for 00:11:03.75)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.812 |  0.179 |                   39 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.47  |  0.175 |                   39 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.383 |  0.163 |                   16 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.275 |  0.144 |                   15 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17939019203186035
[2m[36m(func pid=51709)[0m mae:  0.1316300332546234
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4623 | Steps: 2 | Val loss: 0.3581 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=57869)[0m rmse: 0.14340798556804657
[2m[36m(func pid=57869)[0m mae:  0.09191323816776276
[2m[36m(func pid=57869)[0m rmse_per_class: [0.078, 0.269, 0.039, 0.273, 0.053, 0.147, 0.219, 0.116, 0.127, 0.113]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3737 | Steps: 2 | Val loss: 0.2994 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.8100 | Steps: 2 | Val loss: 0.6371 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=51774)[0m rmse: 0.17515066266059875
[2m[36m(func pid=51774)[0m mae:  0.1278633177280426
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.259, 0.093, 0.328, 0.094, 0.19, 0.281, 0.139, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.16179725527763367
[2m[36m(func pid=57346)[0m mae:  0.11676816642284393
[2m[36m(func pid=57346)[0m rmse_per_class: [0.112, 0.25, 0.066, 0.313, 0.066, 0.184, 0.255, 0.131, 0.141, 0.099]
== Status ==
Current time: 2024-01-07 09:35:47 (running for 00:11:08.85)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.812 |  0.179 |                   39 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.462 |  0.175 |                   40 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.374 |  0.162 |                   17 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.273 |  0.143 |                   16 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.2586 | Steps: 2 | Val loss: 0.2699 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=51709)[0m rmse: 0.1793959140777588
[2m[36m(func pid=51709)[0m mae:  0.1316346973180771
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4574 | Steps: 2 | Val loss: 0.3552 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=57869)[0m rmse: 0.14495566487312317
[2m[36m(func pid=57869)[0m mae:  0.09248539060354233
[2m[36m(func pid=57869)[0m rmse_per_class: [0.078, 0.279, 0.031, 0.276, 0.055, 0.157, 0.213, 0.118, 0.129, 0.114]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17500470578670502
[2m[36m(func pid=51774)[0m mae:  0.12776421010494232
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.259, 0.093, 0.328, 0.094, 0.19, 0.281, 0.138, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3726 | Steps: 2 | Val loss: 0.2985 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.8086 | Steps: 2 | Val loss: 0.6353 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 09:35:53 (running for 00:11:14.28)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.81  |  0.179 |                   40 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.457 |  0.175 |                   41 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.373 |  0.161 |                   18 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.259 |  0.145 |                   17 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m rmse: 0.16093021631240845
[2m[36m(func pid=57346)[0m mae:  0.11601191759109497
[2m[36m(func pid=57346)[0m rmse_per_class: [0.111, 0.249, 0.063, 0.31, 0.065, 0.183, 0.257, 0.131, 0.141, 0.098]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2550 | Steps: 2 | Val loss: 0.2708 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=51709)[0m rmse: 0.1794055998325348
[2m[36m(func pid=51709)[0m mae:  0.1316380351781845
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.109, 0.19, 0.293, 0.14, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4563 | Steps: 2 | Val loss: 0.3523 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=57869)[0m rmse: 0.14433695375919342
[2m[36m(func pid=57869)[0m mae:  0.09144333750009537
[2m[36m(func pid=57869)[0m rmse_per_class: [0.075, 0.277, 0.026, 0.277, 0.059, 0.161, 0.207, 0.112, 0.134, 0.113]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.1748708039522171
[2m[36m(func pid=51774)[0m mae:  0.1276635229587555
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.259, 0.092, 0.328, 0.094, 0.19, 0.281, 0.138, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3734 | Steps: 2 | Val loss: 0.2968 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.8067 | Steps: 2 | Val loss: 0.6332 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.2597 | Steps: 2 | Val loss: 0.2685 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 09:35:58 (running for 00:11:19.79)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.809 |  0.179 |                   41 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.456 |  0.175 |                   42 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.373 |  0.16  |                   19 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.255 |  0.144 |                   18 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m rmse: 0.15967464447021484
[2m[36m(func pid=57346)[0m mae:  0.11486922204494476
[2m[36m(func pid=57346)[0m rmse_per_class: [0.11, 0.248, 0.061, 0.308, 0.064, 0.183, 0.255, 0.13, 0.141, 0.097]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4527 | Steps: 2 | Val loss: 0.3496 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=51709)[0m rmse: 0.17938409745693207
[2m[36m(func pid=51709)[0m mae:  0.13162091374397278
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.109, 0.19, 0.293, 0.14, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14190524816513062
[2m[36m(func pid=57869)[0m mae:  0.08929229527711868
[2m[36m(func pid=57869)[0m rmse_per_class: [0.073, 0.266, 0.025, 0.28, 0.066, 0.156, 0.204, 0.107, 0.135, 0.107]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17462588846683502
[2m[36m(func pid=51774)[0m mae:  0.12743403017520905
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.259, 0.092, 0.327, 0.094, 0.19, 0.28, 0.138, 0.143, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3625 | Steps: 2 | Val loss: 0.2939 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8034 | Steps: 2 | Val loss: 0.6316 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.2519 | Steps: 2 | Val loss: 0.2681 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 09:36:04 (running for 00:11:25.24)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.807 |  0.179 |                   42 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.453 |  0.175 |                   43 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.362 |  0.158 |                   20 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.26  |  0.142 |                   19 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m rmse: 0.1581675410270691
[2m[36m(func pid=57346)[0m mae:  0.1135430559515953
[2m[36m(func pid=57346)[0m rmse_per_class: [0.108, 0.246, 0.059, 0.307, 0.063, 0.182, 0.249, 0.129, 0.14, 0.097]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4500 | Steps: 2 | Val loss: 0.3478 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=51709)[0m rmse: 0.17941464483737946
[2m[36m(func pid=51709)[0m mae:  0.13164213299751282
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.337, 0.109, 0.19, 0.293, 0.14, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14143848419189453
[2m[36m(func pid=57869)[0m mae:  0.08835951238870621
[2m[36m(func pid=57869)[0m rmse_per_class: [0.07, 0.263, 0.024, 0.282, 0.073, 0.154, 0.199, 0.108, 0.134, 0.107]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17454621195793152
[2m[36m(func pid=51774)[0m mae:  0.12737375497817993
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.259, 0.092, 0.327, 0.093, 0.189, 0.279, 0.138, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3621 | Steps: 2 | Val loss: 0.2907 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8023 | Steps: 2 | Val loss: 0.6299 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.2570 | Steps: 2 | Val loss: 0.2700 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 09:36:09 (running for 00:11:30.56)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.803 |  0.179 |                   43 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.45  |  0.175 |                   44 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.362 |  0.157 |                   21 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.252 |  0.141 |                   20 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m rmse: 0.15650425851345062
[2m[36m(func pid=57346)[0m mae:  0.11202211678028107
[2m[36m(func pid=57346)[0m rmse_per_class: [0.106, 0.245, 0.058, 0.304, 0.063, 0.181, 0.244, 0.127, 0.14, 0.097]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4452 | Steps: 2 | Val loss: 0.3455 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=51709)[0m rmse: 0.17939555644989014
[2m[36m(func pid=51709)[0m mae:  0.13161902129650116
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.109, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14300861954689026
[2m[36m(func pid=57869)[0m mae:  0.08856917172670364
[2m[36m(func pid=57869)[0m rmse_per_class: [0.067, 0.268, 0.024, 0.285, 0.081, 0.152, 0.197, 0.11, 0.134, 0.111]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17430853843688965
[2m[36m(func pid=51774)[0m mae:  0.12716856598854065
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.259, 0.091, 0.327, 0.092, 0.189, 0.279, 0.138, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3600 | Steps: 2 | Val loss: 0.2880 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.8004 | Steps: 2 | Val loss: 0.6278 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.2484 | Steps: 2 | Val loss: 0.2753 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:36:14 (running for 00:11:35.84)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.802 |  0.179 |                   44 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.445 |  0.174 |                   45 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.36  |  0.155 |                   22 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.257 |  0.143 |                   21 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m rmse: 0.1551523506641388
[2m[36m(func pid=57346)[0m mae:  0.11082705110311508
[2m[36m(func pid=57346)[0m rmse_per_class: [0.105, 0.245, 0.056, 0.303, 0.062, 0.18, 0.24, 0.125, 0.139, 0.097]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4433 | Steps: 2 | Val loss: 0.3439 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=51709)[0m rmse: 0.17936758697032928
[2m[36m(func pid=51709)[0m mae:  0.13158875703811646
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.109, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14642539620399475
[2m[36m(func pid=57869)[0m mae:  0.09039462357759476
[2m[36m(func pid=57869)[0m rmse_per_class: [0.069, 0.279, 0.026, 0.294, 0.087, 0.149, 0.2, 0.111, 0.134, 0.116]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17424169182777405
[2m[36m(func pid=51774)[0m mae:  0.12712815403938293
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.259, 0.091, 0.327, 0.091, 0.189, 0.278, 0.138, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3528 | Steps: 2 | Val loss: 0.2858 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.7990 | Steps: 2 | Val loss: 0.6255 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2513 | Steps: 2 | Val loss: 0.2780 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 09:36:20 (running for 00:11:41.50)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.8   |  0.179 |                   45 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.443 |  0.174 |                   46 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.353 |  0.154 |                   23 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.248 |  0.146 |                   22 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4397 | Steps: 2 | Val loss: 0.3417 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=57346)[0m rmse: 0.15417930483818054
[2m[36m(func pid=57346)[0m mae:  0.10994984954595566
[2m[36m(func pid=57346)[0m rmse_per_class: [0.104, 0.244, 0.055, 0.302, 0.061, 0.18, 0.237, 0.124, 0.139, 0.097]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17932943999767303
[2m[36m(func pid=51709)[0m mae:  0.1315603256225586
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14851486682891846
[2m[36m(func pid=57869)[0m mae:  0.09109043329954147
[2m[36m(func pid=57869)[0m rmse_per_class: [0.07, 0.29, 0.027, 0.295, 0.09, 0.147, 0.202, 0.112, 0.133, 0.119]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17400680482387543
[2m[36m(func pid=51774)[0m mae:  0.12692217528820038
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.259, 0.091, 0.326, 0.091, 0.189, 0.278, 0.138, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3543 | Steps: 2 | Val loss: 0.2836 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.7955 | Steps: 2 | Val loss: 0.6234 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2533 | Steps: 2 | Val loss: 0.2769 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4393 | Steps: 2 | Val loss: 0.3392 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 09:36:25 (running for 00:11:47.01)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.799 |  0.179 |                   46 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.44  |  0.174 |                   47 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.354 |  0.153 |                   24 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.251 |  0.149 |                   23 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m rmse: 0.15326018631458282
[2m[36m(func pid=57346)[0m mae:  0.10904137790203094
[2m[36m(func pid=57346)[0m rmse_per_class: [0.103, 0.243, 0.053, 0.301, 0.061, 0.18, 0.234, 0.123, 0.138, 0.097]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17930036783218384
[2m[36m(func pid=51709)[0m mae:  0.13153909146785736
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.1491916924715042
[2m[36m(func pid=57869)[0m mae:  0.09174396097660065
[2m[36m(func pid=57869)[0m rmse_per_class: [0.073, 0.289, 0.025, 0.291, 0.09, 0.15, 0.205, 0.116, 0.136, 0.117]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.1737273633480072
[2m[36m(func pid=51774)[0m mae:  0.12667304277420044
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.259, 0.09, 0.326, 0.091, 0.189, 0.276, 0.138, 0.142, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3451 | Steps: 2 | Val loss: 0.2817 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7941 | Steps: 2 | Val loss: 0.6216 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2490 | Steps: 2 | Val loss: 0.2747 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4349 | Steps: 2 | Val loss: 0.3378 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 09:36:31 (running for 00:11:52.48)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.796 |  0.179 |                   47 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.439 |  0.174 |                   48 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.345 |  0.153 |                   25 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.253 |  0.149 |                   24 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m rmse: 0.15250900387763977
[2m[36m(func pid=57346)[0m mae:  0.10843577235937119
[2m[36m(func pid=57346)[0m rmse_per_class: [0.102, 0.243, 0.052, 0.299, 0.061, 0.179, 0.234, 0.121, 0.138, 0.096]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17931391298770905
[2m[36m(func pid=51709)[0m mae:  0.13154371082782745
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14832362532615662
[2m[36m(func pid=57869)[0m mae:  0.09205229580402374
[2m[36m(func pid=57869)[0m rmse_per_class: [0.072, 0.283, 0.024, 0.289, 0.093, 0.15, 0.21, 0.115, 0.133, 0.115]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17370212078094482
[2m[36m(func pid=51774)[0m mae:  0.12666282057762146
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.259, 0.09, 0.326, 0.09, 0.189, 0.277, 0.138, 0.142, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3411 | Steps: 2 | Val loss: 0.2805 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7907 | Steps: 2 | Val loss: 0.6203 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2422 | Steps: 2 | Val loss: 0.2736 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4305 | Steps: 2 | Val loss: 0.3360 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 09:36:36 (running for 00:11:57.99)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.794 |  0.179 |                   48 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.435 |  0.174 |                   49 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.341 |  0.152 |                   26 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.249 |  0.148 |                   25 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m rmse: 0.15205100178718567
[2m[36m(func pid=57346)[0m mae:  0.10807903110980988
[2m[36m(func pid=57346)[0m rmse_per_class: [0.101, 0.243, 0.051, 0.299, 0.06, 0.178, 0.235, 0.121, 0.138, 0.096]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17931658029556274
[2m[36m(func pid=51709)[0m mae:  0.13154329359531403
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.1480000764131546
[2m[36m(func pid=57869)[0m mae:  0.0923178493976593
[2m[36m(func pid=57869)[0m rmse_per_class: [0.072, 0.278, 0.024, 0.283, 0.097, 0.152, 0.215, 0.115, 0.129, 0.116]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17361903190612793
[2m[36m(func pid=51774)[0m mae:  0.12659905850887299
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.259, 0.09, 0.325, 0.09, 0.189, 0.277, 0.138, 0.142, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7899 | Steps: 2 | Val loss: 0.6182 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3422 | Steps: 2 | Val loss: 0.2799 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2483 | Steps: 2 | Val loss: 0.2684 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4324 | Steps: 2 | Val loss: 0.3342 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 09:36:42 (running for 00:12:03.39)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.79  |  0.179 |                   50 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.431 |  0.174 |                   50 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.341 |  0.152 |                   26 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.242 |  0.148 |                   26 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.1792851984500885
[2m[36m(func pid=51709)[0m mae:  0.13152170181274414
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.1518615335226059
[2m[36m(func pid=57346)[0m mae:  0.10802619159221649
[2m[36m(func pid=57346)[0m rmse_per_class: [0.1, 0.243, 0.05, 0.298, 0.06, 0.176, 0.237, 0.12, 0.138, 0.095]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.1452566683292389
[2m[36m(func pid=57869)[0m mae:  0.08965799957513809
[2m[36m(func pid=57869)[0m rmse_per_class: [0.069, 0.262, 0.023, 0.269, 0.094, 0.153, 0.214, 0.117, 0.127, 0.123]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17351871728897095
[2m[36m(func pid=51774)[0m mae:  0.12651368975639343
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.259, 0.09, 0.325, 0.09, 0.189, 0.277, 0.138, 0.142, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2406 | Steps: 2 | Val loss: 0.2663 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7885 | Steps: 2 | Val loss: 0.6168 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3377 | Steps: 2 | Val loss: 0.2791 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4311 | Steps: 2 | Val loss: 0.3335 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=57869)[0m rmse: 0.14373165369033813
[2m[36m(func pid=57869)[0m mae:  0.08791861683130264
[2m[36m(func pid=57869)[0m rmse_per_class: [0.069, 0.253, 0.024, 0.266, 0.089, 0.152, 0.21, 0.119, 0.128, 0.128]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:36:47 (running for 00:12:08.68)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.79  |  0.179 |                   50 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.432 |  0.174 |                   51 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.342 |  0.152 |                   27 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.241 |  0.144 |                   28 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m rmse: 0.1515771746635437
[2m[36m(func pid=57346)[0m mae:  0.10780060291290283
[2m[36m(func pid=57346)[0m rmse_per_class: [0.1, 0.243, 0.049, 0.297, 0.06, 0.176, 0.239, 0.119, 0.138, 0.095]
[2m[36m(func pid=51709)[0m rmse: 0.17931053042411804
[2m[36m(func pid=51709)[0m mae:  0.13153690099716187
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17354397475719452
[2m[36m(func pid=51774)[0m mae:  0.1265847533941269
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.259, 0.09, 0.325, 0.09, 0.188, 0.277, 0.138, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2384 | Steps: 2 | Val loss: 0.2659 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7865 | Steps: 2 | Val loss: 0.6147 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3305 | Steps: 2 | Val loss: 0.2787 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4252 | Steps: 2 | Val loss: 0.3315 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=57869)[0m rmse: 0.14299710094928741
[2m[36m(func pid=57869)[0m mae:  0.0869862362742424
[2m[36m(func pid=57869)[0m rmse_per_class: [0.075, 0.248, 0.025, 0.268, 0.081, 0.147, 0.204, 0.119, 0.129, 0.133]
== Status ==
Current time: 2024-01-07 09:36:52 (running for 00:12:13.86)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.789 |  0.179 |                   51 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.431 |  0.174 |                   52 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.338 |  0.152 |                   28 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.143 |                   29 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17930665612220764
[2m[36m(func pid=51709)[0m mae:  0.13154006004333496
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.1513763815164566
[2m[36m(func pid=57346)[0m mae:  0.10767356306314468
[2m[36m(func pid=57346)[0m rmse_per_class: [0.099, 0.243, 0.048, 0.297, 0.06, 0.175, 0.24, 0.119, 0.138, 0.095]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17324818670749664
[2m[36m(func pid=51774)[0m mae:  0.12633632123470306
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.259, 0.089, 0.324, 0.09, 0.188, 0.277, 0.138, 0.143, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2460 | Steps: 2 | Val loss: 0.2667 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.7828 | Steps: 2 | Val loss: 0.6138 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3333 | Steps: 2 | Val loss: 0.2778 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4259 | Steps: 2 | Val loss: 0.3301 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:36:58 (running for 00:12:19.20)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.786 |  0.179 |                   52 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.425 |  0.173 |                   53 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.331 |  0.151 |                   29 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.246 |  0.143 |                   30 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.143093541264534
[2m[36m(func pid=57869)[0m mae:  0.08679099380970001
[2m[36m(func pid=57869)[0m rmse_per_class: [0.083, 0.246, 0.026, 0.273, 0.074, 0.146, 0.201, 0.118, 0.131, 0.134]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.15101966261863708
[2m[36m(func pid=57346)[0m mae:  0.10747851431369781
[2m[36m(func pid=57346)[0m rmse_per_class: [0.099, 0.243, 0.048, 0.295, 0.059, 0.173, 0.241, 0.118, 0.138, 0.095]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.179255872964859
[2m[36m(func pid=51709)[0m mae:  0.13148468732833862
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17303325235843658
[2m[36m(func pid=51774)[0m mae:  0.126155823469162
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.258, 0.089, 0.324, 0.09, 0.188, 0.276, 0.138, 0.142, 0.11]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2380 | Steps: 2 | Val loss: 0.2701 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.7834 | Steps: 2 | Val loss: 0.6122 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3266 | Steps: 2 | Val loss: 0.2765 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4206 | Steps: 2 | Val loss: 0.3285 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=57869)[0m rmse: 0.14462468028068542
[2m[36m(func pid=57869)[0m mae:  0.0882134661078453
[2m[36m(func pid=57869)[0m rmse_per_class: [0.086, 0.255, 0.025, 0.286, 0.075, 0.146, 0.199, 0.116, 0.134, 0.123]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:37:04 (running for 00:12:25.18)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.783 |  0.179 |                   53 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.426 |  0.173 |                   54 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.327 |  0.15  |                   31 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.145 |                   31 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17929627001285553
[2m[36m(func pid=51709)[0m mae:  0.1315118819475174
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.1503288447856903
[2m[36m(func pid=57346)[0m mae:  0.1069551557302475
[2m[36m(func pid=57346)[0m rmse_per_class: [0.099, 0.243, 0.046, 0.294, 0.059, 0.172, 0.24, 0.118, 0.138, 0.095]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17290619015693665
[2m[36m(func pid=51774)[0m mae:  0.1260680854320526
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.258, 0.088, 0.324, 0.089, 0.188, 0.276, 0.138, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2467 | Steps: 2 | Val loss: 0.2764 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.7788 | Steps: 2 | Val loss: 0.6106 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3343 | Steps: 2 | Val loss: 0.2756 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4207 | Steps: 2 | Val loss: 0.3273 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=57869)[0m rmse: 0.14755317568778992
[2m[36m(func pid=57869)[0m mae:  0.09080006927251816
[2m[36m(func pid=57869)[0m rmse_per_class: [0.086, 0.266, 0.024, 0.303, 0.082, 0.145, 0.2, 0.114, 0.134, 0.12]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:37:09 (running for 00:12:30.52)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.779 |  0.179 |                   55 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.421 |  0.173 |                   55 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.327 |  0.15  |                   31 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.247 |  0.148 |                   32 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17928890883922577
[2m[36m(func pid=51709)[0m mae:  0.13151754438877106
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14981083571910858
[2m[36m(func pid=57346)[0m mae:  0.1065531000494957
[2m[36m(func pid=57346)[0m rmse_per_class: [0.099, 0.242, 0.045, 0.293, 0.059, 0.171, 0.24, 0.117, 0.137, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17276343703269958
[2m[36m(func pid=51774)[0m mae:  0.1259876936674118
[2m[36m(func pid=51774)[0m rmse_per_class: [0.116, 0.258, 0.088, 0.324, 0.089, 0.188, 0.276, 0.138, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2357 | Steps: 2 | Val loss: 0.2795 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.7753 | Steps: 2 | Val loss: 0.6092 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4206 | Steps: 2 | Val loss: 0.3257 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3262 | Steps: 2 | Val loss: 0.2747 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=57869)[0m rmse: 0.14945872128009796
[2m[36m(func pid=57869)[0m mae:  0.09221461415290833
[2m[36m(func pid=57869)[0m rmse_per_class: [0.087, 0.274, 0.025, 0.308, 0.084, 0.147, 0.202, 0.112, 0.135, 0.121]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:37:14 (running for 00:12:35.85)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.775 |  0.179 |                   56 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.421 |  0.173 |                   56 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.334 |  0.15  |                   32 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.236 |  0.149 |                   33 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.1792393922805786
[2m[36m(func pid=51709)[0m mae:  0.13148006796836853
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17251381278038025
[2m[36m(func pid=51774)[0m mae:  0.12577074766159058
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.258, 0.087, 0.324, 0.088, 0.188, 0.275, 0.138, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.1492740660905838
[2m[36m(func pid=57346)[0m mae:  0.1060839295387268
[2m[36m(func pid=57346)[0m rmse_per_class: [0.099, 0.242, 0.044, 0.292, 0.059, 0.17, 0.239, 0.116, 0.137, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2384 | Steps: 2 | Val loss: 0.2772 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7729 | Steps: 2 | Val loss: 0.6074 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3257 | Steps: 2 | Val loss: 0.2741 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4184 | Steps: 2 | Val loss: 0.3250 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=57869)[0m rmse: 0.1488088071346283
[2m[36m(func pid=57869)[0m mae:  0.09148658812046051
[2m[36m(func pid=57869)[0m rmse_per_class: [0.082, 0.275, 0.025, 0.297, 0.08, 0.148, 0.203, 0.115, 0.14, 0.123]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:37:20 (running for 00:12:41.23)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.773 |  0.179 |                   57 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.421 |  0.173 |                   57 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.326 |  0.149 |                   33 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.149 |                   34 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17922347784042358
[2m[36m(func pid=51709)[0m mae:  0.13145682215690613
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.1724444329738617
[2m[36m(func pid=51774)[0m mae:  0.12572887539863586
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.258, 0.087, 0.323, 0.088, 0.188, 0.275, 0.138, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14878730475902557
[2m[36m(func pid=57346)[0m mae:  0.10567746311426163
[2m[36m(func pid=57346)[0m rmse_per_class: [0.098, 0.242, 0.043, 0.291, 0.059, 0.169, 0.238, 0.116, 0.137, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2426 | Steps: 2 | Val loss: 0.2741 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7718 | Steps: 2 | Val loss: 0.6051 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4144 | Steps: 2 | Val loss: 0.3241 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3209 | Steps: 2 | Val loss: 0.2735 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=57869)[0m rmse: 0.1475638449192047
[2m[36m(func pid=57869)[0m mae:  0.08993645757436752
[2m[36m(func pid=57869)[0m rmse_per_class: [0.081, 0.264, 0.025, 0.282, 0.075, 0.148, 0.203, 0.118, 0.151, 0.13]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:37:25 (running for 00:12:46.46)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.772 |  0.179 |                   58 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.418 |  0.172 |                   58 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.326 |  0.149 |                   34 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.243 |  0.148 |                   35 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17916926741600037
[2m[36m(func pid=51709)[0m mae:  0.13140057027339935
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.108]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17236347496509552
[2m[36m(func pid=51774)[0m mae:  0.12565599381923676
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.258, 0.087, 0.323, 0.088, 0.188, 0.275, 0.138, 0.143, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14840173721313477
[2m[36m(func pid=57346)[0m mae:  0.10534273087978363
[2m[36m(func pid=57346)[0m rmse_per_class: [0.098, 0.242, 0.043, 0.291, 0.059, 0.168, 0.238, 0.115, 0.137, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2350 | Steps: 2 | Val loss: 0.2750 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7713 | Steps: 2 | Val loss: 0.6045 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4154 | Steps: 2 | Val loss: 0.3228 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3196 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=57869)[0m rmse: 0.1481458991765976
[2m[36m(func pid=57869)[0m mae:  0.09000538289546967
[2m[36m(func pid=57869)[0m rmse_per_class: [0.082, 0.262, 0.025, 0.279, 0.073, 0.148, 0.204, 0.119, 0.155, 0.133]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:37:30 (running for 00:12:51.93)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.771 |  0.179 |                   59 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.414 |  0.172 |                   59 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.321 |  0.148 |                   35 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.235 |  0.148 |                   36 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17921185493469238
[2m[36m(func pid=51709)[0m mae:  0.13142792880535126
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.142, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17208793759346008
[2m[36m(func pid=51774)[0m mae:  0.12542417645454407
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.257, 0.086, 0.323, 0.088, 0.188, 0.275, 0.138, 0.143, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14801231026649475
[2m[36m(func pid=57346)[0m mae:  0.10495362430810928
[2m[36m(func pid=57346)[0m rmse_per_class: [0.097, 0.242, 0.042, 0.29, 0.06, 0.167, 0.238, 0.114, 0.137, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2437 | Steps: 2 | Val loss: 0.2763 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7693 | Steps: 2 | Val loss: 0.6025 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4153 | Steps: 2 | Val loss: 0.3218 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3175 | Steps: 2 | Val loss: 0.2722 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=57869)[0m rmse: 0.14907731115818024
[2m[36m(func pid=57869)[0m mae:  0.09009838104248047
[2m[36m(func pid=57869)[0m rmse_per_class: [0.082, 0.268, 0.025, 0.28, 0.073, 0.149, 0.205, 0.12, 0.155, 0.132]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:37:36 (running for 00:12:57.42)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.771 |  0.179 |                   59 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.415 |  0.172 |                   61 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.32  |  0.148 |                   36 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.244 |  0.149 |                   37 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.1792355477809906
[2m[36m(func pid=51709)[0m mae:  0.13143253326416016
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17204402387142181
[2m[36m(func pid=51774)[0m mae:  0.12540870904922485
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.257, 0.086, 0.323, 0.088, 0.188, 0.275, 0.138, 0.143, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14759914577007294
[2m[36m(func pid=57346)[0m mae:  0.10444936901330948
[2m[36m(func pid=57346)[0m rmse_per_class: [0.096, 0.242, 0.041, 0.29, 0.06, 0.166, 0.236, 0.113, 0.136, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2371 | Steps: 2 | Val loss: 0.2753 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.7660 | Steps: 2 | Val loss: 0.6008 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4109 | Steps: 2 | Val loss: 0.3208 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=57869)[0m rmse: 0.14902515709400177
[2m[36m(func pid=57869)[0m mae:  0.09008282423019409
[2m[36m(func pid=57869)[0m rmse_per_class: [0.083, 0.272, 0.025, 0.284, 0.074, 0.151, 0.203, 0.124, 0.145, 0.13]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3189 | Steps: 2 | Val loss: 0.2717 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 09:37:41 (running for 00:13:02.84)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.766 |  0.179 |                   61 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.415 |  0.172 |                   61 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.317 |  0.148 |                   37 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.237 |  0.149 |                   38 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17921584844589233
[2m[36m(func pid=51709)[0m mae:  0.13140736520290375
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m rmse: 0.17190317809581757
[2m[36m(func pid=51774)[0m mae:  0.12528930604457855
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.257, 0.086, 0.323, 0.088, 0.188, 0.275, 0.138, 0.143, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.1472564935684204
[2m[36m(func pid=57346)[0m mae:  0.1040298119187355
[2m[36m(func pid=57346)[0m rmse_per_class: [0.096, 0.242, 0.041, 0.29, 0.06, 0.167, 0.234, 0.112, 0.136, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2409 | Steps: 2 | Val loss: 0.2758 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4086 | Steps: 2 | Val loss: 0.3198 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7645 | Steps: 2 | Val loss: 0.5997 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=57869)[0m rmse: 0.14918065071105957
[2m[36m(func pid=57869)[0m mae:  0.09019958972930908
[2m[36m(func pid=57869)[0m rmse_per_class: [0.082, 0.276, 0.024, 0.291, 0.072, 0.152, 0.201, 0.129, 0.135, 0.13]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3159 | Steps: 2 | Val loss: 0.2717 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 09:37:47 (running for 00:13:08.29)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.766 |  0.179 |                   61 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.409 |  0.172 |                   63 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.319 |  0.147 |                   38 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.241 |  0.149 |                   39 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.1717633306980133
[2m[36m(func pid=51774)[0m mae:  0.12516504526138306
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.257, 0.086, 0.323, 0.087, 0.187, 0.275, 0.137, 0.142, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.179310142993927
[2m[36m(func pid=51709)[0m mae:  0.13146989047527313
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14724674820899963
[2m[36m(func pid=57346)[0m mae:  0.10401828587055206
[2m[36m(func pid=57346)[0m rmse_per_class: [0.096, 0.242, 0.041, 0.289, 0.061, 0.166, 0.235, 0.112, 0.136, 0.095]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2433 | Steps: 2 | Val loss: 0.2773 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4098 | Steps: 2 | Val loss: 0.3188 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=57869)[0m rmse: 0.149850994348526
[2m[36m(func pid=57869)[0m mae:  0.09079261869192123
[2m[36m(func pid=57869)[0m rmse_per_class: [0.085, 0.277, 0.024, 0.297, 0.074, 0.15, 0.201, 0.127, 0.134, 0.129]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.7659 | Steps: 2 | Val loss: 0.5977 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3113 | Steps: 2 | Val loss: 0.2716 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 09:37:52 (running for 00:13:13.69)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.765 |  0.179 |                   62 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.41  |  0.172 |                   64 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.316 |  0.147 |                   39 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.243 |  0.15  |                   40 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.1715335100889206
[2m[36m(func pid=51774)[0m mae:  0.12495468556880951
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.257, 0.085, 0.323, 0.087, 0.187, 0.274, 0.137, 0.142, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14712154865264893
[2m[36m(func pid=57346)[0m mae:  0.1038905531167984
[2m[36m(func pid=57346)[0m rmse_per_class: [0.096, 0.242, 0.04, 0.289, 0.061, 0.165, 0.236, 0.112, 0.136, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2369 | Steps: 2 | Val loss: 0.2759 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=51709)[0m rmse: 0.1793001890182495
[2m[36m(func pid=51709)[0m mae:  0.13146530091762543
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4040 | Steps: 2 | Val loss: 0.3179 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=57869)[0m rmse: 0.14865900576114655
[2m[36m(func pid=57869)[0m mae:  0.0899367555975914
[2m[36m(func pid=57869)[0m rmse_per_class: [0.084, 0.277, 0.024, 0.295, 0.071, 0.15, 0.2, 0.126, 0.134, 0.124]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.7611 | Steps: 2 | Val loss: 0.5961 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3088 | Steps: 2 | Val loss: 0.2718 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 09:37:58 (running for 00:13:19.10)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.766 |  0.179 |                   63 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.404 |  0.171 |                   65 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.311 |  0.147 |                   40 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.237 |  0.149 |                   41 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.1713983565568924
[2m[36m(func pid=51774)[0m mae:  0.12483260780572891
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.257, 0.085, 0.322, 0.087, 0.187, 0.273, 0.137, 0.142, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14714758098125458
[2m[36m(func pid=57346)[0m mae:  0.10395820438861847
[2m[36m(func pid=57346)[0m rmse_per_class: [0.096, 0.242, 0.04, 0.289, 0.062, 0.165, 0.237, 0.111, 0.136, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17927968502044678
[2m[36m(func pid=51709)[0m mae:  0.13145479559898376
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2383 | Steps: 2 | Val loss: 0.2725 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=57869)[0m rmse: 0.14650413393974304
[2m[36m(func pid=57869)[0m mae:  0.08895488828420639
[2m[36m(func pid=57869)[0m rmse_per_class: [0.086, 0.267, 0.024, 0.291, 0.071, 0.148, 0.2, 0.124, 0.133, 0.123]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4067 | Steps: 2 | Val loss: 0.3176 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7574 | Steps: 2 | Val loss: 0.5942 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3053 | Steps: 2 | Val loss: 0.2716 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 09:38:03 (running for 00:13:24.63)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.761 |  0.179 |                   64 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.407 |  0.171 |                   66 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.309 |  0.147 |                   41 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.147 |                   42 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.1711946576833725
[2m[36m(func pid=51774)[0m mae:  0.12468228489160538
[2m[36m(func pid=51774)[0m rmse_per_class: [0.115, 0.256, 0.084, 0.322, 0.087, 0.187, 0.273, 0.137, 0.142, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2359 | Steps: 2 | Val loss: 0.2694 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=51709)[0m rmse: 0.17926537990570068
[2m[36m(func pid=51709)[0m mae:  0.13143755495548248
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14693093299865723
[2m[36m(func pid=57346)[0m mae:  0.10384345054626465
[2m[36m(func pid=57346)[0m rmse_per_class: [0.095, 0.241, 0.039, 0.288, 0.062, 0.164, 0.238, 0.111, 0.136, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14369818568229675
[2m[36m(func pid=57869)[0m mae:  0.08764893561601639
[2m[36m(func pid=57869)[0m rmse_per_class: [0.083, 0.263, 0.024, 0.288, 0.07, 0.146, 0.199, 0.115, 0.133, 0.116]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4043 | Steps: 2 | Val loss: 0.3166 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7568 | Steps: 2 | Val loss: 0.5924 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3064 | Steps: 2 | Val loss: 0.2715 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 09:38:08 (running for 00:13:30.00)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.757 |  0.179 |                   65 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.404 |  0.171 |                   67 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.305 |  0.147 |                   42 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.236 |  0.144 |                   43 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.1709844321012497
[2m[36m(func pid=51774)[0m mae:  0.12449238449335098
[2m[36m(func pid=51774)[0m rmse_per_class: [0.114, 0.256, 0.084, 0.322, 0.086, 0.187, 0.272, 0.137, 0.142, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2363 | Steps: 2 | Val loss: 0.2669 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=51709)[0m rmse: 0.17921853065490723
[2m[36m(func pid=51709)[0m mae:  0.13138645887374878
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14669354259967804
[2m[36m(func pid=57346)[0m mae:  0.10358335077762604
[2m[36m(func pid=57346)[0m rmse_per_class: [0.094, 0.241, 0.039, 0.288, 0.062, 0.163, 0.238, 0.111, 0.136, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14142771065235138
[2m[36m(func pid=57869)[0m mae:  0.08632493764162064
[2m[36m(func pid=57869)[0m rmse_per_class: [0.074, 0.263, 0.023, 0.284, 0.068, 0.145, 0.198, 0.112, 0.133, 0.113]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4010 | Steps: 2 | Val loss: 0.3157 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.7536 | Steps: 2 | Val loss: 0.5901 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3057 | Steps: 2 | Val loss: 0.2714 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2338 | Steps: 2 | Val loss: 0.2670 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 09:38:14 (running for 00:13:35.44)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.757 |  0.179 |                   66 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.401 |  0.171 |                   68 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.306 |  0.147 |                   43 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.236 |  0.141 |                   44 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.17088547348976135
[2m[36m(func pid=51774)[0m mae:  0.12439246475696564
[2m[36m(func pid=51774)[0m rmse_per_class: [0.114, 0.256, 0.084, 0.322, 0.086, 0.187, 0.272, 0.137, 0.142, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17923018336296082
[2m[36m(func pid=51709)[0m mae:  0.13140085339546204
[2m[36m(func pid=51709)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14661550521850586
[2m[36m(func pid=57346)[0m mae:  0.10349353402853012
[2m[36m(func pid=57346)[0m rmse_per_class: [0.093, 0.242, 0.038, 0.287, 0.063, 0.163, 0.238, 0.111, 0.136, 0.095]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.1412528157234192
[2m[36m(func pid=57869)[0m mae:  0.0863427072763443
[2m[36m(func pid=57869)[0m rmse_per_class: [0.07, 0.265, 0.023, 0.284, 0.067, 0.147, 0.199, 0.113, 0.134, 0.109]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4052 | Steps: 2 | Val loss: 0.3152 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7525 | Steps: 2 | Val loss: 0.5886 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3035 | Steps: 2 | Val loss: 0.2711 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 09:38:19 (running for 00:13:40.68)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.754 |  0.179 |                   67 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.405 |  0.171 |                   69 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.306 |  0.147 |                   44 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.234 |  0.141 |                   45 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.17065317928791046
[2m[36m(func pid=51774)[0m mae:  0.12418518215417862
[2m[36m(func pid=51774)[0m rmse_per_class: [0.114, 0.256, 0.084, 0.321, 0.086, 0.187, 0.271, 0.137, 0.142, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2343 | Steps: 2 | Val loss: 0.2672 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=51709)[0m rmse: 0.17921222746372223
[2m[36m(func pid=51709)[0m mae:  0.13138236105442047
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14631280303001404
[2m[36m(func pid=57346)[0m mae:  0.10311967134475708
[2m[36m(func pid=57346)[0m rmse_per_class: [0.093, 0.242, 0.038, 0.288, 0.063, 0.163, 0.237, 0.11, 0.136, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14120252430438995
[2m[36m(func pid=57869)[0m mae:  0.08613735437393188
[2m[36m(func pid=57869)[0m rmse_per_class: [0.068, 0.263, 0.023, 0.282, 0.068, 0.147, 0.199, 0.114, 0.136, 0.112]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4011 | Steps: 2 | Val loss: 0.3148 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7522 | Steps: 2 | Val loss: 0.5875 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3001 | Steps: 2 | Val loss: 0.2707 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2380 | Steps: 2 | Val loss: 0.2673 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 09:38:24 (running for 00:13:45.93)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.753 |  0.179 |                   68 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.401 |  0.171 |                   70 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.303 |  0.146 |                   45 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.234 |  0.141 |                   46 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.17063859105110168
[2m[36m(func pid=51774)[0m mae:  0.12416648864746094
[2m[36m(func pid=51774)[0m rmse_per_class: [0.114, 0.256, 0.084, 0.321, 0.085, 0.187, 0.271, 0.137, 0.142, 0.109]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17920055985450745
[2m[36m(func pid=51709)[0m mae:  0.1313595026731491
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14599768817424774
[2m[36m(func pid=57346)[0m mae:  0.10271928459405899
[2m[36m(func pid=57346)[0m rmse_per_class: [0.092, 0.242, 0.038, 0.288, 0.064, 0.163, 0.235, 0.109, 0.136, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14086264371871948
[2m[36m(func pid=57869)[0m mae:  0.08527201414108276
[2m[36m(func pid=57869)[0m rmse_per_class: [0.066, 0.26, 0.023, 0.279, 0.069, 0.146, 0.197, 0.114, 0.137, 0.117]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3969 | Steps: 2 | Val loss: 0.3138 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.7510 | Steps: 2 | Val loss: 0.5856 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3010 | Steps: 2 | Val loss: 0.2705 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2355 | Steps: 2 | Val loss: 0.2683 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 09:38:30 (running for 00:13:51.38)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.752 |  0.179 |                   69 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.397 |  0.17  |                   71 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.3   |  0.146 |                   46 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                   47 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.17048978805541992
[2m[36m(func pid=51774)[0m mae:  0.12404264509677887
[2m[36m(func pid=51774)[0m rmse_per_class: [0.114, 0.256, 0.084, 0.321, 0.085, 0.187, 0.271, 0.137, 0.142, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17919602990150452
[2m[36m(func pid=51709)[0m mae:  0.13135786354541779
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14580164849758148
[2m[36m(func pid=57346)[0m mae:  0.10237671434879303
[2m[36m(func pid=57346)[0m rmse_per_class: [0.092, 0.243, 0.037, 0.288, 0.064, 0.163, 0.233, 0.109, 0.135, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14117088913917542
[2m[36m(func pid=57869)[0m mae:  0.0851781964302063
[2m[36m(func pid=57869)[0m rmse_per_class: [0.067, 0.262, 0.023, 0.281, 0.069, 0.145, 0.196, 0.115, 0.136, 0.118]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4001 | Steps: 2 | Val loss: 0.3130 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.7472 | Steps: 2 | Val loss: 0.5840 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2977 | Steps: 2 | Val loss: 0.2701 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2397 | Steps: 2 | Val loss: 0.2709 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 09:38:35 (running for 00:13:56.81)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.751 |  0.179 |                   70 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.4   |  0.17  |                   72 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.301 |  0.146 |                   47 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.236 |  0.141 |                   48 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.17028959095478058
[2m[36m(func pid=51774)[0m mae:  0.12389764934778214
[2m[36m(func pid=51774)[0m rmse_per_class: [0.114, 0.256, 0.083, 0.321, 0.085, 0.187, 0.27, 0.137, 0.142, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17915037274360657
[2m[36m(func pid=51709)[0m mae:  0.13132384419441223
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14552423357963562
[2m[36m(func pid=57346)[0m mae:  0.10187951475381851
[2m[36m(func pid=57346)[0m rmse_per_class: [0.091, 0.244, 0.037, 0.288, 0.064, 0.162, 0.231, 0.108, 0.134, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.1429683268070221
[2m[36m(func pid=57869)[0m mae:  0.08619391173124313
[2m[36m(func pid=57869)[0m rmse_per_class: [0.069, 0.265, 0.023, 0.285, 0.07, 0.144, 0.197, 0.118, 0.134, 0.124]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4005 | Steps: 2 | Val loss: 0.3125 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.7462 | Steps: 2 | Val loss: 0.5832 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2909 | Steps: 2 | Val loss: 0.2695 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2389 | Steps: 2 | Val loss: 0.2743 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 09:38:40 (running for 00:14:02.00)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.747 |  0.179 |                   71 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.401 |  0.17  |                   73 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.298 |  0.146 |                   48 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.24  |  0.143 |                   49 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.1702093482017517
[2m[36m(func pid=51774)[0m mae:  0.12385530769824982
[2m[36m(func pid=51774)[0m rmse_per_class: [0.114, 0.256, 0.083, 0.321, 0.085, 0.187, 0.27, 0.137, 0.142, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.17918449640274048
[2m[36m(func pid=51709)[0m mae:  0.13134363293647766
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14506784081459045
[2m[36m(func pid=57346)[0m mae:  0.10133882611989975
[2m[36m(func pid=57346)[0m rmse_per_class: [0.092, 0.244, 0.037, 0.287, 0.065, 0.162, 0.229, 0.108, 0.134, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14577049016952515
[2m[36m(func pid=57869)[0m mae:  0.08776883035898209
[2m[36m(func pid=57869)[0m rmse_per_class: [0.076, 0.261, 0.025, 0.286, 0.071, 0.146, 0.199, 0.123, 0.135, 0.136]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3964 | Steps: 2 | Val loss: 0.3116 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.7431 | Steps: 2 | Val loss: 0.5814 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2924 | Steps: 2 | Val loss: 0.2693 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2391 | Steps: 2 | Val loss: 0.2786 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 09:38:46 (running for 00:14:07.23)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: -0.1640000008046627
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.746 |  0.179 |                   72 |
| train_32e5a_00005 | RUNNING    | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.396 |  0.17  |                   74 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.291 |  0.145 |                   49 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.239 |  0.146 |                   50 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51774)[0m rmse: 0.1700846254825592
[2m[36m(func pid=51774)[0m mae:  0.12376047670841217
[2m[36m(func pid=51774)[0m rmse_per_class: [0.114, 0.256, 0.083, 0.32, 0.085, 0.186, 0.27, 0.137, 0.142, 0.108]
[2m[36m(func pid=51774)[0m 
[2m[36m(func pid=51709)[0m rmse: 0.1791171282529831
[2m[36m(func pid=51709)[0m mae:  0.13128767907619476
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.1448785960674286
[2m[36m(func pid=57346)[0m mae:  0.10105601698160172
[2m[36m(func pid=57346)[0m rmse_per_class: [0.092, 0.244, 0.037, 0.286, 0.065, 0.161, 0.228, 0.108, 0.134, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14865413308143616
[2m[36m(func pid=57869)[0m mae:  0.08948618173599243
[2m[36m(func pid=57869)[0m rmse_per_class: [0.08, 0.262, 0.026, 0.291, 0.071, 0.147, 0.202, 0.129, 0.135, 0.143]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51774)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3976 | Steps: 2 | Val loss: 0.3115 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7443 | Steps: 2 | Val loss: 0.5796 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3045 | Steps: 2 | Val loss: 0.2684 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2310 | Steps: 2 | Val loss: 0.2824 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=51774)[0m rmse: 0.17011207342147827
[2m[36m(func pid=51774)[0m mae:  0.12380743026733398
[2m[36m(func pid=51774)[0m rmse_per_class: [0.114, 0.256, 0.083, 0.321, 0.084, 0.186, 0.271, 0.136, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 09:38:51 (running for 00:14:12.45)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 3 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.743 |  0.179 |                   73 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.292 |  0.145 |                   50 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.239 |  0.149 |                   51 |
| train_32e5a_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=51709)[0m rmse: 0.17918606102466583
[2m[36m(func pid=51709)[0m mae:  0.13134361803531647
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=51709)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14425714313983917
[2m[36m(func pid=57346)[0m mae:  0.10040830075740814
[2m[36m(func pid=57346)[0m rmse_per_class: [0.091, 0.244, 0.036, 0.284, 0.065, 0.161, 0.227, 0.108, 0.133, 0.094]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.1510053426027298
[2m[36m(func pid=57869)[0m mae:  0.0911720022559166
[2m[36m(func pid=57869)[0m rmse_per_class: [0.085, 0.259, 0.027, 0.295, 0.073, 0.149, 0.203, 0.132, 0.136, 0.15]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=51709)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.7371 | Steps: 2 | Val loss: 0.5782 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2341 | Steps: 2 | Val loss: 0.2830 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2897 | Steps: 2 | Val loss: 0.2686 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=51709)[0m rmse: 0.17916128039360046
[2m[36m(func pid=51709)[0m mae:  0.13132211565971375
[2m[36m(func pid=51709)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=57869)[0m rmse: 0.15131142735481262
[2m[36m(func pid=57869)[0m mae:  0.09137025475502014
[2m[36m(func pid=57869)[0m rmse_per_class: [0.086, 0.258, 0.027, 0.296, 0.074, 0.15, 0.202, 0.131, 0.138, 0.15]
[2m[36m(func pid=57346)[0m rmse: 0.1442454606294632
[2m[36m(func pid=57346)[0m mae:  0.1003594622015953
[2m[36m(func pid=57346)[0m rmse_per_class: [0.09, 0.244, 0.036, 0.283, 0.065, 0.16, 0.228, 0.108, 0.133, 0.095]
== Status ==
Current time: 2024-01-07 09:38:58 (running for 00:14:19.29)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00004 | RUNNING    | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.744 |  0.179 |                   74 |
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.305 |  0.144 |                   51 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.231 |  0.151 |                   52 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=69319)[0m Dataloader to compute accuracy: val

[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69319)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=69319)[0m Configuration completed!
[2m[36m(func pid=69319)[0m New optimizer parameters:
[2m[36m(func pid=69319)[0m SGD (
[2m[36m(func pid=69319)[0m Parameter Group 0
[2m[36m(func pid=69319)[0m     dampening: 0
[2m[36m(func pid=69319)[0m     differentiable: False
[2m[36m(func pid=69319)[0m     foreach: None
[2m[36m(func pid=69319)[0m     lr: 0.0001
[2m[36m(func pid=69319)[0m     maximize: False
[2m[36m(func pid=69319)[0m     momentum: 0.99
[2m[36m(func pid=69319)[0m     nesterov: False
[2m[36m(func pid=69319)[0m     weight_decay: 0.0001
[2m[36m(func pid=69319)[0m )
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2903 | Steps: 2 | Val loss: 0.2688 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2378 | Steps: 2 | Val loss: 0.2800 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8962 | Steps: 2 | Val loss: 0.7094 | Batch size: 32 | lr: 0.0001 | Duration: 4.88s
[2m[36m(func pid=57346)[0m rmse: 0.1442352682352066
[2m[36m(func pid=57346)[0m mae:  0.10033339262008667
[2m[36m(func pid=57346)[0m rmse_per_class: [0.089, 0.244, 0.035, 0.283, 0.065, 0.16, 0.229, 0.108, 0.133, 0.095]
[2m[36m(func pid=57869)[0m rmse: 0.14928501844406128
[2m[36m(func pid=57869)[0m mae:  0.09012262523174286
[2m[36m(func pid=57869)[0m rmse_per_class: [0.088, 0.26, 0.026, 0.294, 0.072, 0.149, 0.198, 0.124, 0.138, 0.142]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.18273505568504333
[2m[36m(func pid=69319)[0m mae:  0.13446635007858276
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2346 | Steps: 2 | Val loss: 0.2767 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 09:39:06 (running for 00:14:27.13)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.29  |  0.144 |                   52 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.149 |                   54 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69905)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=69905)[0m Configuration completed!
[2m[36m(func pid=69905)[0m New optimizer parameters:
[2m[36m(func pid=69905)[0m SGD (
[2m[36m(func pid=69905)[0m Parameter Group 0
[2m[36m(func pid=69905)[0m     dampening: 0
[2m[36m(func pid=69905)[0m     differentiable: False
[2m[36m(func pid=69905)[0m     foreach: None
[2m[36m(func pid=69905)[0m     lr: 0.001
[2m[36m(func pid=69905)[0m     maximize: False
[2m[36m(func pid=69905)[0m     momentum: 0.99
[2m[36m(func pid=69905)[0m     nesterov: False
[2m[36m(func pid=69905)[0m     weight_decay: 0.0001
[2m[36m(func pid=69905)[0m )
[2m[36m(func pid=69905)[0m 
== Status ==
Current time: 2024-01-07 09:39:11 (running for 00:14:32.66)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.29  |  0.144 |                   53 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.235 |  0.147 |                   55 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.896 |  0.183 |                    1 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14717619121074677
[2m[36m(func pid=57869)[0m mae:  0.08909548819065094
[2m[36m(func pid=57869)[0m rmse_per_class: [0.09, 0.26, 0.025, 0.295, 0.069, 0.146, 0.196, 0.119, 0.141, 0.131]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8962 | Steps: 2 | Val loss: 0.7055 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2978 | Steps: 2 | Val loss: 0.2694 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8965 | Steps: 2 | Val loss: 0.7081 | Batch size: 32 | lr: 0.001 | Duration: 4.72s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2321 | Steps: 2 | Val loss: 0.2751 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=69319)[0m rmse: 0.1824944168329239
[2m[36m(func pid=69319)[0m mae:  0.13432781398296356
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.113]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.1446441113948822
[2m[36m(func pid=57346)[0m mae:  0.10075680166482925
[2m[36m(func pid=57346)[0m rmse_per_class: [0.088, 0.244, 0.035, 0.282, 0.065, 0.161, 0.233, 0.109, 0.134, 0.096]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.18271858990192413
[2m[36m(func pid=69905)[0m mae:  0.13445551693439484
[2m[36m(func pid=69905)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.144, 0.144, 0.113]
[2m[36m(func pid=69905)[0m 
== Status ==
Current time: 2024-01-07 09:39:16 (running for 00:14:38.01)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.298 |  0.145 |                   54 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.232 |  0.146 |                   56 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.896 |  0.182 |                    2 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.897 |  0.183 |                    1 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14599579572677612
[2m[36m(func pid=57869)[0m mae:  0.08888544887304306
[2m[36m(func pid=57869)[0m rmse_per_class: [0.092, 0.26, 0.024, 0.299, 0.069, 0.144, 0.195, 0.115, 0.14, 0.122]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8962 | Steps: 2 | Val loss: 0.7021 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2912 | Steps: 2 | Val loss: 0.2697 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8927 | Steps: 2 | Val loss: 0.6996 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2332 | Steps: 2 | Val loss: 0.2724 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=69319)[0m rmse: 0.18214230239391327
[2m[36m(func pid=69319)[0m mae:  0.13405944406986237
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.1447552889585495
[2m[36m(func pid=57346)[0m mae:  0.10080288350582123
[2m[36m(func pid=57346)[0m rmse_per_class: [0.088, 0.244, 0.035, 0.281, 0.065, 0.161, 0.234, 0.109, 0.134, 0.096]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.18238317966461182
[2m[36m(func pid=69905)[0m mae:  0.13424360752105713
[2m[36m(func pid=69905)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.143, 0.144, 0.113]
[2m[36m(func pid=69905)[0m 
== Status ==
Current time: 2024-01-07 09:39:22 (running for 00:14:43.29)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.291 |  0.145 |                   55 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.233 |  0.144 |                   57 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.896 |  0.182 |                    3 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.893 |  0.182 |                    2 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.1440127044916153
[2m[36m(func pid=57869)[0m mae:  0.08789382874965668
[2m[36m(func pid=57869)[0m rmse_per_class: [0.091, 0.256, 0.024, 0.297, 0.066, 0.143, 0.193, 0.113, 0.14, 0.116]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2904 | Steps: 2 | Val loss: 0.2693 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8939 | Steps: 2 | Val loss: 0.6988 | Batch size: 32 | lr: 0.0001 | Duration: 3.26s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8793 | Steps: 2 | Val loss: 0.6882 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2358 | Steps: 2 | Val loss: 0.2694 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=57346)[0m rmse: 0.14453817903995514
[2m[36m(func pid=57346)[0m mae:  0.10048677772283554
[2m[36m(func pid=57346)[0m rmse_per_class: [0.089, 0.244, 0.035, 0.281, 0.065, 0.16, 0.233, 0.109, 0.134, 0.095]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.18176522850990295
[2m[36m(func pid=69319)[0m mae:  0.13376258313655853
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.338, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.18193629384040833
[2m[36m(func pid=69905)[0m mae:  0.13388404250144958
[2m[36m(func pid=69905)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.339, 0.112, 0.19, 0.293, 0.142, 0.144, 0.112]
[2m[36m(func pid=69905)[0m 
== Status ==
Current time: 2024-01-07 09:39:27 (running for 00:14:48.53)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.29  |  0.145 |                   56 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.236 |  0.142 |                   58 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.894 |  0.182 |                    4 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.879 |  0.182 |                    3 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14212869107723236
[2m[36m(func pid=57869)[0m mae:  0.08628598600625992
[2m[36m(func pid=57869)[0m rmse_per_class: [0.085, 0.252, 0.024, 0.288, 0.065, 0.143, 0.192, 0.113, 0.141, 0.119]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2864 | Steps: 2 | Val loss: 0.2688 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8931 | Steps: 2 | Val loss: 0.6964 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8646 | Steps: 2 | Val loss: 0.6743 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2275 | Steps: 2 | Val loss: 0.2681 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=57346)[0m rmse: 0.14413347840309143
[2m[36m(func pid=57346)[0m mae:  0.09986910223960876
[2m[36m(func pid=57346)[0m rmse_per_class: [0.088, 0.245, 0.035, 0.282, 0.065, 0.16, 0.23, 0.109, 0.133, 0.095]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.18142573535442352
[2m[36m(func pid=69319)[0m mae:  0.13347111642360687
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.112, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.1814960539340973
[2m[36m(func pid=69905)[0m mae:  0.13351315259933472
[2m[36m(func pid=69905)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.338, 0.111, 0.19, 0.292, 0.142, 0.143, 0.112]
[2m[36m(func pid=69905)[0m 
== Status ==
Current time: 2024-01-07 09:39:32 (running for 00:14:53.85)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.286 |  0.144 |                   57 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.228 |  0.141 |                   59 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.893 |  0.181 |                    5 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.865 |  0.181 |                    4 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.1412857621908188
[2m[36m(func pid=57869)[0m mae:  0.08548088371753693
[2m[36m(func pid=57869)[0m rmse_per_class: [0.079, 0.25, 0.024, 0.283, 0.066, 0.143, 0.192, 0.113, 0.143, 0.12]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2866 | Steps: 2 | Val loss: 0.2683 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8892 | Steps: 2 | Val loss: 0.6946 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8443 | Steps: 2 | Val loss: 0.6577 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2358 | Steps: 2 | Val loss: 0.2681 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=57346)[0m rmse: 0.14380434155464172
[2m[36m(func pid=57346)[0m mae:  0.09940173476934433
[2m[36m(func pid=57346)[0m rmse_per_class: [0.088, 0.246, 0.034, 0.282, 0.064, 0.16, 0.228, 0.108, 0.133, 0.095]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.18113581836223602
[2m[36m(func pid=69319)[0m mae:  0.13320298492908478
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.112, 0.19, 0.293, 0.142, 0.142, 0.111]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.18106156587600708
[2m[36m(func pid=69905)[0m mae:  0.13312393426895142
[2m[36m(func pid=69905)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.11, 0.19, 0.291, 0.141, 0.143, 0.112]
[2m[36m(func pid=69905)[0m 
== Status ==
Current time: 2024-01-07 09:39:38 (running for 00:14:59.10)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.287 |  0.144 |                   58 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.236 |  0.142 |                   60 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.889 |  0.181 |                    6 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.844 |  0.181 |                    5 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.1415177881717682
[2m[36m(func pid=57869)[0m mae:  0.08506147563457489
[2m[36m(func pid=57869)[0m rmse_per_class: [0.076, 0.251, 0.024, 0.278, 0.065, 0.144, 0.194, 0.115, 0.147, 0.122]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2836 | Steps: 2 | Val loss: 0.2673 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8871 | Steps: 2 | Val loss: 0.6920 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8225 | Steps: 2 | Val loss: 0.6378 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2297 | Steps: 2 | Val loss: 0.2672 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=57346)[0m rmse: 0.1431235820055008
[2m[36m(func pid=57346)[0m mae:  0.09846962988376617
[2m[36m(func pid=57346)[0m rmse_per_class: [0.087, 0.247, 0.034, 0.282, 0.064, 0.159, 0.224, 0.107, 0.132, 0.095]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.18082696199417114
[2m[36m(func pid=69319)[0m mae:  0.13293185830116272
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.264, 0.103, 0.338, 0.112, 0.19, 0.293, 0.141, 0.142, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.18063853681087494
[2m[36m(func pid=69905)[0m mae:  0.13272875547409058
[2m[36m(func pid=69905)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.108, 0.19, 0.29, 0.141, 0.143, 0.112]
[2m[36m(func pid=69905)[0m 
== Status ==
Current time: 2024-01-07 09:39:43 (running for 00:15:04.40)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.284 |  0.143 |                   59 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.23  |  0.141 |                   61 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.887 |  0.181 |                    7 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.822 |  0.181 |                    6 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.1411566287279129
[2m[36m(func pid=57869)[0m mae:  0.08442655950784683
[2m[36m(func pid=57869)[0m rmse_per_class: [0.074, 0.248, 0.025, 0.271, 0.065, 0.145, 0.195, 0.117, 0.15, 0.122]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2930 | Steps: 2 | Val loss: 0.2673 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8818 | Steps: 2 | Val loss: 0.6886 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7949 | Steps: 2 | Val loss: 0.6157 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2316 | Steps: 2 | Val loss: 0.2669 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=57346)[0m rmse: 0.14309489727020264
[2m[36m(func pid=57346)[0m mae:  0.09824620932340622
[2m[36m(func pid=57346)[0m rmse_per_class: [0.086, 0.248, 0.033, 0.283, 0.064, 0.16, 0.222, 0.107, 0.131, 0.097]
[2m[36m(func pid=57346)[0m 
== Status ==
Current time: 2024-01-07 09:39:48 (running for 00:15:09.42)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.293 |  0.143 |                   60 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.23  |  0.141 |                   61 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.887 |  0.181 |                    7 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.795 |  0.18  |                    7 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14098873734474182
[2m[36m(func pid=57869)[0m mae:  0.08420976251363754
[2m[36m(func pid=57869)[0m rmse_per_class: [0.072, 0.249, 0.024, 0.27, 0.065, 0.144, 0.195, 0.117, 0.148, 0.123]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.18021909892559052
[2m[36m(func pid=69905)[0m mae:  0.1323232352733612
[2m[36m(func pid=69905)[0m rmse_per_class: [0.116, 0.264, 0.103, 0.337, 0.107, 0.19, 0.289, 0.141, 0.143, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.18053202331066132
[2m[36m(func pid=69319)[0m mae:  0.13267864286899567
[2m[36m(func pid=69319)[0m rmse_per_class: [0.115, 0.264, 0.102, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2851 | Steps: 2 | Val loss: 0.2674 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2334 | Steps: 2 | Val loss: 0.2660 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7663 | Steps: 2 | Val loss: 0.5919 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8784 | Steps: 2 | Val loss: 0.6859 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=57346)[0m rmse: 0.1431066244840622
[2m[36m(func pid=57346)[0m mae:  0.0980762392282486
[2m[36m(func pid=57346)[0m rmse_per_class: [0.086, 0.249, 0.033, 0.284, 0.064, 0.159, 0.222, 0.106, 0.131, 0.097]
[2m[36m(func pid=57346)[0m 
== Status ==
Current time: 2024-01-07 09:39:53 (running for 00:15:14.82)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.285 |  0.143 |                   61 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.233 |  0.14  |                   63 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.882 |  0.181 |                    8 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.795 |  0.18  |                    7 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14029401540756226
[2m[36m(func pid=57869)[0m mae:  0.0838245153427124
[2m[36m(func pid=57869)[0m rmse_per_class: [0.072, 0.248, 0.024, 0.269, 0.064, 0.144, 0.194, 0.117, 0.146, 0.125]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17983512580394745
[2m[36m(func pid=69905)[0m mae:  0.1319432109594345
[2m[36m(func pid=69905)[0m rmse_per_class: [0.117, 0.263, 0.102, 0.337, 0.106, 0.19, 0.288, 0.141, 0.143, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.18028509616851807
[2m[36m(func pid=69319)[0m mae:  0.13246920704841614
[2m[36m(func pid=69319)[0m rmse_per_class: [0.115, 0.263, 0.102, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2884 | Steps: 2 | Val loss: 0.2674 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2373 | Steps: 2 | Val loss: 0.2654 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7351 | Steps: 2 | Val loss: 0.5662 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8757 | Steps: 2 | Val loss: 0.6834 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 09:39:58 (running for 00:15:20.01)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.285 |  0.143 |                   61 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.233 |  0.14  |                   63 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.878 |  0.18  |                    9 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.766 |  0.18  |                    8 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m rmse: 0.14310023188591003
[2m[36m(func pid=57346)[0m mae:  0.09809866547584534
[2m[36m(func pid=57346)[0m rmse_per_class: [0.084, 0.249, 0.032, 0.284, 0.064, 0.158, 0.224, 0.106, 0.131, 0.098]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.1397310495376587
[2m[36m(func pid=57869)[0m mae:  0.08376947790384293
[2m[36m(func pid=57869)[0m rmse_per_class: [0.072, 0.249, 0.024, 0.268, 0.065, 0.144, 0.193, 0.114, 0.144, 0.123]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17947165668010712
[2m[36m(func pid=69905)[0m mae:  0.13157406449317932
[2m[36m(func pid=69905)[0m rmse_per_class: [0.117, 0.263, 0.102, 0.336, 0.105, 0.189, 0.287, 0.141, 0.143, 0.112]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.18010814487934113
[2m[36m(func pid=69319)[0m mae:  0.13230709731578827
[2m[36m(func pid=69319)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2323 | Steps: 2 | Val loss: 0.2661 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2964 | Steps: 2 | Val loss: 0.2680 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7020 | Steps: 2 | Val loss: 0.5404 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8688 | Steps: 2 | Val loss: 0.6803 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 09:40:04 (running for 00:15:25.35)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.288 |  0.143 |                   62 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.232 |  0.14  |                   65 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.876 |  0.18  |                   10 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.735 |  0.179 |                    9 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.1401192992925644
[2m[36m(func pid=57869)[0m mae:  0.08439479768276215
[2m[36m(func pid=57869)[0m rmse_per_class: [0.075, 0.251, 0.024, 0.27, 0.067, 0.144, 0.193, 0.113, 0.141, 0.123]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14353735744953156
[2m[36m(func pid=57346)[0m mae:  0.09857085347175598
[2m[36m(func pid=57346)[0m rmse_per_class: [0.084, 0.249, 0.033, 0.282, 0.064, 0.158, 0.227, 0.106, 0.132, 0.1]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17897354066371918
[2m[36m(func pid=69905)[0m mae:  0.13108739256858826
[2m[36m(func pid=69905)[0m rmse_per_class: [0.117, 0.262, 0.1, 0.336, 0.104, 0.189, 0.286, 0.141, 0.143, 0.112]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.1799888163805008
[2m[36m(func pid=69319)[0m mae:  0.13220085203647614
[2m[36m(func pid=69319)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2362 | Steps: 2 | Val loss: 0.2677 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6705 | Steps: 2 | Val loss: 0.5141 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2811 | Steps: 2 | Val loss: 0.2684 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8654 | Steps: 2 | Val loss: 0.6763 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 09:40:09 (running for 00:15:30.55)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.296 |  0.144 |                   63 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.236 |  0.141 |                   66 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.869 |  0.18  |                   11 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.702 |  0.179 |                   10 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14092609286308289
[2m[36m(func pid=57869)[0m mae:  0.08501742780208588
[2m[36m(func pid=57869)[0m rmse_per_class: [0.077, 0.254, 0.024, 0.272, 0.065, 0.145, 0.193, 0.113, 0.139, 0.127]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.1786581575870514
[2m[36m(func pid=69905)[0m mae:  0.13078294694423676
[2m[36m(func pid=69905)[0m rmse_per_class: [0.117, 0.262, 0.099, 0.335, 0.103, 0.19, 0.285, 0.141, 0.143, 0.112]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14359793066978455
[2m[36m(func pid=57346)[0m mae:  0.09870058298110962
[2m[36m(func pid=57346)[0m rmse_per_class: [0.084, 0.248, 0.033, 0.283, 0.064, 0.156, 0.229, 0.107, 0.132, 0.099]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17985263466835022
[2m[36m(func pid=69319)[0m mae:  0.13207773864269257
[2m[36m(func pid=69319)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.11, 0.19, 0.292, 0.141, 0.142, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2379 | Steps: 2 | Val loss: 0.2706 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.6336 | Steps: 2 | Val loss: 0.4881 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2787 | Steps: 2 | Val loss: 0.2682 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8592 | Steps: 2 | Val loss: 0.6722 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 09:40:14 (running for 00:15:36.02)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.281 |  0.144 |                   64 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.143 |                   67 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.865 |  0.18  |                   12 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.671 |  0.179 |                   11 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14270123839378357
[2m[36m(func pid=57869)[0m mae:  0.08626120537519455
[2m[36m(func pid=57869)[0m rmse_per_class: [0.079, 0.261, 0.024, 0.28, 0.066, 0.145, 0.194, 0.113, 0.137, 0.128]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.1782478541135788
[2m[36m(func pid=69905)[0m mae:  0.13041263818740845
[2m[36m(func pid=69905)[0m rmse_per_class: [0.117, 0.262, 0.098, 0.334, 0.102, 0.19, 0.285, 0.141, 0.143, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.1434062421321869
[2m[36m(func pid=57346)[0m mae:  0.09855840355157852
[2m[36m(func pid=57346)[0m rmse_per_class: [0.084, 0.248, 0.032, 0.283, 0.065, 0.155, 0.229, 0.107, 0.132, 0.098]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17982079088687897
[2m[36m(func pid=69319)[0m mae:  0.13204674422740936
[2m[36m(func pid=69319)[0m rmse_per_class: [0.115, 0.261, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2297 | Steps: 2 | Val loss: 0.2724 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.6048 | Steps: 2 | Val loss: 0.4629 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2806 | Steps: 2 | Val loss: 0.2676 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8559 | Steps: 2 | Val loss: 0.6674 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 09:40:20 (running for 00:15:41.27)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.279 |  0.143 |                   65 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.23  |  0.144 |                   68 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.859 |  0.18  |                   13 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.634 |  0.178 |                   12 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.1436176598072052
[2m[36m(func pid=57869)[0m mae:  0.0868651494383812
[2m[36m(func pid=57869)[0m rmse_per_class: [0.08, 0.265, 0.024, 0.284, 0.066, 0.145, 0.194, 0.112, 0.137, 0.128]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17787930369377136
[2m[36m(func pid=69905)[0m mae:  0.1300787329673767
[2m[36m(func pid=69905)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.101, 0.19, 0.284, 0.141, 0.143, 0.112]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14296294748783112
[2m[36m(func pid=57346)[0m mae:  0.09810987859964371
[2m[36m(func pid=57346)[0m rmse_per_class: [0.083, 0.247, 0.032, 0.282, 0.064, 0.155, 0.228, 0.107, 0.132, 0.099]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17971785366535187
[2m[36m(func pid=69319)[0m mae:  0.13195204734802246
[2m[36m(func pid=69319)[0m rmse_per_class: [0.115, 0.261, 0.101, 0.337, 0.11, 0.19, 0.292, 0.141, 0.142, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2331 | Steps: 2 | Val loss: 0.2721 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5752 | Steps: 2 | Val loss: 0.4388 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2891 | Steps: 2 | Val loss: 0.2675 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8483 | Steps: 2 | Val loss: 0.6626 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 09:40:25 (running for 00:15:46.64)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.281 |  0.143 |                   66 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.233 |  0.143 |                   69 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.856 |  0.18  |                   14 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.605 |  0.178 |                   13 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14329048991203308
[2m[36m(func pid=57869)[0m mae:  0.08618331700563431
[2m[36m(func pid=57869)[0m rmse_per_class: [0.078, 0.263, 0.024, 0.282, 0.067, 0.146, 0.193, 0.115, 0.136, 0.128]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17750248312950134
[2m[36m(func pid=69905)[0m mae:  0.12974530458450317
[2m[36m(func pid=69905)[0m rmse_per_class: [0.117, 0.261, 0.096, 0.333, 0.1, 0.19, 0.283, 0.14, 0.143, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14290742576122284
[2m[36m(func pid=57346)[0m mae:  0.09808414429426193
[2m[36m(func pid=57346)[0m rmse_per_class: [0.083, 0.246, 0.032, 0.281, 0.064, 0.155, 0.229, 0.107, 0.133, 0.101]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17962254583835602
[2m[36m(func pid=69319)[0m mae:  0.13186901807785034
[2m[36m(func pid=69319)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.109, 0.19, 0.292, 0.141, 0.142, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2288 | Steps: 2 | Val loss: 0.2719 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5425 | Steps: 2 | Val loss: 0.4167 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2860 | Steps: 2 | Val loss: 0.2665 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8399 | Steps: 2 | Val loss: 0.6577 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 09:40:30 (running for 00:15:51.93)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.289 |  0.143 |                   67 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.229 |  0.143 |                   70 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.848 |  0.18  |                   15 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.575 |  0.178 |                   14 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14306136965751648
[2m[36m(func pid=57869)[0m mae:  0.08566958457231522
[2m[36m(func pid=57869)[0m rmse_per_class: [0.077, 0.261, 0.025, 0.279, 0.067, 0.147, 0.193, 0.118, 0.134, 0.129]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17718592286109924
[2m[36m(func pid=69905)[0m mae:  0.12947788834571838
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.261, 0.095, 0.332, 0.098, 0.191, 0.282, 0.14, 0.143, 0.112]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14221502840518951
[2m[36m(func pid=57346)[0m mae:  0.09730450063943863
[2m[36m(func pid=57346)[0m rmse_per_class: [0.081, 0.246, 0.032, 0.279, 0.063, 0.155, 0.226, 0.107, 0.132, 0.102]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17956431210041046
[2m[36m(func pid=69319)[0m mae:  0.13180838525295258
[2m[36m(func pid=69319)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.109, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2277 | Steps: 2 | Val loss: 0.2720 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5187 | Steps: 2 | Val loss: 0.3960 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2771 | Steps: 2 | Val loss: 0.2657 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8351 | Steps: 2 | Val loss: 0.6524 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 09:40:35 (running for 00:15:57.04)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.286 |  0.142 |                   68 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.228 |  0.143 |                   71 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.84  |  0.18  |                   16 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.543 |  0.177 |                   15 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14323973655700684
[2m[36m(func pid=57869)[0m mae:  0.08561964333057404
[2m[36m(func pid=57869)[0m rmse_per_class: [0.079, 0.258, 0.025, 0.275, 0.068, 0.148, 0.195, 0.121, 0.134, 0.129]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17665031552314758
[2m[36m(func pid=69905)[0m mae:  0.1290217787027359
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.261, 0.094, 0.331, 0.096, 0.191, 0.281, 0.14, 0.143, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14162328839302063
[2m[36m(func pid=57346)[0m mae:  0.09651435911655426
[2m[36m(func pid=57346)[0m rmse_per_class: [0.081, 0.246, 0.031, 0.279, 0.063, 0.154, 0.223, 0.106, 0.131, 0.102]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17952032387256622
[2m[36m(func pid=69319)[0m mae:  0.13175757229328156
[2m[36m(func pid=69319)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.109, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2401 | Steps: 2 | Val loss: 0.2704 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4946 | Steps: 2 | Val loss: 0.3781 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2771 | Steps: 2 | Val loss: 0.2653 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8282 | Steps: 2 | Val loss: 0.6467 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 09:40:41 (running for 00:16:02.30)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.277 |  0.142 |                   69 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.24  |  0.142 |                   72 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.835 |  0.18  |                   17 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.519 |  0.177 |                   16 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14237019419670105
[2m[36m(func pid=57869)[0m mae:  0.08456961065530777
[2m[36m(func pid=57869)[0m rmse_per_class: [0.076, 0.256, 0.026, 0.272, 0.069, 0.148, 0.194, 0.121, 0.133, 0.128]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17631685733795166
[2m[36m(func pid=69905)[0m mae:  0.12873056530952454
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.261, 0.094, 0.331, 0.094, 0.191, 0.28, 0.14, 0.143, 0.112]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14128383994102478
[2m[36m(func pid=57346)[0m mae:  0.09603394567966461
[2m[36m(func pid=57346)[0m rmse_per_class: [0.08, 0.246, 0.031, 0.279, 0.064, 0.155, 0.221, 0.105, 0.13, 0.101]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.1794937551021576
[2m[36m(func pid=69319)[0m mae:  0.13171690702438354
[2m[36m(func pid=69319)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.109, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2353 | Steps: 2 | Val loss: 0.2672 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4733 | Steps: 2 | Val loss: 0.3614 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2750 | Steps: 2 | Val loss: 0.2652 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8195 | Steps: 2 | Val loss: 0.6412 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 09:40:46 (running for 00:16:07.41)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.277 |  0.141 |                   70 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.235 |  0.141 |                   73 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.828 |  0.179 |                   18 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.495 |  0.176 |                   17 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14068862795829773
[2m[36m(func pid=57869)[0m mae:  0.08339272439479828
[2m[36m(func pid=57869)[0m rmse_per_class: [0.076, 0.248, 0.026, 0.27, 0.07, 0.146, 0.192, 0.118, 0.133, 0.128]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17574521899223328
[2m[36m(func pid=69905)[0m mae:  0.1282377392053604
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.26, 0.093, 0.33, 0.093, 0.191, 0.279, 0.139, 0.143, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14120075106620789
[2m[36m(func pid=57346)[0m mae:  0.09578858315944672
[2m[36m(func pid=57346)[0m rmse_per_class: [0.081, 0.247, 0.031, 0.279, 0.064, 0.155, 0.221, 0.104, 0.13, 0.101]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17941594123840332
[2m[36m(func pid=69319)[0m mae:  0.131646528840065
[2m[36m(func pid=69319)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.109, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2303 | Steps: 2 | Val loss: 0.2648 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4552 | Steps: 2 | Val loss: 0.3482 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2745 | Steps: 2 | Val loss: 0.2654 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 09:40:51 (running for 00:16:12.69)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.17000000178813934
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.275 |  0.141 |                   71 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.23  |  0.139 |                   74 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.819 |  0.179 |                   19 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.473 |  0.176 |                   18 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.13944657146930695
[2m[36m(func pid=57869)[0m mae:  0.08285227417945862
[2m[36m(func pid=57869)[0m rmse_per_class: [0.076, 0.246, 0.026, 0.27, 0.067, 0.145, 0.191, 0.116, 0.133, 0.125]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8138 | Steps: 2 | Val loss: 0.6353 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=69905)[0m rmse: 0.17529970407485962
[2m[36m(func pid=69905)[0m mae:  0.12783807516098022
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.26, 0.092, 0.329, 0.091, 0.191, 0.278, 0.139, 0.143, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14141999185085297
[2m[36m(func pid=57346)[0m mae:  0.0959012359380722
[2m[36m(func pid=57346)[0m rmse_per_class: [0.08, 0.247, 0.031, 0.28, 0.064, 0.156, 0.221, 0.105, 0.13, 0.1]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.1793852150440216
[2m[36m(func pid=69319)[0m mae:  0.13161373138427734
[2m[36m(func pid=69319)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.109, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2354 | Steps: 2 | Val loss: 0.2647 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4391 | Steps: 2 | Val loss: 0.3369 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2821 | Steps: 2 | Val loss: 0.2658 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 09:40:56 (running for 00:16:17.77)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15799999982118607
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.275 |  0.141 |                   72 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.235 |  0.139 |                   75 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.814 |  0.179 |                   20 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.455 |  0.175 |                   19 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.13929125666618347
[2m[36m(func pid=57869)[0m mae:  0.08291558176279068
[2m[36m(func pid=57869)[0m rmse_per_class: [0.076, 0.246, 0.026, 0.272, 0.064, 0.144, 0.19, 0.115, 0.134, 0.126]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8037 | Steps: 2 | Val loss: 0.6289 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=69905)[0m rmse: 0.17491775751113892
[2m[36m(func pid=69905)[0m mae:  0.1275126039981842
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.26, 0.092, 0.328, 0.089, 0.191, 0.277, 0.139, 0.143, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14182144403457642
[2m[36m(func pid=57346)[0m mae:  0.09623531997203827
[2m[36m(func pid=57346)[0m rmse_per_class: [0.08, 0.246, 0.031, 0.278, 0.065, 0.155, 0.224, 0.105, 0.131, 0.102]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2270 | Steps: 2 | Val loss: 0.2653 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=69319)[0m rmse: 0.17935475707054138
[2m[36m(func pid=69319)[0m mae:  0.13158729672431946
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.142, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4245 | Steps: 2 | Val loss: 0.3278 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 09:41:01 (running for 00:16:23.05)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15799999982118607
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.282 |  0.142 |                   73 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.227 |  0.14  |                   76 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.804 |  0.179 |                   21 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.439 |  0.175 |                   20 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2721 | Steps: 2 | Val loss: 0.2659 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=57869)[0m rmse: 0.13976433873176575
[2m[36m(func pid=57869)[0m mae:  0.08347104489803314
[2m[36m(func pid=57869)[0m rmse_per_class: [0.078, 0.248, 0.026, 0.274, 0.063, 0.144, 0.192, 0.114, 0.133, 0.127]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17442889511585236
[2m[36m(func pid=69905)[0m mae:  0.12709946930408478
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.26, 0.091, 0.327, 0.087, 0.191, 0.276, 0.139, 0.143, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.7996 | Steps: 2 | Val loss: 0.6230 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=57346)[0m rmse: 0.14197000861167908
[2m[36m(func pid=57346)[0m mae:  0.09632135927677155
[2m[36m(func pid=57346)[0m rmse_per_class: [0.081, 0.246, 0.031, 0.278, 0.066, 0.155, 0.224, 0.105, 0.132, 0.102]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2293 | Steps: 2 | Val loss: 0.2668 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4145 | Steps: 2 | Val loss: 0.3203 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=69319)[0m rmse: 0.179351806640625
[2m[36m(func pid=69319)[0m mae:  0.13157980144023895
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=69319)[0m 
== Status ==
Current time: 2024-01-07 09:41:07 (running for 00:16:28.18)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.15799999982118607
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.272 |  0.142 |                   74 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.229 |  0.141 |                   77 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.8   |  0.179 |                   22 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.425 |  0.174 |                   21 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14067618548870087
[2m[36m(func pid=57869)[0m mae:  0.08434592187404633
[2m[36m(func pid=57869)[0m rmse_per_class: [0.08, 0.249, 0.026, 0.276, 0.063, 0.144, 0.194, 0.114, 0.134, 0.128]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2709 | Steps: 2 | Val loss: 0.2652 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=69905)[0m rmse: 0.17394334077835083
[2m[36m(func pid=69905)[0m mae:  0.12668612599372864
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.261, 0.09, 0.326, 0.086, 0.191, 0.275, 0.138, 0.143, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.7886 | Steps: 2 | Val loss: 0.6160 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=57346)[0m rmse: 0.14139720797538757
[2m[36m(func pid=57346)[0m mae:  0.09571976214647293
[2m[36m(func pid=57346)[0m rmse_per_class: [0.081, 0.245, 0.03, 0.277, 0.065, 0.154, 0.222, 0.106, 0.131, 0.102]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2283 | Steps: 2 | Val loss: 0.2692 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4081 | Steps: 2 | Val loss: 0.3147 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=69319)[0m rmse: 0.17936871945858002
[2m[36m(func pid=69319)[0m mae:  0.1315884292125702
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=69319)[0m 
== Status ==
Current time: 2024-01-07 09:41:12 (running for 00:16:33.40)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.271 |  0.141 |                   75 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.228 |  0.142 |                   78 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.789 |  0.179 |                   23 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.415 |  0.174 |                   22 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.1420280635356903
[2m[36m(func pid=57869)[0m mae:  0.08528264611959457
[2m[36m(func pid=57869)[0m rmse_per_class: [0.081, 0.252, 0.026, 0.279, 0.064, 0.144, 0.196, 0.114, 0.134, 0.131]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2645 | Steps: 2 | Val loss: 0.2653 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=69905)[0m rmse: 0.1733398735523224
[2m[36m(func pid=69905)[0m mae:  0.12616518139839172
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.26, 0.09, 0.325, 0.084, 0.191, 0.273, 0.138, 0.143, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7832 | Steps: 2 | Val loss: 0.6094 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=57346)[0m rmse: 0.1413802206516266
[2m[36m(func pid=57346)[0m mae:  0.09565447270870209
[2m[36m(func pid=57346)[0m rmse_per_class: [0.081, 0.245, 0.03, 0.278, 0.066, 0.154, 0.222, 0.106, 0.131, 0.101]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2400 | Steps: 2 | Val loss: 0.2704 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4000 | Steps: 2 | Val loss: 0.3112 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=69319)[0m rmse: 0.17936208844184875
[2m[36m(func pid=69319)[0m mae:  0.13157624006271362
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14228521287441254
[2m[36m(func pid=57869)[0m mae:  0.08556419610977173
== Status ==
Current time: 2024-01-07 09:41:17 (running for 00:16:38.51)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.264 |  0.141 |                   76 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.24  |  0.142 |                   79 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.783 |  0.179 |                   24 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.408 |  0.173 |                   23 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=57869)[0m rmse_per_class: [0.078, 0.256, 0.024, 0.282, 0.064, 0.145, 0.197, 0.114, 0.134, 0.129]

[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17280802130699158
[2m[36m(func pid=69905)[0m mae:  0.1257012039422989
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.26, 0.089, 0.324, 0.083, 0.191, 0.272, 0.138, 0.143, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2661 | Steps: 2 | Val loss: 0.2651 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7686 | Steps: 2 | Val loss: 0.6029 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2385 | Steps: 2 | Val loss: 0.2702 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=57346)[0m rmse: 0.1412799209356308
[2m[36m(func pid=57346)[0m mae:  0.09539203345775604
[2m[36m(func pid=57346)[0m rmse_per_class: [0.08, 0.246, 0.03, 0.277, 0.067, 0.154, 0.221, 0.106, 0.131, 0.102]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3941 | Steps: 2 | Val loss: 0.3095 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=69319)[0m rmse: 0.17932608723640442
[2m[36m(func pid=69319)[0m mae:  0.13155227899551392
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.337, 0.107, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=69319)[0m 
== Status ==
Current time: 2024-01-07 09:41:22 (running for 00:16:43.85)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.266 |  0.141 |                   77 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.142 |                   80 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.769 |  0.179 |                   25 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.4   |  0.173 |                   24 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14225485920906067
[2m[36m(func pid=57869)[0m mae:  0.08572785556316376
[2m[36m(func pid=57869)[0m rmse_per_class: [0.077, 0.259, 0.023, 0.281, 0.063, 0.145, 0.196, 0.117, 0.136, 0.127]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17240814864635468
[2m[36m(func pid=69905)[0m mae:  0.125343456864357
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.261, 0.088, 0.323, 0.081, 0.191, 0.271, 0.138, 0.142, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2617 | Steps: 2 | Val loss: 0.2650 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7629 | Steps: 2 | Val loss: 0.5957 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2379 | Steps: 2 | Val loss: 0.2713 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=57346)[0m rmse: 0.14116455614566803
[2m[36m(func pid=57346)[0m mae:  0.09517021477222443
[2m[36m(func pid=57346)[0m rmse_per_class: [0.081, 0.246, 0.03, 0.278, 0.067, 0.154, 0.219, 0.106, 0.131, 0.102]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3966 | Steps: 2 | Val loss: 0.3087 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=69319)[0m rmse: 0.17928293347358704
[2m[36m(func pid=69319)[0m mae:  0.13150851428508759
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.337, 0.107, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=69319)[0m 
== Status ==
Current time: 2024-01-07 09:41:27 (running for 00:16:49.04)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.262 |  0.141 |                   78 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.143 |                   81 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.763 |  0.179 |                   26 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.394 |  0.172 |                   25 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.1430128514766693
[2m[36m(func pid=57869)[0m mae:  0.08628363162279129
[2m[36m(func pid=57869)[0m rmse_per_class: [0.079, 0.262, 0.023, 0.284, 0.062, 0.146, 0.195, 0.117, 0.137, 0.125]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17170864343643188
[2m[36m(func pid=69905)[0m mae:  0.12474675476551056
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.26, 0.087, 0.322, 0.079, 0.191, 0.269, 0.138, 0.142, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2663 | Steps: 2 | Val loss: 0.2648 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7532 | Steps: 2 | Val loss: 0.5892 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2314 | Steps: 2 | Val loss: 0.2705 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=57346)[0m rmse: 0.1411537379026413
[2m[36m(func pid=57346)[0m mae:  0.0950452983379364
[2m[36m(func pid=57346)[0m rmse_per_class: [0.08, 0.246, 0.03, 0.277, 0.067, 0.154, 0.219, 0.106, 0.131, 0.102]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3939 | Steps: 2 | Val loss: 0.3094 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 09:41:33 (running for 00:16:54.10)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.266 |  0.141 |                   79 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.143 |                   81 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.753 |  0.179 |                   27 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.397 |  0.172 |                   26 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69319)[0m rmse: 0.17927363514900208
[2m[36m(func pid=69319)[0m mae:  0.131483793258667
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.29, 0.141, 0.143, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14269661903381348
[2m[36m(func pid=57869)[0m mae:  0.08608215302228928
[2m[36m(func pid=57869)[0m rmse_per_class: [0.082, 0.261, 0.023, 0.284, 0.062, 0.145, 0.195, 0.115, 0.137, 0.123]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17113561928272247
[2m[36m(func pid=69905)[0m mae:  0.1242678165435791
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.26, 0.086, 0.321, 0.078, 0.191, 0.267, 0.138, 0.142, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2661 | Steps: 2 | Val loss: 0.2650 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2330 | Steps: 2 | Val loss: 0.2702 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.7473 | Steps: 2 | Val loss: 0.5828 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=57346)[0m rmse: 0.1412605196237564
[2m[36m(func pid=57346)[0m mae:  0.09499682486057281
[2m[36m(func pid=57346)[0m rmse_per_class: [0.081, 0.246, 0.03, 0.277, 0.067, 0.154, 0.219, 0.105, 0.131, 0.103]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3944 | Steps: 2 | Val loss: 0.3111 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=57869)[0m rmse: 0.14255774021148682
[2m[36m(func pid=57869)[0m mae:  0.08564125001430511
[2m[36m(func pid=57869)[0m rmse_per_class: [0.083, 0.257, 0.023, 0.284, 0.06, 0.144, 0.193, 0.114, 0.139, 0.127]
== Status ==
Current time: 2024-01-07 09:41:38 (running for 00:16:59.53)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.266 |  0.141 |                   80 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.233 |  0.143 |                   83 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.753 |  0.179 |                   27 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.394 |  0.171 |                   27 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17921102046966553
[2m[36m(func pid=69319)[0m mae:  0.13141191005706787
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.106, 0.19, 0.29, 0.141, 0.143, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.17060816287994385
[2m[36m(func pid=69905)[0m mae:  0.1238243579864502
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.26, 0.084, 0.32, 0.076, 0.19, 0.266, 0.138, 0.142, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2723 | Steps: 2 | Val loss: 0.2653 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2301 | Steps: 2 | Val loss: 0.2719 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7359 | Steps: 2 | Val loss: 0.5755 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=57346)[0m rmse: 0.14156611263751984
[2m[36m(func pid=57346)[0m mae:  0.09509817510843277
[2m[36m(func pid=57346)[0m rmse_per_class: [0.081, 0.247, 0.029, 0.278, 0.066, 0.154, 0.218, 0.105, 0.132, 0.105]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3965 | Steps: 2 | Val loss: 0.3135 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 09:41:43 (running for 00:17:04.78)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.272 |  0.142 |                   81 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.23  |  0.144 |                   84 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.747 |  0.179 |                   28 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.394 |  0.171 |                   28 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14405101537704468
[2m[36m(func pid=57869)[0m mae:  0.08672259002923965
[2m[36m(func pid=57869)[0m rmse_per_class: [0.088, 0.255, 0.023, 0.288, 0.061, 0.145, 0.194, 0.116, 0.142, 0.129]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17915181815624237
[2m[36m(func pid=69319)[0m mae:  0.1313476711511612
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.106, 0.19, 0.29, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.1700499951839447
[2m[36m(func pid=69905)[0m mae:  0.1233794316649437
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.26, 0.083, 0.319, 0.075, 0.19, 0.265, 0.138, 0.142, 0.111]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2699 | Steps: 2 | Val loss: 0.2649 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2333 | Steps: 2 | Val loss: 0.2741 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.7283 | Steps: 2 | Val loss: 0.5682 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=57346)[0m rmse: 0.14132720232009888
[2m[36m(func pid=57346)[0m mae:  0.09468619525432587
[2m[36m(func pid=57346)[0m rmse_per_class: [0.08, 0.248, 0.029, 0.277, 0.066, 0.154, 0.218, 0.105, 0.131, 0.105]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.4021 | Steps: 2 | Val loss: 0.3162 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 09:41:49 (running for 00:17:10.12)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.27  |  0.141 |                   82 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.233 |  0.146 |                   85 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.736 |  0.179 |                   29 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.397 |  0.17  |                   29 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14566071331501007
[2m[36m(func pid=57869)[0m mae:  0.08774921298027039
[2m[36m(func pid=57869)[0m rmse_per_class: [0.09, 0.256, 0.023, 0.291, 0.062, 0.146, 0.197, 0.117, 0.144, 0.132]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.1790774017572403
[2m[36m(func pid=69319)[0m mae:  0.13127335906028748
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.106, 0.19, 0.29, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.16931042075157166
[2m[36m(func pid=69905)[0m mae:  0.12276744842529297
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.259, 0.081, 0.318, 0.073, 0.19, 0.263, 0.138, 0.142, 0.11]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2665 | Steps: 2 | Val loss: 0.2650 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2389 | Steps: 2 | Val loss: 0.2745 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4012 | Steps: 2 | Val loss: 0.3195 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=57346)[0m rmse: 0.14136169850826263
[2m[36m(func pid=57346)[0m mae:  0.09456928819417953
[2m[36m(func pid=57346)[0m rmse_per_class: [0.079, 0.249, 0.029, 0.278, 0.066, 0.154, 0.218, 0.105, 0.131, 0.106]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.7193 | Steps: 2 | Val loss: 0.5607 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 09:41:54 (running for 00:17:15.33)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.267 |  0.141 |                   83 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.239 |  0.146 |                   86 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.728 |  0.179 |                   30 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.402 |  0.169 |                   30 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14560994505882263
[2m[36m(func pid=57869)[0m mae:  0.08746524155139923
[2m[36m(func pid=57869)[0m rmse_per_class: [0.087, 0.256, 0.023, 0.288, 0.063, 0.147, 0.197, 0.118, 0.144, 0.133]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.16859489679336548
[2m[36m(func pid=69905)[0m mae:  0.12217824161052704
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.259, 0.08, 0.317, 0.072, 0.19, 0.261, 0.137, 0.142, 0.11]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17895887792110443
[2m[36m(func pid=69319)[0m mae:  0.13114376366138458
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.106, 0.19, 0.289, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2654 | Steps: 2 | Val loss: 0.2659 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2421 | Steps: 2 | Val loss: 0.2745 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=57346)[0m rmse: 0.1418384462594986
[2m[36m(func pid=57346)[0m mae:  0.094957634806633
[2m[36m(func pid=57346)[0m rmse_per_class: [0.078, 0.249, 0.029, 0.28, 0.066, 0.154, 0.22, 0.105, 0.132, 0.106]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4061 | Steps: 2 | Val loss: 0.3236 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7084 | Steps: 2 | Val loss: 0.5534 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 09:41:59 (running for 00:17:20.85)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.265 |  0.142 |                   84 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.242 |  0.145 |                   87 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.719 |  0.179 |                   31 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.401 |  0.169 |                   31 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14514610171318054
[2m[36m(func pid=57869)[0m mae:  0.0871879830956459
[2m[36m(func pid=57869)[0m rmse_per_class: [0.089, 0.256, 0.023, 0.287, 0.065, 0.146, 0.195, 0.115, 0.141, 0.133]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.16786940395832062
[2m[36m(func pid=69905)[0m mae:  0.12155495584011078
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.258, 0.079, 0.316, 0.07, 0.19, 0.259, 0.137, 0.142, 0.11]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.1788739264011383
[2m[36m(func pid=69319)[0m mae:  0.13107238709926605
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.336, 0.105, 0.19, 0.289, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2654 | Steps: 2 | Val loss: 0.2669 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2461 | Steps: 2 | Val loss: 0.2727 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=57346)[0m rmse: 0.14259783923625946
[2m[36m(func pid=57346)[0m mae:  0.09569305181503296
[2m[36m(func pid=57346)[0m rmse_per_class: [0.078, 0.249, 0.029, 0.279, 0.066, 0.154, 0.224, 0.106, 0.134, 0.108]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4081 | Steps: 2 | Val loss: 0.3277 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7002 | Steps: 2 | Val loss: 0.5458 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 09:42:05 (running for 00:17:26.07)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.265 |  0.143 |                   85 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.246 |  0.143 |                   88 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.708 |  0.179 |                   32 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.406 |  0.168 |                   32 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14348796010017395
[2m[36m(func pid=57869)[0m mae:  0.08590994775295258
[2m[36m(func pid=57869)[0m rmse_per_class: [0.085, 0.254, 0.025, 0.285, 0.067, 0.144, 0.193, 0.112, 0.136, 0.133]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.1672453135251999
[2m[36m(func pid=69905)[0m mae:  0.12103370577096939
[2m[36m(func pid=69905)[0m rmse_per_class: [0.118, 0.258, 0.077, 0.316, 0.069, 0.189, 0.258, 0.137, 0.142, 0.109]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17882929742336273
[2m[36m(func pid=69319)[0m mae:  0.13102535903453827
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.336, 0.105, 0.19, 0.289, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2609 | Steps: 2 | Val loss: 0.2676 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2355 | Steps: 2 | Val loss: 0.2696 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4150 | Steps: 2 | Val loss: 0.3317 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=57346)[0m rmse: 0.1431058943271637
[2m[36m(func pid=57346)[0m mae:  0.09601391106843948
[2m[36m(func pid=57346)[0m rmse_per_class: [0.077, 0.249, 0.029, 0.28, 0.066, 0.154, 0.225, 0.107, 0.135, 0.109]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.6923 | Steps: 2 | Val loss: 0.5384 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 09:42:10 (running for 00:17:31.32)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.261 |  0.143 |                   86 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.235 |  0.141 |                   89 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.7   |  0.179 |                   33 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.408 |  0.167 |                   33 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14146974682807922
[2m[36m(func pid=57869)[0m mae:  0.08427605032920837
[2m[36m(func pid=57869)[0m rmse_per_class: [0.077, 0.251, 0.025, 0.278, 0.068, 0.144, 0.192, 0.113, 0.134, 0.133]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.16629177331924438
[2m[36m(func pid=69905)[0m mae:  0.12023122608661652
[2m[36m(func pid=69905)[0m rmse_per_class: [0.117, 0.257, 0.075, 0.314, 0.068, 0.189, 0.256, 0.137, 0.142, 0.109]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2605 | Steps: 2 | Val loss: 0.2674 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=69319)[0m rmse: 0.17879128456115723
[2m[36m(func pid=69319)[0m mae:  0.13098515570163727
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.336, 0.104, 0.19, 0.289, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2375 | Steps: 2 | Val loss: 0.2682 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=57346)[0m rmse: 0.14305442571640015
[2m[36m(func pid=57346)[0m mae:  0.09590547531843185
[2m[36m(func pid=57346)[0m rmse_per_class: [0.077, 0.248, 0.029, 0.278, 0.066, 0.154, 0.225, 0.107, 0.135, 0.111]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4188 | Steps: 2 | Val loss: 0.3365 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.6846 | Steps: 2 | Val loss: 0.5312 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 09:42:15 (running for 00:17:36.69)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.26  |  0.143 |                   87 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.14  |                   90 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.692 |  0.179 |                   34 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.415 |  0.166 |                   34 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14031560719013214
[2m[36m(func pid=57869)[0m mae:  0.08342672884464264
[2m[36m(func pid=57869)[0m rmse_per_class: [0.073, 0.25, 0.026, 0.275, 0.067, 0.144, 0.191, 0.114, 0.133, 0.13]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.16542460024356842
[2m[36m(func pid=69905)[0m mae:  0.11946290731430054
[2m[36m(func pid=69905)[0m rmse_per_class: [0.117, 0.256, 0.073, 0.314, 0.066, 0.189, 0.253, 0.136, 0.141, 0.108]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2618 | Steps: 2 | Val loss: 0.2669 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=69319)[0m rmse: 0.1786988377571106
[2m[36m(func pid=69319)[0m mae:  0.13089501857757568
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.098, 0.335, 0.104, 0.19, 0.288, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2313 | Steps: 2 | Val loss: 0.2674 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4241 | Steps: 2 | Val loss: 0.3414 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=57346)[0m rmse: 0.1426793783903122
[2m[36m(func pid=57346)[0m mae:  0.0955197885632515
[2m[36m(func pid=57346)[0m rmse_per_class: [0.077, 0.248, 0.029, 0.277, 0.066, 0.154, 0.225, 0.106, 0.134, 0.111]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6727 | Steps: 2 | Val loss: 0.5237 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 09:42:20 (running for 00:17:41.92)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.262 |  0.143 |                   88 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.231 |  0.14  |                   91 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.685 |  0.179 |                   35 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.419 |  0.165 |                   35 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.13985781371593475
[2m[36m(func pid=57869)[0m mae:  0.08343847095966339
[2m[36m(func pid=57869)[0m rmse_per_class: [0.073, 0.252, 0.026, 0.275, 0.068, 0.144, 0.192, 0.113, 0.133, 0.123]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.1647205650806427
[2m[36m(func pid=69905)[0m mae:  0.11884455382823944
[2m[36m(func pid=69905)[0m rmse_per_class: [0.117, 0.256, 0.072, 0.313, 0.065, 0.188, 0.251, 0.136, 0.141, 0.108]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2679 | Steps: 2 | Val loss: 0.2658 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=69319)[0m rmse: 0.17863647639751434
[2m[36m(func pid=69319)[0m mae:  0.13082587718963623
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.335, 0.104, 0.19, 0.288, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2294 | Steps: 2 | Val loss: 0.2686 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4310 | Steps: 2 | Val loss: 0.3461 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=57346)[0m rmse: 0.1420208215713501
[2m[36m(func pid=57346)[0m mae:  0.0946645513176918
[2m[36m(func pid=57346)[0m rmse_per_class: [0.076, 0.247, 0.029, 0.276, 0.066, 0.154, 0.221, 0.106, 0.132, 0.113]
[2m[36m(func pid=57346)[0m 
== Status ==
Current time: 2024-01-07 09:42:25 (running for 00:17:47.03)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.268 |  0.142 |                   89 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.229 |  0.141 |                   92 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.673 |  0.179 |                   36 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.424 |  0.165 |                   36 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14091017842292786
[2m[36m(func pid=57869)[0m mae:  0.08447027206420898
[2m[36m(func pid=57869)[0m rmse_per_class: [0.077, 0.253, 0.025, 0.277, 0.069, 0.145, 0.194, 0.114, 0.134, 0.122]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6656 | Steps: 2 | Val loss: 0.5171 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=69905)[0m rmse: 0.1638985127210617
[2m[36m(func pid=69905)[0m mae:  0.11810284852981567
[2m[36m(func pid=69905)[0m rmse_per_class: [0.116, 0.255, 0.071, 0.312, 0.064, 0.188, 0.249, 0.136, 0.141, 0.107]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2621 | Steps: 2 | Val loss: 0.2643 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=69319)[0m rmse: 0.17858241498470306
[2m[36m(func pid=69319)[0m mae:  0.1307692676782608
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.335, 0.104, 0.19, 0.288, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2288 | Steps: 2 | Val loss: 0.2698 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=57346)[0m rmse: 0.14101620018482208
[2m[36m(func pid=57346)[0m mae:  0.09362713992595673
[2m[36m(func pid=57346)[0m rmse_per_class: [0.076, 0.247, 0.029, 0.274, 0.065, 0.154, 0.217, 0.105, 0.131, 0.112]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4315 | Steps: 2 | Val loss: 0.3504 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 09:42:31 (running for 00:17:52.28)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.262 |  0.141 |                   90 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.229 |  0.142 |                   93 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.666 |  0.179 |                   37 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.431 |  0.164 |                   37 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.1418071985244751
[2m[36m(func pid=57869)[0m mae:  0.08518587052822113
[2m[36m(func pid=57869)[0m rmse_per_class: [0.08, 0.253, 0.025, 0.277, 0.07, 0.146, 0.196, 0.115, 0.134, 0.123]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6561 | Steps: 2 | Val loss: 0.5103 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=69905)[0m rmse: 0.16304370760917664
[2m[36m(func pid=69905)[0m mae:  0.1173146516084671
[2m[36m(func pid=69905)[0m rmse_per_class: [0.115, 0.254, 0.069, 0.311, 0.063, 0.188, 0.247, 0.135, 0.141, 0.106]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2584 | Steps: 2 | Val loss: 0.2636 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2436 | Steps: 2 | Val loss: 0.2704 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=69319)[0m rmse: 0.17850592732429504
[2m[36m(func pid=69319)[0m mae:  0.1306854486465454
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.335, 0.103, 0.19, 0.287, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14041152596473694
[2m[36m(func pid=57346)[0m mae:  0.09295399487018585
[2m[36m(func pid=57346)[0m rmse_per_class: [0.076, 0.247, 0.029, 0.273, 0.065, 0.153, 0.215, 0.105, 0.131, 0.111]
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4367 | Steps: 2 | Val loss: 0.3551 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=57346)[0m 
== Status ==
Current time: 2024-01-07 09:42:36 (running for 00:17:57.59)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                   91 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.244 |  0.142 |                   94 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.656 |  0.179 |                   38 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.432 |  0.163 |                   38 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.1421329826116562
[2m[36m(func pid=57869)[0m mae:  0.08513288199901581
[2m[36m(func pid=57869)[0m rmse_per_class: [0.081, 0.254, 0.026, 0.279, 0.068, 0.145, 0.194, 0.114, 0.131, 0.129]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6478 | Steps: 2 | Val loss: 0.5039 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=69905)[0m rmse: 0.16223318874835968
[2m[36m(func pid=69905)[0m mae:  0.11658082157373428
[2m[36m(func pid=69905)[0m rmse_per_class: [0.115, 0.253, 0.068, 0.31, 0.062, 0.188, 0.245, 0.135, 0.141, 0.106]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2646 | Steps: 2 | Val loss: 0.2634 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2309 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=69319)[0m rmse: 0.178431898355484
[2m[36m(func pid=69319)[0m mae:  0.13061265647411346
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.261, 0.098, 0.334, 0.103, 0.19, 0.287, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4426 | Steps: 2 | Val loss: 0.3596 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 09:42:41 (running for 00:18:02.68)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.265 |  0.14  |                   92 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.244 |  0.142 |                   94 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.648 |  0.178 |                   39 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.437 |  0.162 |                   39 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m rmse: 0.1403297632932663
[2m[36m(func pid=57346)[0m mae:  0.09292249381542206
[2m[36m(func pid=57346)[0m rmse_per_class: [0.076, 0.247, 0.029, 0.273, 0.065, 0.153, 0.216, 0.105, 0.131, 0.11]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=57869)[0m rmse: 0.14425159990787506
[2m[36m(func pid=57869)[0m mae:  0.087288036942482
[2m[36m(func pid=57869)[0m rmse_per_class: [0.09, 0.248, 0.026, 0.285, 0.074, 0.144, 0.2, 0.114, 0.13, 0.131]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6409 | Steps: 2 | Val loss: 0.4964 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=69905)[0m rmse: 0.16138005256652832
[2m[36m(func pid=69905)[0m mae:  0.11582257598638535
[2m[36m(func pid=69905)[0m rmse_per_class: [0.114, 0.253, 0.066, 0.31, 0.061, 0.187, 0.243, 0.135, 0.141, 0.105]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2308 | Steps: 2 | Val loss: 0.2770 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2582 | Steps: 2 | Val loss: 0.2645 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=69319)[0m rmse: 0.1781962811946869
[2m[36m(func pid=69319)[0m mae:  0.13040649890899658
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.334, 0.103, 0.19, 0.287, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4392 | Steps: 2 | Val loss: 0.3636 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:42:46 (running for 00:18:07.83)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.265 |  0.14  |                   92 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.231 |  0.148 |                   96 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.641 |  0.178 |                   40 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.443 |  0.161 |                   40 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.1479489654302597
[2m[36m(func pid=57869)[0m mae:  0.09003244340419769
[2m[36m(func pid=57869)[0m rmse_per_class: [0.099, 0.245, 0.027, 0.29, 0.078, 0.146, 0.205, 0.118, 0.13, 0.141]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14102596044540405
[2m[36m(func pid=57346)[0m mae:  0.09349720180034637
[2m[36m(func pid=57346)[0m rmse_per_class: [0.075, 0.248, 0.028, 0.276, 0.065, 0.152, 0.219, 0.105, 0.132, 0.111]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6292 | Steps: 2 | Val loss: 0.4898 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=69905)[0m rmse: 0.16065678000450134
[2m[36m(func pid=69905)[0m mae:  0.11519155651330948
[2m[36m(func pid=69905)[0m rmse_per_class: [0.114, 0.252, 0.064, 0.309, 0.06, 0.187, 0.241, 0.134, 0.141, 0.104]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2332 | Steps: 2 | Val loss: 0.2781 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2628 | Steps: 2 | Val loss: 0.2666 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=69319)[0m rmse: 0.17814283072948456
[2m[36m(func pid=69319)[0m mae:  0.1303766667842865
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.102, 0.19, 0.286, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4467 | Steps: 2 | Val loss: 0.3673 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=57869)[0m rmse: 0.14952702820301056
[2m[36m(func pid=57869)[0m mae:  0.09074892103672028
[2m[36m(func pid=57869)[0m rmse_per_class: [0.103, 0.245, 0.027, 0.291, 0.081, 0.147, 0.205, 0.119, 0.131, 0.145]== Status ==
Current time: 2024-01-07 09:42:52 (running for 00:18:13.07)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.141 |                   93 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.233 |  0.15  |                   97 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.629 |  0.178 |                   41 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.439 |  0.161 |                   41 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14239443838596344
[2m[36m(func pid=57346)[0m mae:  0.09479837119579315
[2m[36m(func pid=57346)[0m rmse_per_class: [0.076, 0.248, 0.028, 0.28, 0.064, 0.152, 0.224, 0.106, 0.134, 0.111]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6226 | Steps: 2 | Val loss: 0.4823 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=69905)[0m rmse: 0.1599433571100235
[2m[36m(func pid=69905)[0m mae:  0.11450952291488647
[2m[36m(func pid=69905)[0m rmse_per_class: [0.113, 0.251, 0.063, 0.308, 0.059, 0.187, 0.24, 0.134, 0.141, 0.103]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2352 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2596 | Steps: 2 | Val loss: 0.2675 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=69319)[0m rmse: 0.17808368802070618
[2m[36m(func pid=69319)[0m mae:  0.13032236695289612
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.102, 0.19, 0.286, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4476 | Steps: 2 | Val loss: 0.3702 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=57869)[0m rmse: 0.14648769795894623
[2m[36m(func pid=57869)[0m mae:  0.08882585912942886
[2m[36m(func pid=57869)[0m rmse_per_class: [0.099, 0.245, 0.026, 0.287, 0.076, 0.147, 0.2, 0.113, 0.134, 0.137]
[2m[36m(func pid=57869)[0m 
== Status ==
Current time: 2024-01-07 09:42:57 (running for 00:18:18.16)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.263 |  0.142 |                   94 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.235 |  0.146 |                   98 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.623 |  0.178 |                   42 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.447 |  0.16  |                   42 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m rmse: 0.14300963282585144
[2m[36m(func pid=57346)[0m mae:  0.09525509178638458
[2m[36m(func pid=57346)[0m rmse_per_class: [0.077, 0.249, 0.028, 0.282, 0.064, 0.152, 0.226, 0.107, 0.135, 0.11]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.1592373251914978
[2m[36m(func pid=69905)[0m mae:  0.11386638879776001
[2m[36m(func pid=69905)[0m rmse_per_class: [0.113, 0.25, 0.061, 0.307, 0.059, 0.186, 0.239, 0.133, 0.141, 0.103]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6132 | Steps: 2 | Val loss: 0.4755 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2300 | Steps: 2 | Val loss: 0.2686 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2569 | Steps: 2 | Val loss: 0.2666 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 09:43:02 (running for 00:18:23.19)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.26  |  0.143 |                   95 |
| train_32e5a_00007 | RUNNING    | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.235 |  0.146 |                   98 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.613 |  0.178 |                   43 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.448 |  0.159 |                   43 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69319)[0m rmse: 0.17799820005893707
[2m[36m(func pid=69319)[0m mae:  0.1302485466003418
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.102, 0.191, 0.286, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4515 | Steps: 2 | Val loss: 0.3732 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=57869)[0m rmse: 0.1425924003124237
[2m[36m(func pid=57869)[0m mae:  0.08666084706783295
[2m[36m(func pid=57869)[0m rmse_per_class: [0.092, 0.246, 0.024, 0.282, 0.068, 0.146, 0.195, 0.109, 0.137, 0.126]
[2m[36m(func pid=57869)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14242134988307953
[2m[36m(func pid=57346)[0m mae:  0.09443141520023346
[2m[36m(func pid=57346)[0m rmse_per_class: [0.076, 0.25, 0.027, 0.281, 0.064, 0.152, 0.222, 0.106, 0.135, 0.11]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.15842291712760925
[2m[36m(func pid=69905)[0m mae:  0.11311084032058716
[2m[36m(func pid=69905)[0m rmse_per_class: [0.113, 0.249, 0.059, 0.306, 0.058, 0.186, 0.237, 0.133, 0.141, 0.102]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=57869)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2384 | Steps: 2 | Val loss: 0.2654 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.6064 | Steps: 2 | Val loss: 0.4685 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2623 | Steps: 2 | Val loss: 0.2653 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 09:43:07 (running for 00:18:28.69)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 3 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.257 |  0.142 |                   96 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.613 |  0.178 |                   43 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.452 |  0.158 |                   44 |
| train_32e5a_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57869)[0m rmse: 0.14055512845516205
[2m[36m(func pid=57869)[0m mae:  0.08510155230760574
[2m[36m(func pid=57869)[0m rmse_per_class: [0.085, 0.248, 0.024, 0.277, 0.065, 0.147, 0.192, 0.108, 0.14, 0.12]
[2m[36m(func pid=69319)[0m rmse: 0.17787152528762817
[2m[36m(func pid=69319)[0m mae:  0.1301342099905014
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.334, 0.102, 0.191, 0.286, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4520 | Steps: 2 | Val loss: 0.3757 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=57346)[0m rmse: 0.14142698049545288
[2m[36m(func pid=57346)[0m mae:  0.09318049997091293
[2m[36m(func pid=57346)[0m rmse_per_class: [0.076, 0.252, 0.027, 0.279, 0.064, 0.152, 0.217, 0.105, 0.132, 0.11]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.1577329784631729
[2m[36m(func pid=69905)[0m mae:  0.1124558299779892
[2m[36m(func pid=69905)[0m rmse_per_class: [0.112, 0.249, 0.058, 0.305, 0.058, 0.185, 0.236, 0.133, 0.141, 0.101]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5966 | Steps: 2 | Val loss: 0.4617 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2653 | Steps: 2 | Val loss: 0.2641 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4498 | Steps: 2 | Val loss: 0.3774 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=69319)[0m rmse: 0.17775247991085052
[2m[36m(func pid=69319)[0m mae:  0.1300305426120758
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.334, 0.101, 0.191, 0.286, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57346)[0m rmse: 0.14070522785186768
[2m[36m(func pid=57346)[0m mae:  0.09250899404287338
[2m[36m(func pid=57346)[0m rmse_per_class: [0.076, 0.25, 0.027, 0.277, 0.065, 0.153, 0.214, 0.105, 0.131, 0.109]
[2m[36m(func pid=69905)[0m rmse: 0.15713095664978027
[2m[36m(func pid=69905)[0m mae:  0.11188854277133942
[2m[36m(func pid=69905)[0m rmse_per_class: [0.112, 0.248, 0.056, 0.304, 0.057, 0.185, 0.236, 0.132, 0.141, 0.1]
== Status ==
Current time: 2024-01-07 09:43:13 (running for 00:18:34.34)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.262 |  0.141 |                   97 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.597 |  0.178 |                   45 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.452 |  0.158 |                   45 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=79815)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=79815)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=79815)[0m Configuration completed!
[2m[36m(func pid=79815)[0m New optimizer parameters:
[2m[36m(func pid=79815)[0m SGD (
[2m[36m(func pid=79815)[0m Parameter Group 0
[2m[36m(func pid=79815)[0m     dampening: 0
[2m[36m(func pid=79815)[0m     differentiable: False
[2m[36m(func pid=79815)[0m     foreach: None
[2m[36m(func pid=79815)[0m     lr: 0.01
[2m[36m(func pid=79815)[0m     maximize: False
[2m[36m(func pid=79815)[0m     momentum: 0.99
[2m[36m(func pid=79815)[0m     nesterov: False
[2m[36m(func pid=79815)[0m     weight_decay: 0.0001
[2m[36m(func pid=79815)[0m )
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5928 | Steps: 2 | Val loss: 0.4550 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 09:43:18 (running for 00:18:39.82)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.265 |  0.141 |                   98 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.593 |  0.178 |                   46 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.45  |  0.157 |                   46 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69319)[0m rmse: 0.17765209078788757
[2m[36m(func pid=69319)[0m mae:  0.12993940711021423
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.101, 0.191, 0.285, 0.141, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2572 | Steps: 2 | Val loss: 0.2632 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4591 | Steps: 2 | Val loss: 0.3802 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8914 | Steps: 2 | Val loss: 0.6854 | Batch size: 32 | lr: 0.01 | Duration: 4.39s
[2m[36m(func pid=57346)[0m rmse: 0.13993525505065918
[2m[36m(func pid=57346)[0m mae:  0.09179776161909103
[2m[36m(func pid=57346)[0m rmse_per_class: [0.075, 0.249, 0.027, 0.276, 0.064, 0.153, 0.211, 0.105, 0.13, 0.109]
[2m[36m(func pid=57346)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.15635882318019867
[2m[36m(func pid=69905)[0m mae:  0.1110859364271164
[2m[36m(func pid=69905)[0m rmse_per_class: [0.111, 0.247, 0.055, 0.304, 0.057, 0.185, 0.234, 0.132, 0.14, 0.099]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5821 | Steps: 2 | Val loss: 0.4496 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=79815)[0m rmse: 0.1825832575559616
[2m[36m(func pid=79815)[0m mae:  0.13426315784454346
[2m[36m(func pid=79815)[0m rmse_per_class: [0.115, 0.267, 0.111, 0.339, 0.11, 0.191, 0.294, 0.144, 0.143, 0.112]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=57346)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2581 | Steps: 2 | Val loss: 0.2633 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4557 | Steps: 2 | Val loss: 0.3817 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=69319)[0m rmse: 0.1776021420955658
[2m[36m(func pid=69319)[0m mae:  0.12989068031311035
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.333, 0.101, 0.191, 0.286, 0.14, 0.143, 0.11]
== Status ==
Current time: 2024-01-07 09:43:24 (running for 00:18:45.47)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00006 | RUNNING    | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.257 |  0.14  |                   99 |
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.582 |  0.178 |                   47 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.459 |  0.156 |                   47 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.891 |  0.183 |                    1 |
| train_32e5a_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8390 | Steps: 2 | Val loss: 0.6312 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=57346)[0m rmse: 0.13990464806556702
[2m[36m(func pid=57346)[0m mae:  0.0917089581489563
[2m[36m(func pid=57346)[0m rmse_per_class: [0.075, 0.248, 0.027, 0.276, 0.064, 0.152, 0.21, 0.105, 0.13, 0.111]
[2m[36m(func pid=69905)[0m rmse: 0.15577390789985657
[2m[36m(func pid=69905)[0m mae:  0.11045706272125244
[2m[36m(func pid=69905)[0m rmse_per_class: [0.11, 0.247, 0.054, 0.303, 0.056, 0.185, 0.233, 0.132, 0.14, 0.099]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5755 | Steps: 2 | Val loss: 0.4434 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=79815)[0m rmse: 0.1814052015542984
[2m[36m(func pid=79815)[0m mae:  0.13331389427185059
[2m[36m(func pid=79815)[0m rmse_per_class: [0.114, 0.267, 0.109, 0.338, 0.109, 0.191, 0.29, 0.141, 0.143, 0.112]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4575 | Steps: 2 | Val loss: 0.3819 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=69319)[0m rmse: 0.1774894744157791
[2m[36m(func pid=69319)[0m mae:  0.1297924816608429
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.333, 0.101, 0.191, 0.285, 0.14, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7493 | Steps: 2 | Val loss: 0.5530 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=69905)[0m rmse: 0.15501336753368378
[2m[36m(func pid=69905)[0m mae:  0.10969910770654678
[2m[36m(func pid=69905)[0m rmse_per_class: [0.109, 0.246, 0.052, 0.301, 0.056, 0.184, 0.233, 0.131, 0.14, 0.098]
[2m[36m(func pid=79815)[0m rmse: 0.18015523254871368
[2m[36m(func pid=79815)[0m mae:  0.13206325471401215
[2m[36m(func pid=79815)[0m rmse_per_class: [0.113, 0.266, 0.11, 0.337, 0.106, 0.191, 0.285, 0.141, 0.143, 0.111]
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5675 | Steps: 2 | Val loss: 0.4376 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 09:43:29 (running for 00:18:51.01)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.575 |  0.177 |                   48 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.456 |  0.156 |                   48 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.839 |  0.181 |                    2 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=80805)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=80805)[0m Configuration completed!
[2m[36m(func pid=80805)[0m New optimizer parameters:
[2m[36m(func pid=80805)[0m SGD (
[2m[36m(func pid=80805)[0m Parameter Group 0
[2m[36m(func pid=80805)[0m     dampening: 0
[2m[36m(func pid=80805)[0m     differentiable: False
[2m[36m(func pid=80805)[0m     foreach: None
[2m[36m(func pid=80805)[0m     lr: 0.1
[2m[36m(func pid=80805)[0m     maximize: False
[2m[36m(func pid=80805)[0m     momentum: 0.99
[2m[36m(func pid=80805)[0m     nesterov: False
[2m[36m(func pid=80805)[0m     weight_decay: 0.0001
[2m[36m(func pid=80805)[0m )
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:43:35 (running for 00:18:56.51)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.568 |  0.177 |                   49 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.458 |  0.155 |                   49 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.749 |  0.18  |                    3 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69319)[0m rmse: 0.1773761510848999
[2m[36m(func pid=69319)[0m mae:  0.1296912133693695
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.333, 0.1, 0.191, 0.285, 0.14, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4627 | Steps: 2 | Val loss: 0.3818 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6409 | Steps: 2 | Val loss: 0.4691 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5611 | Steps: 2 | Val loss: 0.4316 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8366 | Steps: 2 | Val loss: 0.5093 | Batch size: 32 | lr: 0.1 | Duration: 4.94s
[2m[36m(func pid=79815)[0m rmse: 0.17902015149593353
[2m[36m(func pid=79815)[0m mae:  0.1309908777475357
[2m[36m(func pid=79815)[0m rmse_per_class: [0.113, 0.265, 0.106, 0.335, 0.104, 0.19, 0.282, 0.14, 0.143, 0.111]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.15461833775043488
[2m[36m(func pid=69905)[0m mae:  0.10919886827468872
[2m[36m(func pid=69905)[0m rmse_per_class: [0.108, 0.245, 0.051, 0.301, 0.056, 0.184, 0.233, 0.131, 0.14, 0.098]
[2m[36m(func pid=69905)[0m 
== Status ==
Current time: 2024-01-07 09:43:40 (running for 00:19:01.81)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.561 |  0.177 |                   50 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.463 |  0.155 |                   50 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.641 |  0.179 |                    4 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |        |        |                      |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69319)[0m rmse: 0.17726781964302063
[2m[36m(func pid=69319)[0m mae:  0.129595547914505
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.333, 0.1, 0.191, 0.284, 0.14, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.18033117055892944
[2m[36m(func pid=80805)[0m mae:  0.1320260763168335
[2m[36m(func pid=80805)[0m rmse_per_class: [0.108, 0.266, 0.117, 0.337, 0.103, 0.19, 0.283, 0.145, 0.144, 0.11]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5381 | Steps: 2 | Val loss: 0.4018 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4519 | Steps: 2 | Val loss: 0.3825 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5548 | Steps: 2 | Val loss: 0.4257 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5230 | Steps: 2 | Val loss: 0.3349 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=79815)[0m rmse: 0.17791025340557098
[2m[36m(func pid=79815)[0m mae:  0.13004173338413239
[2m[36m(func pid=79815)[0m rmse_per_class: [0.115, 0.265, 0.103, 0.332, 0.1, 0.19, 0.28, 0.139, 0.143, 0.111]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.15413543581962585
[2m[36m(func pid=69905)[0m mae:  0.10865800082683563
[2m[36m(func pid=69905)[0m rmse_per_class: [0.108, 0.245, 0.05, 0.3, 0.055, 0.183, 0.233, 0.131, 0.14, 0.097]
[2m[36m(func pid=69905)[0m 
== Status ==
Current time: 2024-01-07 09:43:46 (running for 00:19:07.50)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.555 |  0.177 |                   51 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.452 |  0.154 |                   51 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.538 |  0.178 |                    5 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.837 |  0.18  |                    1 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69319)[0m rmse: 0.1771582067012787
[2m[36m(func pid=69319)[0m mae:  0.1294921487569809
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.333, 0.099, 0.191, 0.284, 0.14, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.1761145442724228
[2m[36m(func pid=80805)[0m mae:  0.1286715567111969
[2m[36m(func pid=80805)[0m rmse_per_class: [0.113, 0.266, 0.105, 0.331, 0.087, 0.189, 0.274, 0.142, 0.143, 0.11]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4605 | Steps: 2 | Val loss: 0.3521 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4537 | Steps: 2 | Val loss: 0.3829 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5452 | Steps: 2 | Val loss: 0.4203 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=79815)[0m rmse: 0.17637380957603455
[2m[36m(func pid=79815)[0m mae:  0.12876392900943756
[2m[36m(func pid=79815)[0m rmse_per_class: [0.116, 0.265, 0.1, 0.33, 0.094, 0.19, 0.278, 0.138, 0.143, 0.11]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4009 | Steps: 2 | Val loss: 0.3188 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=69905)[0m rmse: 0.15368621051311493
[2m[36m(func pid=69905)[0m mae:  0.10821183025836945
[2m[36m(func pid=69905)[0m rmse_per_class: [0.107, 0.244, 0.049, 0.299, 0.055, 0.183, 0.233, 0.131, 0.139, 0.096]
[2m[36m(func pid=69905)[0m 
== Status ==
Current time: 2024-01-07 09:43:52 (running for 00:19:13.16)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.545 |  0.177 |                   52 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.454 |  0.154 |                   52 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.46  |  0.176 |                    6 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.523 |  0.176 |                    2 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69319)[0m rmse: 0.17708171904087067
[2m[36m(func pid=69319)[0m mae:  0.12940613925457
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.332, 0.099, 0.191, 0.284, 0.14, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.1693945974111557
[2m[36m(func pid=80805)[0m mae:  0.12306282669305801
[2m[36m(func pid=80805)[0m rmse_per_class: [0.118, 0.265, 0.086, 0.321, 0.072, 0.19, 0.256, 0.136, 0.143, 0.106]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4137 | Steps: 2 | Val loss: 0.3213 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4529 | Steps: 2 | Val loss: 0.3834 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=79815)[0m rmse: 0.17444120347499847
[2m[36m(func pid=79815)[0m mae:  0.12710878252983093
[2m[36m(func pid=79815)[0m rmse_per_class: [0.117, 0.266, 0.095, 0.326, 0.088, 0.19, 0.273, 0.138, 0.142, 0.109]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5397 | Steps: 2 | Val loss: 0.4146 | Batch size: 32 | lr: 0.0001 | Duration: 3.43s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4749 | Steps: 2 | Val loss: 0.3749 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
[2m[36m(func pid=69905)[0m rmse: 0.15316203236579895
[2m[36m(func pid=69905)[0m mae:  0.10760830342769623
[2m[36m(func pid=69905)[0m rmse_per_class: [0.106, 0.244, 0.047, 0.298, 0.055, 0.183, 0.234, 0.13, 0.139, 0.095]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.16013208031654358
[2m[36m(func pid=80805)[0m mae:  0.11485759913921356
[2m[36m(func pid=80805)[0m rmse_per_class: [0.119, 0.256, 0.064, 0.309, 0.059, 0.191, 0.238, 0.126, 0.143, 0.098]
== Status ==
Current time: 2024-01-07 09:43:58 (running for 00:19:19.14)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.545 |  0.177 |                   52 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.453 |  0.153 |                   53 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.414 |  0.174 |                    7 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.475 |  0.16  |                    4 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69319)[0m rmse: 0.17698971927165985
[2m[36m(func pid=69319)[0m mae:  0.1293153315782547
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.332, 0.099, 0.191, 0.284, 0.14, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3971 | Steps: 2 | Val loss: 0.3101 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4520 | Steps: 2 | Val loss: 0.3829 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=79815)[0m rmse: 0.17235662043094635
[2m[36m(func pid=79815)[0m mae:  0.12528428435325623
[2m[36m(func pid=79815)[0m rmse_per_class: [0.117, 0.266, 0.09, 0.322, 0.081, 0.19, 0.269, 0.137, 0.142, 0.109]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5485 | Steps: 2 | Val loss: 0.4320 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5337 | Steps: 2 | Val loss: 0.4096 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=69905)[0m rmse: 0.1527041494846344
[2m[36m(func pid=69905)[0m mae:  0.10710906982421875
[2m[36m(func pid=69905)[0m rmse_per_class: [0.105, 0.244, 0.046, 0.298, 0.055, 0.182, 0.234, 0.129, 0.139, 0.095]
[2m[36m(func pid=69905)[0m 
== Status ==
Current time: 2024-01-07 09:44:03 (running for 00:19:24.54)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.54  |  0.177 |                   53 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.452 |  0.153 |                   54 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.397 |  0.172 |                    8 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.549 |  0.151 |                    5 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4029 | Steps: 2 | Val loss: 0.3145 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=80805)[0m rmse: 0.15115337073802948
[2m[36m(func pid=80805)[0m mae:  0.10487176477909088
[2m[36m(func pid=80805)[0m rmse_per_class: [0.109, 0.249, 0.043, 0.285, 0.054, 0.191, 0.232, 0.124, 0.136, 0.088]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17690269649028778
[2m[36m(func pid=69319)[0m mae:  0.1292344629764557
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.332, 0.099, 0.191, 0.283, 0.14, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4533 | Steps: 2 | Val loss: 0.3826 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=79815)[0m rmse: 0.1700381338596344
[2m[36m(func pid=79815)[0m mae:  0.12323278188705444
[2m[36m(func pid=79815)[0m rmse_per_class: [0.117, 0.266, 0.085, 0.318, 0.076, 0.19, 0.263, 0.136, 0.141, 0.109]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5712 | Steps: 2 | Val loss: 0.4692 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5285 | Steps: 2 | Val loss: 0.4052 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=69905)[0m rmse: 0.15238119661808014
[2m[36m(func pid=69905)[0m mae:  0.10667278617620468
[2m[36m(func pid=69905)[0m rmse_per_class: [0.105, 0.243, 0.046, 0.298, 0.055, 0.182, 0.235, 0.129, 0.139, 0.094]
[2m[36m(func pid=69905)[0m 
== Status ==
Current time: 2024-01-07 09:44:08 (running for 00:19:29.95)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.534 |  0.177 |                   54 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.453 |  0.152 |                   55 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.403 |  0.17  |                    9 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.571 |  0.148 |                    6 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4203 | Steps: 2 | Val loss: 0.3285 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=80805)[0m rmse: 0.1475735753774643
[2m[36m(func pid=80805)[0m mae:  0.09747295081615448
[2m[36m(func pid=80805)[0m rmse_per_class: [0.095, 0.247, 0.041, 0.263, 0.055, 0.198, 0.238, 0.122, 0.133, 0.085]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17675134539604187
[2m[36m(func pid=69319)[0m mae:  0.12912917137145996
[2m[36m(func pid=69319)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.332, 0.099, 0.191, 0.283, 0.139, 0.143, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4494 | Steps: 2 | Val loss: 0.3813 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=79815)[0m rmse: 0.16717343032360077
[2m[36m(func pid=79815)[0m mae:  0.12069541215896606
[2m[36m(func pid=79815)[0m rmse_per_class: [0.116, 0.264, 0.079, 0.313, 0.07, 0.19, 0.255, 0.135, 0.141, 0.109]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5411 | Steps: 2 | Val loss: 0.4587 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5192 | Steps: 2 | Val loss: 0.4003 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=69905)[0m rmse: 0.1520194262266159
[2m[36m(func pid=69905)[0m mae:  0.10615358501672745
[2m[36m(func pid=69905)[0m rmse_per_class: [0.103, 0.243, 0.045, 0.298, 0.054, 0.181, 0.235, 0.129, 0.138, 0.093]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4421 | Steps: 2 | Val loss: 0.3480 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=80805)[0m rmse: 0.1557798534631729
[2m[36m(func pid=80805)[0m mae:  0.09975379705429077
[2m[36m(func pid=80805)[0m rmse_per_class: [0.09, 0.246, 0.038, 0.283, 0.055, 0.209, 0.297, 0.121, 0.133, 0.086]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:44:14 (running for 00:19:35.19)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.528 |  0.177 |                   55 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.449 |  0.152 |                   56 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.42  |  0.167 |                   10 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.541 |  0.156 |                    7 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69319)[0m rmse: 0.1767166554927826
[2m[36m(func pid=69319)[0m mae:  0.12910184264183044
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.332, 0.098, 0.191, 0.283, 0.139, 0.143, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4495 | Steps: 2 | Val loss: 0.3802 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=79815)[0m rmse: 0.1641935110092163
[2m[36m(func pid=79815)[0m mae:  0.11805444955825806
[2m[36m(func pid=79815)[0m rmse_per_class: [0.116, 0.262, 0.072, 0.309, 0.065, 0.191, 0.246, 0.133, 0.141, 0.108]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4919 | Steps: 2 | Val loss: 0.4295 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=69905)[0m rmse: 0.15163688361644745
[2m[36m(func pid=69905)[0m mae:  0.10552811622619629
[2m[36m(func pid=69905)[0m rmse_per_class: [0.102, 0.242, 0.044, 0.297, 0.054, 0.181, 0.237, 0.129, 0.138, 0.093]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5144 | Steps: 2 | Val loss: 0.3952 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 09:44:19 (running for 00:19:40.39)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.519 |  0.177 |                   56 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.45  |  0.152 |                   57 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.442 |  0.164 |                   11 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.492 |  0.173 |                    8 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=80805)[0m rmse: 0.17317184805870056
[2m[36m(func pid=80805)[0m mae:  0.11028041690587997
[2m[36m(func pid=80805)[0m rmse_per_class: [0.099, 0.254, 0.038, 0.337, 0.056, 0.213, 0.39, 0.123, 0.135, 0.088]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4693 | Steps: 2 | Val loss: 0.3698 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=69319)[0m rmse: 0.17659859359264374
[2m[36m(func pid=69319)[0m mae:  0.12898966670036316
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.094, 0.331, 0.098, 0.191, 0.283, 0.139, 0.143, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4423 | Steps: 2 | Val loss: 0.3789 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=79815)[0m rmse: 0.1609097421169281
[2m[36m(func pid=79815)[0m mae:  0.11503811925649643
[2m[36m(func pid=79815)[0m rmse_per_class: [0.114, 0.258, 0.066, 0.305, 0.061, 0.191, 0.237, 0.132, 0.14, 0.106]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4157 | Steps: 2 | Val loss: 0.5165 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=69905)[0m rmse: 0.15143708884716034
[2m[36m(func pid=69905)[0m mae:  0.10516785085201263
[2m[36m(func pid=69905)[0m rmse_per_class: [0.101, 0.241, 0.043, 0.298, 0.054, 0.181, 0.238, 0.128, 0.137, 0.092]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5105 | Steps: 2 | Val loss: 0.3904 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4913 | Steps: 2 | Val loss: 0.3921 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:44:24 (running for 00:19:45.70)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.514 |  0.177 |                   57 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.442 |  0.151 |                   58 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.469 |  0.161 |                   12 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.416 |  0.194 |                    9 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=80805)[0m rmse: 0.19405271112918854
[2m[36m(func pid=80805)[0m mae:  0.12099780887365341
[2m[36m(func pid=80805)[0m rmse_per_class: [0.099, 0.265, 0.05, 0.361, 0.056, 0.215, 0.55, 0.118, 0.137, 0.09]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.1764555275440216
[2m[36m(func pid=69319)[0m mae:  0.12885478138923645
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.094, 0.331, 0.098, 0.191, 0.282, 0.139, 0.143, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4412 | Steps: 2 | Val loss: 0.3781 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=79815)[0m rmse: 0.15784789621829987
[2m[36m(func pid=79815)[0m mae:  0.11211679875850677
[2m[36m(func pid=79815)[0m rmse_per_class: [0.112, 0.254, 0.059, 0.301, 0.057, 0.19, 0.23, 0.131, 0.14, 0.104]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3637 | Steps: 2 | Val loss: 0.6048 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=69905)[0m rmse: 0.15114109218120575
[2m[36m(func pid=69905)[0m mae:  0.10479570925235748
[2m[36m(func pid=69905)[0m rmse_per_class: [0.101, 0.241, 0.042, 0.298, 0.054, 0.181, 0.239, 0.128, 0.137, 0.091]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5021 | Steps: 2 | Val loss: 0.3857 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5148 | Steps: 2 | Val loss: 0.4113 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 09:44:29 (running for 00:19:51.04)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.511 |  0.176 |                   58 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.441 |  0.151 |                   59 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.491 |  0.158 |                   13 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.364 |  0.204 |                   10 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=80805)[0m rmse: 0.2037467658519745
[2m[36m(func pid=80805)[0m mae:  0.12640324234962463
[2m[36m(func pid=80805)[0m rmse_per_class: [0.105, 0.276, 0.052, 0.366, 0.056, 0.213, 0.614, 0.126, 0.138, 0.092]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4371 | Steps: 2 | Val loss: 0.3762 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=69319)[0m rmse: 0.176385760307312
[2m[36m(func pid=69319)[0m mae:  0.12879350781440735
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.094, 0.331, 0.097, 0.191, 0.282, 0.139, 0.143, 0.109]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15496590733528137
[2m[36m(func pid=79815)[0m mae:  0.10929135978221893
[2m[36m(func pid=79815)[0m rmse_per_class: [0.11, 0.251, 0.052, 0.297, 0.056, 0.19, 0.224, 0.13, 0.139, 0.101]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3567 | Steps: 2 | Val loss: 0.5618 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=69905)[0m rmse: 0.15081845223903656
[2m[36m(func pid=69905)[0m mae:  0.10436101257801056
[2m[36m(func pid=69905)[0m rmse_per_class: [0.1, 0.241, 0.042, 0.298, 0.054, 0.18, 0.239, 0.127, 0.137, 0.09]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4958 | Steps: 2 | Val loss: 0.3814 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5267 | Steps: 2 | Val loss: 0.4290 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:44:35 (running for 00:19:56.22)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.502 |  0.176 |                   59 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.437 |  0.151 |                   60 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.515 |  0.155 |                   14 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.357 |  0.2   |                   11 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=80805)[0m rmse: 0.19975410401821136
[2m[36m(func pid=80805)[0m mae:  0.1227044016122818
[2m[36m(func pid=80805)[0m rmse_per_class: [0.103, 0.272, 0.055, 0.359, 0.056, 0.204, 0.581, 0.138, 0.137, 0.091]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4296 | Steps: 2 | Val loss: 0.3737 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=69319)[0m rmse: 0.17628547549247742
[2m[36m(func pid=69319)[0m mae:  0.12871159613132477
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.094, 0.331, 0.097, 0.191, 0.282, 0.139, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15276604890823364
[2m[36m(func pid=79815)[0m mae:  0.1071036085486412
[2m[36m(func pid=79815)[0m rmse_per_class: [0.109, 0.249, 0.046, 0.293, 0.055, 0.189, 0.221, 0.13, 0.138, 0.098]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3477 | Steps: 2 | Val loss: 0.4560 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=69905)[0m rmse: 0.15062782168388367
[2m[36m(func pid=69905)[0m mae:  0.10399377346038818
[2m[36m(func pid=69905)[0m rmse_per_class: [0.098, 0.242, 0.041, 0.298, 0.054, 0.18, 0.24, 0.126, 0.137, 0.09]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4900 | Steps: 2 | Val loss: 0.3771 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5386 | Steps: 2 | Val loss: 0.4433 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 09:44:40 (running for 00:20:01.57)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.496 |  0.176 |                   60 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.43  |  0.151 |                   61 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.527 |  0.153 |                   15 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.348 |  0.194 |                   12 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=80805)[0m rmse: 0.19367077946662903
[2m[36m(func pid=80805)[0m mae:  0.11724384874105453
[2m[36m(func pid=80805)[0m rmse_per_class: [0.139, 0.264, 0.052, 0.357, 0.056, 0.196, 0.499, 0.15, 0.136, 0.088]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4239 | Steps: 2 | Val loss: 0.3718 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=69319)[0m rmse: 0.1761264055967331
[2m[36m(func pid=69319)[0m mae:  0.1285664141178131
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.094, 0.33, 0.096, 0.191, 0.281, 0.139, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15050306916236877
[2m[36m(func pid=79815)[0m mae:  0.10465061664581299
[2m[36m(func pid=79815)[0m rmse_per_class: [0.107, 0.246, 0.041, 0.287, 0.054, 0.188, 0.218, 0.131, 0.137, 0.094]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3399 | Steps: 2 | Val loss: 0.3743 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=69905)[0m rmse: 0.15046992897987366
[2m[36m(func pid=69905)[0m mae:  0.10356219857931137
[2m[36m(func pid=69905)[0m rmse_per_class: [0.097, 0.241, 0.04, 0.298, 0.054, 0.181, 0.242, 0.126, 0.136, 0.089]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4869 | Steps: 2 | Val loss: 0.3729 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5375 | Steps: 2 | Val loss: 0.4539 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 09:44:45 (running for 00:20:06.98)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.49  |  0.176 |                   61 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.424 |  0.15  |                   62 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.539 |  0.151 |                   16 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.34  |  0.185 |                   13 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=80805)[0m rmse: 0.1849958300590515
[2m[36m(func pid=80805)[0m mae:  0.11360148340463638
[2m[36m(func pid=80805)[0m rmse_per_class: [0.133, 0.263, 0.054, 0.349, 0.056, 0.193, 0.404, 0.178, 0.135, 0.085]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4303 | Steps: 2 | Val loss: 0.3691 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=69319)[0m rmse: 0.17605334520339966
[2m[36m(func pid=69319)[0m mae:  0.1284918338060379
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.094, 0.33, 0.096, 0.191, 0.281, 0.139, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.14837300777435303
[2m[36m(func pid=79815)[0m mae:  0.101936936378479
[2m[36m(func pid=79815)[0m rmse_per_class: [0.104, 0.244, 0.039, 0.281, 0.054, 0.188, 0.215, 0.13, 0.136, 0.091]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3276 | Steps: 2 | Val loss: 0.3422 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=69905)[0m rmse: 0.1502232849597931
[2m[36m(func pid=69905)[0m mae:  0.10318511724472046
[2m[36m(func pid=69905)[0m rmse_per_class: [0.097, 0.241, 0.04, 0.299, 0.054, 0.181, 0.242, 0.125, 0.136, 0.088]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4806 | Steps: 2 | Val loss: 0.3693 | Batch size: 32 | lr: 0.0001 | Duration: 3.27s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.5355 | Steps: 2 | Val loss: 0.4597 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 09:44:51 (running for 00:20:12.46)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.487 |  0.176 |                   62 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.43  |  0.15  |                   63 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.537 |  0.148 |                   17 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.328 |  0.177 |                   14 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=80805)[0m rmse: 0.17664919793605804
[2m[36m(func pid=80805)[0m mae:  0.1102292537689209
[2m[36m(func pid=80805)[0m rmse_per_class: [0.101, 0.269, 0.06, 0.339, 0.056, 0.179, 0.336, 0.207, 0.138, 0.082]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4175 | Steps: 2 | Val loss: 0.3659 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=79815)[0m rmse: 0.1473744660615921
[2m[36m(func pid=79815)[0m mae:  0.09972012042999268
[2m[36m(func pid=79815)[0m rmse_per_class: [0.101, 0.242, 0.038, 0.279, 0.054, 0.19, 0.217, 0.129, 0.135, 0.088]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17598262429237366
[2m[36m(func pid=69319)[0m mae:  0.1284061074256897
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.094, 0.33, 0.096, 0.191, 0.281, 0.139, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.1502637267112732
[2m[36m(func pid=69905)[0m mae:  0.10303571075201035
[2m[36m(func pid=69905)[0m rmse_per_class: [0.096, 0.241, 0.039, 0.299, 0.054, 0.181, 0.244, 0.124, 0.136, 0.088]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3083 | Steps: 2 | Val loss: 0.3432 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.5297 | Steps: 2 | Val loss: 0.4611 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4781 | Steps: 2 | Val loss: 0.3657 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=80805)[0m rmse: 0.17306122183799744
[2m[36m(func pid=80805)[0m mae:  0.10842114686965942
[2m[36m(func pid=80805)[0m rmse_per_class: [0.097, 0.271, 0.052, 0.331, 0.056, 0.17, 0.285, 0.247, 0.138, 0.083]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:44:56 (running for 00:20:17.88)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.481 |  0.176 |                   63 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.417 |  0.15  |                   64 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.535 |  0.147 |                   18 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.308 |  0.173 |                   15 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4138 | Steps: 2 | Val loss: 0.3633 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=79815)[0m rmse: 0.14811693131923676
[2m[36m(func pid=79815)[0m mae:  0.09878886491060257
[2m[36m(func pid=79815)[0m rmse_per_class: [0.098, 0.24, 0.038, 0.282, 0.055, 0.191, 0.229, 0.128, 0.134, 0.086]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17582565546035767
[2m[36m(func pid=69319)[0m mae:  0.12828630208969116
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.093, 0.33, 0.095, 0.191, 0.281, 0.139, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.15024280548095703
[2m[36m(func pid=69905)[0m mae:  0.10277388244867325
[2m[36m(func pid=69905)[0m rmse_per_class: [0.095, 0.241, 0.039, 0.3, 0.054, 0.181, 0.245, 0.124, 0.135, 0.088]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3054 | Steps: 2 | Val loss: 0.3366 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5428 | Steps: 2 | Val loss: 0.4598 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4722 | Steps: 2 | Val loss: 0.3622 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 09:45:02 (running for 00:20:23.23)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.478 |  0.176 |                   64 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.414 |  0.15  |                   65 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.53  |  0.148 |                   19 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.305 |  0.167 |                   16 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=80805)[0m rmse: 0.16663199663162231
[2m[36m(func pid=80805)[0m mae:  0.10535520315170288
[2m[36m(func pid=80805)[0m rmse_per_class: [0.094, 0.262, 0.047, 0.323, 0.056, 0.165, 0.247, 0.234, 0.149, 0.09]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4111 | Steps: 2 | Val loss: 0.3605 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=79815)[0m rmse: 0.1498596966266632
[2m[36m(func pid=79815)[0m mae:  0.0988682210445404
[2m[36m(func pid=79815)[0m rmse_per_class: [0.096, 0.239, 0.038, 0.289, 0.055, 0.191, 0.245, 0.126, 0.134, 0.085]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.1757318079471588
[2m[36m(func pid=69319)[0m mae:  0.12822012603282928
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.093, 0.329, 0.095, 0.191, 0.281, 0.139, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.15020069479942322
[2m[36m(func pid=69905)[0m mae:  0.10247929394245148
[2m[36m(func pid=69905)[0m rmse_per_class: [0.094, 0.24, 0.038, 0.301, 0.054, 0.181, 0.247, 0.123, 0.135, 0.088]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3361 | Steps: 2 | Val loss: 0.3819 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5047 | Steps: 2 | Val loss: 0.4533 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4653 | Steps: 2 | Val loss: 0.3587 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 09:45:07 (running for 00:20:28.67)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.472 |  0.176 |                   65 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.411 |  0.15  |                   66 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.543 |  0.15  |                   20 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.336 |  0.184 |                   17 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=80805)[0m rmse: 0.18443724513053894
[2m[36m(func pid=80805)[0m mae:  0.11252856254577637
[2m[36m(func pid=80805)[0m rmse_per_class: [0.099, 0.266, 0.139, 0.331, 0.056, 0.188, 0.237, 0.271, 0.16, 0.098]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4048 | Steps: 2 | Val loss: 0.3569 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=79815)[0m rmse: 0.15258827805519104
[2m[36m(func pid=79815)[0m mae:  0.09983642399311066
[2m[36m(func pid=79815)[0m rmse_per_class: [0.093, 0.239, 0.038, 0.298, 0.055, 0.191, 0.268, 0.125, 0.134, 0.085]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.1756366789340973
[2m[36m(func pid=69319)[0m mae:  0.12813805043697357
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.093, 0.329, 0.095, 0.191, 0.28, 0.139, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.1501692831516266
[2m[36m(func pid=69905)[0m mae:  0.10234628617763519
[2m[36m(func pid=69905)[0m rmse_per_class: [0.093, 0.24, 0.038, 0.302, 0.054, 0.181, 0.248, 0.123, 0.135, 0.087]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3058 | Steps: 2 | Val loss: 0.3901 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4929 | Steps: 2 | Val loss: 0.4415 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4648 | Steps: 2 | Val loss: 0.3559 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 09:45:13 (running for 00:20:34.16)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.465 |  0.176 |                   66 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.405 |  0.15  |                   67 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.505 |  0.153 |                   21 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.306 |  0.183 |                   18 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=80805)[0m rmse: 0.18343304097652435
[2m[36m(func pid=80805)[0m mae:  0.11444330215454102
[2m[36m(func pid=80805)[0m rmse_per_class: [0.092, 0.269, 0.05, 0.321, 0.056, 0.219, 0.243, 0.318, 0.158, 0.108]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4017 | Steps: 2 | Val loss: 0.3534 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=79815)[0m rmse: 0.15501448512077332
[2m[36m(func pid=79815)[0m mae:  0.1008790135383606
[2m[36m(func pid=79815)[0m rmse_per_class: [0.09, 0.239, 0.035, 0.307, 0.055, 0.19, 0.288, 0.125, 0.135, 0.085]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.175619438290596
[2m[36m(func pid=69319)[0m mae:  0.12811803817749023
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.093, 0.329, 0.094, 0.191, 0.28, 0.139, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.15012803673744202
[2m[36m(func pid=69905)[0m mae:  0.10213299095630646
[2m[36m(func pid=69905)[0m rmse_per_class: [0.093, 0.24, 0.038, 0.303, 0.054, 0.181, 0.25, 0.122, 0.135, 0.087]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.2864 | Steps: 2 | Val loss: 0.3721 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4678 | Steps: 2 | Val loss: 0.4256 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4597 | Steps: 2 | Val loss: 0.3529 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3937 | Steps: 2 | Val loss: 0.3494 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 09:45:18 (running for 00:20:39.77)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.465 |  0.176 |                   67 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.402 |  0.15  |                   68 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.493 |  0.155 |                   22 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.286 |  0.179 |                   19 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=80805)[0m rmse: 0.1793706715106964
[2m[36m(func pid=80805)[0m mae:  0.10945849120616913
[2m[36m(func pid=80805)[0m rmse_per_class: [0.106, 0.273, 0.049, 0.305, 0.056, 0.22, 0.245, 0.304, 0.138, 0.098]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15668635070323944
[2m[36m(func pid=79815)[0m mae:  0.1016610711812973
[2m[36m(func pid=79815)[0m rmse_per_class: [0.089, 0.239, 0.032, 0.315, 0.055, 0.19, 0.302, 0.124, 0.135, 0.085]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.1499985158443451
[2m[36m(func pid=69905)[0m mae:  0.10204925388097763
[2m[36m(func pid=69905)[0m rmse_per_class: [0.093, 0.24, 0.037, 0.304, 0.054, 0.181, 0.25, 0.121, 0.135, 0.087]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17552092671394348
[2m[36m(func pid=69319)[0m mae:  0.12802399694919586
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.093, 0.329, 0.094, 0.191, 0.28, 0.139, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.2847 | Steps: 2 | Val loss: 0.3599 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4637 | Steps: 2 | Val loss: 0.4072 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3891 | Steps: 2 | Val loss: 0.3455 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 09:45:23 (running for 00:20:44.94)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.46  |  0.176 |                   68 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.394 |  0.15  |                   69 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.468 |  0.157 |                   23 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.285 |  0.178 |                   20 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=80805)[0m rmse: 0.17795245349407196
[2m[36m(func pid=80805)[0m mae:  0.10554978996515274
[2m[36m(func pid=80805)[0m rmse_per_class: [0.109, 0.276, 0.049, 0.347, 0.055, 0.199, 0.243, 0.263, 0.138, 0.1]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4530 | Steps: 2 | Val loss: 0.3495 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=79815)[0m rmse: 0.1572594940662384
[2m[36m(func pid=79815)[0m mae:  0.10189364850521088
[2m[36m(func pid=79815)[0m rmse_per_class: [0.089, 0.239, 0.028, 0.321, 0.055, 0.19, 0.307, 0.122, 0.136, 0.085]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.14972227811813354
[2m[36m(func pid=69905)[0m mae:  0.10180624574422836
[2m[36m(func pid=69905)[0m rmse_per_class: [0.092, 0.24, 0.037, 0.304, 0.054, 0.18, 0.25, 0.12, 0.135, 0.087]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17534536123275757
[2m[36m(func pid=69319)[0m mae:  0.12787984311580658
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.259, 0.093, 0.329, 0.093, 0.191, 0.28, 0.139, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.2813 | Steps: 2 | Val loss: 0.3574 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4223 | Steps: 2 | Val loss: 0.3853 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3902 | Steps: 2 | Val loss: 0.3416 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=80805)[0m rmse: 0.17590466141700745
[2m[36m(func pid=80805)[0m mae:  0.10208549350500107
[2m[36m(func pid=80805)[0m rmse_per_class: [0.11, 0.273, 0.049, 0.374, 0.054, 0.186, 0.241, 0.222, 0.138, 0.112]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:45:29 (running for 00:20:50.41)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.453 |  0.175 |                   69 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.389 |  0.15  |                   70 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.464 |  0.157 |                   24 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.281 |  0.176 |                   21 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4520 | Steps: 2 | Val loss: 0.3473 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=79815)[0m rmse: 0.15841862559318542
[2m[36m(func pid=79815)[0m mae:  0.10227010399103165
[2m[36m(func pid=79815)[0m rmse_per_class: [0.089, 0.24, 0.026, 0.325, 0.055, 0.191, 0.316, 0.121, 0.135, 0.085]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=69905)[0m rmse: 0.14955416321754456
[2m[36m(func pid=69905)[0m mae:  0.10163756459951401
[2m[36m(func pid=69905)[0m rmse_per_class: [0.092, 0.239, 0.036, 0.304, 0.054, 0.179, 0.25, 0.12, 0.135, 0.087]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.1753445565700531
[2m[36m(func pid=69319)[0m mae:  0.1278880536556244
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.093, 0.329, 0.093, 0.191, 0.28, 0.139, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.2990 | Steps: 2 | Val loss: 0.3358 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4064 | Steps: 2 | Val loss: 0.3665 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3872 | Steps: 2 | Val loss: 0.3373 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 09:45:34 (running for 00:20:55.85)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.452 |  0.175 |                   70 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.39  |  0.15  |                   71 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.406 |  0.16  |                   26 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.281 |  0.176 |                   21 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79815)[0m rmse: 0.15951022505760193
[2m[36m(func pid=79815)[0m mae:  0.10266143083572388
[2m[36m(func pid=79815)[0m rmse_per_class: [0.087, 0.241, 0.025, 0.328, 0.056, 0.193, 0.322, 0.12, 0.135, 0.087]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.173160120844841
[2m[36m(func pid=80805)[0m mae:  0.10011889040470123
[2m[36m(func pid=80805)[0m rmse_per_class: [0.102, 0.269, 0.049, 0.334, 0.054, 0.186, 0.236, 0.21, 0.139, 0.153]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4472 | Steps: 2 | Val loss: 0.3445 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=69905)[0m rmse: 0.14909256994724274
[2m[36m(func pid=69905)[0m mae:  0.10120014101266861
[2m[36m(func pid=69905)[0m rmse_per_class: [0.091, 0.239, 0.036, 0.303, 0.054, 0.179, 0.249, 0.119, 0.134, 0.086]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.1753162145614624
[2m[36m(func pid=69319)[0m mae:  0.12785734236240387
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.26, 0.093, 0.328, 0.093, 0.191, 0.28, 0.139, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3891 | Steps: 2 | Val loss: 0.3475 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2696 | Steps: 2 | Val loss: 0.3447 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3821 | Steps: 2 | Val loss: 0.3330 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=79815)[0m rmse: 0.1596362292766571
[2m[36m(func pid=79815)[0m mae:  0.10279589891433716
[2m[36m(func pid=79815)[0m rmse_per_class: [0.088, 0.242, 0.025, 0.329, 0.056, 0.194, 0.318, 0.119, 0.136, 0.089]
[2m[36m(func pid=79815)[0m 
== Status ==
Current time: 2024-01-07 09:45:40 (running for 00:21:01.19)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.447 |  0.175 |                   71 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.387 |  0.149 |                   72 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.389 |  0.16  |                   27 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.299 |  0.173 |                   22 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=80805)[0m rmse: 0.18021388351917267
[2m[36m(func pid=80805)[0m mae:  0.1093958392739296
[2m[36m(func pid=80805)[0m rmse_per_class: [0.103, 0.27, 0.033, 0.318, 0.055, 0.175, 0.241, 0.151, 0.212, 0.243]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4457 | Steps: 2 | Val loss: 0.3419 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=69905)[0m rmse: 0.14870600402355194
[2m[36m(func pid=69905)[0m mae:  0.10084065049886703
[2m[36m(func pid=69905)[0m rmse_per_class: [0.091, 0.239, 0.036, 0.302, 0.054, 0.178, 0.249, 0.118, 0.134, 0.086]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3622 | Steps: 2 | Val loss: 0.3307 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=69319)[0m rmse: 0.1751875877380371
[2m[36m(func pid=69319)[0m mae:  0.12775246798992157
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.259, 0.092, 0.328, 0.092, 0.191, 0.279, 0.139, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2782 | Steps: 2 | Val loss: 0.4767 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 09:45:45 (running for 00:21:06.46)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.446 |  0.175 |                   72 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.382 |  0.149 |                   73 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.362 |  0.159 |                   28 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.27  |  0.18  |                   23 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=79815)[0m rmse: 0.15908892452716827
[2m[36m(func pid=79815)[0m mae:  0.10255970060825348
[2m[36m(func pid=79815)[0m rmse_per_class: [0.09, 0.243, 0.026, 0.329, 0.056, 0.194, 0.309, 0.118, 0.137, 0.091]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3799 | Steps: 2 | Val loss: 0.3284 | Batch size: 32 | lr: 0.001 | Duration: 3.18s
[2m[36m(func pid=80805)[0m rmse: 0.23071280121803284
[2m[36m(func pid=80805)[0m mae:  0.14492115378379822
[2m[36m(func pid=80805)[0m rmse_per_class: [0.118, 0.278, 0.093, 0.341, 0.055, 0.204, 0.306, 0.151, 0.262, 0.499]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4407 | Steps: 2 | Val loss: 0.3397 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=69905)[0m rmse: 0.14835771918296814
[2m[36m(func pid=69905)[0m mae:  0.10068067163228989
[2m[36m(func pid=69905)[0m rmse_per_class: [0.091, 0.24, 0.035, 0.302, 0.054, 0.177, 0.247, 0.117, 0.134, 0.086]
[2m[36m(func pid=69905)[0m 
[2m[36m(func pid=69319)[0m rmse: 0.17503248155117035
[2m[36m(func pid=69319)[0m mae:  0.1276131570339203
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.259, 0.092, 0.328, 0.092, 0.191, 0.279, 0.139, 0.143, 0.11]
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3497 | Steps: 2 | Val loss: 0.3171 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2776 | Steps: 2 | Val loss: 0.5417 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 09:45:50 (running for 00:21:11.97)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: -0.14474999904632568
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.441 |  0.175 |                   73 |
| train_32e5a_00009 | RUNNING    | 192.168.7.53:69905 | 0.001  |       0.99 |         0.0001 |  0.38  |  0.148 |                   74 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.35  |  0.158 |                   29 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.278 |  0.231 |                   24 |
| train_32e5a_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=69905)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3789 | Steps: 2 | Val loss: 0.3241 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=79815)[0m rmse: 0.15826579928398132
[2m[36m(func pid=79815)[0m mae:  0.10217289626598358
[2m[36m(func pid=79815)[0m rmse_per_class: [0.091, 0.244, 0.026, 0.328, 0.056, 0.192, 0.299, 0.118, 0.137, 0.091]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.24062196910381317
[2m[36m(func pid=80805)[0m mae:  0.15334489941596985
[2m[36m(func pid=80805)[0m rmse_per_class: [0.122, 0.275, 0.106, 0.356, 0.055, 0.217, 0.319, 0.153, 0.278, 0.526]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4384 | Steps: 2 | Val loss: 0.3375 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=69905)[0m rmse: 0.14794668555259705
[2m[36m(func pid=69905)[0m mae:  0.10043513774871826
[2m[36m(func pid=69905)[0m rmse_per_class: [0.091, 0.239, 0.035, 0.302, 0.054, 0.176, 0.245, 0.117, 0.134, 0.086]
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3399 | Steps: 2 | Val loss: 0.3058 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=69319)[0m rmse: 0.1749567687511444
[2m[36m(func pid=69319)[0m mae:  0.12754669785499573
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.259, 0.092, 0.328, 0.092, 0.191, 0.279, 0.139, 0.143, 0.11]
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2831 | Steps: 2 | Val loss: 0.4394 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=69319)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15686173737049103
[2m[36m(func pid=79815)[0m mae:  0.10153476148843765
[2m[36m(func pid=79815)[0m rmse_per_class: [0.093, 0.244, 0.026, 0.327, 0.056, 0.188, 0.289, 0.116, 0.138, 0.091]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.21063026785850525
[2m[36m(func pid=80805)[0m mae:  0.1246567964553833
[2m[36m(func pid=80805)[0m rmse_per_class: [0.121, 0.278, 0.089, 0.356, 0.056, 0.216, 0.235, 0.21, 0.299, 0.247]
[2m[36m(func pid=69319)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4355 | Steps: 2 | Val loss: 0.3354 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3494 | Steps: 2 | Val loss: 0.2992 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=69319)[0m rmse: 0.17481276392936707
[2m[36m(func pid=69319)[0m mae:  0.1274266242980957
[2m[36m(func pid=69319)[0m rmse_per_class: [0.117, 0.259, 0.092, 0.327, 0.091, 0.191, 0.279, 0.139, 0.143, 0.11]
[2m[36m(func pid=79815)[0m rmse: 0.15594351291656494
[2m[36m(func pid=79815)[0m mae:  0.10114596784114838
[2m[36m(func pid=79815)[0m rmse_per_class: [0.095, 0.245, 0.026, 0.328, 0.056, 0.187, 0.281, 0.115, 0.138, 0.089]
== Status ==
Current time: 2024-01-07 09:45:56 (running for 00:21:17.26)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.1459999978542328
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.438 |  0.175 |                   74 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.34  |  0.157 |                   30 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.278 |  0.241 |                   25 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=86819)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=86819)[0m Configuration completed!
[2m[36m(func pid=86819)[0m New optimizer parameters:
[2m[36m(func pid=86819)[0m SGD (
[2m[36m(func pid=86819)[0m Parameter Group 0
[2m[36m(func pid=86819)[0m     dampening: 0
[2m[36m(func pid=86819)[0m     differentiable: False
[2m[36m(func pid=86819)[0m     foreach: None
[2m[36m(func pid=86819)[0m     lr: 0.0001
[2m[36m(func pid=86819)[0m     maximize: False
[2m[36m(func pid=86819)[0m     momentum: 0.9
[2m[36m(func pid=86819)[0m     nesterov: False
[2m[36m(func pid=86819)[0m     weight_decay: 0.0001
[2m[36m(func pid=86819)[0m )
[2m[36m(func pid=86819)[0m 
== Status ==
Current time: 2024-01-07 09:46:01 (running for 00:21:22.72)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: -0.1459999978542328
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00008 | RUNNING    | 192.168.7.53:69319 | 0.0001 |       0.99 |         0.0001 |  0.438 |  0.175 |                   74 |
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.34  |  0.157 |                   30 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.283 |  0.211 |                   26 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2793 | Steps: 2 | Val loss: 0.3846 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3290 | Steps: 2 | Val loss: 0.2920 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8987 | Steps: 2 | Val loss: 0.7106 | Batch size: 32 | lr: 0.0001 | Duration: 4.99s
[2m[36m(func pid=79815)[0m rmse: 0.15406306087970734
[2m[36m(func pid=79815)[0m mae:  0.1001986712217331
[2m[36m(func pid=79815)[0m rmse_per_class: [0.096, 0.244, 0.026, 0.327, 0.056, 0.182, 0.27, 0.113, 0.139, 0.089]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.1951165348291397
[2m[36m(func pid=80805)[0m mae:  0.115300253033638
[2m[36m(func pid=80805)[0m rmse_per_class: [0.123, 0.302, 0.075, 0.35, 0.061, 0.202, 0.232, 0.212, 0.175, 0.217]
[2m[36m(func pid=86819)[0m rmse: 0.1827744096517563
[2m[36m(func pid=86819)[0m mae:  0.13447943329811096
[2m[36m(func pid=86819)[0m rmse_per_class: [0.116, 0.267, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3294 | Steps: 2 | Val loss: 0.2868 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 09:46:07 (running for 00:21:28.10)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.329 |  0.154 |                   32 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.283 |  0.211 |                   26 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=87355)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=87355)[0m Configuration completed!
[2m[36m(func pid=87355)[0m New optimizer parameters:
[2m[36m(func pid=87355)[0m SGD (
[2m[36m(func pid=87355)[0m Parameter Group 0
[2m[36m(func pid=87355)[0m     dampening: 0
[2m[36m(func pid=87355)[0m     differentiable: False
[2m[36m(func pid=87355)[0m     foreach: None
[2m[36m(func pid=87355)[0m     lr: 0.001
[2m[36m(func pid=87355)[0m     maximize: False
[2m[36m(func pid=87355)[0m     momentum: 0.9
[2m[36m(func pid=87355)[0m     nesterov: False
[2m[36m(func pid=87355)[0m     weight_decay: 0.0001
[2m[36m(func pid=87355)[0m )
[2m[36m(func pid=87355)[0m 
== Status ==
Current time: 2024-01-07 09:46:12 (running for 00:21:33.26)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.329 |  0.152 |                   33 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.279 |  0.195 |                   27 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.899 |  0.183 |                    1 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=79815)[0m rmse: 0.15243127942085266
[2m[36m(func pid=79815)[0m mae:  0.0993504673242569
[2m[36m(func pid=79815)[0m rmse_per_class: [0.096, 0.245, 0.026, 0.324, 0.056, 0.177, 0.262, 0.112, 0.139, 0.088]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2799 | Steps: 2 | Val loss: 0.3842 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8934 | Steps: 2 | Val loss: 0.7080 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8964 | Steps: 2 | Val loss: 0.7073 | Batch size: 32 | lr: 0.001 | Duration: 4.74s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3150 | Steps: 2 | Val loss: 0.2836 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=80805)[0m rmse: 0.1949204057455063
[2m[36m(func pid=80805)[0m mae:  0.1163438931107521
[2m[36m(func pid=80805)[0m rmse_per_class: [0.137, 0.334, 0.046, 0.347, 0.063, 0.202, 0.242, 0.219, 0.159, 0.201]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.18268552422523499
[2m[36m(func pid=86819)[0m mae:  0.13444674015045166
[2m[36m(func pid=86819)[0m rmse_per_class: [0.116, 0.267, 0.108, 0.339, 0.112, 0.19, 0.294, 0.144, 0.143, 0.113]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.18268439173698425
[2m[36m(func pid=87355)[0m mae:  0.13442684710025787
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=87355)[0m 
== Status ==
Current time: 2024-01-07 09:46:17 (running for 00:21:38.64)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.315 |  0.151 |                   34 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.28  |  0.195 |                   28 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.893 |  0.183 |                    2 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.896 |  0.183 |                    1 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=79815)[0m rmse: 0.15126550197601318
[2m[36m(func pid=79815)[0m mae:  0.0988340973854065
[2m[36m(func pid=79815)[0m rmse_per_class: [0.097, 0.246, 0.025, 0.321, 0.056, 0.173, 0.257, 0.111, 0.139, 0.088]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2937 | Steps: 2 | Val loss: 0.4174 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8955 | Steps: 2 | Val loss: 0.7038 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8915 | Steps: 2 | Val loss: 0.7004 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3150 | Steps: 2 | Val loss: 0.2824 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=80805)[0m rmse: 0.20294280350208282
[2m[36m(func pid=80805)[0m mae:  0.12069883197546005
[2m[36m(func pid=80805)[0m rmse_per_class: [0.137, 0.399, 0.033, 0.355, 0.061, 0.207, 0.243, 0.241, 0.157, 0.196]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.1823076605796814
[2m[36m(func pid=86819)[0m mae:  0.13417163491249084
[2m[36m(func pid=86819)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.113, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.18251748383045197
[2m[36m(func pid=87355)[0m mae:  0.13433000445365906
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.267, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.112]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15068577229976654
[2m[36m(func pid=79815)[0m mae:  0.09840051829814911
[2m[36m(func pid=79815)[0m rmse_per_class: [0.096, 0.246, 0.026, 0.317, 0.056, 0.169, 0.262, 0.11, 0.138, 0.088]
[2m[36m(func pid=79815)[0m 
== Status ==
Current time: 2024-01-07 09:46:22 (running for 00:21:43.80)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.315 |  0.151 |                   35 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.294 |  0.203 |                   29 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.896 |  0.182 |                    3 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.891 |  0.183 |                    2 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2846 | Steps: 2 | Val loss: 0.4806 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8950 | Steps: 2 | Val loss: 0.7005 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8824 | Steps: 2 | Val loss: 0.6894 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3192 | Steps: 2 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=80805)[0m rmse: 0.21191847324371338
[2m[36m(func pid=80805)[0m mae:  0.12485464662313461
[2m[36m(func pid=80805)[0m rmse_per_class: [0.122, 0.467, 0.049, 0.368, 0.061, 0.209, 0.249, 0.247, 0.171, 0.174]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.18200001120567322
[2m[36m(func pid=86819)[0m mae:  0.1339341551065445
[2m[36m(func pid=86819)[0m rmse_per_class: [0.116, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.18216128647327423
[2m[36m(func pid=87355)[0m mae:  0.13406413793563843
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15067875385284424
[2m[36m(func pid=79815)[0m mae:  0.09805627167224884
[2m[36m(func pid=79815)[0m rmse_per_class: [0.093, 0.247, 0.026, 0.313, 0.056, 0.166, 0.268, 0.111, 0.137, 0.09]
== Status ==
Current time: 2024-01-07 09:46:27 (running for 00:21:49.00)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.319 |  0.151 |                   36 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.285 |  0.212 |                   30 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.895 |  0.182 |                    4 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.882 |  0.182 |                    3 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2763 | Steps: 2 | Val loss: 0.4714 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8935 | Steps: 2 | Val loss: 0.6972 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8719 | Steps: 2 | Val loss: 0.6771 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3195 | Steps: 2 | Val loss: 0.2851 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=80805)[0m rmse: 0.21462932229042053
[2m[36m(func pid=80805)[0m mae:  0.12726452946662903
[2m[36m(func pid=80805)[0m rmse_per_class: [0.121, 0.456, 0.049, 0.366, 0.061, 0.208, 0.265, 0.177, 0.184, 0.26]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.1815566122531891
[2m[36m(func pid=86819)[0m mae:  0.13357970118522644
[2m[36m(func pid=86819)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.338, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.1818021535873413
[2m[36m(func pid=87355)[0m mae:  0.13377684354782104
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.338, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
== Status ==
Current time: 2024-01-07 09:46:32 (running for 00:21:54.01)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.319 |  0.151 |                   36 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.276 |  0.215 |                   31 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.894 |  0.182 |                    5 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.872 |  0.182 |                    4 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15128374099731445
[2m[36m(func pid=79815)[0m mae:  0.09817924350500107
[2m[36m(func pid=79815)[0m rmse_per_class: [0.092, 0.247, 0.027, 0.31, 0.056, 0.165, 0.274, 0.113, 0.138, 0.092]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2719 | Steps: 2 | Val loss: 0.4087 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8917 | Steps: 2 | Val loss: 0.6953 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8546 | Steps: 2 | Val loss: 0.6639 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3252 | Steps: 2 | Val loss: 0.2863 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=80805)[0m rmse: 0.20593181252479553
[2m[36m(func pid=80805)[0m mae:  0.12159315496683121
[2m[36m(func pid=80805)[0m rmse_per_class: [0.152, 0.372, 0.05, 0.358, 0.061, 0.202, 0.256, 0.18, 0.184, 0.243]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.1811475306749344
[2m[36m(func pid=86819)[0m mae:  0.13324275612831116
[2m[36m(func pid=86819)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.112, 0.19, 0.294, 0.142, 0.142, 0.11]
[2m[36m(func pid=86819)[0m 
== Status ==
Current time: 2024-01-07 09:46:38 (running for 00:21:59.24)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.32  |  0.151 |                   37 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.272 |  0.206 |                   32 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.892 |  0.181 |                    6 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.855 |  0.181 |                    5 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.18135099112987518
[2m[36m(func pid=87355)[0m mae:  0.13340619206428528
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.112, 0.19, 0.293, 0.142, 0.142, 0.111]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.1518789529800415
[2m[36m(func pid=79815)[0m mae:  0.09848353266716003
[2m[36m(func pid=79815)[0m rmse_per_class: [0.091, 0.248, 0.029, 0.306, 0.056, 0.164, 0.273, 0.118, 0.139, 0.095]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2784 | Steps: 2 | Val loss: 0.3893 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8883 | Steps: 2 | Val loss: 0.6943 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8421 | Steps: 2 | Val loss: 0.6502 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3187 | Steps: 2 | Val loss: 0.2873 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=80805)[0m rmse: 0.201586052775383
[2m[36m(func pid=80805)[0m mae:  0.11839987337589264
[2m[36m(func pid=80805)[0m rmse_per_class: [0.204, 0.292, 0.049, 0.361, 0.059, 0.195, 0.244, 0.186, 0.22, 0.205]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.18082842230796814
[2m[36m(func pid=86819)[0m mae:  0.13295862078666687
[2m[36m(func pid=86819)[0m rmse_per_class: [0.116, 0.264, 0.103, 0.338, 0.111, 0.19, 0.293, 0.142, 0.142, 0.11]
[2m[36m(func pid=86819)[0m 
== Status ==
Current time: 2024-01-07 09:46:43 (running for 00:22:04.61)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.325 |  0.152 |                   38 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.278 |  0.202 |                   33 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.888 |  0.181 |                    7 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.842 |  0.181 |                    6 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.18092955648899078
[2m[36m(func pid=87355)[0m mae:  0.13305559754371643
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.111, 0.19, 0.293, 0.141, 0.142, 0.11]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15287987887859344
[2m[36m(func pid=79815)[0m mae:  0.0989922359585762
[2m[36m(func pid=79815)[0m rmse_per_class: [0.091, 0.248, 0.035, 0.304, 0.056, 0.163, 0.269, 0.123, 0.14, 0.1]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2680 | Steps: 2 | Val loss: 0.4069 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8865 | Steps: 2 | Val loss: 0.6928 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8241 | Steps: 2 | Val loss: 0.6359 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3256 | Steps: 2 | Val loss: 0.2866 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=80805)[0m rmse: 0.20356270670890808
[2m[36m(func pid=80805)[0m mae:  0.11862187087535858
[2m[36m(func pid=80805)[0m rmse_per_class: [0.284, 0.242, 0.049, 0.364, 0.058, 0.194, 0.234, 0.203, 0.251, 0.157]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.1805296242237091
[2m[36m(func pid=86819)[0m mae:  0.13269980251789093
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.264, 0.102, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=86819)[0m 
== Status ==
Current time: 2024-01-07 09:46:48 (running for 00:22:09.93)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.319 |  0.153 |                   39 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.268 |  0.204 |                   34 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.887 |  0.181 |                    8 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.824 |  0.18  |                    7 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.18044234812259674
[2m[36m(func pid=87355)[0m mae:  0.1326354444026947
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.11]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15328386425971985
[2m[36m(func pid=79815)[0m mae:  0.09925521165132523
[2m[36m(func pid=79815)[0m rmse_per_class: [0.091, 0.248, 0.045, 0.298, 0.056, 0.164, 0.259, 0.129, 0.141, 0.103]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2670 | Steps: 2 | Val loss: 0.4222 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8829 | Steps: 2 | Val loss: 0.6916 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8062 | Steps: 2 | Val loss: 0.6229 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3230 | Steps: 2 | Val loss: 0.2876 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=80805)[0m rmse: 0.2072276771068573
[2m[36m(func pid=80805)[0m mae:  0.11994244158267975
[2m[36m(func pid=80805)[0m rmse_per_class: [0.33, 0.242, 0.048, 0.363, 0.058, 0.198, 0.229, 0.216, 0.233, 0.156]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:46:53 (running for 00:22:15.00)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.326 |  0.153 |                   40 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.267 |  0.207 |                   35 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.883 |  0.18  |                    9 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.824 |  0.18  |                    7 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=86819)[0m rmse: 0.1803089827299118
[2m[36m(func pid=86819)[0m mae:  0.1324930191040039
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.263, 0.102, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.18012909591197968
[2m[36m(func pid=87355)[0m mae:  0.13236942887306213
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.263, 0.102, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15383446216583252
[2m[36m(func pid=79815)[0m mae:  0.10002939403057098
[2m[36m(func pid=79815)[0m rmse_per_class: [0.092, 0.248, 0.047, 0.293, 0.055, 0.165, 0.253, 0.137, 0.141, 0.106]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2616 | Steps: 2 | Val loss: 0.4101 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8807 | Steps: 2 | Val loss: 0.6903 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7856 | Steps: 2 | Val loss: 0.6111 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3100 | Steps: 2 | Val loss: 0.2894 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=80805)[0m rmse: 0.20478515326976776
[2m[36m(func pid=80805)[0m mae:  0.1185002475976944
[2m[36m(func pid=80805)[0m rmse_per_class: [0.31, 0.243, 0.047, 0.359, 0.059, 0.201, 0.227, 0.224, 0.216, 0.161]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:46:59 (running for 00:22:20.41)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.323 |  0.154 |                   41 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.262 |  0.205 |                   36 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.883 |  0.18  |                    9 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.786 |  0.18  |                    9 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.17983749508857727
[2m[36m(func pid=87355)[0m mae:  0.13211047649383545
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.263, 0.101, 0.337, 0.108, 0.19, 0.292, 0.14, 0.142, 0.11]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.18013693392276764
[2m[36m(func pid=86819)[0m mae:  0.1323467195034027
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.263, 0.101, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15506145358085632
[2m[36m(func pid=79815)[0m mae:  0.10080967098474503
[2m[36m(func pid=79815)[0m rmse_per_class: [0.093, 0.248, 0.048, 0.29, 0.055, 0.166, 0.25, 0.148, 0.142, 0.111]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2747 | Steps: 2 | Val loss: 0.3850 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7662 | Steps: 2 | Val loss: 0.5977 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8802 | Steps: 2 | Val loss: 0.6892 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3280 | Steps: 2 | Val loss: 0.2922 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=80805)[0m rmse: 0.19789883494377136
[2m[36m(func pid=80805)[0m mae:  0.11431501060724258
[2m[36m(func pid=80805)[0m rmse_per_class: [0.252, 0.246, 0.047, 0.35, 0.059, 0.202, 0.226, 0.232, 0.205, 0.16]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:47:04 (running for 00:22:25.69)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.31  |  0.155 |                   42 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.275 |  0.198 |                   37 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.881 |  0.18  |                   10 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.766 |  0.18  |                   10 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.1797391176223755
[2m[36m(func pid=87355)[0m mae:  0.13200406730175018
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.263, 0.101, 0.337, 0.107, 0.19, 0.291, 0.141, 0.143, 0.11]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17999713122844696
[2m[36m(func pid=86819)[0m mae:  0.13223667442798615
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.1569887399673462
[2m[36m(func pid=79815)[0m mae:  0.10150705277919769
[2m[36m(func pid=79815)[0m rmse_per_class: [0.092, 0.249, 0.048, 0.29, 0.055, 0.168, 0.248, 0.163, 0.141, 0.115]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2679 | Steps: 2 | Val loss: 0.3656 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7493 | Steps: 2 | Val loss: 0.5858 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8783 | Steps: 2 | Val loss: 0.6878 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3298 | Steps: 2 | Val loss: 0.2958 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=80805)[0m rmse: 0.1905176043510437
[2m[36m(func pid=80805)[0m mae:  0.10919580608606339
[2m[36m(func pid=80805)[0m rmse_per_class: [0.185, 0.254, 0.046, 0.339, 0.058, 0.203, 0.226, 0.233, 0.187, 0.174]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:47:09 (running for 00:22:30.95)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.328 |  0.157 |                   43 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.268 |  0.191 |                   38 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.88  |  0.18  |                   11 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.749 |  0.18  |                   11 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.17962589859962463
[2m[36m(func pid=87355)[0m mae:  0.1318671554327011
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.262, 0.101, 0.336, 0.106, 0.19, 0.29, 0.141, 0.143, 0.11]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17991775274276733
[2m[36m(func pid=86819)[0m mae:  0.13215725123882294
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15968212485313416
[2m[36m(func pid=79815)[0m mae:  0.10237350314855576
[2m[36m(func pid=79815)[0m rmse_per_class: [0.091, 0.251, 0.048, 0.292, 0.055, 0.171, 0.247, 0.183, 0.142, 0.117]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2607 | Steps: 2 | Val loss: 0.3611 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7312 | Steps: 2 | Val loss: 0.5738 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3038 | Steps: 2 | Val loss: 0.3035 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8771 | Steps: 2 | Val loss: 0.6857 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=80805)[0m rmse: 0.18968893587589264
[2m[36m(func pid=80805)[0m mae:  0.10876744985580444
[2m[36m(func pid=80805)[0m rmse_per_class: [0.137, 0.27, 0.045, 0.335, 0.056, 0.202, 0.23, 0.23, 0.169, 0.222]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:47:15 (running for 00:22:36.37)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.33  |  0.16  |                   44 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.261 |  0.19  |                   39 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.878 |  0.18  |                   12 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.731 |  0.18  |                   12 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.17954383790493011
[2m[36m(func pid=87355)[0m mae:  0.13175150752067566
[2m[36m(func pid=87355)[0m rmse_per_class: [0.117, 0.262, 0.101, 0.336, 0.105, 0.19, 0.29, 0.141, 0.143, 0.111]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.164369598031044
[2m[36m(func pid=79815)[0m mae:  0.1040436178445816
[2m[36m(func pid=79815)[0m rmse_per_class: [0.089, 0.254, 0.047, 0.299, 0.055, 0.174, 0.248, 0.213, 0.142, 0.124]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.1798146665096283
[2m[36m(func pid=86819)[0m mae:  0.13207128643989563
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.11, 0.19, 0.294, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2816 | Steps: 2 | Val loss: 0.3693 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7169 | Steps: 2 | Val loss: 0.5608 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3113 | Steps: 2 | Val loss: 0.3124 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8730 | Steps: 2 | Val loss: 0.6842 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=80805)[0m rmse: 0.19256024062633514
[2m[36m(func pid=80805)[0m mae:  0.11193720251321793
[2m[36m(func pid=80805)[0m rmse_per_class: [0.119, 0.287, 0.043, 0.338, 0.056, 0.2, 0.237, 0.222, 0.154, 0.27]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:47:20 (running for 00:22:41.53)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.304 |  0.164 |                   45 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.282 |  0.193 |                   40 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.877 |  0.18  |                   13 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.717 |  0.179 |                   13 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.17938852310180664
[2m[36m(func pid=87355)[0m mae:  0.13159754872322083
[2m[36m(func pid=87355)[0m rmse_per_class: [0.117, 0.262, 0.1, 0.336, 0.104, 0.19, 0.289, 0.141, 0.143, 0.111]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.16907958686351776
[2m[36m(func pid=79815)[0m mae:  0.10627344995737076
[2m[36m(func pid=79815)[0m rmse_per_class: [0.088, 0.256, 0.046, 0.305, 0.055, 0.174, 0.249, 0.24, 0.144, 0.134]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17977432906627655
[2m[36m(func pid=86819)[0m mae:  0.13200944662094116
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2626 | Steps: 2 | Val loss: 0.3806 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.6976 | Steps: 2 | Val loss: 0.5473 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3056 | Steps: 2 | Val loss: 0.3158 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8704 | Steps: 2 | Val loss: 0.6831 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=80805)[0m rmse: 0.19554224610328674
[2m[36m(func pid=80805)[0m mae:  0.1149270161986351
[2m[36m(func pid=80805)[0m rmse_per_class: [0.11, 0.305, 0.04, 0.341, 0.056, 0.2, 0.24, 0.228, 0.147, 0.288]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:47:25 (running for 00:22:46.72)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.311 |  0.169 |                   46 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.263 |  0.196 |                   41 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.873 |  0.18  |                   14 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.698 |  0.179 |                   14 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.1792021095752716
[2m[36m(func pid=87355)[0m mae:  0.13141176104545593
[2m[36m(func pid=87355)[0m rmse_per_class: [0.117, 0.262, 0.099, 0.336, 0.104, 0.19, 0.288, 0.141, 0.143, 0.112]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.17120009660720825
[2m[36m(func pid=79815)[0m mae:  0.10775484889745712
[2m[36m(func pid=79815)[0m rmse_per_class: [0.087, 0.257, 0.042, 0.308, 0.055, 0.174, 0.252, 0.247, 0.146, 0.144]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17970430850982666
[2m[36m(func pid=86819)[0m mae:  0.13194043934345245
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2902 | Steps: 2 | Val loss: 0.3884 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.6817 | Steps: 2 | Val loss: 0.5346 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2978 | Steps: 2 | Val loss: 0.3121 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8674 | Steps: 2 | Val loss: 0.6810 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=80805)[0m rmse: 0.19645924866199493
[2m[36m(func pid=80805)[0m mae:  0.11523598432540894
[2m[36m(func pid=80805)[0m rmse_per_class: [0.111, 0.32, 0.037, 0.345, 0.056, 0.199, 0.232, 0.234, 0.159, 0.271]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:47:30 (running for 00:22:51.96)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.306 |  0.171 |                   47 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.29  |  0.196 |                   42 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.87  |  0.18  |                   15 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.682 |  0.179 |                   15 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.17907099425792694
[2m[36m(func pid=87355)[0m mae:  0.13126006722450256
[2m[36m(func pid=87355)[0m rmse_per_class: [0.117, 0.262, 0.099, 0.336, 0.103, 0.19, 0.288, 0.141, 0.143, 0.112]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.17065386474132538
[2m[36m(func pid=79815)[0m mae:  0.10783028602600098
[2m[36m(func pid=79815)[0m rmse_per_class: [0.088, 0.257, 0.039, 0.305, 0.055, 0.173, 0.253, 0.24, 0.149, 0.146]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17963001132011414
[2m[36m(func pid=86819)[0m mae:  0.13188418745994568
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2618 | Steps: 2 | Val loss: 0.3912 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6681 | Steps: 2 | Val loss: 0.5220 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2950 | Steps: 2 | Val loss: 0.3062 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8663 | Steps: 2 | Val loss: 0.6794 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=80805)[0m rmse: 0.18944744765758514
[2m[36m(func pid=80805)[0m mae:  0.11025382578372955
[2m[36m(func pid=80805)[0m rmse_per_class: [0.118, 0.312, 0.038, 0.346, 0.057, 0.194, 0.215, 0.218, 0.202, 0.194]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:47:36 (running for 00:22:57.41)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.298 |  0.171 |                   48 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.262 |  0.189 |                   43 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.867 |  0.18  |                   16 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.668 |  0.179 |                   16 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.17887377738952637
[2m[36m(func pid=87355)[0m mae:  0.1310608685016632
[2m[36m(func pid=87355)[0m rmse_per_class: [0.117, 0.262, 0.098, 0.335, 0.103, 0.19, 0.287, 0.141, 0.143, 0.112]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.1691749393939972
[2m[36m(func pid=79815)[0m mae:  0.10721489042043686
[2m[36m(func pid=79815)[0m rmse_per_class: [0.088, 0.256, 0.04, 0.3, 0.055, 0.169, 0.256, 0.222, 0.152, 0.154]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17960162460803986
[2m[36m(func pid=86819)[0m mae:  0.1318577229976654
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2585 | Steps: 2 | Val loss: 0.3859 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6515 | Steps: 2 | Val loss: 0.5105 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2914 | Steps: 2 | Val loss: 0.3005 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8618 | Steps: 2 | Val loss: 0.6771 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=80805)[0m rmse: 0.18658781051635742
[2m[36m(func pid=80805)[0m mae:  0.10907623916864395
[2m[36m(func pid=80805)[0m rmse_per_class: [0.121, 0.308, 0.04, 0.346, 0.057, 0.195, 0.212, 0.189, 0.228, 0.17]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:47:41 (running for 00:23:02.66)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.295 |  0.169 |                   49 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.259 |  0.187 |                   44 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.866 |  0.18  |                   17 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.652 |  0.179 |                   17 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.1787690967321396
[2m[36m(func pid=87355)[0m mae:  0.13095450401306152
[2m[36m(func pid=87355)[0m rmse_per_class: [0.118, 0.262, 0.098, 0.335, 0.102, 0.19, 0.287, 0.141, 0.143, 0.112]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.16679981350898743
[2m[36m(func pid=79815)[0m mae:  0.10612261295318604
[2m[36m(func pid=79815)[0m rmse_per_class: [0.089, 0.254, 0.043, 0.294, 0.055, 0.164, 0.257, 0.205, 0.155, 0.153]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17956706881523132
[2m[36m(func pid=86819)[0m mae:  0.13183271884918213
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2582 | Steps: 2 | Val loss: 0.3664 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6371 | Steps: 2 | Val loss: 0.4993 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3004 | Steps: 2 | Val loss: 0.2961 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8610 | Steps: 2 | Val loss: 0.6752 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=80805)[0m rmse: 0.18287594616413116
[2m[36m(func pid=80805)[0m mae:  0.10797407478094101
[2m[36m(func pid=80805)[0m rmse_per_class: [0.121, 0.299, 0.044, 0.344, 0.057, 0.197, 0.21, 0.179, 0.212, 0.166]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17864497005939484
[2m[36m(func pid=87355)[0m mae:  0.13082052767276764
[2m[36m(func pid=87355)[0m rmse_per_class: [0.118, 0.262, 0.098, 0.335, 0.102, 0.19, 0.286, 0.141, 0.143, 0.112]
[2m[36m(func pid=87355)[0m 
== Status ==
Current time: 2024-01-07 09:47:47 (running for 00:23:08.06)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.291 |  0.167 |                   50 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.258 |  0.183 |                   45 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.862 |  0.18  |                   18 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.637 |  0.179 |                   18 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=79815)[0m rmse: 0.16438421607017517
[2m[36m(func pid=79815)[0m mae:  0.10501090437173843
[2m[36m(func pid=79815)[0m rmse_per_class: [0.09, 0.252, 0.045, 0.287, 0.054, 0.161, 0.258, 0.185, 0.16, 0.151]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17954103648662567
[2m[36m(func pid=86819)[0m mae:  0.13180525600910187
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2652 | Steps: 2 | Val loss: 0.3471 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6212 | Steps: 2 | Val loss: 0.4879 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2844 | Steps: 2 | Val loss: 0.2922 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=80805)[0m rmse: 0.17640233039855957
[2m[36m(func pid=80805)[0m mae:  0.10468169301748276
[2m[36m(func pid=80805)[0m rmse_per_class: [0.125, 0.282, 0.046, 0.338, 0.057, 0.201, 0.208, 0.161, 0.193, 0.153]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8592 | Steps: 2 | Val loss: 0.6732 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 09:47:52 (running for 00:23:13.48)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.3   |  0.164 |                   51 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.265 |  0.176 |                   46 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.861 |  0.18  |                   19 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.621 |  0.178 |                   19 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.17837992310523987
[2m[36m(func pid=87355)[0m mae:  0.13056230545043945
[2m[36m(func pid=87355)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.102, 0.19, 0.286, 0.141, 0.143, 0.112]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.16221681237220764
[2m[36m(func pid=79815)[0m mae:  0.10427067428827286
[2m[36m(func pid=79815)[0m rmse_per_class: [0.092, 0.251, 0.045, 0.283, 0.054, 0.16, 0.257, 0.171, 0.162, 0.146]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17956289649009705
[2m[36m(func pid=86819)[0m mae:  0.13183268904685974
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2533 | Steps: 2 | Val loss: 0.3374 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6116 | Steps: 2 | Val loss: 0.4772 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2844 | Steps: 2 | Val loss: 0.2906 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=80805)[0m rmse: 0.17213164269924164
[2m[36m(func pid=80805)[0m mae:  0.10206818580627441
[2m[36m(func pid=80805)[0m rmse_per_class: [0.131, 0.265, 0.047, 0.331, 0.057, 0.2, 0.212, 0.14, 0.194, 0.145]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8587 | Steps: 2 | Val loss: 0.6712 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 09:47:57 (running for 00:23:18.93)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.284 |  0.162 |                   52 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.253 |  0.172 |                   47 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.859 |  0.18  |                   20 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.612 |  0.178 |                   20 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.17820462584495544
[2m[36m(func pid=87355)[0m mae:  0.13039806485176086
[2m[36m(func pid=87355)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.102, 0.19, 0.286, 0.141, 0.143, 0.112]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.16154150664806366
[2m[36m(func pid=79815)[0m mae:  0.10422239452600479
[2m[36m(func pid=79815)[0m rmse_per_class: [0.094, 0.25, 0.046, 0.282, 0.053, 0.161, 0.258, 0.162, 0.164, 0.146]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17952623963356018
[2m[36m(func pid=86819)[0m mae:  0.1317957490682602
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2523 | Steps: 2 | Val loss: 0.3323 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5955 | Steps: 2 | Val loss: 0.4670 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2793 | Steps: 2 | Val loss: 0.2886 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=80805)[0m rmse: 0.17111049592494965
[2m[36m(func pid=80805)[0m mae:  0.10081905126571655
[2m[36m(func pid=80805)[0m rmse_per_class: [0.139, 0.26, 0.047, 0.323, 0.056, 0.196, 0.216, 0.126, 0.189, 0.159]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8550 | Steps: 2 | Val loss: 0.6695 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 09:48:03 (running for 00:23:24.19)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.284 |  0.162 |                   53 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.252 |  0.171 |                   48 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.859 |  0.18  |                   21 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.595 |  0.178 |                   21 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.17806681990623474
[2m[36m(func pid=87355)[0m mae:  0.13027098774909973
[2m[36m(func pid=87355)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.101, 0.191, 0.285, 0.141, 0.143, 0.111]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.16095498204231262
[2m[36m(func pid=79815)[0m mae:  0.10362396389245987
[2m[36m(func pid=79815)[0m rmse_per_class: [0.094, 0.249, 0.047, 0.279, 0.053, 0.163, 0.256, 0.161, 0.162, 0.146]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17953510582447052
[2m[36m(func pid=86819)[0m mae:  0.13180160522460938
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2642 | Steps: 2 | Val loss: 0.3352 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5877 | Steps: 2 | Val loss: 0.4568 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2823 | Steps: 2 | Val loss: 0.2872 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=80805)[0m rmse: 0.1741471141576767
[2m[36m(func pid=80805)[0m mae:  0.10195516049861908
[2m[36m(func pid=80805)[0m rmse_per_class: [0.151, 0.26, 0.048, 0.319, 0.057, 0.198, 0.214, 0.122, 0.187, 0.186]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8520 | Steps: 2 | Val loss: 0.6676 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 09:48:08 (running for 00:23:29.48)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.279 |  0.161 |                   54 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.264 |  0.174 |                   49 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.855 |  0.18  |                   22 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.588 |  0.178 |                   22 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.17780394852161407
[2m[36m(func pid=87355)[0m mae:  0.13003131747245789
[2m[36m(func pid=87355)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.333, 0.101, 0.191, 0.285, 0.14, 0.143, 0.111]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.16006380319595337
[2m[36m(func pid=79815)[0m mae:  0.10329239070415497
[2m[36m(func pid=79815)[0m rmse_per_class: [0.096, 0.248, 0.047, 0.278, 0.052, 0.165, 0.255, 0.151, 0.162, 0.146]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17955084145069122
[2m[36m(func pid=86819)[0m mae:  0.13180606067180634
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2506 | Steps: 2 | Val loss: 0.3368 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5749 | Steps: 2 | Val loss: 0.4475 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2698 | Steps: 2 | Val loss: 0.2851 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=80805)[0m rmse: 0.17669416964054108
[2m[36m(func pid=80805)[0m mae:  0.10360793769359589
[2m[36m(func pid=80805)[0m rmse_per_class: [0.154, 0.258, 0.046, 0.315, 0.058, 0.201, 0.211, 0.137, 0.178, 0.208]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.8487 | Steps: 2 | Val loss: 0.6664 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 09:48:14 (running for 00:23:35.11)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.282 |  0.16  |                   55 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.251 |  0.177 |                   50 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.852 |  0.18  |                   23 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.575 |  0.178 |                   23 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.17760786414146423
[2m[36m(func pid=87355)[0m mae:  0.1298803687095642
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.261, 0.096, 0.333, 0.101, 0.191, 0.284, 0.14, 0.143, 0.111]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.1589963436126709
[2m[36m(func pid=79815)[0m mae:  0.10234570503234863
[2m[36m(func pid=79815)[0m rmse_per_class: [0.097, 0.247, 0.047, 0.275, 0.052, 0.168, 0.252, 0.146, 0.161, 0.145]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2630 | Steps: 2 | Val loss: 0.3338 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=86819)[0m rmse: 0.1794845014810562
[2m[36m(func pid=86819)[0m mae:  0.13175222277641296
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5661 | Steps: 2 | Val loss: 0.4396 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2882 | Steps: 2 | Val loss: 0.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=80805)[0m rmse: 0.17728257179260254
[2m[36m(func pid=80805)[0m mae:  0.10384037345647812
[2m[36m(func pid=80805)[0m rmse_per_class: [0.156, 0.256, 0.049, 0.31, 0.059, 0.197, 0.211, 0.151, 0.176, 0.208]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.8469 | Steps: 2 | Val loss: 0.6636 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 09:48:19 (running for 00:23:40.39)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.27  |  0.159 |                   56 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.263 |  0.177 |                   51 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.849 |  0.179 |                   24 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.566 |  0.178 |                   24 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.17756564915180206
[2m[36m(func pid=87355)[0m mae:  0.12985089421272278
[2m[36m(func pid=87355)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.333, 0.1, 0.191, 0.284, 0.14, 0.143, 0.11]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15798676013946533
[2m[36m(func pid=79815)[0m mae:  0.10148316621780396
[2m[36m(func pid=79815)[0m rmse_per_class: [0.097, 0.246, 0.047, 0.273, 0.05, 0.172, 0.249, 0.144, 0.159, 0.143]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2535 | Steps: 2 | Val loss: 0.3286 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=86819)[0m rmse: 0.1794651746749878
[2m[36m(func pid=86819)[0m mae:  0.13175496459007263
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2775 | Steps: 2 | Val loss: 0.2821 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5587 | Steps: 2 | Val loss: 0.4312 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=80805)[0m rmse: 0.17610694468021393
[2m[36m(func pid=80805)[0m mae:  0.1033223420381546
[2m[36m(func pid=80805)[0m rmse_per_class: [0.152, 0.252, 0.044, 0.301, 0.059, 0.192, 0.214, 0.16, 0.176, 0.211]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.8447 | Steps: 2 | Val loss: 0.6618 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:48:24 (running for 00:23:45.67)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.288 |  0.158 |                   57 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.253 |  0.176 |                   52 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.847 |  0.179 |                   25 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.559 |  0.177 |                   25 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.17734450101852417
[2m[36m(func pid=87355)[0m mae:  0.1296524852514267
[2m[36m(func pid=87355)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.1, 0.191, 0.284, 0.14, 0.143, 0.11]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.1565476953983307
[2m[36m(func pid=79815)[0m mae:  0.10084738582372665
[2m[36m(func pid=79815)[0m rmse_per_class: [0.101, 0.245, 0.046, 0.272, 0.05, 0.173, 0.248, 0.131, 0.159, 0.141]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2512 | Steps: 2 | Val loss: 0.3222 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=86819)[0m rmse: 0.17947645485401154
[2m[36m(func pid=86819)[0m mae:  0.13176141679286957
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2653 | Steps: 2 | Val loss: 0.2815 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5468 | Steps: 2 | Val loss: 0.4240 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=80805)[0m rmse: 0.17373932898044586
[2m[36m(func pid=80805)[0m mae:  0.10191474109888077
[2m[36m(func pid=80805)[0m rmse_per_class: [0.147, 0.25, 0.039, 0.295, 0.059, 0.185, 0.216, 0.158, 0.177, 0.212]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.8415 | Steps: 2 | Val loss: 0.6601 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=79815)[0m rmse: 0.15591943264007568
[2m[36m(func pid=79815)[0m mae:  0.10016953945159912
[2m[36m(func pid=79815)[0m rmse_per_class: [0.103, 0.245, 0.044, 0.27, 0.049, 0.175, 0.245, 0.126, 0.157, 0.145]
== Status ==
Current time: 2024-01-07 09:48:29 (running for 00:23:50.88)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.265 |  0.156 |                   59 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.251 |  0.174 |                   53 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.845 |  0.179 |                   26 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.559 |  0.177 |                   25 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.1771913766860962
[2m[36m(func pid=87355)[0m mae:  0.1295059323310852
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.332, 0.099, 0.191, 0.284, 0.14, 0.143, 0.11]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2606 | Steps: 2 | Val loss: 0.3176 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=86819)[0m rmse: 0.17946161329746246
[2m[36m(func pid=86819)[0m mae:  0.13173893094062805
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2641 | Steps: 2 | Val loss: 0.2808 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5382 | Steps: 2 | Val loss: 0.4170 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=80805)[0m rmse: 0.17152728140354156
[2m[36m(func pid=80805)[0m mae:  0.10079667717218399
[2m[36m(func pid=80805)[0m rmse_per_class: [0.143, 0.248, 0.036, 0.293, 0.06, 0.182, 0.215, 0.147, 0.179, 0.212]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.8388 | Steps: 2 | Val loss: 0.6582 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=79815)[0m rmse: 0.15511979162693024
[2m[36m(func pid=79815)[0m mae:  0.09926016628742218
[2m[36m(func pid=79815)[0m rmse_per_class: [0.102, 0.245, 0.043, 0.268, 0.049, 0.176, 0.242, 0.122, 0.156, 0.147]
== Status ==
Current time: 2024-01-07 09:48:35 (running for 00:23:56.17)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.264 |  0.155 |                   60 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.261 |  0.172 |                   54 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.842 |  0.179 |                   27 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.547 |  0.177 |                   26 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.1770147830247879
[2m[36m(func pid=87355)[0m mae:  0.1293666958808899
[2m[36m(func pid=87355)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.332, 0.099, 0.191, 0.283, 0.14, 0.143, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2523 | Steps: 2 | Val loss: 0.3200 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=86819)[0m rmse: 0.17947125434875488
[2m[36m(func pid=86819)[0m mae:  0.13173529505729675
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2634 | Steps: 2 | Val loss: 0.2804 | Batch size: 32 | lr: 0.01 | Duration: 2.70s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5327 | Steps: 2 | Val loss: 0.4111 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=80805)[0m rmse: 0.17344805598258972
[2m[36m(func pid=80805)[0m mae:  0.1032470241189003
[2m[36m(func pid=80805)[0m rmse_per_class: [0.137, 0.246, 0.037, 0.298, 0.062, 0.191, 0.219, 0.139, 0.182, 0.223]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:48:40 (running for 00:24:01.18)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.263 |  0.154 |                   61 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.252 |  0.173 |                   55 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.839 |  0.179 |                   28 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.538 |  0.177 |                   27 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=79815)[0m rmse: 0.15434591472148895
[2m[36m(func pid=79815)[0m mae:  0.09848178923130035
[2m[36m(func pid=79815)[0m rmse_per_class: [0.103, 0.245, 0.042, 0.268, 0.05, 0.176, 0.24, 0.117, 0.156, 0.147]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.8351 | Steps: 2 | Val loss: 0.6561 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=87355)[0m rmse: 0.17693644762039185
[2m[36m(func pid=87355)[0m mae:  0.12929582595825195
[2m[36m(func pid=87355)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.332, 0.099, 0.191, 0.283, 0.139, 0.144, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2570 | Steps: 2 | Val loss: 0.3291 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=86819)[0m rmse: 0.17944788932800293
[2m[36m(func pid=86819)[0m mae:  0.1317056566476822
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2638 | Steps: 2 | Val loss: 0.2805 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5241 | Steps: 2 | Val loss: 0.4050 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=80805)[0m rmse: 0.17914949357509613
[2m[36m(func pid=80805)[0m mae:  0.10809149593114853
[2m[36m(func pid=80805)[0m rmse_per_class: [0.134, 0.244, 0.036, 0.31, 0.065, 0.198, 0.23, 0.135, 0.179, 0.263]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:48:45 (running for 00:24:06.39)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.264 |  0.154 |                   62 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.257 |  0.179 |                   56 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.835 |  0.179 |                   29 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.533 |  0.177 |                   28 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=79815)[0m rmse: 0.1540210247039795
[2m[36m(func pid=79815)[0m mae:  0.09782140702009201
[2m[36m(func pid=79815)[0m rmse_per_class: [0.103, 0.247, 0.041, 0.269, 0.051, 0.174, 0.238, 0.112, 0.155, 0.149]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.8342 | Steps: 2 | Val loss: 0.6546 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=87355)[0m rmse: 0.1767270416021347
[2m[36m(func pid=87355)[0m mae:  0.12914623320102692
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.331, 0.098, 0.191, 0.283, 0.139, 0.144, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2563 | Steps: 2 | Val loss: 0.3269 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2578 | Steps: 2 | Val loss: 0.2791 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=86819)[0m rmse: 0.17938223481178284
[2m[36m(func pid=86819)[0m mae:  0.13166072964668274
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5154 | Steps: 2 | Val loss: 0.3992 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 09:48:50 (running for 00:24:11.40)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.264 |  0.154 |                   62 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.256 |  0.178 |                   57 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.834 |  0.179 |                   30 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.524 |  0.177 |                   29 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=80805)[0m rmse: 0.17825153470039368
[2m[36m(func pid=80805)[0m mae:  0.10807839781045914
[2m[36m(func pid=80805)[0m rmse_per_class: [0.131, 0.245, 0.034, 0.318, 0.067, 0.199, 0.226, 0.135, 0.175, 0.251]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15271440148353577
[2m[36m(func pid=79815)[0m mae:  0.09614764899015427
[2m[36m(func pid=79815)[0m rmse_per_class: [0.102, 0.247, 0.038, 0.267, 0.054, 0.173, 0.234, 0.111, 0.154, 0.147]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.8334 | Steps: 2 | Val loss: 0.6531 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=87355)[0m rmse: 0.17660823464393616
[2m[36m(func pid=87355)[0m mae:  0.12903612852096558
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.331, 0.098, 0.191, 0.283, 0.139, 0.144, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2494 | Steps: 2 | Val loss: 0.3110 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2681 | Steps: 2 | Val loss: 0.2783 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=86819)[0m rmse: 0.17943598330020905
[2m[36m(func pid=86819)[0m mae:  0.13169558346271515
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5076 | Steps: 2 | Val loss: 0.3936 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 09:48:55 (running for 00:24:16.72)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.258 |  0.153 |                   63 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.249 |  0.168 |                   58 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.833 |  0.179 |                   31 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.515 |  0.177 |                   30 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=80805)[0m rmse: 0.16835619509220123
[2m[36m(func pid=80805)[0m mae:  0.10077760368585587
[2m[36m(func pid=80805)[0m rmse_per_class: [0.133, 0.247, 0.037, 0.319, 0.07, 0.19, 0.205, 0.135, 0.177, 0.17]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15191270411014557
[2m[36m(func pid=79815)[0m mae:  0.09473859518766403
[2m[36m(func pid=79815)[0m rmse_per_class: [0.098, 0.25, 0.037, 0.268, 0.057, 0.17, 0.229, 0.11, 0.154, 0.146]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17651477456092834
[2m[36m(func pid=87355)[0m mae:  0.12895379960536957
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.331, 0.097, 0.191, 0.283, 0.139, 0.144, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.8303 | Steps: 2 | Val loss: 0.6507 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2560 | Steps: 2 | Val loss: 0.3081 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2512 | Steps: 2 | Val loss: 0.2767 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=86819)[0m rmse: 0.17938709259033203
[2m[36m(func pid=86819)[0m mae:  0.13166101276874542
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5065 | Steps: 2 | Val loss: 0.3888 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 09:49:01 (running for 00:24:22.07)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.268 |  0.152 |                   64 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.256 |  0.168 |                   59 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.83  |  0.179 |                   32 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.508 |  0.177 |                   31 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=80805)[0m rmse: 0.16769693791866302
[2m[36m(func pid=80805)[0m mae:  0.0996670052409172
[2m[36m(func pid=80805)[0m rmse_per_class: [0.135, 0.254, 0.035, 0.326, 0.072, 0.182, 0.205, 0.144, 0.169, 0.156]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15065421164035797
[2m[36m(func pid=79815)[0m mae:  0.09315064549446106
[2m[36m(func pid=79815)[0m rmse_per_class: [0.097, 0.251, 0.033, 0.268, 0.061, 0.167, 0.226, 0.108, 0.151, 0.143]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8271 | Steps: 2 | Val loss: 0.6491 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=87355)[0m rmse: 0.1763780117034912
[2m[36m(func pid=87355)[0m mae:  0.1288386881351471
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.331, 0.097, 0.191, 0.282, 0.139, 0.144, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2425 | Steps: 2 | Val loss: 0.3088 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2537 | Steps: 2 | Val loss: 0.2764 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=86819)[0m rmse: 0.17939028143882751
[2m[36m(func pid=86819)[0m mae:  0.13166382908821106
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4963 | Steps: 2 | Val loss: 0.3838 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 09:49:06 (running for 00:24:27.31)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.251 |  0.151 |                   65 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.242 |  0.17  |                   60 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.827 |  0.179 |                   33 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.507 |  0.176 |                   32 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=80805)[0m rmse: 0.16964729130268097
[2m[36m(func pid=80805)[0m mae:  0.10062004625797272
[2m[36m(func pid=80805)[0m rmse_per_class: [0.142, 0.259, 0.034, 0.33, 0.072, 0.179, 0.206, 0.15, 0.164, 0.16]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15063251554965973
[2m[36m(func pid=79815)[0m mae:  0.0922447070479393
[2m[36m(func pid=79815)[0m rmse_per_class: [0.095, 0.255, 0.032, 0.271, 0.068, 0.164, 0.223, 0.107, 0.148, 0.143]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17627544701099396
[2m[36m(func pid=87355)[0m mae:  0.12874196469783783
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.331, 0.097, 0.191, 0.282, 0.139, 0.144, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8254 | Steps: 2 | Val loss: 0.6472 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2554 | Steps: 2 | Val loss: 0.3083 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2570 | Steps: 2 | Val loss: 0.2763 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=86819)[0m rmse: 0.17940321564674377
[2m[36m(func pid=86819)[0m mae:  0.13168089091777802
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4928 | Steps: 2 | Val loss: 0.3798 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 09:49:11 (running for 00:24:32.52)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.254 |  0.151 |                   66 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.255 |  0.171 |                   61 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.825 |  0.179 |                   34 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.496 |  0.176 |                   33 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=80805)[0m rmse: 0.17101211845874786
[2m[36m(func pid=80805)[0m mae:  0.10159045457839966
[2m[36m(func pid=80805)[0m rmse_per_class: [0.145, 0.261, 0.034, 0.328, 0.073, 0.182, 0.205, 0.16, 0.161, 0.161]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.1505327820777893
[2m[36m(func pid=79815)[0m mae:  0.09114721417427063
[2m[36m(func pid=79815)[0m rmse_per_class: [0.093, 0.256, 0.031, 0.272, 0.074, 0.163, 0.22, 0.107, 0.147, 0.143]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.1761961132287979
[2m[36m(func pid=87355)[0m mae:  0.12866297364234924
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.33, 0.097, 0.191, 0.282, 0.139, 0.144, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8246 | Steps: 2 | Val loss: 0.6457 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2480 | Steps: 2 | Val loss: 0.3106 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2574 | Steps: 2 | Val loss: 0.2777 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=86819)[0m rmse: 0.17936445772647858
[2m[36m(func pid=86819)[0m mae:  0.13163822889328003
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4874 | Steps: 2 | Val loss: 0.3759 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 09:49:16 (running for 00:24:37.66)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.257 |  0.151 |                   67 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.248 |  0.173 |                   62 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.825 |  0.179 |                   35 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.493 |  0.176 |                   34 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=80805)[0m rmse: 0.1732657253742218
[2m[36m(func pid=80805)[0m mae:  0.10309040546417236
[2m[36m(func pid=80805)[0m rmse_per_class: [0.151, 0.259, 0.036, 0.324, 0.073, 0.188, 0.206, 0.162, 0.166, 0.167]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15135762095451355
[2m[36m(func pid=79815)[0m mae:  0.09067538380622864
[2m[36m(func pid=79815)[0m rmse_per_class: [0.094, 0.258, 0.032, 0.273, 0.079, 0.161, 0.218, 0.106, 0.146, 0.147]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.1759941726922989
[2m[36m(func pid=87355)[0m mae:  0.12848761677742004
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.33, 0.097, 0.191, 0.282, 0.139, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.8194 | Steps: 2 | Val loss: 0.6439 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2636 | Steps: 2 | Val loss: 0.3119 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2530 | Steps: 2 | Val loss: 0.2789 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=86819)[0m rmse: 0.17936207354068756
[2m[36m(func pid=86819)[0m mae:  0.13162705302238464
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4812 | Steps: 2 | Val loss: 0.3720 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 09:49:22 (running for 00:24:43.08)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.257 |  0.151 |                   68 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.264 |  0.174 |                   63 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.819 |  0.179 |                   36 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.487 |  0.176 |                   35 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=80805)[0m rmse: 0.17426560819149017
[2m[36m(func pid=80805)[0m mae:  0.10299650579690933
[2m[36m(func pid=80805)[0m rmse_per_class: [0.151, 0.261, 0.039, 0.317, 0.077, 0.188, 0.208, 0.158, 0.173, 0.171]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15238232910633087
[2m[36m(func pid=79815)[0m mae:  0.09026484936475754
[2m[36m(func pid=79815)[0m rmse_per_class: [0.092, 0.262, 0.034, 0.277, 0.086, 0.159, 0.216, 0.106, 0.146, 0.146]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17590372264385223
[2m[36m(func pid=87355)[0m mae:  0.12841811776161194
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.33, 0.096, 0.19, 0.281, 0.139, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.8200 | Steps: 2 | Val loss: 0.6421 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2579 | Steps: 2 | Val loss: 0.3124 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2663 | Steps: 2 | Val loss: 0.2809 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=86819)[0m rmse: 0.17939308285713196
[2m[36m(func pid=86819)[0m mae:  0.13163861632347107
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4766 | Steps: 2 | Val loss: 0.3679 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 09:49:27 (running for 00:24:48.22)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.253 |  0.152 |                   69 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.258 |  0.175 |                   64 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.82  |  0.179 |                   37 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.481 |  0.176 |                   36 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=80805)[0m rmse: 0.1750994622707367
[2m[36m(func pid=80805)[0m mae:  0.10227151960134506
[2m[36m(func pid=80805)[0m rmse_per_class: [0.16, 0.263, 0.042, 0.309, 0.08, 0.183, 0.212, 0.148, 0.191, 0.164]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.1536182463169098
[2m[36m(func pid=79815)[0m mae:  0.09044293314218521
[2m[36m(func pid=79815)[0m rmse_per_class: [0.093, 0.265, 0.035, 0.279, 0.091, 0.158, 0.216, 0.105, 0.143, 0.15]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.1757810413837433
[2m[36m(func pid=87355)[0m mae:  0.12831062078475952
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.259, 0.095, 0.33, 0.096, 0.19, 0.281, 0.139, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.8152 | Steps: 2 | Val loss: 0.6399 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2488 | Steps: 2 | Val loss: 0.3081 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2610 | Steps: 2 | Val loss: 0.2826 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4742 | Steps: 2 | Val loss: 0.3646 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=86819)[0m rmse: 0.179353728890419
[2m[36m(func pid=86819)[0m mae:  0.13161225616931915
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
== Status ==
Current time: 2024-01-07 09:49:32 (running for 00:24:53.37)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.266 |  0.154 |                   70 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.249 |  0.174 |                   65 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.815 |  0.179 |                   38 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.477 |  0.176 |                   37 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=80805)[0m rmse: 0.17388716340065002
[2m[36m(func pid=80805)[0m mae:  0.10086128860712051
[2m[36m(func pid=80805)[0m rmse_per_class: [0.176, 0.262, 0.043, 0.305, 0.085, 0.174, 0.213, 0.138, 0.196, 0.147]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15452221035957336
[2m[36m(func pid=79815)[0m mae:  0.0902404859662056
[2m[36m(func pid=79815)[0m rmse_per_class: [0.092, 0.267, 0.036, 0.281, 0.096, 0.158, 0.215, 0.104, 0.139, 0.156]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17554505169391632
[2m[36m(func pid=87355)[0m mae:  0.12811526656150818
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.329, 0.096, 0.19, 0.281, 0.139, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2458 | Steps: 2 | Val loss: 0.3044 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8126 | Steps: 2 | Val loss: 0.6377 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2538 | Steps: 2 | Val loss: 0.2841 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4680 | Steps: 2 | Val loss: 0.3616 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 09:49:37 (running for 00:24:58.74)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.261 |  0.155 |                   71 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.249 |  0.174 |                   65 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.813 |  0.179 |                   39 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.474 |  0.176 |                   38 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=80805)[0m rmse: 0.17197580635547638
[2m[36m(func pid=80805)[0m mae:  0.09972628951072693
[2m[36m(func pid=80805)[0m rmse_per_class: [0.178, 0.262, 0.042, 0.303, 0.09, 0.173, 0.212, 0.136, 0.189, 0.134]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17935094237327576
[2m[36m(func pid=86819)[0m mae:  0.13161000609397888
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15520726144313812
[2m[36m(func pid=79815)[0m mae:  0.09002704173326492
[2m[36m(func pid=79815)[0m rmse_per_class: [0.093, 0.267, 0.038, 0.281, 0.099, 0.158, 0.214, 0.102, 0.137, 0.161]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17545625567436218
[2m[36m(func pid=87355)[0m mae:  0.12804993987083435
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.329, 0.095, 0.19, 0.28, 0.139, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2556 | Steps: 2 | Val loss: 0.3011 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.8114 | Steps: 2 | Val loss: 0.6359 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2515 | Steps: 2 | Val loss: 0.2851 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4661 | Steps: 2 | Val loss: 0.3584 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 09:49:43 (running for 00:25:04.24)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.254 |  0.155 |                   72 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.246 |  0.172 |                   66 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.811 |  0.179 |                   40 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.468 |  0.175 |                   39 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=80805)[0m rmse: 0.17066647112369537
[2m[36m(func pid=80805)[0m mae:  0.099174365401268
[2m[36m(func pid=80805)[0m rmse_per_class: [0.184, 0.263, 0.04, 0.304, 0.096, 0.169, 0.213, 0.13, 0.179, 0.13]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17936208844184875
[2m[36m(func pid=86819)[0m mae:  0.13162897527217865
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15605956315994263
[2m[36m(func pid=79815)[0m mae:  0.09013214707374573
[2m[36m(func pid=79815)[0m rmse_per_class: [0.096, 0.266, 0.039, 0.281, 0.103, 0.158, 0.214, 0.101, 0.137, 0.165]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17534327507019043
[2m[36m(func pid=87355)[0m mae:  0.1279504895210266
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.329, 0.095, 0.19, 0.28, 0.139, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2611 | Steps: 2 | Val loss: 0.2989 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.8107 | Steps: 2 | Val loss: 0.6343 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2466 | Steps: 2 | Val loss: 0.2860 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4580 | Steps: 2 | Val loss: 0.3553 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 09:49:48 (running for 00:25:09.61)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.252 |  0.156 |                   73 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.261 |  0.169 |                   68 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.811 |  0.179 |                   40 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.466 |  0.175 |                   40 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=80805)[0m rmse: 0.16912071406841278
[2m[36m(func pid=80805)[0m mae:  0.09824330359697342
[2m[36m(func pid=80805)[0m rmse_per_class: [0.176, 0.269, 0.039, 0.305, 0.096, 0.167, 0.214, 0.121, 0.171, 0.134]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17932961881160736
[2m[36m(func pid=86819)[0m mae:  0.13159996271133423
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15709185600280762
[2m[36m(func pid=79815)[0m mae:  0.09026594460010529
[2m[36m(func pid=79815)[0m rmse_per_class: [0.096, 0.265, 0.039, 0.282, 0.113, 0.158, 0.214, 0.101, 0.137, 0.167]
[2m[36m(func pid=79815)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17513501644134521
[2m[36m(func pid=87355)[0m mae:  0.1277870386838913
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.329, 0.094, 0.19, 0.28, 0.139, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.8072 | Steps: 2 | Val loss: 0.6335 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2501 | Steps: 2 | Val loss: 0.2992 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=79815)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2618 | Steps: 2 | Val loss: 0.2864 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4578 | Steps: 2 | Val loss: 0.3525 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 09:49:53 (running for 00:25:14.91)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00010 | RUNNING    | 192.168.7.53:79815 | 0.01   |       0.99 |         0.0001 |  0.247 |  0.157 |                   74 |
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.261 |  0.169 |                   68 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.807 |  0.179 |                   42 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.458 |  0.175 |                   41 |
| train_32e5a_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=86819)[0m rmse: 0.17930243909358978
[2m[36m(func pid=86819)[0m mae:  0.1315673589706421
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.16871075332164764
[2m[36m(func pid=80805)[0m mae:  0.09824785590171814
[2m[36m(func pid=80805)[0m rmse_per_class: [0.16, 0.278, 0.037, 0.307, 0.097, 0.167, 0.216, 0.116, 0.173, 0.136]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=79815)[0m rmse: 0.15743044018745422
[2m[36m(func pid=79815)[0m mae:  0.09009287506341934
[2m[36m(func pid=79815)[0m rmse_per_class: [0.1, 0.265, 0.039, 0.283, 0.118, 0.157, 0.213, 0.101, 0.138, 0.161]
[2m[36m(func pid=87355)[0m rmse: 0.1749362349510193
[2m[36m(func pid=87355)[0m mae:  0.127625972032547
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.259, 0.093, 0.328, 0.094, 0.19, 0.279, 0.139, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8045 | Steps: 2 | Val loss: 0.6315 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2425 | Steps: 2 | Val loss: 0.3021 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4541 | Steps: 2 | Val loss: 0.3498 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=86819)[0m rmse: 0.17922770977020264
[2m[36m(func pid=86819)[0m mae:  0.13150374591350555
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.14, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.17002062499523163
[2m[36m(func pid=80805)[0m mae:  0.09906090795993805
[2m[36m(func pid=80805)[0m rmse_per_class: [0.158, 0.287, 0.036, 0.312, 0.099, 0.166, 0.218, 0.112, 0.189, 0.125]
[2m[36m(func pid=87355)[0m rmse: 0.17473608255386353
[2m[36m(func pid=87355)[0m mae:  0.1274576187133789
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.259, 0.092, 0.328, 0.094, 0.19, 0.279, 0.139, 0.143, 0.108]
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8033 | Steps: 2 | Val loss: 0.6297 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:49:59 (running for 00:25:20.23)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1469999998807907
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.25  |  0.169 |                   69 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.804 |  0.179 |                   43 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.458 |  0.175 |                   42 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=97073)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=97073)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=97073)[0m Configuration completed!
[2m[36m(func pid=97073)[0m New optimizer parameters:
[2m[36m(func pid=97073)[0m SGD (
[2m[36m(func pid=97073)[0m Parameter Group 0
[2m[36m(func pid=97073)[0m     dampening: 0
[2m[36m(func pid=97073)[0m     differentiable: False
[2m[36m(func pid=97073)[0m     foreach: None
[2m[36m(func pid=97073)[0m     lr: 0.01
[2m[36m(func pid=97073)[0m     maximize: False
[2m[36m(func pid=97073)[0m     momentum: 0.9
[2m[36m(func pid=97073)[0m     nesterov: False
[2m[36m(func pid=97073)[0m     weight_decay: 0.0001
[2m[36m(func pid=97073)[0m )
[2m[36m(func pid=97073)[0m 
== Status ==
Current time: 2024-01-07 09:50:04 (running for 00:25:25.55)
Memory usage on this node: 23.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1469999998807907
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.243 |  0.17  |                   70 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.803 |  0.179 |                   44 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.454 |  0.175 |                   43 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=86819)[0m rmse: 0.17923736572265625
[2m[36m(func pid=86819)[0m mae:  0.13151559233665466
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4508 | Steps: 2 | Val loss: 0.3476 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2448 | Steps: 2 | Val loss: 0.3065 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.7991 | Steps: 2 | Val loss: 0.6276 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8896 | Steps: 2 | Val loss: 0.6867 | Batch size: 32 | lr: 0.01 | Duration: 4.56s
[2m[36m(func pid=87355)[0m rmse: 0.17453798651695251
[2m[36m(func pid=87355)[0m mae:  0.12731119990348816
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.259, 0.092, 0.328, 0.093, 0.19, 0.279, 0.139, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.17184603214263916
[2m[36m(func pid=80805)[0m mae:  0.10020671039819717
[2m[36m(func pid=80805)[0m rmse_per_class: [0.152, 0.295, 0.035, 0.316, 0.103, 0.166, 0.218, 0.112, 0.202, 0.121]
[2m[36m(func pid=80805)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17926430702209473
[2m[36m(func pid=86819)[0m mae:  0.13152454793453217
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.098, 0.336, 0.109, 0.19, 0.292, 0.141, 0.142, 0.108]
== Status ==
Current time: 2024-01-07 09:50:09 (running for 00:25:30.84)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1469999998807907
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.245 |  0.172 |                   71 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.799 |  0.179 |                   45 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.451 |  0.175 |                   44 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=97073)[0m rmse: 0.1825575828552246
[2m[36m(func pid=97073)[0m mae:  0.1342858374118805
[2m[36m(func pid=97073)[0m rmse_per_class: [0.116, 0.266, 0.11, 0.339, 0.11, 0.191, 0.294, 0.144, 0.143, 0.113]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4453 | Steps: 2 | Val loss: 0.3454 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2506 | Steps: 2 | Val loss: 0.3069 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.7989 | Steps: 2 | Val loss: 0.6258 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8438 | Steps: 2 | Val loss: 0.6347 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=87355)[0m rmse: 0.17440897226333618
[2m[36m(func pid=87355)[0m mae:  0.1272231936454773
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.259, 0.092, 0.327, 0.093, 0.189, 0.279, 0.139, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.17170825600624084
[2m[36m(func pid=80805)[0m mae:  0.10012378543615341
[2m[36m(func pid=80805)[0m rmse_per_class: [0.154, 0.298, 0.036, 0.321, 0.105, 0.163, 0.215, 0.112, 0.193, 0.121]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:50:15 (running for 00:25:36.20)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1469999998807907
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.251 |  0.172 |                   72 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.799 |  0.179 |                   46 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.445 |  0.174 |                   45 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.89  |  0.183 |                    1 |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=86819)[0m rmse: 0.17931866645812988
[2m[36m(func pid=86819)[0m mae:  0.1315619796514511
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=97073)[0m rmse: 0.1820160448551178
[2m[36m(func pid=97073)[0m mae:  0.1338338404893875
[2m[36m(func pid=97073)[0m rmse_per_class: [0.116, 0.267, 0.109, 0.339, 0.11, 0.19, 0.292, 0.142, 0.143, 0.112]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4450 | Steps: 2 | Val loss: 0.3440 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2522 | Steps: 2 | Val loss: 0.3030 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.7944 | Steps: 2 | Val loss: 0.6238 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7661 | Steps: 2 | Val loss: 0.5727 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=87355)[0m rmse: 0.1743529886007309
[2m[36m(func pid=87355)[0m mae:  0.127193883061409
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.259, 0.091, 0.327, 0.093, 0.189, 0.278, 0.138, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.16897699236869812
[2m[36m(func pid=80805)[0m mae:  0.09822408109903336
[2m[36m(func pid=80805)[0m rmse_per_class: [0.156, 0.293, 0.041, 0.319, 0.105, 0.158, 0.211, 0.112, 0.173, 0.121]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:50:20 (running for 00:25:41.49)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1469999998807907
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.252 |  0.169 |                   73 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.794 |  0.179 |                   47 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.445 |  0.174 |                   46 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.844 |  0.182 |                    2 |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=86819)[0m rmse: 0.17932955920696259
[2m[36m(func pid=86819)[0m mae:  0.13156720995903015
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=97073)[0m rmse: 0.18080022931098938
[2m[36m(func pid=97073)[0m mae:  0.13281096518039703
[2m[36m(func pid=97073)[0m rmse_per_class: [0.115, 0.266, 0.108, 0.338, 0.107, 0.191, 0.288, 0.14, 0.143, 0.112]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4394 | Steps: 2 | Val loss: 0.3419 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2560 | Steps: 2 | Val loss: 0.3026 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7946 | Steps: 2 | Val loss: 0.6224 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6762 | Steps: 2 | Val loss: 0.5023 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=87355)[0m rmse: 0.1741674542427063
[2m[36m(func pid=87355)[0m mae:  0.12702743709087372
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.259, 0.091, 0.327, 0.092, 0.189, 0.278, 0.138, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.16880296170711517
[2m[36m(func pid=80805)[0m mae:  0.0982765480875969
[2m[36m(func pid=80805)[0m rmse_per_class: [0.15, 0.292, 0.041, 0.321, 0.1, 0.154, 0.213, 0.134, 0.152, 0.13]
[2m[36m(func pid=80805)[0m 
== Status ==
Current time: 2024-01-07 09:50:25 (running for 00:25:46.83)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: -0.1469999998807907
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00011 | RUNNING    | 192.168.7.53:80805 | 0.1    |       0.99 |         0.0001 |  0.256 |  0.169 |                   74 |
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.795 |  0.179 |                   48 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.439 |  0.174 |                   47 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.766 |  0.181 |                    3 |
| train_32e5a_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=86819)[0m rmse: 0.1793045848608017
[2m[36m(func pid=86819)[0m mae:  0.13154050707817078
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=97073)[0m rmse: 0.1794360727071762
[2m[36m(func pid=97073)[0m mae:  0.13145314157009125
[2m[36m(func pid=97073)[0m rmse_per_class: [0.114, 0.265, 0.106, 0.336, 0.104, 0.19, 0.283, 0.141, 0.144, 0.112]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4373 | Steps: 2 | Val loss: 0.3398 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=80805)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2398 | Steps: 2 | Val loss: 0.3082 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7916 | Steps: 2 | Val loss: 0.6207 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5887 | Steps: 2 | Val loss: 0.4420 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=87355)[0m rmse: 0.17400690913200378
[2m[36m(func pid=87355)[0m mae:  0.12688425183296204
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.259, 0.091, 0.327, 0.092, 0.189, 0.277, 0.138, 0.142, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=80805)[0m rmse: 0.17149746417999268
[2m[36m(func pid=80805)[0m mae:  0.10004036128520966
[2m[36m(func pid=80805)[0m rmse_per_class: [0.15, 0.29, 0.042, 0.325, 0.1, 0.153, 0.216, 0.155, 0.144, 0.14]
[2m[36m(func pid=97073)[0m rmse: 0.1784050166606903
[2m[36m(func pid=97073)[0m mae:  0.1305094063282013
[2m[36m(func pid=97073)[0m rmse_per_class: [0.115, 0.265, 0.103, 0.335, 0.102, 0.19, 0.28, 0.14, 0.144, 0.111]
[2m[36m(func pid=86819)[0m rmse: 0.17929567396640778
[2m[36m(func pid=86819)[0m mae:  0.13153058290481567
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4372 | Steps: 2 | Val loss: 0.3385 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=87355)[0m rmse: 0.1739228069782257
[2m[36m(func pid=87355)[0m mae:  0.1268380731344223
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.259, 0.091, 0.327, 0.092, 0.189, 0.277, 0.139, 0.142, 0.109]
== Status ==
Current time: 2024-01-07 09:50:31 (running for 00:25:52.23)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.795 |  0.179 |                   48 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.437 |  0.174 |                   48 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.676 |  0.179 |                    4 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 09:50:39 (running for 00:26:00.17)
Memory usage on this node: 23.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.795 |  0.179 |                   48 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.437 |  0.174 |                   48 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.589 |  0.178 |                    5 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=98661)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=98661)[0m Configuration completed!
[2m[36m(func pid=98661)[0m New optimizer parameters:
[2m[36m(func pid=98661)[0m SGD (
[2m[36m(func pid=98661)[0m Parameter Group 0
[2m[36m(func pid=98661)[0m     dampening: 0
[2m[36m(func pid=98661)[0m     differentiable: False
[2m[36m(func pid=98661)[0m     foreach: None
[2m[36m(func pid=98661)[0m     lr: 0.1
[2m[36m(func pid=98661)[0m     maximize: False
[2m[36m(func pid=98661)[0m     momentum: 0.9
[2m[36m(func pid=98661)[0m     nesterov: False
[2m[36m(func pid=98661)[0m     weight_decay: 0.0001
[2m[36m(func pid=98661)[0m )
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7866 | Steps: 2 | Val loss: 0.6188 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5177 | Steps: 2 | Val loss: 0.3972 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4331 | Steps: 2 | Val loss: 0.3364 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8374 | Steps: 2 | Val loss: 0.5265 | Batch size: 32 | lr: 0.1 | Duration: 4.83s
== Status ==
Current time: 2024-01-07 09:50:44 (running for 00:26:05.18)
Memory usage on this node: 25.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.792 |  0.179 |                   49 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.437 |  0.174 |                   49 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.589 |  0.178 |                    5 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |        |        |                      |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.17729437351226807
[2m[36m(func pid=97073)[0m mae:  0.12955769896507263
[2m[36m(func pid=97073)[0m rmse_per_class: [0.116, 0.264, 0.099, 0.333, 0.099, 0.191, 0.279, 0.139, 0.143, 0.111]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17928990721702576
[2m[36m(func pid=86819)[0m mae:  0.1315404325723648
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.173723965883255
[2m[36m(func pid=87355)[0m mae:  0.12665650248527527
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.258, 0.09, 0.327, 0.091, 0.189, 0.276, 0.139, 0.142, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.18178798258304596
[2m[36m(func pid=98661)[0m mae:  0.1335371434688568
[2m[36m(func pid=98661)[0m rmse_per_class: [0.113, 0.268, 0.117, 0.338, 0.101, 0.191, 0.291, 0.145, 0.143, 0.111]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4664 | Steps: 2 | Val loss: 0.3646 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4345 | Steps: 2 | Val loss: 0.3346 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7861 | Steps: 2 | Val loss: 0.6170 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5342 | Steps: 2 | Val loss: 0.3483 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 09:50:49 (running for 00:26:10.81)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.787 |  0.179 |                   50 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.433 |  0.174 |                   50 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.466 |  0.176 |                    7 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.837 |  0.182 |                    1 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=87355)[0m rmse: 0.17355868220329285
[2m[36m(func pid=87355)[0m mae:  0.1265179067850113
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.259, 0.09, 0.327, 0.091, 0.189, 0.275, 0.139, 0.142, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=97073)[0m rmse: 0.17620691657066345
[2m[36m(func pid=97073)[0m mae:  0.12864795327186584
[2m[36m(func pid=97073)[0m rmse_per_class: [0.116, 0.264, 0.097, 0.331, 0.095, 0.191, 0.277, 0.138, 0.143, 0.11]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.1792808622121811
[2m[36m(func pid=86819)[0m mae:  0.1315382868051529
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.17731109261512756
[2m[36m(func pid=98661)[0m mae:  0.12969030439853668
[2m[36m(func pid=98661)[0m rmse_per_class: [0.114, 0.266, 0.105, 0.335, 0.093, 0.19, 0.277, 0.139, 0.144, 0.111]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4350 | Steps: 2 | Val loss: 0.3400 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4283 | Steps: 2 | Val loss: 0.3333 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7857 | Steps: 2 | Val loss: 0.6158 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4014 | Steps: 2 | Val loss: 0.3152 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 09:50:54 (running for 00:26:15.88)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.786 |  0.179 |                   51 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.434 |  0.174 |                   51 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.435 |  0.175 |                    8 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.534 |  0.177 |                    2 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.17513972520828247
[2m[36m(func pid=97073)[0m mae:  0.12772585451602936
[2m[36m(func pid=97073)[0m rmse_per_class: [0.117, 0.264, 0.095, 0.328, 0.092, 0.191, 0.275, 0.138, 0.142, 0.11]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17342162132263184
[2m[36m(func pid=87355)[0m mae:  0.12643450498580933
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.258, 0.089, 0.326, 0.09, 0.189, 0.275, 0.139, 0.142, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.1792694479227066
[2m[36m(func pid=86819)[0m mae:  0.13151653110980988
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.1730702668428421
[2m[36m(func pid=98661)[0m mae:  0.1263473927974701
[2m[36m(func pid=98661)[0m rmse_per_class: [0.117, 0.266, 0.092, 0.332, 0.078, 0.19, 0.267, 0.136, 0.144, 0.109]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4081 | Steps: 2 | Val loss: 0.3231 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4286 | Steps: 2 | Val loss: 0.3322 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.7806 | Steps: 2 | Val loss: 0.6147 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4258 | Steps: 2 | Val loss: 0.3325 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:50:59 (running for 00:26:20.94)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.786 |  0.179 |                   52 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.428 |  0.173 |                   52 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.408 |  0.174 |                    9 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.401 |  0.173 |                    3 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.17383769154548645
[2m[36m(func pid=97073)[0m mae:  0.12661278247833252
[2m[36m(func pid=97073)[0m rmse_per_class: [0.117, 0.264, 0.092, 0.326, 0.087, 0.19, 0.272, 0.137, 0.142, 0.11]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17333166301250458
[2m[36m(func pid=87355)[0m mae:  0.12635692954063416
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.258, 0.089, 0.326, 0.09, 0.189, 0.275, 0.139, 0.142, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17929662764072418
[2m[36m(func pid=86819)[0m mae:  0.13154008984565735
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.16728530824184418
[2m[36m(func pid=98661)[0m mae:  0.12158177047967911
[2m[36m(func pid=98661)[0m rmse_per_class: [0.122, 0.259, 0.077, 0.328, 0.065, 0.191, 0.258, 0.127, 0.143, 0.104]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3979 | Steps: 2 | Val loss: 0.3130 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4252 | Steps: 2 | Val loss: 0.3309 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.7825 | Steps: 2 | Val loss: 0.6132 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4519 | Steps: 2 | Val loss: 0.3376 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 09:51:05 (running for 00:26:26.30)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.781 |  0.179 |                   53 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.429 |  0.173 |                   53 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.398 |  0.172 |                   10 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.426 |  0.167 |                    4 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.17249275743961334
[2m[36m(func pid=97073)[0m mae:  0.12548887729644775
[2m[36m(func pid=97073)[0m rmse_per_class: [0.117, 0.264, 0.09, 0.324, 0.084, 0.189, 0.269, 0.137, 0.143, 0.109]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17324276268482208
[2m[36m(func pid=87355)[0m mae:  0.1262689232826233
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.258, 0.09, 0.326, 0.089, 0.189, 0.275, 0.139, 0.142, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17928259074687958
[2m[36m(func pid=86819)[0m mae:  0.1315160095691681
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.16033217310905457
[2m[36m(func pid=98661)[0m mae:  0.11572978645563126
[2m[36m(func pid=98661)[0m rmse_per_class: [0.121, 0.251, 0.054, 0.32, 0.058, 0.186, 0.252, 0.125, 0.14, 0.096]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3922 | Steps: 2 | Val loss: 0.3073 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4238 | Steps: 2 | Val loss: 0.3294 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.7772 | Steps: 2 | Val loss: 0.6114 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4431 | Steps: 2 | Val loss: 0.3327 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 09:51:10 (running for 00:26:31.65)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.782 |  0.179 |                   54 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.425 |  0.173 |                   54 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.392 |  0.171 |                   11 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.452 |  0.16  |                    5 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.17097580432891846
[2m[36m(func pid=97073)[0m mae:  0.12425496429204941
[2m[36m(func pid=97073)[0m rmse_per_class: [0.117, 0.262, 0.087, 0.321, 0.08, 0.189, 0.266, 0.136, 0.143, 0.109]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17308871448040009
[2m[36m(func pid=87355)[0m mae:  0.12613409757614136
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.258, 0.089, 0.325, 0.089, 0.189, 0.275, 0.138, 0.142, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17931243777275085
[2m[36m(func pid=86819)[0m mae:  0.13153132796287537
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.150217667222023
[2m[36m(func pid=98661)[0m mae:  0.10476948320865631
[2m[36m(func pid=98661)[0m rmse_per_class: [0.108, 0.244, 0.034, 0.302, 0.054, 0.187, 0.228, 0.122, 0.136, 0.087]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3871 | Steps: 2 | Val loss: 0.3043 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4209 | Steps: 2 | Val loss: 0.3283 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.7772 | Steps: 2 | Val loss: 0.6096 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4033 | Steps: 2 | Val loss: 0.3181 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 09:51:15 (running for 00:26:36.75)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.777 |  0.179 |                   55 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.424 |  0.173 |                   55 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.387 |  0.169 |                   12 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.443 |  0.15  |                    6 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.169377863407135
[2m[36m(func pid=97073)[0m mae:  0.12302038818597794
[2m[36m(func pid=97073)[0m rmse_per_class: [0.116, 0.26, 0.084, 0.32, 0.076, 0.188, 0.264, 0.135, 0.143, 0.108]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.1730700433254242
[2m[36m(func pid=87355)[0m mae:  0.1261271834373474
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.258, 0.089, 0.325, 0.089, 0.189, 0.275, 0.138, 0.142, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17926770448684692
[2m[36m(func pid=86819)[0m mae:  0.13149109482765198
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14805598556995392
[2m[36m(func pid=98661)[0m mae:  0.10021809488534927
[2m[36m(func pid=98661)[0m rmse_per_class: [0.098, 0.238, 0.028, 0.303, 0.054, 0.187, 0.236, 0.121, 0.132, 0.083]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3863 | Steps: 2 | Val loss: 0.3032 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4202 | Steps: 2 | Val loss: 0.3266 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7744 | Steps: 2 | Val loss: 0.6076 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3682 | Steps: 2 | Val loss: 0.2938 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 09:51:20 (running for 00:26:41.91)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.777 |  0.179 |                   56 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.421 |  0.173 |                   56 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.386 |  0.168 |                   13 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.403 |  0.148 |                    7 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1679166704416275
[2m[36m(func pid=97073)[0m mae:  0.12197168916463852
[2m[36m(func pid=97073)[0m rmse_per_class: [0.115, 0.258, 0.08, 0.319, 0.074, 0.187, 0.263, 0.135, 0.143, 0.106]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17276416718959808
[2m[36m(func pid=87355)[0m mae:  0.12589652836322784
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.258, 0.088, 0.325, 0.089, 0.188, 0.274, 0.138, 0.142, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.1792760193347931
[2m[36m(func pid=86819)[0m mae:  0.1315019577741623
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.1467946171760559
[2m[36m(func pid=98661)[0m mae:  0.09869025647640228
[2m[36m(func pid=98661)[0m rmse_per_class: [0.097, 0.241, 0.026, 0.309, 0.055, 0.178, 0.234, 0.116, 0.133, 0.082]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3839 | Steps: 2 | Val loss: 0.3030 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4181 | Steps: 2 | Val loss: 0.3251 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7734 | Steps: 2 | Val loss: 0.6058 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3334 | Steps: 2 | Val loss: 0.2807 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 09:51:26 (running for 00:26:47.28)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.774 |  0.179 |                   57 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.42  |  0.173 |                   57 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.384 |  0.167 |                   14 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.368 |  0.147 |                    8 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.16664378345012665
[2m[36m(func pid=97073)[0m mae:  0.12106826156377792
[2m[36m(func pid=97073)[0m rmse_per_class: [0.115, 0.256, 0.076, 0.318, 0.072, 0.186, 0.263, 0.134, 0.143, 0.105]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17260479927062988
[2m[36m(func pid=87355)[0m mae:  0.12578633427619934
[2m[36m(func pid=87355)[0m rmse_per_class: [0.116, 0.258, 0.088, 0.325, 0.088, 0.188, 0.274, 0.138, 0.143, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17926986515522003
[2m[36m(func pid=86819)[0m mae:  0.13149645924568176
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.148198202252388
[2m[36m(func pid=98661)[0m mae:  0.09918956458568573
[2m[36m(func pid=98661)[0m rmse_per_class: [0.099, 0.247, 0.025, 0.318, 0.055, 0.178, 0.228, 0.119, 0.132, 0.081]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3821 | Steps: 2 | Val loss: 0.3027 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4166 | Steps: 2 | Val loss: 0.3239 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7708 | Steps: 2 | Val loss: 0.6036 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3155 | Steps: 2 | Val loss: 0.2796 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:51:31 (running for 00:26:52.59)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.773 |  0.179 |                   58 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.418 |  0.173 |                   58 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.382 |  0.165 |                   15 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.333 |  0.148 |                    9 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.16526399552822113
[2m[36m(func pid=97073)[0m mae:  0.1200094223022461
[2m[36m(func pid=97073)[0m rmse_per_class: [0.114, 0.254, 0.071, 0.317, 0.07, 0.185, 0.262, 0.134, 0.143, 0.103]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17246241867542267
[2m[36m(func pid=87355)[0m mae:  0.12568530440330505
[2m[36m(func pid=87355)[0m rmse_per_class: [0.115, 0.258, 0.088, 0.324, 0.088, 0.188, 0.274, 0.138, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.1792384684085846
[2m[36m(func pid=86819)[0m mae:  0.1314653754234314
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.15028467774391174
[2m[36m(func pid=98661)[0m mae:  0.10023586452007294
[2m[36m(func pid=98661)[0m rmse_per_class: [0.098, 0.262, 0.025, 0.319, 0.055, 0.18, 0.222, 0.128, 0.131, 0.083]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3752 | Steps: 2 | Val loss: 0.3024 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4144 | Steps: 2 | Val loss: 0.3229 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7694 | Steps: 2 | Val loss: 0.6025 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3121 | Steps: 2 | Val loss: 0.2811 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 09:51:36 (running for 00:26:57.87)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.771 |  0.179 |                   59 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.417 |  0.172 |                   59 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.375 |  0.164 |                   16 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.315 |  0.15  |                   10 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.16415034234523773
[2m[36m(func pid=97073)[0m mae:  0.11910424381494522
[2m[36m(func pid=97073)[0m rmse_per_class: [0.114, 0.252, 0.067, 0.317, 0.069, 0.184, 0.261, 0.134, 0.142, 0.102]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17241868376731873
[2m[36m(func pid=87355)[0m mae:  0.12566380202770233
[2m[36m(func pid=87355)[0m rmse_per_class: [0.115, 0.258, 0.088, 0.324, 0.088, 0.188, 0.275, 0.138, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17922522127628326
[2m[36m(func pid=86819)[0m mae:  0.13144877552986145
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.15076005458831787
[2m[36m(func pid=98661)[0m mae:  0.10091855376958847
[2m[36m(func pid=98661)[0m rmse_per_class: [0.092, 0.275, 0.025, 0.315, 0.055, 0.174, 0.223, 0.129, 0.131, 0.089]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3729 | Steps: 2 | Val loss: 0.3020 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4137 | Steps: 2 | Val loss: 0.3219 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3159 | Steps: 2 | Val loss: 0.2789 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.7691 | Steps: 2 | Val loss: 0.6008 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 09:51:41 (running for 00:27:02.95)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.769 |  0.179 |                   60 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.414 |  0.172 |                   60 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.373 |  0.163 |                   17 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.312 |  0.151 |                   11 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1632140576839447
[2m[36m(func pid=97073)[0m mae:  0.1183343306183815
[2m[36m(func pid=97073)[0m rmse_per_class: [0.113, 0.251, 0.064, 0.316, 0.068, 0.182, 0.261, 0.134, 0.142, 0.101]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.1723092496395111
[2m[36m(func pid=87355)[0m mae:  0.12557834386825562
[2m[36m(func pid=87355)[0m rmse_per_class: [0.115, 0.258, 0.088, 0.324, 0.088, 0.188, 0.275, 0.138, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14929577708244324
[2m[36m(func pid=98661)[0m mae:  0.10029604285955429
[2m[36m(func pid=98661)[0m rmse_per_class: [0.088, 0.278, 0.025, 0.302, 0.054, 0.162, 0.233, 0.122, 0.132, 0.097]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.1792755275964737
[2m[36m(func pid=86819)[0m mae:  0.13148987293243408
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3713 | Steps: 2 | Val loss: 0.3006 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4115 | Steps: 2 | Val loss: 0.3207 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.2953 | Steps: 2 | Val loss: 0.2759 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 09:51:46 (running for 00:27:08.03)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.769 |  0.179 |                   61 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.414 |  0.172 |                   61 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.371 |  0.162 |                   18 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.316 |  0.149 |                   12 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1619483083486557
[2m[36m(func pid=97073)[0m mae:  0.11723776906728745
[2m[36m(func pid=97073)[0m rmse_per_class: [0.111, 0.25, 0.062, 0.315, 0.067, 0.181, 0.26, 0.133, 0.141, 0.1]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7663 | Steps: 2 | Val loss: 0.5987 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=87355)[0m rmse: 0.17205433547496796
[2m[36m(func pid=87355)[0m mae:  0.1253686100244522
[2m[36m(func pid=87355)[0m rmse_per_class: [0.115, 0.257, 0.087, 0.324, 0.087, 0.188, 0.274, 0.138, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.148189976811409
[2m[36m(func pid=98661)[0m mae:  0.09987939894199371
[2m[36m(func pid=98661)[0m rmse_per_class: [0.085, 0.275, 0.027, 0.287, 0.054, 0.156, 0.245, 0.117, 0.134, 0.101]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.1792675256729126
[2m[36m(func pid=86819)[0m mae:  0.13147714734077454
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3679 | Steps: 2 | Val loss: 0.2976 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4103 | Steps: 2 | Val loss: 0.3203 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 09:51:52 (running for 00:27:13.29)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.766 |  0.179 |                   62 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.412 |  0.172 |                   62 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.368 |  0.16  |                   19 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.295 |  0.148 |                   13 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.16026076674461365
[2m[36m(func pid=97073)[0m mae:  0.11580757796764374
[2m[36m(func pid=97073)[0m rmse_per_class: [0.109, 0.249, 0.06, 0.313, 0.065, 0.18, 0.254, 0.131, 0.141, 0.1]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.2851 | Steps: 2 | Val loss: 0.2742 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=87355)[0m rmse: 0.17207711935043335
[2m[36m(func pid=87355)[0m mae:  0.12537537515163422
[2m[36m(func pid=87355)[0m rmse_per_class: [0.115, 0.257, 0.087, 0.323, 0.087, 0.188, 0.274, 0.138, 0.143, 0.109]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.7600 | Steps: 2 | Val loss: 0.5978 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=98661)[0m rmse: 0.14804264903068542
[2m[36m(func pid=98661)[0m mae:  0.09913044422864914
[2m[36m(func pid=98661)[0m rmse_per_class: [0.083, 0.27, 0.031, 0.277, 0.053, 0.157, 0.247, 0.12, 0.138, 0.105]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3619 | Steps: 2 | Val loss: 0.2950 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=86819)[0m rmse: 0.1792776882648468
[2m[36m(func pid=86819)[0m mae:  0.13148978352546692
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.109]
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4078 | Steps: 2 | Val loss: 0.3193 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=86819)[0m 
== Status ==
Current time: 2024-01-07 09:51:57 (running for 00:27:18.55)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.76  |  0.179 |                   63 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.41  |  0.172 |                   63 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.362 |  0.159 |                   20 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.285 |  0.148 |                   14 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.15880736708641052
[2m[36m(func pid=97073)[0m mae:  0.11455689370632172
[2m[36m(func pid=97073)[0m rmse_per_class: [0.107, 0.247, 0.059, 0.311, 0.064, 0.179, 0.251, 0.13, 0.141, 0.099]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.2809 | Steps: 2 | Val loss: 0.2743 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=87355)[0m rmse: 0.17186012864112854
[2m[36m(func pid=87355)[0m mae:  0.12519195675849915
[2m[36m(func pid=87355)[0m rmse_per_class: [0.115, 0.258, 0.087, 0.323, 0.087, 0.187, 0.273, 0.138, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.7620 | Steps: 2 | Val loss: 0.5963 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=98661)[0m rmse: 0.14854475855827332
[2m[36m(func pid=98661)[0m mae:  0.09801812469959259
[2m[36m(func pid=98661)[0m rmse_per_class: [0.081, 0.274, 0.033, 0.275, 0.052, 0.158, 0.241, 0.117, 0.139, 0.114]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3586 | Steps: 2 | Val loss: 0.2919 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4101 | Steps: 2 | Val loss: 0.3185 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=86819)[0m rmse: 0.17924371361732483
[2m[36m(func pid=86819)[0m mae:  0.13145458698272705
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.109]
[2m[36m(func pid=86819)[0m 
== Status ==
Current time: 2024-01-07 09:52:02 (running for 00:27:23.77)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.762 |  0.179 |                   64 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.408 |  0.172 |                   64 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.359 |  0.157 |                   21 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.281 |  0.149 |                   15 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1572965681552887
[2m[36m(func pid=97073)[0m mae:  0.11316907405853271
[2m[36m(func pid=97073)[0m rmse_per_class: [0.106, 0.247, 0.057, 0.308, 0.063, 0.179, 0.246, 0.128, 0.141, 0.099]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.2680 | Steps: 2 | Val loss: 0.2745 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=87355)[0m rmse: 0.17157062888145447
[2m[36m(func pid=87355)[0m mae:  0.1249307170510292
[2m[36m(func pid=87355)[0m rmse_per_class: [0.115, 0.257, 0.086, 0.323, 0.087, 0.187, 0.272, 0.137, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7577 | Steps: 2 | Val loss: 0.5944 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3548 | Steps: 2 | Val loss: 0.2899 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=98661)[0m rmse: 0.14926312863826752
[2m[36m(func pid=98661)[0m mae:  0.09621064364910126
[2m[36m(func pid=98661)[0m rmse_per_class: [0.076, 0.28, 0.033, 0.276, 0.052, 0.163, 0.23, 0.125, 0.136, 0.121]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4054 | Steps: 2 | Val loss: 0.3177 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=86819)[0m rmse: 0.1792224496603012
[2m[36m(func pid=86819)[0m mae:  0.1314391940832138
[2m[36m(func pid=86819)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.109]
[2m[36m(func pid=86819)[0m 
== Status ==
Current time: 2024-01-07 09:52:07 (running for 00:27:28.99)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.758 |  0.179 |                   65 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.41  |  0.172 |                   65 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.355 |  0.156 |                   22 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.268 |  0.149 |                   16 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.15633967518806458
[2m[36m(func pid=97073)[0m mae:  0.11230410635471344
[2m[36m(func pid=97073)[0m rmse_per_class: [0.104, 0.246, 0.056, 0.306, 0.063, 0.179, 0.244, 0.126, 0.141, 0.099]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.1714712381362915
[2m[36m(func pid=87355)[0m mae:  0.12483227252960205
[2m[36m(func pid=87355)[0m rmse_per_class: [0.115, 0.257, 0.086, 0.323, 0.087, 0.188, 0.272, 0.137, 0.142, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.2626 | Steps: 2 | Val loss: 0.2755 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7562 | Steps: 2 | Val loss: 0.5933 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3518 | Steps: 2 | Val loss: 0.2883 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=98661)[0m rmse: 0.14912010729312897
[2m[36m(func pid=98661)[0m mae:  0.0944751650094986
[2m[36m(func pid=98661)[0m rmse_per_class: [0.074, 0.275, 0.033, 0.272, 0.054, 0.17, 0.216, 0.135, 0.135, 0.128]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4049 | Steps: 2 | Val loss: 0.3168 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=86819)[0m rmse: 0.17917880415916443
[2m[36m(func pid=86819)[0m mae:  0.13140268623828888
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=97073)[0m rmse: 0.1556570678949356
[2m[36m(func pid=97073)[0m mae:  0.11176830530166626
[2m[36m(func pid=97073)[0m rmse_per_class: [0.103, 0.246, 0.054, 0.304, 0.062, 0.178, 0.245, 0.126, 0.14, 0.098]
== Status ==
Current time: 2024-01-07 09:52:13 (running for 00:27:34.29)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.756 |  0.179 |                   66 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.405 |  0.171 |                   66 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.352 |  0.156 |                   23 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.263 |  0.149 |                   17 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17128416895866394
[2m[36m(func pid=87355)[0m mae:  0.12466083467006683
[2m[36m(func pid=87355)[0m rmse_per_class: [0.115, 0.257, 0.085, 0.322, 0.087, 0.188, 0.271, 0.137, 0.142, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2648 | Steps: 2 | Val loss: 0.2749 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.7544 | Steps: 2 | Val loss: 0.5916 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3471 | Steps: 2 | Val loss: 0.2868 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=98661)[0m rmse: 0.1473774015903473
[2m[36m(func pid=98661)[0m mae:  0.09298928827047348
[2m[36m(func pid=98661)[0m rmse_per_class: [0.074, 0.271, 0.03, 0.274, 0.06, 0.169, 0.209, 0.123, 0.14, 0.124]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4042 | Steps: 2 | Val loss: 0.3159 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=86819)[0m rmse: 0.1792122721672058
[2m[36m(func pid=86819)[0m mae:  0.13142386078834534
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=86819)[0m 
== Status ==
Current time: 2024-01-07 09:52:18 (running for 00:27:39.38)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.754 |  0.179 |                   67 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.405 |  0.171 |                   67 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.347 |  0.155 |                   24 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.265 |  0.147 |                   18 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.15492898225784302
[2m[36m(func pid=97073)[0m mae:  0.11114557832479477
[2m[36m(func pid=97073)[0m rmse_per_class: [0.101, 0.246, 0.053, 0.303, 0.062, 0.177, 0.245, 0.125, 0.14, 0.097]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.171128049492836
[2m[36m(func pid=87355)[0m mae:  0.12455375492572784
[2m[36m(func pid=87355)[0m rmse_per_class: [0.115, 0.257, 0.085, 0.322, 0.086, 0.187, 0.272, 0.137, 0.142, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.2601 | Steps: 2 | Val loss: 0.2749 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7536 | Steps: 2 | Val loss: 0.5906 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3429 | Steps: 2 | Val loss: 0.2849 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=98661)[0m rmse: 0.1476963758468628
[2m[36m(func pid=98661)[0m mae:  0.0923483744263649
[2m[36m(func pid=98661)[0m rmse_per_class: [0.074, 0.272, 0.028, 0.275, 0.067, 0.167, 0.204, 0.119, 0.137, 0.133]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4049 | Steps: 2 | Val loss: 0.3154 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=86819)[0m rmse: 0.17922106385231018
[2m[36m(func pid=86819)[0m mae:  0.13141600787639618
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=86819)[0m 
== Status ==
Current time: 2024-01-07 09:52:23 (running for 00:27:44.61)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.754 |  0.179 |                   68 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.404 |  0.171 |                   68 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.343 |  0.154 |                   25 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.26  |  0.148 |                   19 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.15418368577957153
[2m[36m(func pid=97073)[0m mae:  0.11053764820098877
[2m[36m(func pid=97073)[0m rmse_per_class: [0.101, 0.246, 0.051, 0.302, 0.061, 0.176, 0.244, 0.124, 0.14, 0.097]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17105822265148163
[2m[36m(func pid=87355)[0m mae:  0.12450821697711945
[2m[36m(func pid=87355)[0m rmse_per_class: [0.115, 0.257, 0.085, 0.321, 0.086, 0.187, 0.272, 0.137, 0.142, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.2480 | Steps: 2 | Val loss: 0.2739 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7512 | Steps: 2 | Val loss: 0.5886 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3441 | Steps: 2 | Val loss: 0.2828 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=98661)[0m rmse: 0.14823053777217865
[2m[36m(func pid=98661)[0m mae:  0.09216753393411636
[2m[36m(func pid=98661)[0m rmse_per_class: [0.077, 0.267, 0.027, 0.281, 0.076, 0.16, 0.202, 0.114, 0.134, 0.145]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3987 | Steps: 2 | Val loss: 0.3143 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=86819)[0m rmse: 0.17918959259986877
[2m[36m(func pid=86819)[0m mae:  0.13137389719486237
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=86819)[0m 
== Status ==
Current time: 2024-01-07 09:52:28 (running for 00:27:49.83)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.751 |  0.179 |                   69 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.405 |  0.171 |                   69 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.344 |  0.153 |                   26 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.248 |  0.148 |                   20 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.15338896214962006
[2m[36m(func pid=97073)[0m mae:  0.10989052057266235
[2m[36m(func pid=97073)[0m rmse_per_class: [0.1, 0.246, 0.05, 0.3, 0.061, 0.174, 0.243, 0.122, 0.14, 0.097]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17081299424171448
[2m[36m(func pid=87355)[0m mae:  0.1242997869849205
[2m[36m(func pid=87355)[0m rmse_per_class: [0.115, 0.257, 0.084, 0.321, 0.086, 0.187, 0.271, 0.137, 0.142, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.2572 | Steps: 2 | Val loss: 0.2744 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.7483 | Steps: 2 | Val loss: 0.5875 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3333 | Steps: 2 | Val loss: 0.2811 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=98661)[0m rmse: 0.14944294095039368
[2m[36m(func pid=98661)[0m mae:  0.09278714656829834
[2m[36m(func pid=98661)[0m rmse_per_class: [0.081, 0.26, 0.025, 0.288, 0.083, 0.155, 0.203, 0.111, 0.138, 0.151]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4011 | Steps: 2 | Val loss: 0.3136 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 09:52:33 (running for 00:27:55.01)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.748 |  0.179 |                   70 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.399 |  0.171 |                   70 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.344 |  0.153 |                   26 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.257 |  0.149 |                   21 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=86819)[0m rmse: 0.17922334372997284
[2m[36m(func pid=86819)[0m mae:  0.13139626383781433
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=97073)[0m rmse: 0.1526680290699005
[2m[36m(func pid=97073)[0m mae:  0.10923377424478531
[2m[36m(func pid=97073)[0m rmse_per_class: [0.1, 0.245, 0.049, 0.3, 0.061, 0.173, 0.241, 0.121, 0.14, 0.097]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17068380117416382
[2m[36m(func pid=87355)[0m mae:  0.12419555336236954
[2m[36m(func pid=87355)[0m rmse_per_class: [0.115, 0.257, 0.084, 0.321, 0.085, 0.187, 0.271, 0.137, 0.142, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.2464 | Steps: 2 | Val loss: 0.2708 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.7485 | Steps: 2 | Val loss: 0.5857 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3352 | Steps: 2 | Val loss: 0.2799 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=98661)[0m rmse: 0.14651362597942352
[2m[36m(func pid=98661)[0m mae:  0.09039434045553207
[2m[36m(func pid=98661)[0m rmse_per_class: [0.078, 0.256, 0.024, 0.282, 0.081, 0.154, 0.202, 0.108, 0.131, 0.149]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3983 | Steps: 2 | Val loss: 0.3127 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 09:52:39 (running for 00:28:00.31)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.748 |  0.179 |                   70 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.401 |  0.171 |                   71 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.335 |  0.152 |                   28 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.246 |  0.147 |                   22 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.15195301175117493
[2m[36m(func pid=97073)[0m mae:  0.10868363082408905
[2m[36m(func pid=97073)[0m rmse_per_class: [0.099, 0.245, 0.047, 0.3, 0.061, 0.171, 0.242, 0.121, 0.139, 0.095]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.1791970282793045
[2m[36m(func pid=86819)[0m mae:  0.13137581944465637
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17055223882198334
[2m[36m(func pid=87355)[0m mae:  0.12408530712127686
[2m[36m(func pid=87355)[0m rmse_per_class: [0.115, 0.256, 0.084, 0.321, 0.085, 0.187, 0.27, 0.137, 0.142, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2484 | Steps: 2 | Val loss: 0.2713 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3310 | Steps: 2 | Val loss: 0.2792 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.7448 | Steps: 2 | Val loss: 0.5843 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=98661)[0m rmse: 0.14638972282409668
[2m[36m(func pid=98661)[0m mae:  0.08993266522884369
[2m[36m(func pid=98661)[0m rmse_per_class: [0.074, 0.266, 0.024, 0.28, 0.081, 0.151, 0.203, 0.108, 0.128, 0.15]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3984 | Steps: 2 | Val loss: 0.3123 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 09:52:44 (running for 00:28:05.69)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.749 |  0.179 |                   71 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.398 |  0.171 |                   72 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.331 |  0.152 |                   29 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.248 |  0.146 |                   23 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1516350656747818
[2m[36m(func pid=97073)[0m mae:  0.10847906023263931
[2m[36m(func pid=97073)[0m rmse_per_class: [0.098, 0.245, 0.046, 0.299, 0.061, 0.17, 0.242, 0.12, 0.139, 0.095]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17916108667850494
[2m[36m(func pid=86819)[0m mae:  0.13133253157138824
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.17038878798484802
[2m[36m(func pid=87355)[0m mae:  0.12395676225423813
[2m[36m(func pid=87355)[0m rmse_per_class: [0.114, 0.256, 0.083, 0.321, 0.085, 0.187, 0.27, 0.137, 0.142, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2399 | Steps: 2 | Val loss: 0.2732 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3239 | Steps: 2 | Val loss: 0.2780 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.7438 | Steps: 2 | Val loss: 0.5821 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=98661)[0m rmse: 0.14762072265148163
[2m[36m(func pid=98661)[0m mae:  0.09146611392498016
[2m[36m(func pid=98661)[0m rmse_per_class: [0.077, 0.27, 0.024, 0.287, 0.083, 0.148, 0.206, 0.109, 0.128, 0.144]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3977 | Steps: 2 | Val loss: 0.3115 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 09:52:50 (running for 00:28:11.14)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.745 |  0.179 |                   72 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.398 |  0.17  |                   73 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.324 |  0.151 |                   30 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.24  |  0.148 |                   24 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.15098915994167328
[2m[36m(func pid=97073)[0m mae:  0.10790747404098511
[2m[36m(func pid=97073)[0m rmse_per_class: [0.098, 0.244, 0.045, 0.298, 0.061, 0.169, 0.242, 0.12, 0.139, 0.095]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17916767299175262
[2m[36m(func pid=86819)[0m mae:  0.1313328891992569
[2m[36m(func pid=86819)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.1702250987291336
[2m[36m(func pid=87355)[0m mae:  0.12383256107568741
[2m[36m(func pid=87355)[0m rmse_per_class: [0.114, 0.256, 0.083, 0.321, 0.085, 0.187, 0.27, 0.137, 0.143, 0.108]
[2m[36m(func pid=87355)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2446 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3242 | Steps: 2 | Val loss: 0.2766 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7406 | Steps: 2 | Val loss: 0.5800 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=98661)[0m rmse: 0.14807122945785522
[2m[36m(func pid=98661)[0m mae:  0.09198080003261566
[2m[36m(func pid=98661)[0m rmse_per_class: [0.081, 0.267, 0.024, 0.291, 0.081, 0.148, 0.209, 0.109, 0.129, 0.141]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=87355)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3955 | Steps: 2 | Val loss: 0.3108 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 09:52:55 (running for 00:28:16.45)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: -0.14750000089406967
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819 | 0.0001 |       0.9  |         0.0001 |  0.744 |  0.179 |                   73 |
| train_32e5a_00013 | RUNNING    | 192.168.7.53:87355 | 0.001  |       0.9  |         0.0001 |  0.398 |  0.17  |                   74 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073 | 0.01   |       0.9  |         0.0001 |  0.324 |  0.15  |                   31 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661 | 0.1    |       0.9  |         0.0001 |  0.245 |  0.148 |                   25 |
| train_32e5a_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042 | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415 | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838 | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262 | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709 | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774 | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346 | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869 | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.15027925372123718
[2m[36m(func pid=97073)[0m mae:  0.1071106567978859
[2m[36m(func pid=97073)[0m rmse_per_class: [0.097, 0.244, 0.044, 0.297, 0.061, 0.168, 0.238, 0.119, 0.138, 0.096]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.17914333939552307
[2m[36m(func pid=86819)[0m mae:  0.13131532073020935
[2m[36m(func pid=86819)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=86819)[0m 
[2m[36m(func pid=87355)[0m rmse: 0.1701153963804245
[2m[36m(func pid=87355)[0m mae:  0.12373660504817963
[2m[36m(func pid=87355)[0m rmse_per_class: [0.114, 0.256, 0.083, 0.32, 0.084, 0.187, 0.269, 0.137, 0.142, 0.108]
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2456 | Steps: 2 | Val loss: 0.2749 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3266 | Steps: 2 | Val loss: 0.2757 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=86819)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.7398 | Steps: 2 | Val loss: 0.5784 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=98661)[0m rmse: 0.14867158234119415
[2m[36m(func pid=98661)[0m mae:  0.09251339733600616
[2m[36m(func pid=98661)[0m rmse_per_class: [0.085, 0.261, 0.024, 0.293, 0.079, 0.149, 0.21, 0.112, 0.133, 0.14]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m rmse: 0.14975938200950623
[2m[36m(func pid=97073)[0m mae:  0.10666485875844955
[2m[36m(func pid=97073)[0m rmse_per_class: [0.097, 0.244, 0.043, 0.295, 0.061, 0.168, 0.238, 0.118, 0.138, 0.096]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=86819)[0m rmse: 0.1791246384382248
[2m[36m(func pid=86819)[0m mae:  0.1312817931175232
[2m[36m(func pid=86819)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.336, 0.107, 0.19, 0.29, 0.141, 0.143, 0.109]
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2491 | Steps: 2 | Val loss: 0.2757 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3205 | Steps: 2 | Val loss: 0.2760 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=98661)[0m rmse: 0.14944672584533691
[2m[36m(func pid=98661)[0m mae:  0.09281590580940247
[2m[36m(func pid=98661)[0m rmse_per_class: [0.091, 0.254, 0.024, 0.294, 0.076, 0.149, 0.212, 0.113, 0.135, 0.147]
== Status ==
Current time: 2024-01-07 09:53:00 (running for 00:28:21.74)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: -0.14800000190734863
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00012 | RUNNING    | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.741 |  0.179 |                   74 |
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.327 |  0.15  |                   32 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.246 |  0.149 |                   26 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=104571)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=104571)[0m Configuration completed!
[2m[36m(func pid=104571)[0m New optimizer parameters:
[2m[36m(func pid=104571)[0m SGD (
[2m[36m(func pid=104571)[0m Parameter Group 0
[2m[36m(func pid=104571)[0m     dampening: 0
[2m[36m(func pid=104571)[0m     differentiable: False
[2m[36m(func pid=104571)[0m     foreach: None
[2m[36m(func pid=104571)[0m     lr: 0.0001
[2m[36m(func pid=104571)[0m     maximize: False
[2m[36m(func pid=104571)[0m     momentum: 0.99
[2m[36m(func pid=104571)[0m     nesterov: False
[2m[36m(func pid=104571)[0m     weight_decay: 1e-05
[2m[36m(func pid=104571)[0m )
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m rmse: 0.14973919093608856
[2m[36m(func pid=97073)[0m mae:  0.10679608583450317
[2m[36m(func pid=97073)[0m rmse_per_class: [0.096, 0.245, 0.043, 0.294, 0.061, 0.166, 0.242, 0.118, 0.138, 0.095]
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2406 | Steps: 2 | Val loss: 0.2743 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8975 | Steps: 2 | Val loss: 0.7089 | Batch size: 32 | lr: 0.0001 | Duration: 4.96s
[2m[36m(func pid=98661)[0m rmse: 0.14857986569404602
[2m[36m(func pid=98661)[0m mae:  0.09190411865711212
[2m[36m(func pid=98661)[0m rmse_per_class: [0.089, 0.25, 0.023, 0.293, 0.072, 0.148, 0.212, 0.113, 0.134, 0.151]
[2m[36m(func pid=104571)[0m rmse: 0.18270526826381683
[2m[36m(func pid=104571)[0m mae:  0.13443787395954132
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
== Status ==
Current time: 2024-01-07 09:53:06 (running for 00:28:27.30)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.327 |  0.15  |                   32 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.249 |  0.149 |                   27 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 09:53:14 (running for 00:28:35.18)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.321 |  0.15  |                   33 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.249 |  0.149 |                   27 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=105127)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=105127)[0m Configuration completed!
[2m[36m(func pid=105127)[0m New optimizer parameters:
[2m[36m(func pid=105127)[0m SGD (
[2m[36m(func pid=105127)[0m Parameter Group 0
[2m[36m(func pid=105127)[0m     dampening: 0
[2m[36m(func pid=105127)[0m     differentiable: False
[2m[36m(func pid=105127)[0m     foreach: None
[2m[36m(func pid=105127)[0m     lr: 0.001
[2m[36m(func pid=105127)[0m     maximize: False
[2m[36m(func pid=105127)[0m     momentum: 0.99
[2m[36m(func pid=105127)[0m     nesterov: False
[2m[36m(func pid=105127)[0m     weight_decay: 1e-05
[2m[36m(func pid=105127)[0m )
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3205 | Steps: 2 | Val loss: 0.2761 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2401 | Steps: 2 | Val loss: 0.2735 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8985 | Steps: 2 | Val loss: 0.7051 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8971 | Steps: 2 | Val loss: 0.7073 | Batch size: 32 | lr: 0.001 | Duration: 4.70s
== Status ==
Current time: 2024-01-07 09:53:19 (running for 00:28:40.22)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.321 |  0.15  |                   33 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.241 |  0.149 |                   28 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.898 |  0.183 |                    1 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1496826708316803
[2m[36m(func pid=97073)[0m mae:  0.10686119645833969
[2m[36m(func pid=97073)[0m rmse_per_class: [0.096, 0.245, 0.042, 0.295, 0.061, 0.164, 0.245, 0.118, 0.138, 0.094]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14808489382266998
[2m[36m(func pid=98661)[0m mae:  0.09090934693813324
[2m[36m(func pid=98661)[0m rmse_per_class: [0.089, 0.248, 0.024, 0.288, 0.069, 0.148, 0.212, 0.112, 0.133, 0.159]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.18247994780540466
[2m[36m(func pid=104571)[0m mae:  0.1343075931072235
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.113]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.18269972503185272
[2m[36m(func pid=105127)[0m mae:  0.13443322479724884
[2m[36m(func pid=105127)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3165 | Steps: 2 | Val loss: 0.2752 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2434 | Steps: 2 | Val loss: 0.2721 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8948 | Steps: 2 | Val loss: 0.7016 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8940 | Steps: 2 | Val loss: 0.6990 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 09:53:24 (running for 00:28:45.62)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.317 |  0.149 |                   35 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.24  |  0.148 |                   29 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.898 |  0.182 |                    2 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.897 |  0.183 |                    1 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14925719797611237
[2m[36m(func pid=97073)[0m mae:  0.10630428791046143
[2m[36m(func pid=97073)[0m rmse_per_class: [0.096, 0.245, 0.042, 0.293, 0.061, 0.164, 0.243, 0.117, 0.138, 0.095]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14695532619953156
[2m[36m(func pid=98661)[0m mae:  0.08912158012390137
[2m[36m(func pid=98661)[0m rmse_per_class: [0.089, 0.248, 0.023, 0.279, 0.064, 0.149, 0.209, 0.112, 0.137, 0.161]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.18211564421653748
[2m[36m(func pid=104571)[0m mae:  0.13404573500156403
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.113, 0.19, 0.294, 0.142, 0.143, 0.112]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.18248878419399261
[2m[36m(func pid=105127)[0m mae:  0.13431338965892792
[2m[36m(func pid=105127)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.143, 0.144, 0.113]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3140 | Steps: 2 | Val loss: 0.2742 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2378 | Steps: 2 | Val loss: 0.2714 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8930 | Steps: 2 | Val loss: 0.6984 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8814 | Steps: 2 | Val loss: 0.6877 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=97073)[0m rmse: 0.14882713556289673
[2m[36m(func pid=97073)[0m mae:  0.10568702220916748
[2m[36m(func pid=97073)[0m rmse_per_class: [0.096, 0.245, 0.042, 0.291, 0.061, 0.165, 0.239, 0.116, 0.137, 0.096]
[2m[36m(func pid=97073)[0m 
== Status ==
Current time: 2024-01-07 09:53:29 (running for 00:28:50.87)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.314 |  0.149 |                   36 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.243 |  0.147 |                   30 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.895 |  0.182 |                    3 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.894 |  0.182 |                    2 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=98661)[0m rmse: 0.14557310938835144
[2m[36m(func pid=98661)[0m mae:  0.08790707588195801
[2m[36m(func pid=98661)[0m rmse_per_class: [0.086, 0.246, 0.024, 0.274, 0.061, 0.149, 0.207, 0.111, 0.14, 0.158]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.18173877894878387
[2m[36m(func pid=104571)[0m mae:  0.13374599814414978
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.339, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.18212875723838806
[2m[36m(func pid=105127)[0m mae:  0.13403764367103577
[2m[36m(func pid=105127)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.113, 0.19, 0.294, 0.142, 0.143, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3141 | Steps: 2 | Val loss: 0.2738 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2372 | Steps: 2 | Val loss: 0.2697 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8932 | Steps: 2 | Val loss: 0.6948 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8658 | Steps: 2 | Val loss: 0.6729 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 09:53:34 (running for 00:28:55.92)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.314 |  0.148 |                   37 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.238 |  0.146 |                   31 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.893 |  0.182 |                    4 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.881 |  0.182 |                    3 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14833983778953552
[2m[36m(func pid=97073)[0m mae:  0.10533218085765839
[2m[36m(func pid=97073)[0m rmse_per_class: [0.095, 0.245, 0.041, 0.29, 0.061, 0.163, 0.24, 0.115, 0.137, 0.096]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.1432630568742752
[2m[36m(func pid=98661)[0m mae:  0.08642061799764633
[2m[36m(func pid=98661)[0m rmse_per_class: [0.08, 0.249, 0.024, 0.274, 0.061, 0.149, 0.203, 0.109, 0.137, 0.146]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.18137478828430176
[2m[36m(func pid=104571)[0m mae:  0.13344714045524597
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.18162551522254944
[2m[36m(func pid=105127)[0m mae:  0.13362330198287964
[2m[36m(func pid=105127)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.338, 0.112, 0.19, 0.293, 0.142, 0.143, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3200 | Steps: 2 | Val loss: 0.2736 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2387 | Steps: 2 | Val loss: 0.2703 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8912 | Steps: 2 | Val loss: 0.6915 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8463 | Steps: 2 | Val loss: 0.6557 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 09:53:40 (running for 00:29:01.26)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.32  |  0.148 |                   38 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.237 |  0.143 |                   32 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.893 |  0.181 |                    5 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.866 |  0.182 |                    4 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1480570137500763
[2m[36m(func pid=97073)[0m mae:  0.10514726489782333
[2m[36m(func pid=97073)[0m rmse_per_class: [0.094, 0.245, 0.04, 0.289, 0.061, 0.163, 0.242, 0.114, 0.138, 0.096]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14338377118110657
[2m[36m(func pid=98661)[0m mae:  0.08687092363834381
[2m[36m(func pid=98661)[0m rmse_per_class: [0.078, 0.26, 0.024, 0.278, 0.064, 0.148, 0.204, 0.108, 0.133, 0.139]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.18101820349693298
[2m[36m(func pid=104571)[0m mae:  0.133148193359375
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.112, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.18109624087810516
[2m[36m(func pid=105127)[0m mae:  0.13316376507282257
[2m[36m(func pid=105127)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.11, 0.19, 0.292, 0.141, 0.143, 0.111]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3126 | Steps: 2 | Val loss: 0.2735 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2332 | Steps: 2 | Val loss: 0.2715 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8215 | Steps: 2 | Val loss: 0.6367 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8872 | Steps: 2 | Val loss: 0.6894 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 09:53:45 (running for 00:29:06.47)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.313 |  0.148 |                   39 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.239 |  0.143 |                   33 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.891 |  0.181 |                    6 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.846 |  0.181 |                    5 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1477607786655426
[2m[36m(func pid=97073)[0m mae:  0.10493601858615875
[2m[36m(func pid=97073)[0m rmse_per_class: [0.093, 0.245, 0.039, 0.289, 0.061, 0.162, 0.242, 0.114, 0.137, 0.094]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14394815266132355
[2m[36m(func pid=98661)[0m mae:  0.0879673957824707
[2m[36m(func pid=98661)[0m rmse_per_class: [0.079, 0.266, 0.023, 0.285, 0.066, 0.147, 0.205, 0.107, 0.13, 0.132]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.18060636520385742
[2m[36m(func pid=105127)[0m mae:  0.13272029161453247
[2m[36m(func pid=105127)[0m rmse_per_class: [0.116, 0.264, 0.104, 0.338, 0.109, 0.19, 0.29, 0.141, 0.143, 0.111]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.18073350191116333
[2m[36m(func pid=104571)[0m mae:  0.13290563225746155
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.264, 0.103, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3086 | Steps: 2 | Val loss: 0.2724 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2318 | Steps: 2 | Val loss: 0.2721 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.7970 | Steps: 2 | Val loss: 0.6149 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8821 | Steps: 2 | Val loss: 0.6877 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 09:53:50 (running for 00:29:11.63)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.309 |  0.147 |                   40 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.233 |  0.144 |                   34 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.887 |  0.181 |                    7 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.821 |  0.181 |                    6 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1471436768770218
[2m[36m(func pid=97073)[0m mae:  0.10418806225061417
[2m[36m(func pid=97073)[0m rmse_per_class: [0.092, 0.246, 0.039, 0.287, 0.061, 0.162, 0.239, 0.113, 0.137, 0.095]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14408472180366516
[2m[36m(func pid=98661)[0m mae:  0.08822990953922272
[2m[36m(func pid=98661)[0m rmse_per_class: [0.079, 0.268, 0.023, 0.288, 0.068, 0.147, 0.204, 0.107, 0.128, 0.128]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.1801164746284485
[2m[36m(func pid=105127)[0m mae:  0.13224154710769653
[2m[36m(func pid=105127)[0m rmse_per_class: [0.116, 0.264, 0.103, 0.337, 0.107, 0.19, 0.289, 0.141, 0.143, 0.111]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.1804962009191513
[2m[36m(func pid=104571)[0m mae:  0.13269364833831787
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.263, 0.102, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3115 | Steps: 2 | Val loss: 0.2718 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2387 | Steps: 2 | Val loss: 0.2713 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.7653 | Steps: 2 | Val loss: 0.5918 | Batch size: 32 | lr: 0.001 | Duration: 3.24s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8780 | Steps: 2 | Val loss: 0.6853 | Batch size: 32 | lr: 0.0001 | Duration: 3.42s
== Status ==
Current time: 2024-01-07 09:53:56 (running for 00:29:17.12)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.311 |  0.147 |                   41 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.232 |  0.144 |                   35 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.882 |  0.18  |                    8 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.797 |  0.18  |                    7 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1467653214931488
[2m[36m(func pid=97073)[0m mae:  0.1037224680185318
[2m[36m(func pid=97073)[0m rmse_per_class: [0.092, 0.246, 0.038, 0.286, 0.061, 0.163, 0.237, 0.112, 0.137, 0.096]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14371176064014435
[2m[36m(func pid=98661)[0m mae:  0.08752612769603729
[2m[36m(func pid=98661)[0m rmse_per_class: [0.079, 0.263, 0.023, 0.284, 0.069, 0.147, 0.202, 0.108, 0.13, 0.133]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.17973679304122925
[2m[36m(func pid=105127)[0m mae:  0.13185670971870422
[2m[36m(func pid=105127)[0m rmse_per_class: [0.116, 0.263, 0.102, 0.337, 0.106, 0.19, 0.288, 0.141, 0.143, 0.111]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.18033164739608765
[2m[36m(func pid=104571)[0m mae:  0.1325264573097229
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.263, 0.102, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3100 | Steps: 2 | Val loss: 0.2722 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2386 | Steps: 2 | Val loss: 0.2713 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7350 | Steps: 2 | Val loss: 0.5669 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8757 | Steps: 2 | Val loss: 0.6824 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 09:54:01 (running for 00:29:22.54)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.31  |  0.147 |                   42 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.239 |  0.144 |                   36 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.878 |  0.18  |                    9 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.765 |  0.18  |                    8 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14670127630233765
[2m[36m(func pid=97073)[0m mae:  0.10371110588312149
[2m[36m(func pid=97073)[0m rmse_per_class: [0.09, 0.246, 0.037, 0.287, 0.062, 0.161, 0.241, 0.112, 0.137, 0.095]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14373043179512024
[2m[36m(func pid=98661)[0m mae:  0.08736386150121689
[2m[36m(func pid=98661)[0m rmse_per_class: [0.078, 0.259, 0.023, 0.283, 0.07, 0.147, 0.201, 0.109, 0.131, 0.136]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.17934581637382507
[2m[36m(func pid=105127)[0m mae:  0.13147303462028503
[2m[36m(func pid=105127)[0m rmse_per_class: [0.117, 0.263, 0.101, 0.336, 0.105, 0.189, 0.287, 0.141, 0.143, 0.111]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.18014231324195862
[2m[36m(func pid=104571)[0m mae:  0.132352814078331
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.262, 0.101, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2992 | Steps: 2 | Val loss: 0.2734 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2401 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7029 | Steps: 2 | Val loss: 0.5406 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 09:54:06 (running for 00:29:27.87)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.299 |  0.147 |                   43 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.239 |  0.144 |                   37 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.876 |  0.18  |                   10 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.735 |  0.179 |                    9 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14726604521274567
[2m[36m(func pid=97073)[0m mae:  0.10424944013357162
[2m[36m(func pid=97073)[0m rmse_per_class: [0.09, 0.246, 0.036, 0.289, 0.063, 0.16, 0.244, 0.113, 0.137, 0.094]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8690 | Steps: 2 | Val loss: 0.6795 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=98661)[0m rmse: 0.1448141187429428
[2m[36m(func pid=98661)[0m mae:  0.0883529931306839
[2m[36m(func pid=98661)[0m rmse_per_class: [0.083, 0.255, 0.023, 0.285, 0.071, 0.147, 0.202, 0.11, 0.13, 0.141]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.1789902150630951
[2m[36m(func pid=105127)[0m mae:  0.13112439215183258
[2m[36m(func pid=105127)[0m rmse_per_class: [0.117, 0.263, 0.1, 0.336, 0.104, 0.189, 0.286, 0.141, 0.143, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.1799810230731964
[2m[36m(func pid=104571)[0m mae:  0.1322140395641327
[2m[36m(func pid=104571)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3062 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2400 | Steps: 2 | Val loss: 0.2753 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.6685 | Steps: 2 | Val loss: 0.5147 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 09:54:12 (running for 00:29:33.29)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.306 |  0.147 |                   44 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.24  |  0.145 |                   38 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.869 |  0.18  |                   11 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.703 |  0.179 |                   10 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14718131721019745
[2m[36m(func pid=97073)[0m mae:  0.10382300615310669
[2m[36m(func pid=97073)[0m rmse_per_class: [0.09, 0.249, 0.037, 0.289, 0.063, 0.161, 0.241, 0.111, 0.137, 0.095]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14627094566822052
[2m[36m(func pid=98661)[0m mae:  0.08990270644426346
[2m[36m(func pid=98661)[0m rmse_per_class: [0.091, 0.248, 0.023, 0.293, 0.072, 0.146, 0.205, 0.112, 0.131, 0.141]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8651 | Steps: 2 | Val loss: 0.6754 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=105127)[0m rmse: 0.17867116630077362
[2m[36m(func pid=105127)[0m mae:  0.1308165192604065
[2m[36m(func pid=105127)[0m rmse_per_class: [0.117, 0.262, 0.099, 0.335, 0.103, 0.19, 0.285, 0.141, 0.143, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3007 | Steps: 2 | Val loss: 0.2721 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=104571)[0m rmse: 0.17984864115715027
[2m[36m(func pid=104571)[0m mae:  0.13209713995456696
[2m[36m(func pid=104571)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2340 | Steps: 2 | Val loss: 0.2797 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.6386 | Steps: 2 | Val loss: 0.4884 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 09:54:17 (running for 00:29:38.73)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.301 |  0.147 |                   45 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.24  |  0.146 |                   39 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.865 |  0.18  |                   12 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.669 |  0.179 |                   11 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1470116674900055
[2m[36m(func pid=97073)[0m mae:  0.10334555804729462
[2m[36m(func pid=97073)[0m rmse_per_class: [0.09, 0.25, 0.037, 0.288, 0.064, 0.163, 0.235, 0.11, 0.136, 0.098]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14969554543495178
[2m[36m(func pid=98661)[0m mae:  0.09267206490039825
[2m[36m(func pid=98661)[0m rmse_per_class: [0.107, 0.243, 0.024, 0.3, 0.075, 0.146, 0.21, 0.113, 0.131, 0.148]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8620 | Steps: 2 | Val loss: 0.6712 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=105127)[0m rmse: 0.17829999327659607
[2m[36m(func pid=105127)[0m mae:  0.13047519326210022
[2m[36m(func pid=105127)[0m rmse_per_class: [0.117, 0.262, 0.097, 0.334, 0.102, 0.19, 0.284, 0.141, 0.142, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3014 | Steps: 2 | Val loss: 0.2719 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2279 | Steps: 2 | Val loss: 0.2784 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=104571)[0m rmse: 0.1797211468219757
[2m[36m(func pid=104571)[0m mae:  0.13197535276412964
[2m[36m(func pid=104571)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.6032 | Steps: 2 | Val loss: 0.4629 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 09:54:22 (running for 00:29:43.98)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.301 |  0.147 |                   46 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.234 |  0.15  |                   40 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.862 |  0.18  |                   13 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.639 |  0.178 |                   12 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14679795503616333
[2m[36m(func pid=97073)[0m mae:  0.10315511375665665
[2m[36m(func pid=97073)[0m rmse_per_class: [0.089, 0.25, 0.037, 0.287, 0.064, 0.161, 0.237, 0.11, 0.136, 0.097]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14954698085784912
[2m[36m(func pid=98661)[0m mae:  0.09254904091358185
[2m[36m(func pid=98661)[0m rmse_per_class: [0.113, 0.241, 0.024, 0.297, 0.074, 0.147, 0.21, 0.114, 0.13, 0.147]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.1779002994298935
[2m[36m(func pid=105127)[0m mae:  0.13009653985500336
[2m[36m(func pid=105127)[0m rmse_per_class: [0.117, 0.262, 0.097, 0.334, 0.101, 0.19, 0.283, 0.141, 0.142, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8528 | Steps: 2 | Val loss: 0.6660 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2992 | Steps: 2 | Val loss: 0.2728 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2368 | Steps: 2 | Val loss: 0.2746 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=104571)[0m rmse: 0.17963023483753204
[2m[36m(func pid=104571)[0m mae:  0.1318972408771515
[2m[36m(func pid=104571)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5728 | Steps: 2 | Val loss: 0.4395 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 09:54:28 (running for 00:29:49.39)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.299 |  0.147 |                   47 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.228 |  0.15  |                   41 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.853 |  0.18  |                   14 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.603 |  0.178 |                   13 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14676061272621155
[2m[36m(func pid=97073)[0m mae:  0.10345194488763809
[2m[36m(func pid=97073)[0m rmse_per_class: [0.088, 0.248, 0.035, 0.286, 0.065, 0.158, 0.244, 0.112, 0.137, 0.095]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14733383059501648
[2m[36m(func pid=98661)[0m mae:  0.09038902819156647
[2m[36m(func pid=98661)[0m rmse_per_class: [0.106, 0.246, 0.023, 0.287, 0.07, 0.147, 0.208, 0.113, 0.129, 0.145]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.17757293581962585
[2m[36m(func pid=105127)[0m mae:  0.12980102002620697
[2m[36m(func pid=105127)[0m rmse_per_class: [0.117, 0.262, 0.096, 0.333, 0.099, 0.191, 0.283, 0.141, 0.142, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8477 | Steps: 2 | Val loss: 0.6617 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3000 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2433 | Steps: 2 | Val loss: 0.2704 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=104571)[0m rmse: 0.17954055964946747
[2m[36m(func pid=104571)[0m mae:  0.1318127065896988
[2m[36m(func pid=104571)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5435 | Steps: 2 | Val loss: 0.4169 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=97073)[0m rmse: 0.1467486321926117
[2m[36m(func pid=97073)[0m mae:  0.10345375537872314
[2m[36m(func pid=97073)[0m rmse_per_class: [0.088, 0.247, 0.034, 0.285, 0.065, 0.157, 0.246, 0.114, 0.136, 0.096]
[2m[36m(func pid=97073)[0m 
== Status ==
Current time: 2024-01-07 09:54:33 (running for 00:29:54.51)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.3   |  0.147 |                   48 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.237 |  0.147 |                   42 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.848 |  0.18  |                   15 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.573 |  0.178 |                   14 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=98661)[0m rmse: 0.14406992495059967
[2m[36m(func pid=98661)[0m mae:  0.08716918528079987
[2m[36m(func pid=98661)[0m rmse_per_class: [0.095, 0.247, 0.023, 0.275, 0.066, 0.146, 0.204, 0.111, 0.13, 0.143]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.1771697700023651
[2m[36m(func pid=105127)[0m mae:  0.12945206463336945
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.261, 0.095, 0.332, 0.098, 0.191, 0.282, 0.141, 0.142, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8420 | Steps: 2 | Val loss: 0.6562 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2938 | Steps: 2 | Val loss: 0.2707 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2309 | Steps: 2 | Val loss: 0.2703 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5178 | Steps: 2 | Val loss: 0.3962 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=104571)[0m rmse: 0.1794956624507904
[2m[36m(func pid=104571)[0m mae:  0.13177666068077087
[2m[36m(func pid=104571)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.109, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=104571)[0m 
== Status ==
Current time: 2024-01-07 09:54:38 (running for 00:29:59.70)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.294 |  0.146 |                   49 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.243 |  0.144 |                   43 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.842 |  0.179 |                   16 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.543 |  0.177 |                   15 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14572587609291077
[2m[36m(func pid=97073)[0m mae:  0.10216732323169708
[2m[36m(func pid=97073)[0m rmse_per_class: [0.087, 0.249, 0.035, 0.281, 0.065, 0.158, 0.239, 0.11, 0.135, 0.098]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.1436099410057068
[2m[36m(func pid=98661)[0m mae:  0.08670765906572342
[2m[36m(func pid=98661)[0m rmse_per_class: [0.09, 0.249, 0.023, 0.277, 0.065, 0.147, 0.204, 0.111, 0.127, 0.143]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.17672638595104218
[2m[36m(func pid=105127)[0m mae:  0.1290530413389206
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.261, 0.094, 0.331, 0.096, 0.191, 0.281, 0.14, 0.142, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8333 | Steps: 2 | Val loss: 0.6511 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3033 | Steps: 2 | Val loss: 0.2695 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2373 | Steps: 2 | Val loss: 0.2720 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.4916 | Steps: 2 | Val loss: 0.3777 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=104571)[0m rmse: 0.17944595217704773
[2m[36m(func pid=104571)[0m mae:  0.13173161447048187
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.336, 0.109, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m rmse: 0.1452687382698059
[2m[36m(func pid=97073)[0m mae:  0.10137345641851425
[2m[36m(func pid=97073)[0m rmse_per_class: [0.087, 0.249, 0.035, 0.279, 0.065, 0.162, 0.231, 0.11, 0.134, 0.101]
[2m[36m(func pid=97073)[0m 
== Status ==
Current time: 2024-01-07 09:54:43 (running for 00:30:04.80)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.303 |  0.145 |                   50 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.231 |  0.144 |                   44 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.833 |  0.179 |                   17 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.518 |  0.177 |                   16 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=98661)[0m rmse: 0.14495931565761566
[2m[36m(func pid=98661)[0m mae:  0.08775056898593903
[2m[36m(func pid=98661)[0m rmse_per_class: [0.092, 0.251, 0.024, 0.281, 0.065, 0.148, 0.206, 0.109, 0.129, 0.146]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.17631325125694275
[2m[36m(func pid=105127)[0m mae:  0.12870299816131592
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.261, 0.094, 0.33, 0.095, 0.191, 0.28, 0.14, 0.142, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8246 | Steps: 2 | Val loss: 0.6457 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2918 | Steps: 2 | Val loss: 0.2695 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2376 | Steps: 2 | Val loss: 0.2732 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.4754 | Steps: 2 | Val loss: 0.3613 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 09:54:48 (running for 00:30:10.05)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.292 |  0.145 |                   51 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.237 |  0.145 |                   45 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.833 |  0.179 |                   17 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.492 |  0.176 |                   17 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14492860436439514
[2m[36m(func pid=97073)[0m mae:  0.10119060426950455
[2m[36m(func pid=97073)[0m rmse_per_class: [0.086, 0.249, 0.035, 0.279, 0.066, 0.158, 0.234, 0.109, 0.134, 0.098]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.1794152408838272
[2m[36m(func pid=104571)[0m mae:  0.1316845715045929
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.336, 0.109, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.17580991983413696
[2m[36m(func pid=105127)[0m mae:  0.1282711923122406
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.261, 0.093, 0.33, 0.093, 0.191, 0.279, 0.14, 0.143, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14612887799739838
[2m[36m(func pid=98661)[0m mae:  0.08870493620634079
[2m[36m(func pid=98661)[0m rmse_per_class: [0.092, 0.252, 0.024, 0.286, 0.067, 0.148, 0.206, 0.107, 0.133, 0.147]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.3012 | Steps: 2 | Val loss: 0.2707 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8201 | Steps: 2 | Val loss: 0.6400 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4556 | Steps: 2 | Val loss: 0.3480 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2299 | Steps: 2 | Val loss: 0.2740 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=97073)[0m rmse: 0.14521141350269318
== Status ==
Current time: 2024-01-07 09:54:54 (running for 00:30:15.49)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.301 |  0.145 |                   52 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.238 |  0.146 |                   46 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.825 |  0.179 |                   18 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.475 |  0.176 |                   18 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=97073)[0m mae:  0.10163803398609161

[2m[36m(func pid=97073)[0m rmse_per_class: [0.085, 0.247, 0.033, 0.28, 0.066, 0.155, 0.242, 0.111, 0.136, 0.097]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17939305305480957
[2m[36m(func pid=104571)[0m mae:  0.13165289163589478
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14719420671463013
[2m[36m(func pid=98661)[0m mae:  0.08936998248100281
[2m[36m(func pid=98661)[0m rmse_per_class: [0.09, 0.252, 0.024, 0.287, 0.066, 0.149, 0.205, 0.111, 0.144, 0.144]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.17545169591903687
[2m[36m(func pid=105127)[0m mae:  0.12797246873378754
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.261, 0.092, 0.329, 0.091, 0.191, 0.278, 0.14, 0.143, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2869 | Steps: 2 | Val loss: 0.2715 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8125 | Steps: 2 | Val loss: 0.6342 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2333 | Steps: 2 | Val loss: 0.2741 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.4373 | Steps: 2 | Val loss: 0.3366 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 09:54:59 (running for 00:30:20.63)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.287 |  0.146 |                   53 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.23  |  0.147 |                   47 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.82  |  0.179 |                   19 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.456 |  0.175 |                   19 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1457151621580124
[2m[36m(func pid=97073)[0m mae:  0.10196615755558014
[2m[36m(func pid=97073)[0m rmse_per_class: [0.085, 0.246, 0.033, 0.281, 0.067, 0.155, 0.243, 0.112, 0.135, 0.1]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17932595312595367
[2m[36m(func pid=104571)[0m mae:  0.13158275187015533
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.1750793606042862
[2m[36m(func pid=105127)[0m mae:  0.12764421105384827
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.261, 0.092, 0.328, 0.09, 0.191, 0.277, 0.139, 0.143, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14724373817443848
[2m[36m(func pid=98661)[0m mae:  0.0890691876411438
[2m[36m(func pid=98661)[0m rmse_per_class: [0.088, 0.251, 0.024, 0.284, 0.067, 0.15, 0.203, 0.114, 0.151, 0.141]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2961 | Steps: 2 | Val loss: 0.2692 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8040 | Steps: 2 | Val loss: 0.6282 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4243 | Steps: 2 | Val loss: 0.3271 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2324 | Steps: 2 | Val loss: 0.2730 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 09:55:04 (running for 00:30:25.84)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.296 |  0.145 |                   54 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.233 |  0.147 |                   48 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.813 |  0.179 |                   20 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.437 |  0.175 |                   20 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14470188319683075
[2m[36m(func pid=97073)[0m mae:  0.10062432289123535
[2m[36m(func pid=97073)[0m rmse_per_class: [0.085, 0.249, 0.034, 0.279, 0.066, 0.157, 0.234, 0.109, 0.134, 0.1]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17924705147743225
[2m[36m(func pid=104571)[0m mae:  0.13151335716247559
[2m[36m(func pid=104571)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.17456422746181488
[2m[36m(func pid=105127)[0m mae:  0.12720361351966858
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.261, 0.091, 0.327, 0.088, 0.191, 0.275, 0.139, 0.143, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14600515365600586
[2m[36m(func pid=98661)[0m mae:  0.08789214491844177
[2m[36m(func pid=98661)[0m rmse_per_class: [0.086, 0.251, 0.024, 0.279, 0.068, 0.15, 0.201, 0.114, 0.15, 0.138]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2979 | Steps: 2 | Val loss: 0.2694 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8008 | Steps: 2 | Val loss: 0.6220 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4156 | Steps: 2 | Val loss: 0.3198 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2376 | Steps: 2 | Val loss: 0.2728 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 09:55:09 (running for 00:30:31.03)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.298 |  0.145 |                   55 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.232 |  0.146 |                   49 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.804 |  0.179 |                   21 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.424 |  0.175 |                   21 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1448100507259369
[2m[36m(func pid=97073)[0m mae:  0.10056301206350327
[2m[36m(func pid=97073)[0m rmse_per_class: [0.085, 0.25, 0.034, 0.28, 0.066, 0.158, 0.233, 0.109, 0.134, 0.101]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.1740378886461258
[2m[36m(func pid=105127)[0m mae:  0.12674488127231598
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.261, 0.09, 0.326, 0.086, 0.191, 0.274, 0.139, 0.143, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.1792643815279007
[2m[36m(func pid=104571)[0m mae:  0.13151556253433228
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.108]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14530479907989502
[2m[36m(func pid=98661)[0m mae:  0.08743403106927872
[2m[36m(func pid=98661)[0m rmse_per_class: [0.082, 0.256, 0.025, 0.28, 0.07, 0.149, 0.2, 0.112, 0.143, 0.136]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2869 | Steps: 2 | Val loss: 0.2702 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4072 | Steps: 2 | Val loss: 0.3147 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.7888 | Steps: 2 | Val loss: 0.6161 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2290 | Steps: 2 | Val loss: 0.2731 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=97073)[0m rmse: 0.1449592411518097
[2m[36m(func pid=97073)[0m mae:  0.10067884624004364
[2m[36m(func pid=97073)[0m rmse_per_class: [0.084, 0.251, 0.033, 0.282, 0.066, 0.155, 0.237, 0.109, 0.134, 0.099]
[2m[36m(func pid=97073)[0m 
== Status ==
Current time: 2024-01-07 09:55:15 (running for 00:30:36.12)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.287 |  0.145 |                   56 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.238 |  0.145 |                   50 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.801 |  0.179 |                   22 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.416 |  0.174 |                   22 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=105127)[0m rmse: 0.17356343567371368
[2m[36m(func pid=105127)[0m mae:  0.12634380161762238
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.26, 0.089, 0.325, 0.085, 0.191, 0.273, 0.138, 0.143, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.1792500913143158
[2m[36m(func pid=104571)[0m mae:  0.1315026879310608
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.1450556218624115
[2m[36m(func pid=98661)[0m mae:  0.0875241607427597
[2m[36m(func pid=98661)[0m rmse_per_class: [0.084, 0.26, 0.026, 0.282, 0.072, 0.147, 0.202, 0.114, 0.13, 0.134]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2889 | Steps: 2 | Val loss: 0.2713 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3999 | Steps: 2 | Val loss: 0.3113 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2318 | Steps: 2 | Val loss: 0.2771 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7779 | Steps: 2 | Val loss: 0.6092 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 09:55:20 (running for 00:30:41.23)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.289 |  0.145 |                   57 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.229 |  0.145 |                   51 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.789 |  0.179 |                   23 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.407 |  0.174 |                   23 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.145445317029953
[2m[36m(func pid=97073)[0m mae:  0.1010909453034401
[2m[36m(func pid=97073)[0m rmse_per_class: [0.084, 0.25, 0.032, 0.283, 0.067, 0.154, 0.24, 0.111, 0.134, 0.099]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.17300668358802795
[2m[36m(func pid=105127)[0m mae:  0.12588319182395935
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.26, 0.088, 0.324, 0.083, 0.191, 0.272, 0.138, 0.143, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.1475338339805603
[2m[36m(func pid=98661)[0m mae:  0.08983966708183289
[2m[36m(func pid=98661)[0m rmse_per_class: [0.09, 0.268, 0.025, 0.293, 0.076, 0.146, 0.207, 0.116, 0.125, 0.128]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17925940454006195
[2m[36m(func pid=104571)[0m mae:  0.13149306178092957
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2878 | Steps: 2 | Val loss: 0.2708 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3975 | Steps: 2 | Val loss: 0.3093 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2322 | Steps: 2 | Val loss: 0.2787 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 09:55:25 (running for 00:30:46.51)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.288 |  0.145 |                   58 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.232 |  0.148 |                   52 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.778 |  0.179 |                   24 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.4   |  0.173 |                   24 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1452317088842392
[2m[36m(func pid=97073)[0m mae:  0.10086257755756378
[2m[36m(func pid=97073)[0m rmse_per_class: [0.084, 0.251, 0.032, 0.282, 0.067, 0.154, 0.238, 0.111, 0.134, 0.099]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7731 | Steps: 2 | Val loss: 0.6022 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=105127)[0m rmse: 0.17234554886817932
[2m[36m(func pid=105127)[0m mae:  0.12532714009284973
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.26, 0.087, 0.323, 0.081, 0.191, 0.27, 0.138, 0.143, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14855165779590607
[2m[36m(func pid=98661)[0m mae:  0.09108424186706543
[2m[36m(func pid=98661)[0m rmse_per_class: [0.091, 0.265, 0.024, 0.301, 0.08, 0.147, 0.209, 0.117, 0.124, 0.128]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.1792014241218567
[2m[36m(func pid=104571)[0m mae:  0.1314382255077362
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2842 | Steps: 2 | Val loss: 0.2697 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3938 | Steps: 2 | Val loss: 0.3088 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2326 | Steps: 2 | Val loss: 0.2776 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 09:55:30 (running for 00:30:51.78)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.284 |  0.145 |                   59 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.232 |  0.149 |                   53 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.773 |  0.179 |                   25 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.398 |  0.172 |                   25 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14457125961780548
[2m[36m(func pid=97073)[0m mae:  0.10013093799352646
[2m[36m(func pid=97073)[0m rmse_per_class: [0.083, 0.252, 0.032, 0.281, 0.067, 0.155, 0.234, 0.109, 0.134, 0.099]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7618 | Steps: 2 | Val loss: 0.5960 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=105127)[0m rmse: 0.1717730015516281
[2m[36m(func pid=105127)[0m mae:  0.12482358515262604
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.26, 0.086, 0.322, 0.08, 0.191, 0.269, 0.138, 0.143, 0.112]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14791648089885712
[2m[36m(func pid=98661)[0m mae:  0.09103836119174957
[2m[36m(func pid=98661)[0m rmse_per_class: [0.09, 0.261, 0.023, 0.3, 0.079, 0.149, 0.209, 0.115, 0.124, 0.128]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17922385036945343
[2m[36m(func pid=104571)[0m mae:  0.1314390003681183
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2813 | Steps: 2 | Val loss: 0.2692 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3942 | Steps: 2 | Val loss: 0.3096 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 09:55:35 (running for 00:30:57.02)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.281 |  0.144 |                   60 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.233 |  0.148 |                   54 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.762 |  0.179 |                   26 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.394 |  0.172 |                   26 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14422598481178284
[2m[36m(func pid=97073)[0m mae:  0.0997384637594223
[2m[36m(func pid=97073)[0m rmse_per_class: [0.084, 0.253, 0.032, 0.282, 0.067, 0.155, 0.231, 0.108, 0.134, 0.097]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2345 | Steps: 2 | Val loss: 0.2715 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7547 | Steps: 2 | Val loss: 0.5899 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=105127)[0m rmse: 0.17130622267723083
[2m[36m(func pid=105127)[0m mae:  0.12445257604122162
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.26, 0.085, 0.321, 0.078, 0.191, 0.268, 0.138, 0.143, 0.111]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14407895505428314
[2m[36m(func pid=98661)[0m mae:  0.08794532716274261
[2m[36m(func pid=98661)[0m rmse_per_class: [0.083, 0.256, 0.023, 0.285, 0.073, 0.149, 0.205, 0.114, 0.125, 0.128]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2885 | Steps: 2 | Val loss: 0.2686 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=104571)[0m rmse: 0.17924854159355164
[2m[36m(func pid=104571)[0m mae:  0.131444051861763
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3943 | Steps: 2 | Val loss: 0.3113 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 09:55:41 (running for 00:31:02.28)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.288 |  0.144 |                   61 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.235 |  0.144 |                   55 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.755 |  0.179 |                   27 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.394 |  0.171 |                   27 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14365938305854797
[2m[36m(func pid=97073)[0m mae:  0.09917793422937393
[2m[36m(func pid=97073)[0m rmse_per_class: [0.082, 0.251, 0.032, 0.281, 0.067, 0.154, 0.23, 0.108, 0.133, 0.097]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2361 | Steps: 2 | Val loss: 0.2705 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.7465 | Steps: 2 | Val loss: 0.5835 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=105127)[0m rmse: 0.1707647740840912
[2m[36m(func pid=105127)[0m mae:  0.12402299791574478
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.26, 0.085, 0.32, 0.076, 0.19, 0.267, 0.138, 0.143, 0.111]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14339904487133026
[2m[36m(func pid=98661)[0m mae:  0.08708667755126953
[2m[36m(func pid=98661)[0m rmse_per_class: [0.08, 0.253, 0.024, 0.28, 0.072, 0.149, 0.203, 0.114, 0.127, 0.132]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2859 | Steps: 2 | Val loss: 0.2699 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=104571)[0m rmse: 0.17921999096870422
[2m[36m(func pid=104571)[0m mae:  0.13141906261444092
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.336, 0.107, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3961 | Steps: 2 | Val loss: 0.3140 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:55:46 (running for 00:31:07.57)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.286 |  0.144 |                   62 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.143 |                   56 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.746 |  0.179 |                   28 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.394 |  0.171 |                   28 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1442636400461197
[2m[36m(func pid=97073)[0m mae:  0.09969501942396164
[2m[36m(func pid=97073)[0m rmse_per_class: [0.081, 0.25, 0.031, 0.282, 0.068, 0.152, 0.236, 0.11, 0.133, 0.098]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2334 | Steps: 2 | Val loss: 0.2712 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7375 | Steps: 2 | Val loss: 0.5765 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=105127)[0m rmse: 0.170209139585495
[2m[36m(func pid=105127)[0m mae:  0.12356604635715485
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.26, 0.084, 0.319, 0.075, 0.19, 0.266, 0.138, 0.142, 0.111]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14383164048194885
[2m[36m(func pid=98661)[0m mae:  0.08714208751916885
[2m[36m(func pid=98661)[0m rmse_per_class: [0.082, 0.255, 0.024, 0.279, 0.069, 0.149, 0.204, 0.112, 0.127, 0.139]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2808 | Steps: 2 | Val loss: 0.2707 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=104571)[0m rmse: 0.17915138602256775
[2m[36m(func pid=104571)[0m mae:  0.13136181235313416
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.336, 0.106, 0.19, 0.29, 0.141, 0.143, 0.109]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3996 | Steps: 2 | Val loss: 0.3171 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 09:55:51 (running for 00:31:12.93)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.281 |  0.145 |                   63 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.233 |  0.144 |                   57 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.737 |  0.179 |                   29 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.396 |  0.17  |                   29 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=97073)[0m rmse: 0.14484763145446777

[2m[36m(func pid=97073)[0m mae:  0.1001371368765831
[2m[36m(func pid=97073)[0m rmse_per_class: [0.082, 0.249, 0.031, 0.282, 0.069, 0.152, 0.238, 0.111, 0.134, 0.1]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2392 | Steps: 2 | Val loss: 0.2731 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.7280 | Steps: 2 | Val loss: 0.5690 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=105127)[0m rmse: 0.1697118580341339
[2m[36m(func pid=105127)[0m mae:  0.12318233400583267
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.26, 0.082, 0.318, 0.073, 0.19, 0.265, 0.138, 0.142, 0.111]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14503127336502075
[2m[36m(func pid=98661)[0m mae:  0.08787655085325241
[2m[36m(func pid=98661)[0m rmse_per_class: [0.086, 0.259, 0.023, 0.282, 0.065, 0.147, 0.204, 0.11, 0.128, 0.145]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2776 | Steps: 2 | Val loss: 0.2699 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=104571)[0m rmse: 0.17910273373126984
[2m[36m(func pid=104571)[0m mae:  0.13131286203861237
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.106, 0.19, 0.29, 0.141, 0.143, 0.109]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.4014 | Steps: 2 | Val loss: 0.3203 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:55:57 (running for 00:31:18.19)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.278 |  0.144 |                   64 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.239 |  0.145 |                   58 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.728 |  0.179 |                   30 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.4   |  0.17  |                   30 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1444551646709442
[2m[36m(func pid=97073)[0m mae:  0.09957422316074371
[2m[36m(func pid=97073)[0m rmse_per_class: [0.082, 0.251, 0.032, 0.281, 0.069, 0.153, 0.235, 0.109, 0.133, 0.1]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2372 | Steps: 2 | Val loss: 0.2759 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=105127)[0m rmse: 0.16903525590896606
[2m[36m(func pid=105127)[0m mae:  0.12264670431613922
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.259, 0.081, 0.317, 0.072, 0.19, 0.264, 0.138, 0.142, 0.111]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.7178 | Steps: 2 | Val loss: 0.5622 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2755 | Steps: 2 | Val loss: 0.2689 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=98661)[0m rmse: 0.147101491689682
[2m[36m(func pid=98661)[0m mae:  0.08949966728687286
[2m[36m(func pid=98661)[0m rmse_per_class: [0.094, 0.261, 0.023, 0.29, 0.063, 0.145, 0.206, 0.108, 0.129, 0.152]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17908601462841034
[2m[36m(func pid=104571)[0m mae:  0.13127252459526062
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.336, 0.106, 0.19, 0.29, 0.141, 0.143, 0.109]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.4088 | Steps: 2 | Val loss: 0.3243 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 09:56:02 (running for 00:31:23.34)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.276 |  0.144 |                   65 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.237 |  0.147 |                   59 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.718 |  0.179 |                   31 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.401 |  0.169 |                   31 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14385509490966797
[2m[36m(func pid=97073)[0m mae:  0.09883507341146469
[2m[36m(func pid=97073)[0m rmse_per_class: [0.081, 0.252, 0.032, 0.28, 0.068, 0.154, 0.23, 0.107, 0.132, 0.102]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2323 | Steps: 2 | Val loss: 0.2784 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=105127)[0m rmse: 0.1681663990020752
[2m[36m(func pid=105127)[0m mae:  0.12190967798233032
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.258, 0.078, 0.316, 0.07, 0.189, 0.262, 0.138, 0.142, 0.11]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7115 | Steps: 2 | Val loss: 0.5548 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2913 | Steps: 2 | Val loss: 0.2705 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=98661)[0m rmse: 0.148426815867424
[2m[36m(func pid=98661)[0m mae:  0.09046338498592377
[2m[36m(func pid=98661)[0m rmse_per_class: [0.098, 0.26, 0.024, 0.298, 0.063, 0.145, 0.205, 0.107, 0.129, 0.156]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4076 | Steps: 2 | Val loss: 0.3290 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=104571)[0m rmse: 0.1789894849061966
[2m[36m(func pid=104571)[0m mae:  0.13118626177310944
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.336, 0.105, 0.19, 0.289, 0.141, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
== Status ==
Current time: 2024-01-07 09:56:07 (running for 00:31:28.52)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.291 |  0.145 |                   66 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.232 |  0.148 |                   60 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.712 |  0.179 |                   32 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.409 |  0.168 |                   32 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1446983814239502
[2m[36m(func pid=97073)[0m mae:  0.09954750537872314
[2m[36m(func pid=97073)[0m rmse_per_class: [0.082, 0.251, 0.032, 0.281, 0.069, 0.153, 0.235, 0.108, 0.134, 0.102]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2323 | Steps: 2 | Val loss: 0.2794 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=105127)[0m rmse: 0.16744235157966614
[2m[36m(func pid=105127)[0m mae:  0.1213066428899765
[2m[36m(func pid=105127)[0m rmse_per_class: [0.118, 0.258, 0.077, 0.315, 0.069, 0.189, 0.26, 0.137, 0.142, 0.11]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.6981 | Steps: 2 | Val loss: 0.5480 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=98661)[0m rmse: 0.14937534928321838
[2m[36m(func pid=98661)[0m mae:  0.09099063277244568
[2m[36m(func pid=98661)[0m rmse_per_class: [0.104, 0.258, 0.024, 0.301, 0.063, 0.145, 0.204, 0.108, 0.131, 0.156]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2756 | Steps: 2 | Val loss: 0.2711 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4133 | Steps: 2 | Val loss: 0.3335 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 09:56:12 (running for 00:31:33.63)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.291 |  0.145 |                   66 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.232 |  0.149 |                   61 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.698 |  0.179 |                   33 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.408 |  0.167 |                   33 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=104571)[0m rmse: 0.17910102009773254
[2m[36m(func pid=104571)[0m mae:  0.1312607228755951
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.261, 0.099, 0.336, 0.105, 0.19, 0.289, 0.141, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m rmse: 0.14508934319019318
[2m[36m(func pid=97073)[0m mae:  0.09975207597017288
[2m[36m(func pid=97073)[0m rmse_per_class: [0.083, 0.25, 0.031, 0.281, 0.069, 0.153, 0.237, 0.109, 0.134, 0.105]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2304 | Steps: 2 | Val loss: 0.2778 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=105127)[0m rmse: 0.16669002175331116
[2m[36m(func pid=105127)[0m mae:  0.12064827978610992
[2m[36m(func pid=105127)[0m rmse_per_class: [0.117, 0.257, 0.076, 0.314, 0.068, 0.189, 0.258, 0.137, 0.142, 0.109]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14873924851417542
[2m[36m(func pid=98661)[0m mae:  0.09058142453432083
[2m[36m(func pid=98661)[0m rmse_per_class: [0.104, 0.258, 0.024, 0.296, 0.064, 0.145, 0.203, 0.109, 0.132, 0.152]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2726 | Steps: 2 | Val loss: 0.2698 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.6919 | Steps: 2 | Val loss: 0.5407 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4160 | Steps: 2 | Val loss: 0.3382 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 09:56:18 (running for 00:31:39.25)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.273 |  0.144 |                   68 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.23  |  0.149 |                   62 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.698 |  0.179 |                   33 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.413 |  0.167 |                   34 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14448127150535583
[2m[36m(func pid=97073)[0m mae:  0.09905413538217545
[2m[36m(func pid=97073)[0m rmse_per_class: [0.082, 0.251, 0.031, 0.28, 0.068, 0.153, 0.234, 0.108, 0.133, 0.105]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17906302213668823
[2m[36m(func pid=104571)[0m mae:  0.13121472299098969
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.261, 0.099, 0.336, 0.105, 0.19, 0.289, 0.141, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2342 | Steps: 2 | Val loss: 0.2756 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=105127)[0m rmse: 0.1658416986465454
[2m[36m(func pid=105127)[0m mae:  0.11992088705301285
[2m[36m(func pid=105127)[0m rmse_per_class: [0.117, 0.257, 0.074, 0.313, 0.067, 0.189, 0.256, 0.137, 0.142, 0.109]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2725 | Steps: 2 | Val loss: 0.2686 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=98661)[0m rmse: 0.14740926027297974
[2m[36m(func pid=98661)[0m mae:  0.08942772448062897
[2m[36m(func pid=98661)[0m rmse_per_class: [0.101, 0.256, 0.024, 0.288, 0.064, 0.145, 0.202, 0.111, 0.133, 0.15]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.6806 | Steps: 2 | Val loss: 0.5330 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4274 | Steps: 2 | Val loss: 0.3420 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 09:56:23 (running for 00:31:44.67)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.273 |  0.144 |                   69 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.234 |  0.147 |                   63 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.692 |  0.179 |                   34 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.416 |  0.166 |                   35 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14376887679100037
[2m[36m(func pid=97073)[0m mae:  0.09820561110973358
[2m[36m(func pid=97073)[0m rmse_per_class: [0.081, 0.253, 0.031, 0.28, 0.068, 0.154, 0.227, 0.107, 0.132, 0.104]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17894615232944489
[2m[36m(func pid=104571)[0m mae:  0.13109970092773438
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.335, 0.105, 0.19, 0.289, 0.141, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2270 | Steps: 2 | Val loss: 0.2719 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=105127)[0m rmse: 0.1647886335849762
[2m[36m(func pid=105127)[0m mae:  0.11902276426553726
[2m[36m(func pid=105127)[0m rmse_per_class: [0.116, 0.255, 0.071, 0.312, 0.065, 0.188, 0.253, 0.136, 0.141, 0.108]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2702 | Steps: 2 | Val loss: 0.2687 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=98661)[0m rmse: 0.14504000544548035
[2m[36m(func pid=98661)[0m mae:  0.0876333937048912
[2m[36m(func pid=98661)[0m rmse_per_class: [0.098, 0.253, 0.024, 0.277, 0.066, 0.146, 0.2, 0.112, 0.133, 0.141]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6741 | Steps: 2 | Val loss: 0.5256 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4281 | Steps: 2 | Val loss: 0.3466 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 09:56:28 (running for 00:31:49.90)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.27  |  0.144 |                   70 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.227 |  0.145 |                   64 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.681 |  0.179 |                   35 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.427 |  0.165 |                   36 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1438165307044983
[2m[36m(func pid=97073)[0m mae:  0.09816114604473114
[2m[36m(func pid=97073)[0m rmse_per_class: [0.081, 0.254, 0.03, 0.281, 0.068, 0.153, 0.226, 0.107, 0.133, 0.104]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2325 | Steps: 2 | Val loss: 0.2708 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=104571)[0m rmse: 0.1787967085838318
[2m[36m(func pid=104571)[0m mae:  0.1309603452682495
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.261, 0.098, 0.335, 0.104, 0.19, 0.288, 0.141, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.16395516693592072
[2m[36m(func pid=105127)[0m mae:  0.11828770488500595
[2m[36m(func pid=105127)[0m rmse_per_class: [0.116, 0.255, 0.07, 0.311, 0.064, 0.188, 0.251, 0.136, 0.141, 0.108]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2730 | Steps: 2 | Val loss: 0.2692 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=98661)[0m rmse: 0.1440633237361908
[2m[36m(func pid=98661)[0m mae:  0.08654765784740448
[2m[36m(func pid=98661)[0m rmse_per_class: [0.096, 0.251, 0.024, 0.271, 0.068, 0.147, 0.2, 0.114, 0.13, 0.14]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6648 | Steps: 2 | Val loss: 0.5183 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4322 | Steps: 2 | Val loss: 0.3510 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 09:56:34 (running for 00:31:55.12)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.273 |  0.144 |                   71 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.233 |  0.144 |                   65 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.674 |  0.179 |                   36 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.428 |  0.164 |                   37 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14405594766139984
[2m[36m(func pid=97073)[0m mae:  0.09835126250982285
[2m[36m(func pid=97073)[0m rmse_per_class: [0.082, 0.255, 0.03, 0.283, 0.068, 0.153, 0.228, 0.108, 0.134, 0.102]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2399 | Steps: 2 | Val loss: 0.2703 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=104571)[0m rmse: 0.17872287333011627
[2m[36m(func pid=104571)[0m mae:  0.13089492917060852
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.261, 0.098, 0.335, 0.104, 0.191, 0.288, 0.141, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.16310235857963562
[2m[36m(func pid=105127)[0m mae:  0.11754981428384781
[2m[36m(func pid=105127)[0m rmse_per_class: [0.115, 0.254, 0.068, 0.31, 0.063, 0.188, 0.249, 0.135, 0.141, 0.107]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2750 | Steps: 2 | Val loss: 0.2693 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=98661)[0m rmse: 0.1435137838125229
[2m[36m(func pid=98661)[0m mae:  0.08590389788150787
[2m[36m(func pid=98661)[0m rmse_per_class: [0.096, 0.249, 0.024, 0.27, 0.067, 0.146, 0.2, 0.115, 0.131, 0.137]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4361 | Steps: 2 | Val loss: 0.3551 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6569 | Steps: 2 | Val loss: 0.5108 | Batch size: 32 | lr: 0.0001 | Duration: 3.23s
== Status ==
Current time: 2024-01-07 09:56:39 (running for 00:32:00.25)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.275 |  0.144 |                   72 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.24  |  0.144 |                   66 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.665 |  0.179 |                   37 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.432 |  0.163 |                   38 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14408299326896667
[2m[36m(func pid=97073)[0m mae:  0.09831099957227707
[2m[36m(func pid=97073)[0m rmse_per_class: [0.081, 0.254, 0.03, 0.283, 0.067, 0.152, 0.228, 0.108, 0.134, 0.102]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2352 | Steps: 2 | Val loss: 0.2694 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=105127)[0m rmse: 0.16224384307861328
[2m[36m(func pid=105127)[0m mae:  0.11679422855377197
[2m[36m(func pid=105127)[0m rmse_per_class: [0.115, 0.253, 0.066, 0.309, 0.062, 0.187, 0.247, 0.135, 0.141, 0.106]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17859208583831787
[2m[36m(func pid=104571)[0m mae:  0.1307872086763382
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.261, 0.098, 0.335, 0.104, 0.191, 0.288, 0.141, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2760 | Steps: 2 | Val loss: 0.2684 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=98661)[0m rmse: 0.14270654320716858
[2m[36m(func pid=98661)[0m mae:  0.08544594049453735
[2m[36m(func pid=98661)[0m rmse_per_class: [0.094, 0.25, 0.023, 0.273, 0.064, 0.146, 0.199, 0.114, 0.132, 0.132]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4387 | Steps: 2 | Val loss: 0.3587 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.6440 | Steps: 2 | Val loss: 0.5042 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 09:56:44 (running for 00:32:05.41)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.276 |  0.143 |                   73 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.235 |  0.143 |                   67 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.657 |  0.179 |                   38 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.436 |  0.162 |                   39 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14332640171051025
[2m[36m(func pid=97073)[0m mae:  0.097648985683918
[2m[36m(func pid=97073)[0m rmse_per_class: [0.081, 0.253, 0.029, 0.281, 0.067, 0.152, 0.227, 0.108, 0.133, 0.102]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2328 | Steps: 2 | Val loss: 0.2697 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=105127)[0m rmse: 0.16137824952602386
[2m[36m(func pid=105127)[0m mae:  0.11602760851383209
[2m[36m(func pid=105127)[0m rmse_per_class: [0.114, 0.252, 0.065, 0.308, 0.061, 0.187, 0.246, 0.135, 0.141, 0.105]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17861279845237732
[2m[36m(func pid=104571)[0m mae:  0.13080699741840363
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.261, 0.098, 0.335, 0.103, 0.191, 0.288, 0.141, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2679 | Steps: 2 | Val loss: 0.2673 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=98661)[0m rmse: 0.14302298426628113
[2m[36m(func pid=98661)[0m mae:  0.085966095328331
[2m[36m(func pid=98661)[0m rmse_per_class: [0.097, 0.254, 0.023, 0.279, 0.063, 0.145, 0.2, 0.112, 0.131, 0.126]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4424 | Steps: 2 | Val loss: 0.3628 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=97073)[0m rmse: 0.14257970452308655
[2m[36m(func pid=97073)[0m mae:  0.09681730717420578
[2m[36m(func pid=97073)[0m rmse_per_class: [0.079, 0.253, 0.029, 0.279, 0.067, 0.152, 0.224, 0.107, 0.132, 0.102]
[2m[36m(func pid=97073)[0m 
== Status ==
Current time: 2024-01-07 09:56:49 (running for 00:32:10.65)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.15025000274181366
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.268 |  0.143 |                   74 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.233 |  0.143 |                   68 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.644 |  0.179 |                   39 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.439 |  0.161 |                   40 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6384 | Steps: 2 | Val loss: 0.4974 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2279 | Steps: 2 | Val loss: 0.2705 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=105127)[0m rmse: 0.16062146425247192
[2m[36m(func pid=105127)[0m mae:  0.11532539129257202
[2m[36m(func pid=105127)[0m rmse_per_class: [0.114, 0.251, 0.063, 0.308, 0.061, 0.187, 0.244, 0.134, 0.141, 0.105]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17848996818065643
[2m[36m(func pid=104571)[0m mae:  0.1307048797607422
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.261, 0.098, 0.335, 0.103, 0.191, 0.287, 0.141, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2734 | Steps: 2 | Val loss: 0.2669 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=98661)[0m rmse: 0.1430187076330185
[2m[36m(func pid=98661)[0m mae:  0.08636677265167236
[2m[36m(func pid=98661)[0m rmse_per_class: [0.093, 0.258, 0.024, 0.286, 0.063, 0.145, 0.199, 0.111, 0.129, 0.122]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4455 | Steps: 2 | Val loss: 0.3657 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 09:56:54 (running for 00:32:15.87)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1469999998807907
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.273 |  0.142 |                   75 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.228 |  0.143 |                   69 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.638 |  0.178 |                   40 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.442 |  0.161 |                   41 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14239558577537537
[2m[36m(func pid=97073)[0m mae:  0.09641546756029129
[2m[36m(func pid=97073)[0m rmse_per_class: [0.079, 0.254, 0.03, 0.279, 0.066, 0.153, 0.22, 0.106, 0.131, 0.105]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6312 | Steps: 2 | Val loss: 0.4898 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2321 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=105127)[0m rmse: 0.15997451543807983
[2m[36m(func pid=105127)[0m mae:  0.11476568132638931
[2m[36m(func pid=105127)[0m rmse_per_class: [0.113, 0.251, 0.062, 0.307, 0.06, 0.186, 0.243, 0.134, 0.141, 0.104]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2683 | Steps: 2 | Val loss: 0.2680 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=104571)[0m rmse: 0.17835840582847595
[2m[36m(func pid=104571)[0m mae:  0.13058704137802124
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.103, 0.191, 0.287, 0.141, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.1447206288576126
[2m[36m(func pid=98661)[0m mae:  0.08767594397068024
[2m[36m(func pid=98661)[0m rmse_per_class: [0.091, 0.263, 0.024, 0.295, 0.064, 0.145, 0.2, 0.111, 0.128, 0.126]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4464 | Steps: 2 | Val loss: 0.3692 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 09:57:00 (running for 00:32:21.22)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1469999998807907
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.268 |  0.143 |                   76 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.232 |  0.145 |                   70 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.631 |  0.178 |                   41 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.445 |  0.16  |                   42 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.143011212348938
[2m[36m(func pid=97073)[0m mae:  0.09682586044073105
[2m[36m(func pid=97073)[0m rmse_per_class: [0.079, 0.254, 0.03, 0.279, 0.067, 0.152, 0.224, 0.107, 0.132, 0.106]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6216 | Steps: 2 | Val loss: 0.4825 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2345 | Steps: 2 | Val loss: 0.2744 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=105127)[0m rmse: 0.15912915766239166
[2m[36m(func pid=105127)[0m mae:  0.11397471278905869
[2m[36m(func pid=105127)[0m rmse_per_class: [0.113, 0.25, 0.06, 0.306, 0.059, 0.186, 0.241, 0.133, 0.14, 0.103]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2735 | Steps: 2 | Val loss: 0.2691 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=98661)[0m rmse: 0.1452060341835022
[2m[36m(func pid=98661)[0m mae:  0.08778738975524902
[2m[36m(func pid=98661)[0m rmse_per_class: [0.087, 0.265, 0.024, 0.294, 0.064, 0.146, 0.201, 0.112, 0.128, 0.13]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17818425595760345
[2m[36m(func pid=104571)[0m mae:  0.13042616844177246
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.102, 0.191, 0.287, 0.141, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4509 | Steps: 2 | Val loss: 0.3730 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=97073)[0m rmse: 0.1437300145626068
[2m[36m(func pid=97073)[0m mae:  0.09730131924152374
[2m[36m(func pid=97073)[0m rmse_per_class: [0.08, 0.251, 0.03, 0.279, 0.068, 0.151, 0.227, 0.109, 0.132, 0.11]
[2m[36m(func pid=97073)[0m 
== Status ==
Current time: 2024-01-07 09:57:05 (running for 00:32:26.32)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1469999998807907
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.273 |  0.144 |                   77 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.235 |  0.145 |                   71 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.622 |  0.178 |                   42 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.446 |  0.159 |                   43 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2321 | Steps: 2 | Val loss: 0.2743 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6115 | Steps: 2 | Val loss: 0.4755 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=105127)[0m rmse: 0.15829916298389435
[2m[36m(func pid=105127)[0m mae:  0.11317186057567596
[2m[36m(func pid=105127)[0m rmse_per_class: [0.113, 0.249, 0.058, 0.305, 0.059, 0.185, 0.239, 0.133, 0.14, 0.102]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2667 | Steps: 2 | Val loss: 0.2695 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=98661)[0m rmse: 0.14522330462932587
[2m[36m(func pid=98661)[0m mae:  0.08750387281179428
[2m[36m(func pid=98661)[0m rmse_per_class: [0.084, 0.267, 0.025, 0.289, 0.065, 0.146, 0.202, 0.112, 0.128, 0.135]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17808905243873596
[2m[36m(func pid=104571)[0m mae:  0.13033005595207214
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.102, 0.19, 0.286, 0.141, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4559 | Steps: 2 | Val loss: 0.3750 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 09:57:10 (running for 00:32:31.59)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1469999998807907
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.267 |  0.144 |                   78 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.232 |  0.145 |                   72 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.612 |  0.178 |                   43 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.451 |  0.158 |                   44 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1441406011581421
[2m[36m(func pid=97073)[0m mae:  0.09742268919944763
[2m[36m(func pid=97073)[0m rmse_per_class: [0.08, 0.252, 0.03, 0.279, 0.069, 0.151, 0.228, 0.109, 0.132, 0.113]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2356 | Steps: 2 | Val loss: 0.2743 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=105127)[0m rmse: 0.15746192634105682
[2m[36m(func pid=105127)[0m mae:  0.11235193908214569
[2m[36m(func pid=105127)[0m rmse_per_class: [0.112, 0.248, 0.057, 0.304, 0.058, 0.185, 0.238, 0.133, 0.14, 0.102]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.6083 | Steps: 2 | Val loss: 0.4687 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2734 | Steps: 2 | Val loss: 0.2675 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=98661)[0m rmse: 0.14524459838867188
[2m[36m(func pid=98661)[0m mae:  0.08735670149326324
[2m[36m(func pid=98661)[0m rmse_per_class: [0.081, 0.264, 0.026, 0.286, 0.066, 0.147, 0.203, 0.114, 0.127, 0.139]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17797492444515228
[2m[36m(func pid=104571)[0m mae:  0.1302224099636078
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.102, 0.191, 0.286, 0.14, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4521 | Steps: 2 | Val loss: 0.3773 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 09:57:15 (running for 00:32:36.95)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1469999998807907
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.273 |  0.143 |                   79 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.145 |                   73 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.608 |  0.178 |                   44 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.456 |  0.157 |                   45 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14307111501693726
[2m[36m(func pid=97073)[0m mae:  0.09636171907186508
[2m[36m(func pid=97073)[0m rmse_per_class: [0.079, 0.254, 0.03, 0.277, 0.068, 0.153, 0.221, 0.107, 0.13, 0.111]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2333 | Steps: 2 | Val loss: 0.2728 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=105127)[0m rmse: 0.1568606197834015
[2m[36m(func pid=105127)[0m mae:  0.11176104843616486
[2m[36m(func pid=105127)[0m rmse_per_class: [0.111, 0.247, 0.056, 0.303, 0.057, 0.185, 0.237, 0.133, 0.14, 0.101]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5970 | Steps: 2 | Val loss: 0.4625 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2732 | Steps: 2 | Val loss: 0.2672 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=98661)[0m rmse: 0.1442231386899948
[2m[36m(func pid=98661)[0m mae:  0.08642663061618805
[2m[36m(func pid=98661)[0m rmse_per_class: [0.083, 0.256, 0.026, 0.279, 0.066, 0.148, 0.202, 0.114, 0.128, 0.142]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4566 | Steps: 2 | Val loss: 0.3794 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=104571)[0m rmse: 0.1778794229030609
[2m[36m(func pid=104571)[0m mae:  0.13013982772827148
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.334, 0.101, 0.191, 0.286, 0.141, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
== Status ==
Current time: 2024-01-07 09:57:21 (running for 00:32:42.22)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.1469999998807907
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.273 |  0.143 |                   80 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.233 |  0.144 |                   74 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.597 |  0.178 |                   45 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.452 |  0.157 |                   46 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14302442967891693
[2m[36m(func pid=97073)[0m mae:  0.09618033468723297
[2m[36m(func pid=97073)[0m rmse_per_class: [0.08, 0.254, 0.03, 0.276, 0.068, 0.155, 0.218, 0.107, 0.13, 0.112]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.15631555020809174
[2m[36m(func pid=105127)[0m mae:  0.11118407547473907
[2m[36m(func pid=105127)[0m rmse_per_class: [0.111, 0.247, 0.054, 0.302, 0.057, 0.184, 0.236, 0.132, 0.14, 0.1]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2325 | Steps: 2 | Val loss: 0.2706 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5878 | Steps: 2 | Val loss: 0.4552 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2755 | Steps: 2 | Val loss: 0.2691 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=98661)[0m rmse: 0.14269773662090302
[2m[36m(func pid=98661)[0m mae:  0.08525292575359344
[2m[36m(func pid=98661)[0m rmse_per_class: [0.082, 0.249, 0.025, 0.273, 0.064, 0.148, 0.202, 0.114, 0.129, 0.141]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4549 | Steps: 2 | Val loss: 0.3818 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=104571)[0m rmse: 0.17772789299488068
[2m[36m(func pid=104571)[0m mae:  0.13000304996967316
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.26, 0.097, 0.333, 0.101, 0.191, 0.286, 0.14, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
== Status ==
Current time: 2024-01-07 09:57:26 (running for 00:32:47.50)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.276 |  0.144 |                   81 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.233 |  0.143 |                   75 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.588 |  0.178 |                   46 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.457 |  0.156 |                   47 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14391469955444336
[2m[36m(func pid=97073)[0m mae:  0.09698565304279327
[2m[36m(func pid=97073)[0m rmse_per_class: [0.08, 0.255, 0.029, 0.28, 0.069, 0.151, 0.227, 0.108, 0.132, 0.108]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.1557641327381134
[2m[36m(func pid=105127)[0m mae:  0.1106012612581253
[2m[36m(func pid=105127)[0m rmse_per_class: [0.11, 0.246, 0.053, 0.302, 0.056, 0.184, 0.235, 0.132, 0.14, 0.1]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2356 | Steps: 2 | Val loss: 0.2702 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5830 | Steps: 2 | Val loss: 0.4489 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2620 | Steps: 2 | Val loss: 0.2722 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4530 | Steps: 2 | Val loss: 0.3830 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=98661)[0m rmse: 0.14250065386295319
[2m[36m(func pid=98661)[0m mae:  0.08530639857053757
[2m[36m(func pid=98661)[0m rmse_per_class: [0.084, 0.248, 0.025, 0.273, 0.063, 0.148, 0.201, 0.113, 0.132, 0.138]
[2m[36m(func pid=98661)[0m 
== Status ==
Current time: 2024-01-07 09:57:31 (running for 00:32:52.64)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.262 |  0.146 |                   82 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.143 |                   76 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.588 |  0.178 |                   46 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.455 |  0.156 |                   48 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14575697481632233
[2m[36m(func pid=97073)[0m mae:  0.09861958771944046
[2m[36m(func pid=97073)[0m rmse_per_class: [0.082, 0.251, 0.028, 0.285, 0.071, 0.15, 0.234, 0.112, 0.134, 0.11]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17757375538349152
[2m[36m(func pid=104571)[0m mae:  0.12986381351947784
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.333, 0.1, 0.191, 0.285, 0.14, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.1550205945968628
[2m[36m(func pid=105127)[0m mae:  0.10982499271631241
[2m[36m(func pid=105127)[0m rmse_per_class: [0.109, 0.245, 0.052, 0.301, 0.056, 0.184, 0.234, 0.132, 0.139, 0.099]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2350 | Steps: 2 | Val loss: 0.2703 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2702 | Steps: 2 | Val loss: 0.2706 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5787 | Steps: 2 | Val loss: 0.4435 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4542 | Steps: 2 | Val loss: 0.3844 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=98661)[0m rmse: 0.14312729239463806
[2m[36m(func pid=98661)[0m mae:  0.08635642379522324
[2m[36m(func pid=98661)[0m rmse_per_class: [0.087, 0.249, 0.025, 0.278, 0.063, 0.148, 0.202, 0.111, 0.131, 0.138]
[2m[36m(func pid=98661)[0m 
== Status ==
Current time: 2024-01-07 09:57:36 (running for 00:32:57.84)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.27  |  0.145 |                   83 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.235 |  0.143 |                   77 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.583 |  0.178 |                   47 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.453 |  0.155 |                   49 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14489547908306122
[2m[36m(func pid=97073)[0m mae:  0.09763254225254059
[2m[36m(func pid=97073)[0m rmse_per_class: [0.08, 0.254, 0.028, 0.284, 0.069, 0.149, 0.23, 0.11, 0.133, 0.11]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17758773267269135
[2m[36m(func pid=104571)[0m mae:  0.12989547848701477
[2m[36m(func pid=104571)[0m rmse_per_class: [0.116, 0.26, 0.096, 0.333, 0.1, 0.191, 0.285, 0.14, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.15458790957927704
[2m[36m(func pid=105127)[0m mae:  0.10935141891241074
[2m[36m(func pid=105127)[0m rmse_per_class: [0.109, 0.244, 0.05, 0.301, 0.056, 0.183, 0.234, 0.132, 0.139, 0.098]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2355 | Steps: 2 | Val loss: 0.2715 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2704 | Steps: 2 | Val loss: 0.2678 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5688 | Steps: 2 | Val loss: 0.4374 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4552 | Steps: 2 | Val loss: 0.3848 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=98661)[0m rmse: 0.14443232119083405
[2m[36m(func pid=98661)[0m mae:  0.08780988305807114
[2m[36m(func pid=98661)[0m rmse_per_class: [0.095, 0.248, 0.024, 0.283, 0.064, 0.148, 0.205, 0.11, 0.133, 0.134]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m rmse: 0.14322978258132935
[2m[36m(func pid=97073)[0m mae:  0.09597651660442352
[2m[36m(func pid=97073)[0m rmse_per_class: [0.079, 0.258, 0.029, 0.281, 0.068, 0.152, 0.22, 0.106, 0.131, 0.109]
[2m[36m(func pid=97073)[0m 
== Status ==
Current time: 2024-01-07 09:57:41 (running for 00:33:02.92)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.27  |  0.143 |                   84 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.235 |  0.144 |                   78 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.579 |  0.178 |                   48 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.454 |  0.155 |                   50 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=104571)[0m rmse: 0.1775323748588562
[2m[36m(func pid=104571)[0m mae:  0.12984874844551086
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.1, 0.191, 0.285, 0.14, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.1540454924106598
[2m[36m(func pid=105127)[0m mae:  0.10878585278987885
[2m[36m(func pid=105127)[0m rmse_per_class: [0.108, 0.243, 0.049, 0.3, 0.055, 0.183, 0.234, 0.131, 0.139, 0.097]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2328 | Steps: 2 | Val loss: 0.2720 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2678 | Steps: 2 | Val loss: 0.2675 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5612 | Steps: 2 | Val loss: 0.4323 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4544 | Steps: 2 | Val loss: 0.3852 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=98661)[0m rmse: 0.14518368244171143
[2m[36m(func pid=98661)[0m mae:  0.08851965516805649
[2m[36m(func pid=98661)[0m rmse_per_class: [0.1, 0.247, 0.024, 0.286, 0.063, 0.148, 0.206, 0.11, 0.137, 0.132]
[2m[36m(func pid=98661)[0m 
== Status ==
Current time: 2024-01-07 09:57:47 (running for 00:33:08.13)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.268 |  0.143 |                   85 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.233 |  0.145 |                   79 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.569 |  0.178 |                   49 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.455 |  0.154 |                   51 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14315569400787354
[2m[36m(func pid=97073)[0m mae:  0.09583364427089691
[2m[36m(func pid=97073)[0m rmse_per_class: [0.078, 0.259, 0.029, 0.279, 0.068, 0.155, 0.218, 0.106, 0.13, 0.11]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.1774454563856125
[2m[36m(func pid=104571)[0m mae:  0.12978044152259827
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.1, 0.191, 0.285, 0.14, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.15355521440505981
[2m[36m(func pid=105127)[0m mae:  0.10820481926202774
[2m[36m(func pid=105127)[0m rmse_per_class: [0.107, 0.243, 0.048, 0.299, 0.055, 0.183, 0.233, 0.131, 0.139, 0.097]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2326 | Steps: 2 | Val loss: 0.2736 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2710 | Steps: 2 | Val loss: 0.2667 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5526 | Steps: 2 | Val loss: 0.4262 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4554 | Steps: 2 | Val loss: 0.3851 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=98661)[0m rmse: 0.14663580060005188
[2m[36m(func pid=98661)[0m mae:  0.08935777097940445
[2m[36m(func pid=98661)[0m rmse_per_class: [0.108, 0.245, 0.023, 0.288, 0.062, 0.149, 0.207, 0.11, 0.142, 0.134]
[2m[36m(func pid=98661)[0m 
== Status ==
Current time: 2024-01-07 09:57:52 (running for 00:33:13.37)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.271 |  0.143 |                   86 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.233 |  0.147 |                   80 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.561 |  0.177 |                   50 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.454 |  0.154 |                   52 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1425349861383438
[2m[36m(func pid=97073)[0m mae:  0.09517962485551834
[2m[36m(func pid=97073)[0m rmse_per_class: [0.077, 0.257, 0.029, 0.277, 0.067, 0.153, 0.219, 0.106, 0.131, 0.111]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17736005783081055
[2m[36m(func pid=104571)[0m mae:  0.12970100343227386
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.099, 0.191, 0.285, 0.14, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.15315724909305573
[2m[36m(func pid=105127)[0m mae:  0.10768379271030426
[2m[36m(func pid=105127)[0m rmse_per_class: [0.106, 0.243, 0.047, 0.299, 0.055, 0.183, 0.233, 0.131, 0.138, 0.097]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2356 | Steps: 2 | Val loss: 0.2736 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2579 | Steps: 2 | Val loss: 0.2672 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4504 | Steps: 2 | Val loss: 0.3847 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5468 | Steps: 2 | Val loss: 0.4206 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 09:57:57 (running for 00:33:18.43)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.271 |  0.143 |                   86 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.147 |                   81 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.553 |  0.177 |                   51 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.455 |  0.153 |                   53 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=98661)[0m rmse: 0.1465872824192047
[2m[36m(func pid=98661)[0m mae:  0.08899270743131638
[2m[36m(func pid=98661)[0m rmse_per_class: [0.108, 0.244, 0.023, 0.286, 0.061, 0.149, 0.205, 0.111, 0.143, 0.136]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m rmse: 0.14284461736679077
[2m[36m(func pid=97073)[0m mae:  0.09539829194545746
[2m[36m(func pid=97073)[0m rmse_per_class: [0.076, 0.254, 0.028, 0.276, 0.068, 0.149, 0.225, 0.107, 0.132, 0.113]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.15263301134109497
[2m[36m(func pid=105127)[0m mae:  0.1070825606584549
[2m[36m(func pid=105127)[0m rmse_per_class: [0.105, 0.242, 0.046, 0.299, 0.055, 0.183, 0.233, 0.13, 0.138, 0.096]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.1772523820400238
[2m[36m(func pid=104571)[0m mae:  0.12960085272789001
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.332, 0.099, 0.191, 0.284, 0.14, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2280 | Steps: 2 | Val loss: 0.2720 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2789 | Steps: 2 | Val loss: 0.2693 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4430 | Steps: 2 | Val loss: 0.3837 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5399 | Steps: 2 | Val loss: 0.4151 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 09:58:02 (running for 00:33:23.71)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.258 |  0.143 |                   87 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.228 |  0.145 |                   82 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.547 |  0.177 |                   52 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.45  |  0.153 |                   54 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=98661)[0m rmse: 0.14546240866184235
[2m[36m(func pid=98661)[0m mae:  0.08803499490022659
[2m[36m(func pid=98661)[0m rmse_per_class: [0.104, 0.245, 0.024, 0.281, 0.061, 0.15, 0.202, 0.11, 0.145, 0.133]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m rmse: 0.14406625926494598
[2m[36m(func pid=97073)[0m mae:  0.09635493904352188
[2m[36m(func pid=97073)[0m rmse_per_class: [0.076, 0.25, 0.027, 0.28, 0.068, 0.149, 0.229, 0.112, 0.133, 0.117]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.15218983590602875
[2m[36m(func pid=105127)[0m mae:  0.10657600313425064
[2m[36m(func pid=105127)[0m rmse_per_class: [0.104, 0.242, 0.045, 0.298, 0.055, 0.183, 0.233, 0.13, 0.138, 0.095]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17713382840156555
[2m[36m(func pid=104571)[0m mae:  0.1295156180858612
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.332, 0.098, 0.191, 0.284, 0.14, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2382 | Steps: 2 | Val loss: 0.2710 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2605 | Steps: 2 | Val loss: 0.2702 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4499 | Steps: 2 | Val loss: 0.3828 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5317 | Steps: 2 | Val loss: 0.4096 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 09:58:08 (running for 00:33:29.11)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.261 |  0.145 |                   89 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.228 |  0.145 |                   82 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.54  |  0.177 |                   53 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.443 |  0.152 |                   55 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.1446666270494461
[2m[36m(func pid=97073)[0m mae:  0.09690248221158981
[2m[36m(func pid=97073)[0m rmse_per_class: [0.078, 0.249, 0.027, 0.281, 0.069, 0.149, 0.232, 0.113, 0.132, 0.116]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14469730854034424
[2m[36m(func pid=98661)[0m mae:  0.0872049480676651
[2m[36m(func pid=98661)[0m rmse_per_class: [0.098, 0.248, 0.024, 0.278, 0.062, 0.151, 0.201, 0.111, 0.143, 0.132]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.15167509019374847
[2m[36m(func pid=105127)[0m mae:  0.10603398084640503
[2m[36m(func pid=105127)[0m rmse_per_class: [0.103, 0.242, 0.044, 0.297, 0.055, 0.182, 0.234, 0.129, 0.138, 0.094]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17700687050819397
[2m[36m(func pid=104571)[0m mae:  0.12940055131912231
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.332, 0.098, 0.191, 0.284, 0.14, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2616 | Steps: 2 | Val loss: 0.2671 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2323 | Steps: 2 | Val loss: 0.2717 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4418 | Steps: 2 | Val loss: 0.3815 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 09:58:13 (running for 00:33:34.35)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.262 |  0.143 |                   90 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.238 |  0.145 |                   83 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.532 |  0.177 |                   54 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.45  |  0.152 |                   56 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14292122423648834
[2m[36m(func pid=97073)[0m mae:  0.0951821506023407
[2m[36m(func pid=97073)[0m rmse_per_class: [0.076, 0.253, 0.028, 0.276, 0.068, 0.149, 0.225, 0.109, 0.131, 0.113]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14537334442138672
[2m[36m(func pid=98661)[0m mae:  0.08803558349609375
[2m[36m(func pid=98661)[0m rmse_per_class: [0.1, 0.253, 0.024, 0.282, 0.066, 0.151, 0.202, 0.111, 0.135, 0.131]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5247 | Steps: 2 | Val loss: 0.4041 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=105127)[0m rmse: 0.1511404663324356
[2m[36m(func pid=105127)[0m mae:  0.10539829730987549
[2m[36m(func pid=105127)[0m rmse_per_class: [0.102, 0.241, 0.043, 0.296, 0.054, 0.182, 0.234, 0.128, 0.137, 0.093]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2714 | Steps: 2 | Val loss: 0.2652 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=104571)[0m rmse: 0.17690613865852356
[2m[36m(func pid=104571)[0m mae:  0.129313126206398
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.332, 0.098, 0.191, 0.284, 0.14, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2290 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4395 | Steps: 2 | Val loss: 0.3796 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 09:58:18 (running for 00:33:39.63)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.271 |  0.142 |                   91 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.232 |  0.145 |                   84 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.525 |  0.177 |                   55 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.442 |  0.151 |                   57 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14189115166664124
[2m[36m(func pid=97073)[0m mae:  0.09411265701055527
[2m[36m(func pid=97073)[0m rmse_per_class: [0.074, 0.253, 0.029, 0.274, 0.068, 0.15, 0.218, 0.109, 0.129, 0.115]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.1467302143573761
[2m[36m(func pid=98661)[0m mae:  0.08926141262054443
[2m[36m(func pid=98661)[0m rmse_per_class: [0.1, 0.259, 0.025, 0.287, 0.07, 0.151, 0.204, 0.112, 0.131, 0.129]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5188 | Steps: 2 | Val loss: 0.3992 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=105127)[0m rmse: 0.1506468504667282
[2m[36m(func pid=105127)[0m mae:  0.10487544536590576
[2m[36m(func pid=105127)[0m rmse_per_class: [0.101, 0.241, 0.042, 0.295, 0.054, 0.182, 0.235, 0.127, 0.137, 0.092]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2642 | Steps: 2 | Val loss: 0.2649 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2397 | Steps: 2 | Val loss: 0.2754 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=104571)[0m rmse: 0.17679691314697266
[2m[36m(func pid=104571)[0m mae:  0.12922140955924988
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.094, 0.332, 0.097, 0.191, 0.283, 0.139, 0.144, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4326 | Steps: 2 | Val loss: 0.3782 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=97073)[0m rmse: 0.14163833856582642
[2m[36m(func pid=97073)[0m mae:  0.09398984909057617
[2m[36m(func pid=97073)[0m rmse_per_class: [0.075, 0.253, 0.029, 0.274, 0.068, 0.15, 0.218, 0.11, 0.129, 0.111]
[2m[36m(func pid=97073)[0m 
== Status ==
Current time: 2024-01-07 09:58:24 (running for 00:33:45.12)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.264 |  0.142 |                   92 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.229 |  0.147 |                   85 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.519 |  0.177 |                   56 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.439 |  0.151 |                   58 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=98661)[0m rmse: 0.14771892130374908
[2m[36m(func pid=98661)[0m mae:  0.09013674408197403
[2m[36m(func pid=98661)[0m rmse_per_class: [0.101, 0.263, 0.025, 0.291, 0.072, 0.15, 0.206, 0.112, 0.13, 0.129]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5133 | Steps: 2 | Val loss: 0.3948 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=105127)[0m rmse: 0.15037314593791962
[2m[36m(func pid=105127)[0m mae:  0.10448286682367325
[2m[36m(func pid=105127)[0m rmse_per_class: [0.1, 0.24, 0.041, 0.295, 0.054, 0.183, 0.236, 0.126, 0.137, 0.092]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2570 | Steps: 2 | Val loss: 0.2661 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2354 | Steps: 2 | Val loss: 0.2748 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=104571)[0m rmse: 0.1766740381717682
[2m[36m(func pid=104571)[0m mae:  0.12910783290863037
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.332, 0.097, 0.191, 0.283, 0.14, 0.144, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4310 | Steps: 2 | Val loss: 0.3759 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 09:58:29 (running for 00:33:50.37)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.257 |  0.142 |                   93 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.24  |  0.148 |                   86 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.513 |  0.177 |                   57 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.433 |  0.15  |                   59 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14245280623435974
[2m[36m(func pid=97073)[0m mae:  0.0945325493812561
[2m[36m(func pid=97073)[0m rmse_per_class: [0.076, 0.253, 0.029, 0.276, 0.068, 0.149, 0.222, 0.109, 0.13, 0.113]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14738735556602478
[2m[36m(func pid=98661)[0m mae:  0.09023772925138474
[2m[36m(func pid=98661)[0m rmse_per_class: [0.102, 0.262, 0.024, 0.292, 0.073, 0.149, 0.207, 0.112, 0.128, 0.125]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.1500512957572937
[2m[36m(func pid=105127)[0m mae:  0.10404983907938004
[2m[36m(func pid=105127)[0m rmse_per_class: [0.099, 0.241, 0.04, 0.295, 0.054, 0.183, 0.236, 0.125, 0.137, 0.091]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.5071 | Steps: 2 | Val loss: 0.3904 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2578 | Steps: 2 | Val loss: 0.2677 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2268 | Steps: 2 | Val loss: 0.2738 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=104571)[0m rmse: 0.1765371859073639
[2m[36m(func pid=104571)[0m mae:  0.12898191809654236
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.331, 0.097, 0.191, 0.283, 0.139, 0.144, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4292 | Steps: 2 | Val loss: 0.3732 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 09:58:34 (running for 00:33:55.61)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.258 |  0.143 |                   94 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.235 |  0.147 |                   87 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.507 |  0.177 |                   58 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.431 |  0.15  |                   60 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14347051084041595
[2m[36m(func pid=97073)[0m mae:  0.09528511762619019
[2m[36m(func pid=97073)[0m rmse_per_class: [0.075, 0.251, 0.028, 0.277, 0.07, 0.149, 0.227, 0.11, 0.131, 0.116]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.1464196741580963
[2m[36m(func pid=98661)[0m mae:  0.08994103968143463
[2m[36m(func pid=98661)[0m rmse_per_class: [0.102, 0.261, 0.024, 0.294, 0.072, 0.148, 0.206, 0.111, 0.125, 0.121]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.14967624843120575
[2m[36m(func pid=105127)[0m mae:  0.10366837680339813
[2m[36m(func pid=105127)[0m rmse_per_class: [0.098, 0.24, 0.039, 0.295, 0.054, 0.183, 0.237, 0.124, 0.137, 0.091]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5017 | Steps: 2 | Val loss: 0.3861 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2605 | Steps: 2 | Val loss: 0.2681 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2314 | Steps: 2 | Val loss: 0.2740 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4265 | Steps: 2 | Val loss: 0.3711 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=104571)[0m rmse: 0.1764373928308487
[2m[36m(func pid=104571)[0m mae:  0.1288948655128479
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.094, 0.331, 0.096, 0.191, 0.282, 0.139, 0.144, 0.11]
[2m[36m(func pid=104571)[0m 
== Status ==
Current time: 2024-01-07 09:58:39 (running for 00:34:00.73)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.261 |  0.144 |                   95 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.227 |  0.146 |                   88 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.502 |  0.176 |                   59 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.429 |  0.15  |                   61 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14362645149230957
[2m[36m(func pid=97073)[0m mae:  0.09534239023923874
[2m[36m(func pid=97073)[0m rmse_per_class: [0.075, 0.251, 0.028, 0.277, 0.07, 0.148, 0.228, 0.11, 0.132, 0.118]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.1461542844772339
[2m[36m(func pid=98661)[0m mae:  0.08965388685464859
[2m[36m(func pid=98661)[0m rmse_per_class: [0.099, 0.261, 0.024, 0.295, 0.072, 0.148, 0.205, 0.111, 0.124, 0.123]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.14959701895713806
[2m[36m(func pid=105127)[0m mae:  0.10345568507909775
[2m[36m(func pid=105127)[0m rmse_per_class: [0.097, 0.24, 0.038, 0.295, 0.054, 0.183, 0.239, 0.123, 0.136, 0.091]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4961 | Steps: 2 | Val loss: 0.3817 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2617 | Steps: 2 | Val loss: 0.2677 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2353 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4198 | Steps: 2 | Val loss: 0.3685 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=104571)[0m rmse: 0.176347553730011
[2m[36m(func pid=104571)[0m mae:  0.12881293892860413
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.094, 0.331, 0.096, 0.191, 0.282, 0.139, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
== Status ==
Current time: 2024-01-07 09:58:44 (running for 00:34:05.80)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.262 |  0.143 |                   96 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.231 |  0.146 |                   89 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.496 |  0.176 |                   60 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.427 |  0.15  |                   62 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14335258305072784
[2m[36m(func pid=97073)[0m mae:  0.0949716791510582
[2m[36m(func pid=97073)[0m rmse_per_class: [0.074, 0.254, 0.028, 0.277, 0.068, 0.149, 0.225, 0.108, 0.132, 0.118]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14552924036979675
[2m[36m(func pid=98661)[0m mae:  0.08875393122434616
[2m[36m(func pid=98661)[0m rmse_per_class: [0.094, 0.263, 0.024, 0.292, 0.069, 0.147, 0.203, 0.113, 0.124, 0.127]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.1495162546634674
[2m[36m(func pid=105127)[0m mae:  0.10316132009029388
[2m[36m(func pid=105127)[0m rmse_per_class: [0.096, 0.24, 0.037, 0.296, 0.054, 0.183, 0.24, 0.123, 0.136, 0.09]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4911 | Steps: 2 | Val loss: 0.3777 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2575 | Steps: 2 | Val loss: 0.2673 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2364 | Steps: 2 | Val loss: 0.2726 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4103 | Steps: 2 | Val loss: 0.3657 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 09:58:49 (running for 00:34:10.83)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.258 |  0.143 |                   97 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.235 |  0.146 |                   90 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.491 |  0.176 |                   61 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.42  |  0.15  |                   63 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14286461472511292
[2m[36m(func pid=97073)[0m mae:  0.09451872110366821
[2m[36m(func pid=97073)[0m rmse_per_class: [0.074, 0.256, 0.027, 0.278, 0.068, 0.149, 0.223, 0.107, 0.131, 0.116]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17635583877563477
[2m[36m(func pid=104571)[0m mae:  0.12882623076438904
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.094, 0.331, 0.096, 0.191, 0.282, 0.139, 0.144, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14466805756092072
[2m[36m(func pid=98661)[0m mae:  0.0873827263712883
[2m[36m(func pid=98661)[0m rmse_per_class: [0.09, 0.263, 0.025, 0.283, 0.067, 0.147, 0.202, 0.114, 0.124, 0.132]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.14938023686408997
[2m[36m(func pid=105127)[0m mae:  0.10296603292226791
[2m[36m(func pid=105127)[0m rmse_per_class: [0.096, 0.24, 0.037, 0.296, 0.054, 0.182, 0.242, 0.122, 0.136, 0.089]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2656 | Steps: 2 | Val loss: 0.2677 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4855 | Steps: 2 | Val loss: 0.3737 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2300 | Steps: 2 | Val loss: 0.2735 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4132 | Steps: 2 | Val loss: 0.3621 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=97073)[0m rmse: 0.1430845409631729
[2m[36m(func pid=97073)[0m mae:  0.09456482529640198
[2m[36m(func pid=97073)[0m rmse_per_class: [0.074, 0.257, 0.027, 0.278, 0.068, 0.149, 0.222, 0.107, 0.131, 0.116]
[2m[36m(func pid=97073)[0m 
== Status ==
Current time: 2024-01-07 09:58:55 (running for 00:34:16.07)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.266 |  0.143 |                   98 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.145 |                   91 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.491 |  0.176 |                   61 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.41  |  0.149 |                   64 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=104571)[0m rmse: 0.17622169852256775
[2m[36m(func pid=104571)[0m mae:  0.12871980667114258
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.094, 0.331, 0.095, 0.191, 0.282, 0.14, 0.144, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.1493215560913086
[2m[36m(func pid=105127)[0m mae:  0.10276881605386734
[2m[36m(func pid=105127)[0m rmse_per_class: [0.095, 0.24, 0.036, 0.297, 0.054, 0.182, 0.243, 0.122, 0.136, 0.089]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14477844536304474
[2m[36m(func pid=98661)[0m mae:  0.08700373023748398
[2m[36m(func pid=98661)[0m rmse_per_class: [0.091, 0.264, 0.026, 0.281, 0.064, 0.147, 0.204, 0.114, 0.124, 0.133]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2600 | Steps: 2 | Val loss: 0.2676 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4827 | Steps: 2 | Val loss: 0.3699 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4128 | Steps: 2 | Val loss: 0.3585 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2290 | Steps: 2 | Val loss: 0.2754 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 09:59:00 (running for 00:34:21.30)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00014 | RUNNING    | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.26  |  0.143 |                   99 |
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.23  |  0.145 |                   92 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.486 |  0.176 |                   62 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.413 |  0.149 |                   65 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14277830719947815
[2m[36m(func pid=97073)[0m mae:  0.09424029290676117
[2m[36m(func pid=97073)[0m rmse_per_class: [0.074, 0.258, 0.027, 0.279, 0.069, 0.148, 0.221, 0.107, 0.13, 0.114]
[2m[36m(func pid=97073)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17605935037136078
[2m[36m(func pid=104571)[0m mae:  0.12856407463550568
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.094, 0.33, 0.095, 0.191, 0.281, 0.139, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.14913225173950195
[2m[36m(func pid=105127)[0m mae:  0.10242629051208496
[2m[36m(func pid=105127)[0m rmse_per_class: [0.094, 0.24, 0.036, 0.296, 0.054, 0.182, 0.243, 0.122, 0.136, 0.089]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14554700255393982
[2m[36m(func pid=98661)[0m mae:  0.08719509840011597
[2m[36m(func pid=98661)[0m rmse_per_class: [0.094, 0.261, 0.027, 0.278, 0.064, 0.147, 0.206, 0.115, 0.124, 0.139]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=97073)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2524 | Steps: 2 | Val loss: 0.2669 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4793 | Steps: 2 | Val loss: 0.3659 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4049 | Steps: 2 | Val loss: 0.3552 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2346 | Steps: 2 | Val loss: 0.2759 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 09:59:05 (running for 00:34:26.42)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 3 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.229 |  0.146 |                   93 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.483 |  0.176 |                   63 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.413 |  0.149 |                   66 |
| train_32e5a_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=97073)[0m rmse: 0.14232106506824493
[2m[36m(func pid=97073)[0m mae:  0.09382722526788712
[2m[36m(func pid=97073)[0m rmse_per_class: [0.074, 0.257, 0.027, 0.277, 0.068, 0.149, 0.22, 0.107, 0.13, 0.115]
[2m[36m(func pid=104571)[0m rmse: 0.17589259147644043
[2m[36m(func pid=104571)[0m mae:  0.1284225434064865
[2m[36m(func pid=104571)[0m rmse_per_class: [0.118, 0.26, 0.093, 0.33, 0.095, 0.191, 0.281, 0.139, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.14911314845085144
[2m[36m(func pid=105127)[0m mae:  0.10224349796772003
[2m[36m(func pid=105127)[0m rmse_per_class: [0.093, 0.24, 0.035, 0.297, 0.054, 0.182, 0.245, 0.121, 0.135, 0.088]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14584164321422577
[2m[36m(func pid=98661)[0m mae:  0.08737000823020935
[2m[36m(func pid=98661)[0m rmse_per_class: [0.096, 0.261, 0.027, 0.277, 0.063, 0.147, 0.207, 0.114, 0.125, 0.142]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4708 | Steps: 2 | Val loss: 0.3626 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4002 | Steps: 2 | Val loss: 0.3519 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2308 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=104571)[0m rmse: 0.175867959856987
[2m[36m(func pid=104571)[0m mae:  0.12839363515377045
[2m[36m(func pid=104571)[0m rmse_per_class: [0.118, 0.26, 0.093, 0.33, 0.094, 0.191, 0.281, 0.139, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.14906349778175354
[2m[36m(func pid=105127)[0m mae:  0.10209473222494125
[2m[36m(func pid=105127)[0m rmse_per_class: [0.092, 0.24, 0.034, 0.298, 0.054, 0.181, 0.247, 0.121, 0.135, 0.088]
[2m[36m(func pid=98661)[0m rmse: 0.14447322487831116
[2m[36m(func pid=98661)[0m mae:  0.08684591948986053
[2m[36m(func pid=98661)[0m rmse_per_class: [0.094, 0.261, 0.026, 0.276, 0.063, 0.146, 0.204, 0.112, 0.127, 0.136]
== Status ==
Current time: 2024-01-07 09:59:11 (running for 00:34:32.95)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.235 |  0.146 |                   94 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.471 |  0.176 |                   65 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.405 |  0.149 |                   67 |
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=119733)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=119733)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=119733)[0m Configuration completed!
[2m[36m(func pid=119733)[0m New optimizer parameters:
[2m[36m(func pid=119733)[0m SGD (
[2m[36m(func pid=119733)[0m Parameter Group 0
[2m[36m(func pid=119733)[0m     dampening: 0
[2m[36m(func pid=119733)[0m     differentiable: False
[2m[36m(func pid=119733)[0m     foreach: None
[2m[36m(func pid=119733)[0m     lr: 0.01
[2m[36m(func pid=119733)[0m     maximize: False
[2m[36m(func pid=119733)[0m     momentum: 0.99
[2m[36m(func pid=119733)[0m     nesterov: False
[2m[36m(func pid=119733)[0m     weight_decay: 1e-05
[2m[36m(func pid=119733)[0m )
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4684 | Steps: 2 | Val loss: 0.3595 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2369 | Steps: 2 | Val loss: 0.2722 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3926 | Steps: 2 | Val loss: 0.3476 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 09:59:17 (running for 00:34:38.51)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.231 |  0.144 |                   95 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.468 |  0.176 |                   66 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.4   |  0.149 |                   68 |
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=104571)[0m rmse: 0.1757129430770874
[2m[36m(func pid=104571)[0m mae:  0.12825560569763184
[2m[36m(func pid=104571)[0m rmse_per_class: [0.118, 0.26, 0.093, 0.329, 0.094, 0.191, 0.28, 0.139, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8899 | Steps: 2 | Val loss: 0.6864 | Batch size: 32 | lr: 0.01 | Duration: 4.69s
[2m[36m(func pid=105127)[0m rmse: 0.14886696636676788
[2m[36m(func pid=105127)[0m mae:  0.10191156715154648
[2m[36m(func pid=105127)[0m rmse_per_class: [0.092, 0.24, 0.033, 0.299, 0.054, 0.181, 0.247, 0.12, 0.135, 0.088]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14386384189128876
[2m[36m(func pid=98661)[0m mae:  0.08706730604171753
[2m[36m(func pid=98661)[0m rmse_per_class: [0.093, 0.263, 0.024, 0.28, 0.062, 0.146, 0.2, 0.111, 0.13, 0.13]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4625 | Steps: 2 | Val loss: 0.3564 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=119733)[0m rmse: 0.18251869082450867
[2m[36m(func pid=119733)[0m mae:  0.13430118560791016
[2m[36m(func pid=119733)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.11, 0.191, 0.294, 0.144, 0.144, 0.113]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.3845 | Steps: 2 | Val loss: 0.3436 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2346 | Steps: 2 | Val loss: 0.2732 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 09:59:22 (running for 00:34:43.86)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.237 |  0.144 |                   96 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.462 |  0.176 |                   67 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.393 |  0.149 |                   69 |
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.89  |  0.183 |                    1 |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=104571)[0m rmse: 0.17567415535449982
[2m[36m(func pid=104571)[0m mae:  0.12819746136665344
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.093, 0.329, 0.094, 0.191, 0.28, 0.139, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8398 | Steps: 2 | Val loss: 0.6308 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=105127)[0m rmse: 0.14860300719738007
[2m[36m(func pid=105127)[0m mae:  0.10168503224849701
[2m[36m(func pid=105127)[0m rmse_per_class: [0.091, 0.24, 0.033, 0.298, 0.054, 0.18, 0.247, 0.12, 0.135, 0.088]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14458347856998444
[2m[36m(func pid=98661)[0m mae:  0.0883561447262764
[2m[36m(func pid=98661)[0m rmse_per_class: [0.09, 0.266, 0.024, 0.289, 0.063, 0.146, 0.201, 0.11, 0.132, 0.127]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4582 | Steps: 2 | Val loss: 0.3530 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=119733)[0m rmse: 0.1813693791627884
[2m[36m(func pid=119733)[0m mae:  0.1333145797252655
[2m[36m(func pid=119733)[0m rmse_per_class: [0.114, 0.266, 0.108, 0.338, 0.109, 0.191, 0.289, 0.141, 0.144, 0.113]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3868 | Steps: 2 | Val loss: 0.3400 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2358 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=104571)[0m rmse: 0.17553505301475525
[2m[36m(func pid=104571)[0m mae:  0.12806718051433563
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.093, 0.329, 0.094, 0.191, 0.28, 0.139, 0.143, 0.11]
== Status ==
Current time: 2024-01-07 09:59:28 (running for 00:34:49.74)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.235 |  0.145 |                   97 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.458 |  0.176 |                   68 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.384 |  0.149 |                   70 |
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.84  |  0.181 |                    2 |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7506 | Steps: 2 | Val loss: 0.5480 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=98661)[0m rmse: 0.1447153389453888
[2m[36m(func pid=98661)[0m mae:  0.08847586810588837
[2m[36m(func pid=98661)[0m rmse_per_class: [0.092, 0.264, 0.024, 0.288, 0.063, 0.146, 0.202, 0.109, 0.131, 0.127]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=105127)[0m rmse: 0.14843730628490448
[2m[36m(func pid=105127)[0m mae:  0.10141954571008682
[2m[36m(func pid=105127)[0m rmse_per_class: [0.091, 0.24, 0.033, 0.299, 0.054, 0.18, 0.248, 0.119, 0.135, 0.087]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4541 | Steps: 2 | Val loss: 0.3496 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=119733)[0m rmse: 0.17992989718914032
[2m[36m(func pid=119733)[0m mae:  0.1317945122718811
[2m[36m(func pid=119733)[0m rmse_per_class: [0.112, 0.265, 0.108, 0.337, 0.107, 0.191, 0.283, 0.141, 0.143, 0.111]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3853 | Steps: 2 | Val loss: 0.3364 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2350 | Steps: 2 | Val loss: 0.2721 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 09:59:34 (running for 00:34:55.26)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.236 |  0.145 |                   98 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.454 |  0.175 |                   69 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.387 |  0.148 |                   71 |
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.751 |  0.18  |                    3 |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=104571)[0m rmse: 0.17538028955459595
[2m[36m(func pid=104571)[0m mae:  0.12794704735279083
[2m[36m(func pid=104571)[0m rmse_per_class: [0.118, 0.26, 0.092, 0.328, 0.093, 0.191, 0.28, 0.139, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6417 | Steps: 2 | Val loss: 0.4642 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=105127)[0m rmse: 0.14817234873771667
[2m[36m(func pid=105127)[0m mae:  0.10113371908664703
[2m[36m(func pid=105127)[0m rmse_per_class: [0.09, 0.239, 0.032, 0.299, 0.054, 0.18, 0.248, 0.119, 0.135, 0.087]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=98661)[0m rmse: 0.14405521750450134
[2m[36m(func pid=98661)[0m mae:  0.0877617746591568
[2m[36m(func pid=98661)[0m rmse_per_class: [0.086, 0.265, 0.024, 0.288, 0.065, 0.147, 0.202, 0.108, 0.13, 0.126]
[2m[36m(func pid=98661)[0m 
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4478 | Steps: 2 | Val loss: 0.3470 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=119733)[0m rmse: 0.17870432138442993
[2m[36m(func pid=119733)[0m mae:  0.1306743323802948
[2m[36m(func pid=119733)[0m rmse_per_class: [0.113, 0.264, 0.106, 0.335, 0.104, 0.191, 0.281, 0.14, 0.144, 0.11]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=98661)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2261 | Steps: 2 | Val loss: 0.2708 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3764 | Steps: 2 | Val loss: 0.3317 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=104571)[0m rmse: 0.17525412142276764
[2m[36m(func pid=104571)[0m mae:  0.1278410255908966
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.092, 0.328, 0.093, 0.191, 0.279, 0.139, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
== Status ==
Current time: 2024-01-07 09:59:39 (running for 00:35:00.85)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00015 | RUNNING    | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.235 |  0.144 |                   99 |
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.448 |  0.175 |                   70 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.385 |  0.148 |                   72 |
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.642 |  0.179 |                    4 |
| train_32e5a_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=105127)[0m rmse: 0.147807776927948
[2m[36m(func pid=105127)[0m mae:  0.10081116110086441
[2m[36m(func pid=105127)[0m rmse_per_class: [0.09, 0.239, 0.032, 0.298, 0.054, 0.179, 0.247, 0.118, 0.135, 0.087]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5369 | Steps: 2 | Val loss: 0.3973 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=98661)[0m rmse: 0.14316847920417786
[2m[36m(func pid=98661)[0m mae:  0.08691085129976273
[2m[36m(func pid=98661)[0m rmse_per_class: [0.085, 0.264, 0.024, 0.283, 0.066, 0.148, 0.201, 0.107, 0.127, 0.125]
[2m[36m(func pid=119733)[0m rmse: 0.17722877860069275
[2m[36m(func pid=119733)[0m mae:  0.12940192222595215
[2m[36m(func pid=119733)[0m rmse_per_class: [0.114, 0.264, 0.102, 0.332, 0.099, 0.191, 0.278, 0.139, 0.144, 0.11]
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4468 | Steps: 2 | Val loss: 0.3447 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3745 | Steps: 2 | Val loss: 0.3276 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=105127)[0m rmse: 0.14750376343727112
[2m[36m(func pid=105127)[0m mae:  0.10057713091373444
[2m[36m(func pid=105127)[0m rmse_per_class: [0.09, 0.239, 0.032, 0.298, 0.054, 0.178, 0.247, 0.118, 0.135, 0.086]
[2m[36m(func pid=105127)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.1752738654613495
[2m[36m(func pid=104571)[0m mae:  0.12786035239696503
[2m[36m(func pid=104571)[0m rmse_per_class: [0.118, 0.26, 0.092, 0.328, 0.093, 0.191, 0.279, 0.139, 0.143, 0.11]
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4596 | Steps: 2 | Val loss: 0.3494 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=105127)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3686 | Steps: 2 | Val loss: 0.3234 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=119733)[0m rmse: 0.17576608061790466
[2m[36m(func pid=119733)[0m mae:  0.12815895676612854
[2m[36m(func pid=119733)[0m rmse_per_class: [0.116, 0.264, 0.098, 0.329, 0.094, 0.191, 0.274, 0.138, 0.143, 0.109]
== Status ==
Current time: 2024-01-07 09:59:45 (running for 00:35:06.62)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: -0.14525000005960464
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.448 |  0.175 |                   70 |
| train_32e5a_00017 | RUNNING    | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.375 |  0.148 |                   74 |
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.537 |  0.177 |                    5 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121379)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=121379)[0m Configuration completed!
[2m[36m(func pid=121379)[0m New optimizer parameters:
[2m[36m(func pid=121379)[0m SGD (
[2m[36m(func pid=121379)[0m Parameter Group 0
[2m[36m(func pid=121379)[0m     dampening: 0
[2m[36m(func pid=121379)[0m     differentiable: False
[2m[36m(func pid=121379)[0m     foreach: None
[2m[36m(func pid=121379)[0m     lr: 0.1
[2m[36m(func pid=121379)[0m     maximize: False
[2m[36m(func pid=121379)[0m     momentum: 0.99
[2m[36m(func pid=121379)[0m     nesterov: False
[2m[36m(func pid=121379)[0m     weight_decay: 1e-05
[2m[36m(func pid=121379)[0m )
[2m[36m(func pid=121379)[0m 
== Status ==
Current time: 2024-01-07 09:59:50 (running for 00:35:11.75)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.1459999978542328
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 3 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.447 |  0.175 |                   71 |
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.46  |  0.176 |                    6 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |        |        |                      |
| train_32e5a_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=105127)[0m rmse: 0.14695216715335846
[2m[36m(func pid=105127)[0m mae:  0.10027389228343964
[2m[36m(func pid=105127)[0m rmse_per_class: [0.09, 0.239, 0.031, 0.298, 0.054, 0.177, 0.244, 0.117, 0.135, 0.085]
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4442 | Steps: 2 | Val loss: 0.3418 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4116 | Steps: 2 | Val loss: 0.3202 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8360 | Steps: 2 | Val loss: 0.5024 | Batch size: 32 | lr: 0.1 | Duration: 4.99s
[2m[36m(func pid=104571)[0m rmse: 0.17512297630310059
[2m[36m(func pid=104571)[0m mae:  0.12773387134075165
[2m[36m(func pid=104571)[0m rmse_per_class: [0.118, 0.26, 0.092, 0.328, 0.092, 0.191, 0.279, 0.139, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.17404243350028992
[2m[36m(func pid=119733)[0m mae:  0.12669113278388977
[2m[36m(func pid=119733)[0m rmse_per_class: [0.117, 0.265, 0.094, 0.326, 0.088, 0.191, 0.27, 0.136, 0.143, 0.109]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.1802530735731125
[2m[36m(func pid=121379)[0m mae:  0.13195447623729706
[2m[36m(func pid=121379)[0m rmse_per_class: [0.109, 0.265, 0.115, 0.337, 0.106, 0.19, 0.285, 0.143, 0.143, 0.11]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3952 | Steps: 2 | Val loss: 0.3094 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4388 | Steps: 2 | Val loss: 0.3393 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 09:59:56 (running for 00:35:17.89)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.1459999978542328
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.444 |  0.175 |                   72 |
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.412 |  0.174 |                    7 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.836 |  0.18  |                    1 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121978)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=121978)[0m Configuration completed!
[2m[36m(func pid=121978)[0m New optimizer parameters:
[2m[36m(func pid=121978)[0m SGD (
[2m[36m(func pid=121978)[0m Parameter Group 0
[2m[36m(func pid=121978)[0m     dampening: 0
[2m[36m(func pid=121978)[0m     differentiable: False
[2m[36m(func pid=121978)[0m     foreach: None
[2m[36m(func pid=121978)[0m     lr: 0.0001
[2m[36m(func pid=121978)[0m     maximize: False
[2m[36m(func pid=121978)[0m     momentum: 0.9
[2m[36m(func pid=121978)[0m     nesterov: False
[2m[36m(func pid=121978)[0m     weight_decay: 1e-05
[2m[36m(func pid=121978)[0m )
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5221 | Steps: 2 | Val loss: 0.3376 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=119733)[0m rmse: 0.1722167730331421
[2m[36m(func pid=119733)[0m mae:  0.12511062622070312
[2m[36m(func pid=119733)[0m rmse_per_class: [0.118, 0.265, 0.091, 0.323, 0.082, 0.192, 0.265, 0.135, 0.143, 0.109]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=104571)[0m rmse: 0.17493125796318054
[2m[36m(func pid=104571)[0m mae:  0.12757791578769684
[2m[36m(func pid=104571)[0m rmse_per_class: [0.118, 0.26, 0.092, 0.328, 0.092, 0.191, 0.279, 0.139, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
== Status ==
Current time: 2024-01-07 10:00:02 (running for 00:35:23.43)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.1459999978542328
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.439 |  0.175 |                   73 |
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.395 |  0.172 |                    8 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.522 |  0.177 |                    2 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121379)[0m rmse: 0.17696209251880646
[2m[36m(func pid=121379)[0m mae:  0.1293606013059616
[2m[36m(func pid=121379)[0m rmse_per_class: [0.115, 0.266, 0.105, 0.333, 0.089, 0.191, 0.275, 0.14, 0.146, 0.109]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4038 | Steps: 2 | Val loss: 0.3143 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.4395 | Steps: 2 | Val loss: 0.3370 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8986 | Steps: 2 | Val loss: 0.7088 | Batch size: 32 | lr: 0.0001 | Duration: 4.59s
[2m[36m(func pid=119733)[0m rmse: 0.1701231300830841
[2m[36m(func pid=119733)[0m mae:  0.1233368068933487
[2m[36m(func pid=119733)[0m rmse_per_class: [0.119, 0.264, 0.086, 0.319, 0.076, 0.192, 0.26, 0.133, 0.144, 0.109]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4073 | Steps: 2 | Val loss: 0.3274 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
[2m[36m(func pid=121978)[0m rmse: 0.18275246024131775
[2m[36m(func pid=121978)[0m mae:  0.13447965681552887
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.267, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=104571)[0m rmse: 0.1747927963733673
[2m[36m(func pid=104571)[0m mae:  0.12744973599910736
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.092, 0.327, 0.091, 0.191, 0.278, 0.139, 0.143, 0.11]
[2m[36m(func pid=104571)[0m 
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.17304210364818573
[2m[36m(func pid=121379)[0m mae:  0.1259927749633789
[2m[36m(func pid=121379)[0m rmse_per_class: [0.125, 0.267, 0.094, 0.326, 0.072, 0.194, 0.264, 0.136, 0.147, 0.107]
== Status ==
Current time: 2024-01-07 10:00:08 (running for 00:35:29.18)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: -0.1459999978542328
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00016 | RUNNING    | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.439 |  0.175 |                   74 |
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.404 |  0.17  |                    9 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.407 |  0.173 |                    3 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.899 |  0.183 |                    1 |
| train_32e5a_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.4167 | Steps: 2 | Val loss: 0.3295 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8972 | Steps: 2 | Val loss: 0.7055 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=104571)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4349 | Steps: 2 | Val loss: 0.3351 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=119733)[0m rmse: 0.16749556362628937
[2m[36m(func pid=119733)[0m mae:  0.12113851308822632
[2m[36m(func pid=119733)[0m rmse_per_class: [0.118, 0.262, 0.08, 0.316, 0.07, 0.192, 0.253, 0.132, 0.144, 0.107]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.18253275752067566
[2m[36m(func pid=121978)[0m mae:  0.13435761630535126
[2m[36m(func pid=121978)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.144, 0.144, 0.112]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4807 | Steps: 2 | Val loss: 0.3860 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=104571)[0m rmse: 0.1746976524591446
[2m[36m(func pid=104571)[0m mae:  0.12737931311130524
[2m[36m(func pid=104571)[0m rmse_per_class: [0.117, 0.26, 0.092, 0.327, 0.091, 0.191, 0.278, 0.138, 0.143, 0.11]
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.4475 | Steps: 2 | Val loss: 0.3500 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=121379)[0m rmse: 0.16914428770542145
[2m[36m(func pid=121379)[0m mae:  0.1222287267446518
[2m[36m(func pid=121379)[0m rmse_per_class: [0.135, 0.261, 0.08, 0.319, 0.059, 0.195, 0.26, 0.131, 0.148, 0.103]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8950 | Steps: 2 | Val loss: 0.7019 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=119733)[0m rmse: 0.16474290192127228
[2m[36m(func pid=119733)[0m mae:  0.1187589019536972
[2m[36m(func pid=119733)[0m rmse_per_class: [0.117, 0.26, 0.074, 0.313, 0.065, 0.192, 0.245, 0.131, 0.145, 0.106]
[2m[36m(func pid=121978)[0m rmse: 0.18218427896499634
[2m[36m(func pid=121978)[0m mae:  0.1341090351343155
[2m[36m(func pid=121978)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.112, 0.19, 0.294, 0.143, 0.143, 0.112]
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5585 | Steps: 2 | Val loss: 0.4371 | Batch size: 32 | lr: 0.1 | Duration: 3.29s
== Status ==
Current time: 2024-01-07 10:00:14 (running for 00:35:35.13)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.417 |  0.167 |                   10 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.481 |  0.169 |                    4 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.897 |  0.183 |                    2 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 10:00:19 (running for 00:35:40.82)
Memory usage on this node: 23.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.417 |  0.167 |                   10 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.481 |  0.169 |                    4 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.895 |  0.182 |                    3 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=123100)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=123100)[0m Configuration completed!
[2m[36m(func pid=123100)[0m New optimizer parameters:
[2m[36m(func pid=123100)[0m SGD (
[2m[36m(func pid=123100)[0m Parameter Group 0
[2m[36m(func pid=123100)[0m     dampening: 0
[2m[36m(func pid=123100)[0m     differentiable: False
[2m[36m(func pid=123100)[0m     foreach: None
[2m[36m(func pid=123100)[0m     lr: 0.001
[2m[36m(func pid=123100)[0m     maximize: False
[2m[36m(func pid=123100)[0m     momentum: 0.9
[2m[36m(func pid=123100)[0m     nesterov: False
[2m[36m(func pid=123100)[0m     weight_decay: 1e-05
[2m[36m(func pid=123100)[0m )
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.1613936424255371
[2m[36m(func pid=121379)[0m mae:  0.11454327404499054
[2m[36m(func pid=121379)[0m rmse_per_class: [0.142, 0.255, 0.052, 0.301, 0.055, 0.195, 0.244, 0.127, 0.149, 0.093]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8943 | Steps: 2 | Val loss: 0.6986 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.4650 | Steps: 2 | Val loss: 0.3740 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5816 | Steps: 2 | Val loss: 0.4827 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8982 | Steps: 2 | Val loss: 0.7064 | Batch size: 32 | lr: 0.001 | Duration: 4.75s
== Status ==
Current time: 2024-01-07 10:00:25 (running for 00:35:46.07)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.448 |  0.165 |                   11 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.559 |  0.161 |                    5 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.895 |  0.182 |                    3 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m rmse: 0.18177059292793274
[2m[36m(func pid=121978)[0m mae:  0.13378150761127472
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.266, 0.105, 0.339, 0.112, 0.19, 0.294, 0.142, 0.143, 0.111]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.1621069610118866
[2m[36m(func pid=119733)[0m mae:  0.11634945869445801
[2m[36m(func pid=119733)[0m rmse_per_class: [0.116, 0.256, 0.067, 0.311, 0.061, 0.192, 0.238, 0.13, 0.146, 0.104]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.15542922914028168
[2m[36m(func pid=121379)[0m mae:  0.10434745252132416
[2m[36m(func pid=121379)[0m rmse_per_class: [0.147, 0.252, 0.03, 0.284, 0.055, 0.204, 0.236, 0.125, 0.137, 0.084]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.18261967599391937
[2m[36m(func pid=123100)[0m mae:  0.13436926901340485
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.266, 0.109, 0.339, 0.111, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.4919 | Steps: 2 | Val loss: 0.3980 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8941 | Steps: 2 | Val loss: 0.6965 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.5371 | Steps: 2 | Val loss: 0.4941 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8917 | Steps: 2 | Val loss: 0.6991 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 10:00:30 (running for 00:35:51.66)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.465 |  0.162 |                   12 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.582 |  0.155 |                    6 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.894 |  0.181 |                    5 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.898 |  0.183 |                    1 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m rmse: 0.18136748671531677
[2m[36m(func pid=121978)[0m mae:  0.13344760239124298
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.112, 0.19, 0.294, 0.142, 0.142, 0.111]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.15947143733501434
[2m[36m(func pid=119733)[0m mae:  0.11374441534280777
[2m[36m(func pid=119733)[0m rmse_per_class: [0.114, 0.253, 0.06, 0.309, 0.058, 0.192, 0.231, 0.13, 0.147, 0.102]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.1652405858039856
[2m[36m(func pid=121379)[0m mae:  0.1058940663933754
[2m[36m(func pid=121379)[0m rmse_per_class: [0.125, 0.253, 0.03, 0.291, 0.056, 0.212, 0.322, 0.141, 0.137, 0.085]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.18245063722133636
[2m[36m(func pid=123100)[0m mae:  0.13429078459739685
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.266, 0.107, 0.339, 0.112, 0.19, 0.294, 0.143, 0.144, 0.113]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8905 | Steps: 2 | Val loss: 0.6952 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5131 | Steps: 2 | Val loss: 0.4205 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4765 | Steps: 2 | Val loss: 0.4958 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.8835 | Steps: 2 | Val loss: 0.6884 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=121978)[0m rmse: 0.18105432391166687
[2m[36m(func pid=121978)[0m mae:  0.13317464292049408
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.112, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=121978)[0m 
== Status ==
Current time: 2024-01-07 10:00:35 (running for 00:35:56.92)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.492 |  0.159 |                   13 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.537 |  0.165 |                    7 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.89  |  0.181 |                    6 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.892 |  0.182 |                    2 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15757988393306732
[2m[36m(func pid=119733)[0m mae:  0.11170568317174911
[2m[36m(func pid=119733)[0m rmse_per_class: [0.114, 0.25, 0.054, 0.308, 0.056, 0.191, 0.226, 0.13, 0.147, 0.1]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.18491531908512115
[2m[36m(func pid=121379)[0m mae:  0.11794930696487427
[2m[36m(func pid=121379)[0m rmse_per_class: [0.114, 0.256, 0.045, 0.337, 0.056, 0.219, 0.439, 0.16, 0.139, 0.085]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.18196287751197815
[2m[36m(func pid=123100)[0m mae:  0.13391819596290588
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.266, 0.106, 0.339, 0.112, 0.19, 0.293, 0.142, 0.143, 0.112]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8898 | Steps: 2 | Val loss: 0.6935 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5212 | Steps: 2 | Val loss: 0.4396 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3826 | Steps: 2 | Val loss: 0.6251 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.8710 | Steps: 2 | Val loss: 0.6762 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:00:41 (running for 00:36:02.09)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.513 |  0.158 |                   14 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.476 |  0.185 |                    8 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.89  |  0.181 |                    7 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.883 |  0.182 |                    3 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m rmse: 0.18076977133750916
[2m[36m(func pid=121978)[0m mae:  0.13292895257472992
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.264, 0.103, 0.338, 0.111, 0.19, 0.294, 0.141, 0.142, 0.11]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.15579727292060852
[2m[36m(func pid=119733)[0m mae:  0.10950926691293716
[2m[36m(func pid=119733)[0m rmse_per_class: [0.112, 0.248, 0.048, 0.307, 0.055, 0.19, 0.224, 0.131, 0.146, 0.097]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.18151606619358063
[2m[36m(func pid=123100)[0m mae:  0.13356205821037292
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.266, 0.104, 0.338, 0.111, 0.19, 0.293, 0.142, 0.143, 0.112]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.1944832056760788
[2m[36m(func pid=121379)[0m mae:  0.12428821623325348
[2m[36m(func pid=121379)[0m rmse_per_class: [0.114, 0.255, 0.049, 0.349, 0.056, 0.224, 0.499, 0.166, 0.145, 0.088]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8886 | Steps: 2 | Val loss: 0.6926 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.5312 | Steps: 2 | Val loss: 0.4540 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.8550 | Steps: 2 | Val loss: 0.6640 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3615 | Steps: 2 | Val loss: 0.7710 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 10:00:46 (running for 00:36:07.51)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.521 |  0.156 |                   15 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.383 |  0.194 |                    9 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.889 |  0.181 |                    8 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.871 |  0.182 |                    4 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m rmse: 0.18053239583969116
[2m[36m(func pid=121978)[0m mae:  0.13271769881248474
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.264, 0.102, 0.337, 0.111, 0.19, 0.294, 0.141, 0.142, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.15425950288772583
[2m[36m(func pid=119733)[0m mae:  0.10738839954137802
[2m[36m(func pid=119733)[0m rmse_per_class: [0.11, 0.245, 0.043, 0.305, 0.055, 0.189, 0.225, 0.131, 0.145, 0.094]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.18109838664531708
[2m[36m(func pid=123100)[0m mae:  0.13320520520210266
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.265, 0.104, 0.338, 0.111, 0.19, 0.293, 0.141, 0.143, 0.111]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.19627954065799713
[2m[36m(func pid=121379)[0m mae:  0.12495331466197968
[2m[36m(func pid=121379)[0m rmse_per_class: [0.115, 0.253, 0.049, 0.354, 0.056, 0.226, 0.501, 0.17, 0.146, 0.09]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.8858 | Steps: 2 | Val loss: 0.6910 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5303 | Steps: 2 | Val loss: 0.4606 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.8406 | Steps: 2 | Val loss: 0.6503 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 10:00:51 (running for 00:36:12.72)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.531 |  0.154 |                   16 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.362 |  0.196 |                   10 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.886 |  0.18  |                    9 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.855 |  0.181 |                    5 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m rmse: 0.18032053112983704
[2m[36m(func pid=121978)[0m mae:  0.13252843916416168
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.263, 0.101, 0.337, 0.111, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3694 | Steps: 2 | Val loss: 0.7054 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=119733)[0m rmse: 0.1522287279367447
[2m[36m(func pid=119733)[0m mae:  0.10487572848796844
[2m[36m(func pid=119733)[0m rmse_per_class: [0.106, 0.244, 0.038, 0.301, 0.054, 0.188, 0.228, 0.129, 0.143, 0.092]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.18066205084323883
[2m[36m(func pid=123100)[0m mae:  0.13283228874206543
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.265, 0.103, 0.338, 0.11, 0.19, 0.292, 0.141, 0.143, 0.111]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.1964164525270462
[2m[36m(func pid=121379)[0m mae:  0.12359446287155151
[2m[36m(func pid=121379)[0m rmse_per_class: [0.118, 0.255, 0.05, 0.352, 0.056, 0.226, 0.475, 0.197, 0.143, 0.091]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8839 | Steps: 2 | Val loss: 0.6897 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.5389 | Steps: 2 | Val loss: 0.4627 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.8213 | Steps: 2 | Val loss: 0.6370 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 10:00:56 (running for 00:36:17.98)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.53  |  0.152 |                   17 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.369 |  0.196 |                   11 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.884 |  0.18  |                   10 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.841 |  0.181 |                    6 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m rmse: 0.1801382303237915
[2m[36m(func pid=121978)[0m mae:  0.13236573338508606
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.15016911923885345
[2m[36m(func pid=119733)[0m mae:  0.1019487977027893
[2m[36m(func pid=119733)[0m rmse_per_class: [0.1, 0.241, 0.034, 0.296, 0.054, 0.19, 0.233, 0.125, 0.14, 0.089]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3496 | Steps: 2 | Val loss: 0.5910 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
[2m[36m(func pid=123100)[0m rmse: 0.18027684092521667
[2m[36m(func pid=123100)[0m mae:  0.13248160481452942
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.264, 0.103, 0.337, 0.109, 0.19, 0.291, 0.141, 0.143, 0.11]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8815 | Steps: 2 | Val loss: 0.6885 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.5188 | Steps: 2 | Val loss: 0.4675 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=121379)[0m rmse: 0.19881664216518402
[2m[36m(func pid=121379)[0m mae:  0.12266705930233002
[2m[36m(func pid=121379)[0m rmse_per_class: [0.118, 0.258, 0.049, 0.343, 0.056, 0.222, 0.472, 0.237, 0.141, 0.09]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.8039 | Steps: 2 | Val loss: 0.6242 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 10:01:02 (running for 00:36:23.36)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.539 |  0.15  |                   18 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.35  |  0.199 |                   12 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.882 |  0.18  |                   11 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.821 |  0.18  |                    7 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m rmse: 0.17995022237300873
[2m[36m(func pid=121978)[0m mae:  0.13220618665218353
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.262, 0.101, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.15009872615337372
[2m[36m(func pid=119733)[0m mae:  0.10036313533782959
[2m[36m(func pid=119733)[0m rmse_per_class: [0.094, 0.24, 0.031, 0.294, 0.054, 0.194, 0.246, 0.123, 0.138, 0.087]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3416 | Steps: 2 | Val loss: 0.5178 | Batch size: 32 | lr: 0.1 | Duration: 3.30s
[2m[36m(func pid=123100)[0m rmse: 0.18000991642475128
[2m[36m(func pid=123100)[0m mae:  0.13222722709178925
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.263, 0.102, 0.337, 0.108, 0.19, 0.291, 0.141, 0.143, 0.11]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.8791 | Steps: 2 | Val loss: 0.6872 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.5138 | Steps: 2 | Val loss: 0.4706 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=121379)[0m rmse: 0.1799066811800003
[2m[36m(func pid=121379)[0m mae:  0.11280305683612823
[2m[36m(func pid=121379)[0m rmse_per_class: [0.127, 0.273, 0.049, 0.325, 0.056, 0.202, 0.373, 0.161, 0.141, 0.092]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.7856 | Steps: 2 | Val loss: 0.6101 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 10:01:07 (running for 00:36:28.82)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.519 |  0.15  |                   19 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.342 |  0.18  |                   13 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.879 |  0.18  |                   12 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.804 |  0.18  |                    8 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m rmse: 0.17981700599193573
[2m[36m(func pid=121978)[0m mae:  0.13208559155464172
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.15248753130435944
[2m[36m(func pid=119733)[0m mae:  0.10066331923007965
[2m[36m(func pid=119733)[0m rmse_per_class: [0.09, 0.24, 0.029, 0.297, 0.055, 0.198, 0.269, 0.124, 0.136, 0.086]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3465 | Steps: 2 | Val loss: 0.4879 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=123100)[0m rmse: 0.17979373037815094
[2m[36m(func pid=123100)[0m mae:  0.13201463222503662
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.263, 0.101, 0.337, 0.107, 0.19, 0.29, 0.141, 0.143, 0.11]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8782 | Steps: 2 | Val loss: 0.6850 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.4986 | Steps: 2 | Val loss: 0.4663 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=121379)[0m rmse: 0.17823748290538788
[2m[36m(func pid=121379)[0m mae:  0.10965921729803085
[2m[36m(func pid=121379)[0m rmse_per_class: [0.124, 0.312, 0.05, 0.336, 0.056, 0.189, 0.274, 0.197, 0.142, 0.101]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.7682 | Steps: 2 | Val loss: 0.5965 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 10:01:12 (running for 00:36:34.04)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.514 |  0.152 |                   20 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.347 |  0.178 |                   14 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.878 |  0.18  |                   13 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.786 |  0.18  |                    9 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m rmse: 0.17976567149162292
[2m[36m(func pid=121978)[0m mae:  0.13204078376293182
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.262, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.15527936816215515
[2m[36m(func pid=119733)[0m mae:  0.10180066525936127
[2m[36m(func pid=119733)[0m rmse_per_class: [0.088, 0.24, 0.029, 0.306, 0.055, 0.199, 0.29, 0.124, 0.135, 0.086]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17964652180671692
[2m[36m(func pid=123100)[0m mae:  0.13186220824718475
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.262, 0.101, 0.337, 0.107, 0.19, 0.29, 0.141, 0.143, 0.11]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3373 | Steps: 2 | Val loss: 0.5952 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8724 | Steps: 2 | Val loss: 0.6842 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.4845 | Steps: 2 | Val loss: 0.4544 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7475 | Steps: 2 | Val loss: 0.5834 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 10:01:18 (running for 00:36:39.14)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.499 |  0.155 |                   21 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.337 |  0.219 |                   15 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.878 |  0.18  |                   13 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.768 |  0.18  |                   10 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121379)[0m rmse: 0.218547061085701
[2m[36m(func pid=121379)[0m mae:  0.13386711478233337
[2m[36m(func pid=121379)[0m rmse_per_class: [0.125, 0.343, 0.053, 0.33, 0.056, 0.213, 0.325, 0.497, 0.144, 0.1]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17971904575824738
[2m[36m(func pid=121978)[0m mae:  0.131989985704422
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.15737009048461914
[2m[36m(func pid=119733)[0m mae:  0.10259503126144409
[2m[36m(func pid=119733)[0m rmse_per_class: [0.086, 0.239, 0.028, 0.313, 0.055, 0.199, 0.311, 0.122, 0.135, 0.085]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17956632375717163
[2m[36m(func pid=123100)[0m mae:  0.13176198303699493
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.262, 0.101, 0.337, 0.106, 0.19, 0.289, 0.141, 0.143, 0.11]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3226 | Steps: 2 | Val loss: 0.5500 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8728 | Steps: 2 | Val loss: 0.6822 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.4724 | Steps: 2 | Val loss: 0.4349 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7344 | Steps: 2 | Val loss: 0.5702 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:01:23 (running for 00:36:44.78)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.484 |  0.157 |                   22 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.337 |  0.219 |                   15 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.873 |  0.18  |                   15 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.748 |  0.18  |                   11 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m rmse: 0.1796949952840805
[2m[36m(func pid=121978)[0m mae:  0.13196083903312683
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.2151525765657425
[2m[36m(func pid=121379)[0m mae:  0.13074728846549988
[2m[36m(func pid=121379)[0m rmse_per_class: [0.125, 0.315, 0.061, 0.311, 0.056, 0.209, 0.332, 0.393, 0.143, 0.205]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.15859757363796234
[2m[36m(func pid=119733)[0m mae:  0.10304126888513565
[2m[36m(func pid=119733)[0m rmse_per_class: [0.085, 0.239, 0.027, 0.319, 0.055, 0.198, 0.321, 0.121, 0.135, 0.085]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17939212918281555
[2m[36m(func pid=123100)[0m mae:  0.13158108294010162
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.262, 0.1, 0.336, 0.105, 0.19, 0.289, 0.141, 0.143, 0.111]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8667 | Steps: 2 | Val loss: 0.6794 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.4458 | Steps: 2 | Val loss: 0.4121 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3213 | Steps: 2 | Val loss: 0.5311 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.7144 | Steps: 2 | Val loss: 0.5571 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=121978)[0m rmse: 0.17960532009601593
[2m[36m(func pid=121978)[0m mae:  0.13187755644321442
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
== Status ==
Current time: 2024-01-07 10:01:29 (running for 00:36:50.14)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.472 |  0.159 |                   23 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.323 |  0.215 |                   16 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.867 |  0.18  |                   16 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.734 |  0.179 |                   12 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.1584264487028122
[2m[36m(func pid=119733)[0m mae:  0.10277052968740463
[2m[36m(func pid=119733)[0m rmse_per_class: [0.084, 0.239, 0.026, 0.322, 0.055, 0.197, 0.32, 0.12, 0.135, 0.085]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.20645742118358612
[2m[36m(func pid=121379)[0m mae:  0.1262361854314804
[2m[36m(func pid=121379)[0m rmse_per_class: [0.122, 0.278, 0.064, 0.297, 0.056, 0.201, 0.325, 0.457, 0.144, 0.12]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17929120361804962
[2m[36m(func pid=123100)[0m mae:  0.13145700097084045
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.262, 0.1, 0.336, 0.105, 0.19, 0.288, 0.141, 0.143, 0.111]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.4227 | Steps: 2 | Val loss: 0.3882 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.8641 | Steps: 2 | Val loss: 0.6777 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3107 | Steps: 2 | Val loss: 0.5212 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.6988 | Steps: 2 | Val loss: 0.5447 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 10:01:34 (running for 00:36:55.40)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.446 |  0.158 |                   24 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.321 |  0.206 |                   17 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.864 |  0.18  |                   17 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.714 |  0.179 |                   13 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m rmse: 0.17959357798099518
[2m[36m(func pid=121978)[0m mae:  0.13186360895633698
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.15815159678459167
[2m[36m(func pid=119733)[0m mae:  0.10255490243434906
[2m[36m(func pid=119733)[0m rmse_per_class: [0.085, 0.241, 0.026, 0.325, 0.055, 0.196, 0.312, 0.12, 0.135, 0.085]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.2027525007724762
[2m[36m(func pid=121379)[0m mae:  0.12436717748641968
[2m[36m(func pid=121379)[0m rmse_per_class: [0.121, 0.255, 0.062, 0.302, 0.056, 0.201, 0.309, 0.464, 0.145, 0.112]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1791558563709259
[2m[36m(func pid=123100)[0m mae:  0.13130848109722137
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.261, 0.099, 0.336, 0.104, 0.19, 0.288, 0.141, 0.143, 0.112]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4044 | Steps: 2 | Val loss: 0.3689 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8645 | Steps: 2 | Val loss: 0.6765 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.2916 | Steps: 2 | Val loss: 0.4555 | Batch size: 32 | lr: 0.1 | Duration: 3.27s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.6806 | Steps: 2 | Val loss: 0.5326 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 10:01:39 (running for 00:37:00.91)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.404 |  0.159 |                   26 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.311 |  0.203 |                   18 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.864 |  0.18  |                   17 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.699 |  0.179 |                   14 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15926143527030945
[2m[36m(func pid=119733)[0m mae:  0.10315631330013275
[2m[36m(func pid=119733)[0m rmse_per_class: [0.087, 0.247, 0.026, 0.33, 0.055, 0.195, 0.306, 0.125, 0.135, 0.085]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.1795644313097
[2m[36m(func pid=121978)[0m mae:  0.13183443248271942
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.19376322627067566
[2m[36m(func pid=121379)[0m mae:  0.11597529798746109
[2m[36m(func pid=121379)[0m rmse_per_class: [0.129, 0.25, 0.076, 0.3, 0.056, 0.194, 0.279, 0.384, 0.153, 0.117]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17895732820034027
[2m[36m(func pid=123100)[0m mae:  0.13111653923988342
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.261, 0.099, 0.335, 0.103, 0.19, 0.287, 0.141, 0.143, 0.112]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3865 | Steps: 2 | Val loss: 0.3527 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8620 | Steps: 2 | Val loss: 0.6752 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3043 | Steps: 2 | Val loss: 0.4200 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.6647 | Steps: 2 | Val loss: 0.5208 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 10:01:45 (running for 00:37:06.20)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.386 |  0.161 |                   27 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.292 |  0.194 |                   19 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.864 |  0.18  |                   18 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.681 |  0.179 |                   15 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.16137346625328064
[2m[36m(func pid=119733)[0m mae:  0.10428707301616669
[2m[36m(func pid=119733)[0m rmse_per_class: [0.088, 0.252, 0.026, 0.332, 0.055, 0.197, 0.307, 0.133, 0.136, 0.087]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17957301437854767
[2m[36m(func pid=121978)[0m mae:  0.13183313608169556
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17886874079704285
[2m[36m(func pid=123100)[0m mae:  0.1310146600008011
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.261, 0.099, 0.335, 0.103, 0.19, 0.287, 0.141, 0.143, 0.112]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.18926317989826202
[2m[36m(func pid=121379)[0m mae:  0.1113896518945694
[2m[36m(func pid=121379)[0m rmse_per_class: [0.166, 0.248, 0.121, 0.317, 0.056, 0.196, 0.224, 0.268, 0.165, 0.132]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3672 | Steps: 2 | Val loss: 0.3441 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8580 | Steps: 2 | Val loss: 0.6738 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6503 | Steps: 2 | Val loss: 0.5088 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.2875 | Steps: 2 | Val loss: 0.4002 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 10:01:50 (running for 00:37:11.40)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.367 |  0.163 |                   28 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.304 |  0.189 |                   20 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.862 |  0.18  |                   19 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.665 |  0.179 |                   16 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.16346901655197144
[2m[36m(func pid=119733)[0m mae:  0.10551537573337555
[2m[36m(func pid=119733)[0m rmse_per_class: [0.089, 0.253, 0.026, 0.333, 0.055, 0.2, 0.315, 0.139, 0.137, 0.088]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17957773804664612
[2m[36m(func pid=121978)[0m mae:  0.13181990385055542
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.261, 0.1, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17873436212539673
[2m[36m(func pid=123100)[0m mae:  0.13090842962265015
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.261, 0.098, 0.335, 0.103, 0.19, 0.287, 0.141, 0.143, 0.112]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.18922539055347443
[2m[36m(func pid=121379)[0m mae:  0.1119481697678566
[2m[36m(func pid=121379)[0m rmse_per_class: [0.213, 0.245, 0.107, 0.331, 0.055, 0.203, 0.218, 0.201, 0.175, 0.143]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8572 | Steps: 2 | Val loss: 0.6723 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3498 | Steps: 2 | Val loss: 0.3326 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.6381 | Steps: 2 | Val loss: 0.4972 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 10:01:55 (running for 00:37:16.60)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.367 |  0.163 |                   28 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.288 |  0.189 |                   21 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.857 |  0.18  |                   21 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.65  |  0.179 |                   17 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.16289293766021729
[2m[36m(func pid=119733)[0m mae:  0.10514217615127563
[2m[36m(func pid=119733)[0m rmse_per_class: [0.09, 0.251, 0.026, 0.329, 0.055, 0.2, 0.31, 0.141, 0.137, 0.089]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.2978 | Steps: 2 | Val loss: 0.3716 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=121978)[0m rmse: 0.17952975630760193
[2m[36m(func pid=121978)[0m mae:  0.1317790448665619
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.261, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1785530000925064
[2m[36m(func pid=123100)[0m mae:  0.13073812425136566
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.261, 0.098, 0.335, 0.102, 0.19, 0.286, 0.141, 0.143, 0.112]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.18636927008628845
[2m[36m(func pid=121379)[0m mae:  0.1099964827299118
[2m[36m(func pid=121379)[0m rmse_per_class: [0.219, 0.244, 0.082, 0.331, 0.054, 0.203, 0.217, 0.19, 0.169, 0.154]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3497 | Steps: 2 | Val loss: 0.3291 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.8522 | Steps: 2 | Val loss: 0.6707 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.6256 | Steps: 2 | Val loss: 0.4865 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 10:02:00 (running for 00:37:21.73)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.35  |  0.163 |                   30 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.298 |  0.186 |                   22 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.857 |  0.18  |                   21 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.638 |  0.179 |                   18 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.1628047674894333
[2m[36m(func pid=119733)[0m mae:  0.10484637320041656
[2m[36m(func pid=119733)[0m rmse_per_class: [0.089, 0.25, 0.026, 0.327, 0.055, 0.201, 0.309, 0.144, 0.138, 0.089]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.1795036643743515
[2m[36m(func pid=121978)[0m mae:  0.13176298141479492
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2822 | Steps: 2 | Val loss: 0.3687 | Batch size: 32 | lr: 0.1 | Duration: 3.27s
[2m[36m(func pid=123100)[0m rmse: 0.17831449210643768
[2m[36m(func pid=123100)[0m mae:  0.13051924109458923
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.101, 0.19, 0.286, 0.141, 0.143, 0.112]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3368 | Steps: 2 | Val loss: 0.3254 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8533 | Steps: 2 | Val loss: 0.6690 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=121379)[0m rmse: 0.18603162467479706
[2m[36m(func pid=121379)[0m mae:  0.10985706001520157
[2m[36m(func pid=121379)[0m rmse_per_class: [0.198, 0.243, 0.065, 0.326, 0.053, 0.205, 0.216, 0.229, 0.159, 0.166]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.6108 | Steps: 2 | Val loss: 0.4760 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 10:02:05 (running for 00:37:27.02)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.337 |  0.161 |                   31 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.282 |  0.186 |                   23 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.852 |  0.18  |                   22 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.626 |  0.178 |                   19 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.16141948103904724
[2m[36m(func pid=119733)[0m mae:  0.10369851440191269
[2m[36m(func pid=119733)[0m rmse_per_class: [0.089, 0.249, 0.025, 0.322, 0.055, 0.2, 0.299, 0.148, 0.138, 0.089]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17945906519889832
[2m[36m(func pid=121978)[0m mae:  0.13172896206378937
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17817728221416473
[2m[36m(func pid=123100)[0m mae:  0.13040713965892792
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.261, 0.097, 0.334, 0.101, 0.19, 0.286, 0.141, 0.143, 0.112]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2873 | Steps: 2 | Val loss: 0.4057 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3388 | Steps: 2 | Val loss: 0.3272 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.8499 | Steps: 2 | Val loss: 0.6672 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=121379)[0m rmse: 0.19948048889636993
[2m[36m(func pid=121379)[0m mae:  0.11785238981246948
[2m[36m(func pid=121379)[0m rmse_per_class: [0.196, 0.251, 0.063, 0.327, 0.053, 0.208, 0.245, 0.324, 0.153, 0.176]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.5968 | Steps: 2 | Val loss: 0.4658 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 10:02:11 (running for 00:37:32.31)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.339 |  0.16  |                   32 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.287 |  0.199 |                   24 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.853 |  0.179 |                   23 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.611 |  0.178 |                   20 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=119733)[0m rmse: 0.16015878319740295

[2m[36m(func pid=119733)[0m mae:  0.10274704545736313
[2m[36m(func pid=119733)[0m rmse_per_class: [0.088, 0.246, 0.025, 0.316, 0.055, 0.2, 0.292, 0.153, 0.138, 0.089]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.1794535219669342
[2m[36m(func pid=121978)[0m mae:  0.13172850012779236
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1780090630054474
[2m[36m(func pid=123100)[0m mae:  0.13026735186576843
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.334, 0.101, 0.191, 0.285, 0.141, 0.143, 0.111]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2789 | Steps: 2 | Val loss: 0.4197 | Batch size: 32 | lr: 0.1 | Duration: 3.23s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3301 | Steps: 2 | Val loss: 0.3200 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.8445 | Steps: 2 | Val loss: 0.6647 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.5855 | Steps: 2 | Val loss: 0.4573 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=121379)[0m rmse: 0.19962581992149353
[2m[36m(func pid=121379)[0m mae:  0.11605194956064224
[2m[36m(func pid=121379)[0m rmse_per_class: [0.167, 0.272, 0.051, 0.314, 0.054, 0.207, 0.261, 0.337, 0.15, 0.183]
[2m[36m(func pid=121379)[0m 
== Status ==
Current time: 2024-01-07 10:02:16 (running for 00:37:37.45)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.33  |  0.157 |                   33 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.279 |  0.2   |                   25 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.85  |  0.179 |                   24 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.597 |  0.178 |                   21 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15708674490451813
[2m[36m(func pid=119733)[0m mae:  0.10072797536849976
[2m[36m(func pid=119733)[0m rmse_per_class: [0.087, 0.245, 0.024, 0.308, 0.055, 0.197, 0.277, 0.152, 0.138, 0.088]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17949771881103516
[2m[36m(func pid=121978)[0m mae:  0.13175705075263977
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.14, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17784202098846436
[2m[36m(func pid=123100)[0m mae:  0.13011568784713745
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.26, 0.097, 0.334, 0.1, 0.191, 0.285, 0.141, 0.143, 0.111]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2686 | Steps: 2 | Val loss: 0.4236 | Batch size: 32 | lr: 0.1 | Duration: 3.23s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3378 | Steps: 2 | Val loss: 0.3152 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.8409 | Steps: 2 | Val loss: 0.6625 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5772 | Steps: 2 | Val loss: 0.4482 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 10:02:21 (running for 00:37:42.73)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.338 |  0.155 |                   34 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.279 |  0.2   |                   25 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.844 |  0.179 |                   25 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.585 |  0.178 |                   22 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15469609200954437
[2m[36m(func pid=119733)[0m mae:  0.09916310757398605
[2m[36m(func pid=119733)[0m rmse_per_class: [0.086, 0.243, 0.024, 0.299, 0.055, 0.194, 0.268, 0.153, 0.137, 0.088]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.20470741391181946
[2m[36m(func pid=121379)[0m mae:  0.1170850545167923
[2m[36m(func pid=121379)[0m rmse_per_class: [0.143, 0.293, 0.048, 0.304, 0.054, 0.213, 0.273, 0.305, 0.143, 0.272]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.1794624626636505
[2m[36m(func pid=121978)[0m mae:  0.13173195719718933
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1776771992444992
[2m[36m(func pid=123100)[0m mae:  0.12997962534427643
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.1, 0.191, 0.285, 0.14, 0.143, 0.111]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3345 | Steps: 2 | Val loss: 0.3054 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.8424 | Steps: 2 | Val loss: 0.6607 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2864 | Steps: 2 | Val loss: 0.4252 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.5676 | Steps: 2 | Val loss: 0.4402 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 10:02:26 (running for 00:37:48.04)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.334 |  0.153 |                   35 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.269 |  0.205 |                   26 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.841 |  0.179 |                   26 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.577 |  0.178 |                   23 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15308909118175507
[2m[36m(func pid=119733)[0m mae:  0.09809143841266632
[2m[36m(func pid=119733)[0m rmse_per_class: [0.084, 0.243, 0.023, 0.291, 0.055, 0.192, 0.265, 0.153, 0.137, 0.088]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17945441603660583
[2m[36m(func pid=121978)[0m mae:  0.13172796368598938
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.2014988660812378
[2m[36m(func pid=121379)[0m mae:  0.11463034152984619
[2m[36m(func pid=121379)[0m rmse_per_class: [0.136, 0.295, 0.049, 0.295, 0.06, 0.213, 0.269, 0.311, 0.14, 0.246]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17758597433567047
[2m[36m(func pid=123100)[0m mae:  0.12989357113838196
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.1, 0.191, 0.285, 0.14, 0.143, 0.111]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3295 | Steps: 2 | Val loss: 0.3006 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.8416 | Steps: 2 | Val loss: 0.6576 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2632 | Steps: 2 | Val loss: 0.4246 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.5576 | Steps: 2 | Val loss: 0.4320 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 10:02:32 (running for 00:37:53.34)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.33  |  0.153 |                   36 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.286 |  0.201 |                   27 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.842 |  0.179 |                   27 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.568 |  0.178 |                   24 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.1533292829990387
[2m[36m(func pid=119733)[0m mae:  0.09796051681041718
[2m[36m(func pid=119733)[0m rmse_per_class: [0.084, 0.243, 0.023, 0.285, 0.055, 0.191, 0.274, 0.152, 0.137, 0.09]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17943276464939117
[2m[36m(func pid=121978)[0m mae:  0.13171640038490295
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.11, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.18863928318023682
[2m[36m(func pid=121379)[0m mae:  0.10673235356807709
[2m[36m(func pid=121379)[0m rmse_per_class: [0.136, 0.293, 0.049, 0.288, 0.071, 0.21, 0.241, 0.278, 0.145, 0.176]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17741529643535614
[2m[36m(func pid=123100)[0m mae:  0.12974520027637482
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.333, 0.099, 0.191, 0.285, 0.14, 0.143, 0.111]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3213 | Steps: 2 | Val loss: 0.2949 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.8371 | Steps: 2 | Val loss: 0.6557 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5494 | Steps: 2 | Val loss: 0.4246 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2953 | Steps: 2 | Val loss: 0.4294 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 10:02:37 (running for 00:37:58.62)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.33  |  0.153 |                   36 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.263 |  0.189 |                   28 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.837 |  0.179 |                   29 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.558 |  0.177 |                   25 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.1533338725566864
[2m[36m(func pid=119733)[0m mae:  0.09777109324932098
[2m[36m(func pid=119733)[0m rmse_per_class: [0.085, 0.243, 0.025, 0.28, 0.055, 0.189, 0.276, 0.15, 0.137, 0.093]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17942923307418823
[2m[36m(func pid=121978)[0m mae:  0.13170726597309113
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17726394534111023
[2m[36m(func pid=123100)[0m mae:  0.12960052490234375
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.332, 0.099, 0.191, 0.284, 0.14, 0.143, 0.111]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.18377842009067535
[2m[36m(func pid=121379)[0m mae:  0.10351276397705078
[2m[36m(func pid=121379)[0m rmse_per_class: [0.143, 0.286, 0.05, 0.284, 0.074, 0.209, 0.225, 0.254, 0.146, 0.168]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3174 | Steps: 2 | Val loss: 0.2883 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.8328 | Steps: 2 | Val loss: 0.6536 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.5382 | Steps: 2 | Val loss: 0.4181 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2775 | Steps: 2 | Val loss: 0.3880 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 10:02:42 (running for 00:38:03.89)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.321 |  0.153 |                   37 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.295 |  0.184 |                   29 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.833 |  0.179 |                   30 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.549 |  0.177 |                   26 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15257427096366882
[2m[36m(func pid=119733)[0m mae:  0.09739381074905396
[2m[36m(func pid=119733)[0m rmse_per_class: [0.086, 0.245, 0.031, 0.277, 0.055, 0.186, 0.266, 0.144, 0.138, 0.098]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17940282821655273
[2m[36m(func pid=121978)[0m mae:  0.13167770206928253
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1771344393491745
[2m[36m(func pid=123100)[0m mae:  0.1294897496700287
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.26, 0.096, 0.332, 0.099, 0.191, 0.284, 0.14, 0.143, 0.11]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.18243645131587982
[2m[36m(func pid=121379)[0m mae:  0.10285444557666779
[2m[36m(func pid=121379)[0m rmse_per_class: [0.164, 0.271, 0.049, 0.285, 0.078, 0.207, 0.219, 0.233, 0.173, 0.145]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.8314 | Steps: 2 | Val loss: 0.6518 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3307 | Steps: 2 | Val loss: 0.2847 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.5303 | Steps: 2 | Val loss: 0.4113 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 10:02:48 (running for 00:38:09.18)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.317 |  0.153 |                   38 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.277 |  0.182 |                   30 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.831 |  0.179 |                   31 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.538 |  0.177 |                   27 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m rmse: 0.1794305145740509
[2m[36m(func pid=121978)[0m mae:  0.13170120120048523
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2677 | Steps: 2 | Val loss: 0.3819 | Batch size: 32 | lr: 0.1 | Duration: 3.50s
[2m[36m(func pid=119733)[0m rmse: 0.15194346010684967
[2m[36m(func pid=119733)[0m mae:  0.0972028449177742
[2m[36m(func pid=119733)[0m rmse_per_class: [0.087, 0.247, 0.039, 0.276, 0.055, 0.183, 0.253, 0.138, 0.139, 0.103]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17702817916870117
[2m[36m(func pid=123100)[0m mae:  0.12939414381980896
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.331, 0.099, 0.191, 0.284, 0.139, 0.144, 0.11]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.1810186505317688
[2m[36m(func pid=121379)[0m mae:  0.10422609746456146
[2m[36m(func pid=121379)[0m rmse_per_class: [0.189, 0.249, 0.051, 0.292, 0.071, 0.206, 0.22, 0.237, 0.154, 0.141]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.8297 | Steps: 2 | Val loss: 0.6495 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3184 | Steps: 2 | Val loss: 0.2831 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.5219 | Steps: 2 | Val loss: 0.4050 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 10:02:53 (running for 00:38:14.54)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.331 |  0.152 |                   39 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.268 |  0.181 |                   31 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.83  |  0.179 |                   32 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.53  |  0.177 |                   28 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m rmse: 0.17940381169319153
[2m[36m(func pid=121978)[0m mae:  0.131679505109787
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.150708869099617
[2m[36m(func pid=119733)[0m mae:  0.09692972153425217
[2m[36m(func pid=119733)[0m rmse_per_class: [0.087, 0.249, 0.044, 0.275, 0.056, 0.178, 0.237, 0.135, 0.14, 0.107]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2734 | Steps: 2 | Val loss: 0.3950 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=123100)[0m rmse: 0.17684102058410645
[2m[36m(func pid=123100)[0m mae:  0.12923851609230042
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.331, 0.098, 0.191, 0.284, 0.139, 0.143, 0.11]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.8248 | Steps: 2 | Val loss: 0.6474 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3262 | Steps: 2 | Val loss: 0.2857 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=121379)[0m rmse: 0.18366143107414246
[2m[36m(func pid=121379)[0m mae:  0.10585182905197144
[2m[36m(func pid=121379)[0m rmse_per_class: [0.206, 0.264, 0.051, 0.311, 0.068, 0.206, 0.22, 0.226, 0.144, 0.141]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5157 | Steps: 2 | Val loss: 0.3987 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=121978)[0m rmse: 0.17943187057971954
[2m[36m(func pid=121978)[0m mae:  0.13169611990451813
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
== Status ==
Current time: 2024-01-07 10:02:58 (running for 00:38:20.01)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.326 |  0.153 |                   41 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.273 |  0.184 |                   32 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.825 |  0.179 |                   33 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.522 |  0.177 |                   29 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15277203917503357
[2m[36m(func pid=119733)[0m mae:  0.09784932434558868
[2m[36m(func pid=119733)[0m rmse_per_class: [0.086, 0.249, 0.046, 0.276, 0.056, 0.176, 0.229, 0.154, 0.141, 0.114]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2808 | Steps: 2 | Val loss: 0.4167 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=123100)[0m rmse: 0.17670689523220062
[2m[36m(func pid=123100)[0m mae:  0.1291184425354004
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.331, 0.098, 0.191, 0.283, 0.139, 0.144, 0.11]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8254 | Steps: 2 | Val loss: 0.6456 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3191 | Steps: 2 | Val loss: 0.2887 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=121379)[0m rmse: 0.18799428641796112
[2m[36m(func pid=121379)[0m mae:  0.10808134078979492
[2m[36m(func pid=121379)[0m rmse_per_class: [0.216, 0.289, 0.05, 0.326, 0.07, 0.204, 0.221, 0.229, 0.138, 0.136]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17938020825386047
[2m[36m(func pid=121978)[0m mae:  0.13165906071662903
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5106 | Steps: 2 | Val loss: 0.3936 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 10:03:04 (running for 00:38:25.40)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.319 |  0.156 |                   42 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.281 |  0.188 |                   33 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.825 |  0.179 |                   34 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.516 |  0.177 |                   30 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15603134036064148
[2m[36m(func pid=119733)[0m mae:  0.09902842342853546
[2m[36m(func pid=119733)[0m rmse_per_class: [0.085, 0.248, 0.047, 0.277, 0.056, 0.172, 0.231, 0.182, 0.143, 0.12]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2845 | Steps: 2 | Val loss: 0.3996 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=123100)[0m rmse: 0.17655959725379944
[2m[36m(func pid=123100)[0m mae:  0.1289864480495453
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.26, 0.095, 0.331, 0.098, 0.191, 0.283, 0.139, 0.144, 0.11]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8222 | Steps: 2 | Val loss: 0.6439 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3133 | Steps: 2 | Val loss: 0.2886 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=121379)[0m rmse: 0.1895952969789505
[2m[36m(func pid=121379)[0m mae:  0.1091226115822792
[2m[36m(func pid=121379)[0m rmse_per_class: [0.21, 0.293, 0.051, 0.329, 0.078, 0.198, 0.224, 0.247, 0.138, 0.129]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5035 | Steps: 2 | Val loss: 0.3879 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=121978)[0m rmse: 0.17936144769191742
[2m[36m(func pid=121978)[0m mae:  0.13164587318897247
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
== Status ==
Current time: 2024-01-07 10:03:09 (running for 00:38:30.87)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.313 |  0.156 |                   43 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.284 |  0.19  |                   34 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.822 |  0.179 |                   35 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.511 |  0.177 |                   31 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.1556238979101181
[2m[36m(func pid=119733)[0m mae:  0.09870658814907074
[2m[36m(func pid=119733)[0m rmse_per_class: [0.085, 0.248, 0.047, 0.277, 0.056, 0.165, 0.235, 0.177, 0.144, 0.123]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1763315498828888
[2m[36m(func pid=123100)[0m mae:  0.12878887355327606
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.26, 0.095, 0.33, 0.097, 0.191, 0.283, 0.139, 0.143, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2726 | Steps: 2 | Val loss: 0.3801 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.8194 | Steps: 2 | Val loss: 0.6422 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3082 | Steps: 2 | Val loss: 0.2898 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=121978)[0m rmse: 0.17934849858283997
[2m[36m(func pid=121978)[0m mae:  0.1316176950931549
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.18740728497505188
[2m[36m(func pid=121379)[0m mae:  0.10792253166437149
[2m[36m(func pid=121379)[0m rmse_per_class: [0.176, 0.286, 0.05, 0.328, 0.091, 0.191, 0.225, 0.261, 0.139, 0.126]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.4971 | Steps: 2 | Val loss: 0.3829 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 10:03:15 (running for 00:38:36.43)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.308 |  0.155 |                   44 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.273 |  0.187 |                   35 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.819 |  0.179 |                   36 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.503 |  0.176 |                   32 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15510961413383484
[2m[36m(func pid=119733)[0m mae:  0.0987095758318901
[2m[36m(func pid=119733)[0m rmse_per_class: [0.086, 0.25, 0.048, 0.279, 0.056, 0.163, 0.238, 0.163, 0.143, 0.127]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.8192 | Steps: 2 | Val loss: 0.6402 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=123100)[0m rmse: 0.1762140691280365
[2m[36m(func pid=123100)[0m mae:  0.12868312001228333
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.26, 0.094, 0.33, 0.097, 0.191, 0.282, 0.139, 0.143, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2688 | Steps: 2 | Val loss: 0.3487 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3085 | Steps: 2 | Val loss: 0.2955 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=121978)[0m rmse: 0.17938299477100372
[2m[36m(func pid=121978)[0m mae:  0.13164618611335754
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.4901 | Steps: 2 | Val loss: 0.3786 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=121379)[0m rmse: 0.18141192197799683
[2m[36m(func pid=121379)[0m mae:  0.10483263432979584
[2m[36m(func pid=121379)[0m rmse_per_class: [0.139, 0.281, 0.049, 0.316, 0.08, 0.185, 0.232, 0.266, 0.14, 0.126]
[2m[36m(func pid=121379)[0m 
== Status ==
Current time: 2024-01-07 10:03:20 (running for 00:38:41.98)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.309 |  0.158 |                   45 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.269 |  0.181 |                   36 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.819 |  0.179 |                   37 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.497 |  0.176 |                   33 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15849123895168304
[2m[36m(func pid=119733)[0m mae:  0.10126937925815582
[2m[36m(func pid=119733)[0m rmse_per_class: [0.086, 0.251, 0.048, 0.285, 0.055, 0.171, 0.244, 0.171, 0.142, 0.132]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1761043518781662
[2m[36m(func pid=123100)[0m mae:  0.12859484553337097
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.26, 0.094, 0.33, 0.097, 0.191, 0.282, 0.139, 0.143, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.8161 | Steps: 2 | Val loss: 0.6394 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2568 | Steps: 2 | Val loss: 0.3208 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3058 | Steps: 2 | Val loss: 0.3047 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=121978)[0m rmse: 0.17942437529563904
[2m[36m(func pid=121978)[0m mae:  0.13166511058807373
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.4860 | Steps: 2 | Val loss: 0.3744 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=121379)[0m rmse: 0.1752292662858963
[2m[36m(func pid=121379)[0m mae:  0.10086643695831299
[2m[36m(func pid=121379)[0m rmse_per_class: [0.118, 0.275, 0.048, 0.3, 0.064, 0.179, 0.236, 0.251, 0.144, 0.136]
[2m[36m(func pid=121379)[0m 
== Status ==
Current time: 2024-01-07 10:03:26 (running for 00:38:47.11)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.306 |  0.164 |                   46 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.257 |  0.175 |                   37 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.816 |  0.179 |                   38 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.49  |  0.176 |                   34 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.1644308716058731
[2m[36m(func pid=119733)[0m mae:  0.1049797311425209
[2m[36m(func pid=119733)[0m rmse_per_class: [0.087, 0.251, 0.048, 0.293, 0.055, 0.183, 0.252, 0.21, 0.14, 0.125]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17597182095050812
[2m[36m(func pid=123100)[0m mae:  0.1284741908311844
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.26, 0.094, 0.329, 0.097, 0.191, 0.282, 0.139, 0.143, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8154 | Steps: 2 | Val loss: 0.6378 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.3293 | Steps: 2 | Val loss: 0.3092 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2630 | Steps: 2 | Val loss: 0.3154 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=121978)[0m rmse: 0.17941465973854065
[2m[36m(func pid=121978)[0m mae:  0.13165174424648285
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.109, 0.19, 0.293, 0.141, 0.143, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4813 | Steps: 2 | Val loss: 0.3709 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 10:03:31 (running for 00:38:52.31)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.329 |  0.168 |                   47 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.257 |  0.175 |                   37 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.815 |  0.179 |                   39 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.486 |  0.176 |                   35 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.1682107150554657
[2m[36m(func pid=119733)[0m mae:  0.10744480043649673
[2m[36m(func pid=119733)[0m rmse_per_class: [0.089, 0.251, 0.048, 0.299, 0.055, 0.186, 0.258, 0.234, 0.139, 0.123]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.16939689218997955
[2m[36m(func pid=121379)[0m mae:  0.09701856970787048
[2m[36m(func pid=121379)[0m rmse_per_class: [0.11, 0.275, 0.049, 0.292, 0.06, 0.175, 0.228, 0.212, 0.145, 0.148]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.8097 | Steps: 2 | Val loss: 0.6370 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=123100)[0m rmse: 0.17589536309242249
[2m[36m(func pid=123100)[0m mae:  0.12841403484344482
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.26, 0.094, 0.329, 0.097, 0.191, 0.282, 0.139, 0.143, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3055 | Steps: 2 | Val loss: 0.3015 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2637 | Steps: 2 | Val loss: 0.3240 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=121978)[0m rmse: 0.17938759922981262
[2m[36m(func pid=121978)[0m mae:  0.13162153959274292
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.11, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.4779 | Steps: 2 | Val loss: 0.3674 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 10:03:36 (running for 00:38:57.72)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.305 |  0.165 |                   48 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.263 |  0.169 |                   38 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.81  |  0.179 |                   40 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.481 |  0.176 |                   36 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.16495364904403687
[2m[36m(func pid=119733)[0m mae:  0.10639975965023041
[2m[36m(func pid=119733)[0m rmse_per_class: [0.087, 0.25, 0.045, 0.298, 0.055, 0.182, 0.26, 0.204, 0.14, 0.129]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.17023207247257233
[2m[36m(func pid=121379)[0m mae:  0.09769423305988312
[2m[36m(func pid=121379)[0m rmse_per_class: [0.117, 0.289, 0.049, 0.295, 0.059, 0.17, 0.226, 0.192, 0.144, 0.161]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1756783127784729
[2m[36m(func pid=123100)[0m mae:  0.12822745740413666
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.259, 0.094, 0.329, 0.097, 0.19, 0.282, 0.138, 0.143, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.8097 | Steps: 2 | Val loss: 0.6345 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2998 | Steps: 2 | Val loss: 0.2937 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2645 | Steps: 2 | Val loss: 0.3282 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=121978)[0m rmse: 0.17937339842319489
[2m[36m(func pid=121978)[0m mae:  0.1316077560186386
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4716 | Steps: 2 | Val loss: 0.3644 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=119733)[0m rmse: 0.16067956387996674
[2m[36m(func pid=119733)[0m mae:  0.10460277646780014
[2m[36m(func pid=119733)[0m rmse_per_class: [0.087, 0.25, 0.042, 0.295, 0.054, 0.174, 0.258, 0.168, 0.142, 0.136]
== Status ==
Current time: 2024-01-07 10:03:42 (running for 00:39:03.10)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.3   |  0.161 |                   49 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.264 |  0.17  |                   39 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.81  |  0.179 |                   41 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.478 |  0.176 |                   37 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.17358368635177612
[2m[36m(func pid=121379)[0m mae:  0.10039134323596954
[2m[36m(func pid=121379)[0m rmse_per_class: [0.154, 0.291, 0.04, 0.311, 0.057, 0.163, 0.224, 0.175, 0.141, 0.179]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.8058 | Steps: 2 | Val loss: 0.6330 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=123100)[0m rmse: 0.17550760507583618
[2m[36m(func pid=123100)[0m mae:  0.1281263828277588
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.259, 0.093, 0.328, 0.096, 0.19, 0.282, 0.138, 0.143, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.3078 | Steps: 2 | Val loss: 0.2899 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=121978)[0m rmse: 0.17931120097637177
[2m[36m(func pid=121978)[0m mae:  0.1315571814775467
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.293, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2585 | Steps: 2 | Val loss: 0.3323 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4656 | Steps: 2 | Val loss: 0.3611 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 10:03:47 (running for 00:39:08.51)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.308 |  0.158 |                   50 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.264 |  0.174 |                   40 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.806 |  0.179 |                   42 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.472 |  0.176 |                   38 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15799535810947418
[2m[36m(func pid=119733)[0m mae:  0.10349728167057037
[2m[36m(func pid=119733)[0m rmse_per_class: [0.089, 0.25, 0.039, 0.295, 0.054, 0.166, 0.256, 0.146, 0.144, 0.141]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.18570055067539215
[2m[36m(func pid=121379)[0m mae:  0.10661908239126205
[2m[36m(func pid=121379)[0m rmse_per_class: [0.204, 0.282, 0.053, 0.33, 0.056, 0.162, 0.231, 0.206, 0.141, 0.194]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8034 | Steps: 2 | Val loss: 0.6309 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=123100)[0m rmse: 0.1753557026386261
[2m[36m(func pid=123100)[0m mae:  0.1279999166727066
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.259, 0.093, 0.328, 0.096, 0.19, 0.282, 0.138, 0.143, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3004 | Steps: 2 | Val loss: 0.2885 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=121978)[0m rmse: 0.17929990589618683
[2m[36m(func pid=121978)[0m mae:  0.13156922161579132
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2867 | Steps: 2 | Val loss: 0.3387 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4639 | Steps: 2 | Val loss: 0.3577 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 10:03:52 (running for 00:39:13.89)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.3   |  0.157 |                   51 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.259 |  0.186 |                   41 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.803 |  0.179 |                   43 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.466 |  0.175 |                   39 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.156854510307312
[2m[36m(func pid=119733)[0m mae:  0.10332496464252472
[2m[36m(func pid=119733)[0m rmse_per_class: [0.093, 0.25, 0.038, 0.297, 0.054, 0.161, 0.254, 0.126, 0.147, 0.148]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8027 | Steps: 2 | Val loss: 0.6294 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=121379)[0m rmse: 0.1905851662158966
[2m[36m(func pid=121379)[0m mae:  0.10808775573968887
[2m[36m(func pid=121379)[0m rmse_per_class: [0.211, 0.282, 0.084, 0.338, 0.057, 0.163, 0.226, 0.194, 0.143, 0.208]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1751326620578766
[2m[36m(func pid=123100)[0m mae:  0.12781834602355957
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.259, 0.093, 0.328, 0.095, 0.19, 0.281, 0.138, 0.143, 0.108]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2861 | Steps: 2 | Val loss: 0.2903 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=121978)[0m rmse: 0.17929653823375702
[2m[36m(func pid=121978)[0m mae:  0.13156206905841827
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2905 | Steps: 2 | Val loss: 0.3273 | Batch size: 32 | lr: 0.1 | Duration: 3.31s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.4601 | Steps: 2 | Val loss: 0.3552 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:03:58 (running for 00:39:19.57)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.286 |  0.158 |                   52 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.287 |  0.191 |                   42 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.803 |  0.179 |                   44 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.464 |  0.175 |                   40 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.1579923927783966
[2m[36m(func pid=119733)[0m mae:  0.10394042730331421
[2m[36m(func pid=119733)[0m rmse_per_class: [0.098, 0.249, 0.04, 0.301, 0.053, 0.159, 0.253, 0.115, 0.15, 0.163]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.8002 | Steps: 2 | Val loss: 0.6273 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=121379)[0m rmse: 0.18639585375785828
[2m[36m(func pid=121379)[0m mae:  0.1057596430182457
[2m[36m(func pid=121379)[0m rmse_per_class: [0.201, 0.269, 0.09, 0.331, 0.061, 0.168, 0.223, 0.154, 0.141, 0.226]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17507195472717285
[2m[36m(func pid=123100)[0m mae:  0.1278008669614792
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.259, 0.092, 0.328, 0.094, 0.19, 0.281, 0.138, 0.143, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2872 | Steps: 2 | Val loss: 0.2914 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=121978)[0m rmse: 0.1793147772550583
[2m[36m(func pid=121978)[0m mae:  0.13158567249774933
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.109, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2627 | Steps: 2 | Val loss: 0.3192 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.4572 | Steps: 2 | Val loss: 0.3528 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=119733)[0m rmse: 0.15893997251987457
[2m[36m(func pid=119733)[0m mae:  0.1040147915482521
[2m[36m(func pid=119733)[0m rmse_per_class: [0.101, 0.249, 0.042, 0.302, 0.053, 0.159, 0.251, 0.11, 0.15, 0.172]
[2m[36m(func pid=119733)[0m 
== Status ==
Current time: 2024-01-07 10:04:03 (running for 00:39:24.96)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.287 |  0.159 |                   53 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.291 |  0.186 |                   43 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.8   |  0.179 |                   45 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.46  |  0.175 |                   41 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.7992 | Steps: 2 | Val loss: 0.6252 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=123100)[0m rmse: 0.17494527995586395
[2m[36m(func pid=123100)[0m mae:  0.127696231007576
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.259, 0.092, 0.328, 0.094, 0.19, 0.281, 0.138, 0.143, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.18005983531475067
[2m[36m(func pid=121379)[0m mae:  0.10378668457269669
[2m[36m(func pid=121379)[0m rmse_per_class: [0.182, 0.247, 0.064, 0.309, 0.064, 0.175, 0.235, 0.121, 0.173, 0.23]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17930755019187927
[2m[36m(func pid=121978)[0m mae:  0.13158003985881805
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.109, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2727 | Steps: 2 | Val loss: 0.2907 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4525 | Steps: 2 | Val loss: 0.3505 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2580 | Steps: 2 | Val loss: 0.3042 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 10:04:09 (running for 00:39:30.38)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.273 |  0.159 |                   54 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.263 |  0.18  |                   44 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.799 |  0.179 |                   46 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.457 |  0.175 |                   42 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15858635306358337
[2m[36m(func pid=119733)[0m mae:  0.1033671647310257
[2m[36m(func pid=119733)[0m rmse_per_class: [0.102, 0.249, 0.044, 0.301, 0.052, 0.159, 0.247, 0.11, 0.148, 0.174]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.7962 | Steps: 2 | Val loss: 0.6236 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=123100)[0m rmse: 0.1748058795928955
[2m[36m(func pid=123100)[0m mae:  0.1275986284017563
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.259, 0.092, 0.327, 0.094, 0.19, 0.28, 0.138, 0.143, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.16811014711856842
[2m[36m(func pid=121379)[0m mae:  0.09774074703454971
[2m[36m(func pid=121379)[0m rmse_per_class: [0.144, 0.243, 0.033, 0.286, 0.066, 0.169, 0.237, 0.129, 0.193, 0.181]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17924150824546814
[2m[36m(func pid=121978)[0m mae:  0.13151821494102478
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2731 | Steps: 2 | Val loss: 0.2892 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.4479 | Steps: 2 | Val loss: 0.3480 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2547 | Steps: 2 | Val loss: 0.2998 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 10:04:14 (running for 00:39:35.75)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.273 |  0.158 |                   55 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.258 |  0.168 |                   45 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.796 |  0.179 |                   47 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.453 |  0.175 |                   43 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15786369144916534
[2m[36m(func pid=119733)[0m mae:  0.1022036224603653
[2m[36m(func pid=119733)[0m rmse_per_class: [0.102, 0.248, 0.043, 0.297, 0.052, 0.159, 0.244, 0.112, 0.147, 0.175]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7954 | Steps: 2 | Val loss: 0.6222 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=123100)[0m rmse: 0.17455919086933136
[2m[36m(func pid=123100)[0m mae:  0.12739364802837372
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.259, 0.091, 0.327, 0.093, 0.19, 0.28, 0.138, 0.143, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.1644662618637085
[2m[36m(func pid=121379)[0m mae:  0.0949968695640564
[2m[36m(func pid=121379)[0m rmse_per_class: [0.13, 0.254, 0.04, 0.283, 0.067, 0.162, 0.231, 0.13, 0.188, 0.16]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.1793379932641983
[2m[36m(func pid=121978)[0m mae:  0.13160017132759094
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.337, 0.108, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2737 | Steps: 2 | Val loss: 0.2873 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.4470 | Steps: 2 | Val loss: 0.3458 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2639 | Steps: 2 | Val loss: 0.3026 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 10:04:19 (running for 00:39:40.92)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.274 |  0.157 |                   56 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.255 |  0.164 |                   46 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.795 |  0.179 |                   48 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.448 |  0.175 |                   44 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15693943202495575
[2m[36m(func pid=119733)[0m mae:  0.10084331035614014
[2m[36m(func pid=119733)[0m rmse_per_class: [0.101, 0.247, 0.043, 0.293, 0.051, 0.16, 0.241, 0.115, 0.145, 0.174]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7915 | Steps: 2 | Val loss: 0.6204 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=123100)[0m rmse: 0.17445723712444305
[2m[36m(func pid=123100)[0m mae:  0.1273030936717987
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.259, 0.091, 0.327, 0.093, 0.19, 0.28, 0.138, 0.143, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.1657460480928421
[2m[36m(func pid=121379)[0m mae:  0.09483884274959564
[2m[36m(func pid=121379)[0m rmse_per_class: [0.123, 0.266, 0.043, 0.287, 0.069, 0.16, 0.23, 0.13, 0.184, 0.164]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17927084863185883
[2m[36m(func pid=121978)[0m mae:  0.1315373331308365
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2776 | Steps: 2 | Val loss: 0.2875 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.4424 | Steps: 2 | Val loss: 0.3440 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=119733)[0m rmse: 0.15731492638587952
[2m[36m(func pid=119733)[0m mae:  0.10043682157993317
[2m[36m(func pid=119733)[0m rmse_per_class: [0.101, 0.247, 0.043, 0.29, 0.05, 0.16, 0.241, 0.116, 0.146, 0.178]
== Status ==
Current time: 2024-01-07 10:04:25 (running for 00:39:46.35)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.278 |  0.157 |                   57 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.264 |  0.166 |                   47 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.792 |  0.179 |                   49 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.447 |  0.174 |                   45 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2595 | Steps: 2 | Val loss: 0.3041 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7891 | Steps: 2 | Val loss: 0.6186 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17430318892002106
[2m[36m(func pid=123100)[0m mae:  0.12718716263771057
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.259, 0.091, 0.326, 0.093, 0.189, 0.279, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17934444546699524
[2m[36m(func pid=121978)[0m mae:  0.13158757984638214
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.108, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.166769340634346
[2m[36m(func pid=121379)[0m mae:  0.09577284008264542
[2m[36m(func pid=121379)[0m rmse_per_class: [0.123, 0.278, 0.045, 0.291, 0.071, 0.16, 0.227, 0.13, 0.174, 0.167]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2844 | Steps: 2 | Val loss: 0.2885 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4432 | Steps: 2 | Val loss: 0.3427 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 10:04:30 (running for 00:39:51.76)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.284 |  0.159 |                   58 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.259 |  0.167 |                   48 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.789 |  0.179 |                   50 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.442 |  0.174 |                   46 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15851332247257233
[2m[36m(func pid=119733)[0m mae:  0.10001873970031738
[2m[36m(func pid=119733)[0m rmse_per_class: [0.098, 0.248, 0.043, 0.286, 0.049, 0.162, 0.241, 0.123, 0.152, 0.182]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.7865 | Steps: 2 | Val loss: 0.6174 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2619 | Steps: 2 | Val loss: 0.3061 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=123100)[0m rmse: 0.1743050366640091
[2m[36m(func pid=123100)[0m mae:  0.12720224261283875
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.259, 0.091, 0.326, 0.092, 0.189, 0.279, 0.138, 0.143, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17928281426429749
[2m[36m(func pid=121978)[0m mae:  0.13153240084648132
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.108, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2700 | Steps: 2 | Val loss: 0.2909 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=121379)[0m rmse: 0.17016109824180603
[2m[36m(func pid=121379)[0m mae:  0.09904682636260986
[2m[36m(func pid=121379)[0m rmse_per_class: [0.132, 0.279, 0.053, 0.3, 0.074, 0.162, 0.229, 0.129, 0.174, 0.169]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.4389 | Steps: 2 | Val loss: 0.3405 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7836 | Steps: 2 | Val loss: 0.6157 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 10:04:36 (running for 00:39:57.08)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.27  |  0.161 |                   59 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.262 |  0.17  |                   49 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.787 |  0.179 |                   51 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.443 |  0.174 |                   47 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.16053824126720428
[2m[36m(func pid=119733)[0m mae:  0.10039152204990387
[2m[36m(func pid=119733)[0m rmse_per_class: [0.094, 0.25, 0.043, 0.285, 0.049, 0.163, 0.244, 0.132, 0.156, 0.189]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2619 | Steps: 2 | Val loss: 0.3130 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
[2m[36m(func pid=123100)[0m rmse: 0.1740601509809494
[2m[36m(func pid=123100)[0m mae:  0.12698179483413696
[2m[36m(func pid=123100)[0m rmse_per_class: [0.117, 0.259, 0.09, 0.326, 0.092, 0.189, 0.278, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17927345633506775
[2m[36m(func pid=121978)[0m mae:  0.13152353465557098
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.108, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2688 | Steps: 2 | Val loss: 0.2923 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=121379)[0m rmse: 0.17557352781295776
[2m[36m(func pid=121379)[0m mae:  0.10372289270162582
[2m[36m(func pid=121379)[0m rmse_per_class: [0.14, 0.271, 0.07, 0.317, 0.079, 0.168, 0.233, 0.128, 0.173, 0.177]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.4352 | Steps: 2 | Val loss: 0.3384 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.7842 | Steps: 2 | Val loss: 0.6145 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 10:04:41 (running for 00:40:02.44)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.269 |  0.162 |                   60 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.262 |  0.176 |                   50 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.784 |  0.179 |                   52 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.439 |  0.174 |                   48 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.16176001727581024
[2m[36m(func pid=119733)[0m mae:  0.10021016746759415
[2m[36m(func pid=119733)[0m rmse_per_class: [0.091, 0.253, 0.042, 0.284, 0.048, 0.163, 0.246, 0.141, 0.161, 0.191]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1738233119249344
[2m[36m(func pid=123100)[0m mae:  0.12678390741348267
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.259, 0.09, 0.326, 0.092, 0.189, 0.278, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2566 | Steps: 2 | Val loss: 0.3159 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=121978)[0m rmse: 0.1792551875114441
[2m[36m(func pid=121978)[0m mae:  0.13150368630886078
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.108, 0.19, 0.292, 0.141, 0.142, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2709 | Steps: 2 | Val loss: 0.2930 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4330 | Steps: 2 | Val loss: 0.3363 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=121379)[0m rmse: 0.17725560069084167
[2m[36m(func pid=121379)[0m mae:  0.10487852990627289
[2m[36m(func pid=121379)[0m rmse_per_class: [0.142, 0.266, 0.089, 0.331, 0.083, 0.168, 0.229, 0.124, 0.156, 0.184]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.7831 | Steps: 2 | Val loss: 0.6128 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 10:04:46 (running for 00:40:07.60)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.271 |  0.162 |                   61 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.257 |  0.177 |                   51 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.784 |  0.179 |                   53 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.435 |  0.174 |                   49 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.16237035393714905
[2m[36m(func pid=119733)[0m mae:  0.09984268248081207
[2m[36m(func pid=119733)[0m rmse_per_class: [0.087, 0.256, 0.04, 0.283, 0.047, 0.163, 0.246, 0.148, 0.161, 0.193]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17358022928237915
[2m[36m(func pid=123100)[0m mae:  0.12655216455459595
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.258, 0.089, 0.325, 0.091, 0.189, 0.277, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2615 | Steps: 2 | Val loss: 0.3262 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=121978)[0m rmse: 0.17927639186382294
[2m[36m(func pid=121978)[0m mae:  0.1315198838710785
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.108, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2693 | Steps: 2 | Val loss: 0.2947 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4306 | Steps: 2 | Val loss: 0.3345 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=121379)[0m rmse: 0.18343225121498108
[2m[36m(func pid=121379)[0m mae:  0.10788854211568832
[2m[36m(func pid=121379)[0m rmse_per_class: [0.151, 0.257, 0.099, 0.334, 0.085, 0.169, 0.235, 0.14, 0.147, 0.219]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.7790 | Steps: 2 | Val loss: 0.6114 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 10:04:51 (running for 00:40:12.71)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.269 |  0.163 |                   62 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.262 |  0.183 |                   52 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.783 |  0.179 |                   54 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.433 |  0.174 |                   50 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.16334855556488037
[2m[36m(func pid=119733)[0m mae:  0.09984467923641205
[2m[36m(func pid=119733)[0m rmse_per_class: [0.086, 0.259, 0.038, 0.284, 0.047, 0.163, 0.246, 0.152, 0.162, 0.195]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1734301745891571
[2m[36m(func pid=123100)[0m mae:  0.12641708552837372
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.258, 0.089, 0.325, 0.091, 0.189, 0.276, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17925116419792175
[2m[36m(func pid=121978)[0m mae:  0.13148941099643707
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.108, 0.19, 0.292, 0.141, 0.143, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2638 | Steps: 2 | Val loss: 0.3485 | Batch size: 32 | lr: 0.1 | Duration: 3.38s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2562 | Steps: 2 | Val loss: 0.2929 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4314 | Steps: 2 | Val loss: 0.3335 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.7766 | Steps: 2 | Val loss: 0.6096 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 10:04:57 (running for 00:40:18.15)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.269 |  0.163 |                   62 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.264 |  0.192 |                   53 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.779 |  0.179 |                   55 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.431 |  0.173 |                   51 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121379)[0m rmse: 0.1916150450706482
[2m[36m(func pid=121379)[0m mae:  0.11172275245189667
[2m[36m(func pid=121379)[0m rmse_per_class: [0.172, 0.256, 0.097, 0.336, 0.086, 0.172, 0.24, 0.209, 0.145, 0.202]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.16210445761680603
[2m[36m(func pid=119733)[0m mae:  0.09866099804639816
[2m[36m(func pid=119733)[0m rmse_per_class: [0.084, 0.263, 0.036, 0.286, 0.048, 0.162, 0.241, 0.155, 0.159, 0.187]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17343536019325256
[2m[36m(func pid=123100)[0m mae:  0.12643840909004211
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.258, 0.089, 0.325, 0.091, 0.189, 0.277, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.1792571246623993
[2m[36m(func pid=121978)[0m mae:  0.1314888894557953
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2615 | Steps: 2 | Val loss: 0.2921 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2531 | Steps: 2 | Val loss: 0.3709 | Batch size: 32 | lr: 0.1 | Duration: 3.21s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4286 | Steps: 2 | Val loss: 0.3318 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7766 | Steps: 2 | Val loss: 0.6078 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 10:05:02 (running for 00:40:23.47)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.261 |  0.161 |                   64 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.264 |  0.192 |                   53 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.777 |  0.179 |                   56 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.431 |  0.173 |                   52 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.16094402968883514
[2m[36m(func pid=119733)[0m mae:  0.09736086428165436
[2m[36m(func pid=119733)[0m rmse_per_class: [0.082, 0.268, 0.034, 0.289, 0.05, 0.161, 0.235, 0.154, 0.157, 0.18]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.19987942278385162
[2m[36m(func pid=121379)[0m mae:  0.11559639126062393
[2m[36m(func pid=121379)[0m rmse_per_class: [0.187, 0.256, 0.097, 0.333, 0.084, 0.182, 0.247, 0.288, 0.145, 0.181]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1731981486082077
[2m[36m(func pid=123100)[0m mae:  0.12623760104179382
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.258, 0.089, 0.325, 0.09, 0.189, 0.276, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17922842502593994
[2m[36m(func pid=121978)[0m mae:  0.1314639002084732
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.1, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2545 | Steps: 2 | Val loss: 0.2902 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2585 | Steps: 2 | Val loss: 0.3936 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4240 | Steps: 2 | Val loss: 0.3300 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7751 | Steps: 2 | Val loss: 0.6060 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 10:05:07 (running for 00:40:28.67)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.254 |  0.159 |                   65 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.253 |  0.2   |                   54 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.777 |  0.179 |                   57 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.429 |  0.173 |                   53 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15912608802318573
[2m[36m(func pid=119733)[0m mae:  0.09587138146162033
[2m[36m(func pid=119733)[0m rmse_per_class: [0.082, 0.272, 0.032, 0.291, 0.053, 0.16, 0.229, 0.148, 0.154, 0.172]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.19470596313476562
[2m[36m(func pid=121379)[0m mae:  0.11186400800943375
[2m[36m(func pid=121379)[0m rmse_per_class: [0.187, 0.26, 0.086, 0.328, 0.082, 0.186, 0.226, 0.263, 0.148, 0.179]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17292720079421997
[2m[36m(func pid=123100)[0m mae:  0.12602205574512482
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.258, 0.088, 0.324, 0.09, 0.188, 0.275, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17923514544963837
[2m[36m(func pid=121978)[0m mae:  0.13147297501564026
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.292, 0.14, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2608 | Steps: 2 | Val loss: 0.2888 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4246 | Steps: 2 | Val loss: 0.3293 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2621 | Steps: 2 | Val loss: 0.3725 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7727 | Steps: 2 | Val loss: 0.6045 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 10:05:12 (running for 00:40:33.92)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.261 |  0.158 |                   66 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.259 |  0.195 |                   55 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.775 |  0.179 |                   58 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.424 |  0.173 |                   54 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15787175297737122
[2m[36m(func pid=119733)[0m mae:  0.09470754861831665
[2m[36m(func pid=119733)[0m rmse_per_class: [0.082, 0.275, 0.03, 0.293, 0.057, 0.159, 0.223, 0.145, 0.151, 0.164]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17288975417613983
[2m[36m(func pid=123100)[0m mae:  0.1260545402765274
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.258, 0.088, 0.324, 0.089, 0.188, 0.276, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.1855616271495819
[2m[36m(func pid=121379)[0m mae:  0.10616781562566757
[2m[36m(func pid=121379)[0m rmse_per_class: [0.177, 0.263, 0.076, 0.322, 0.081, 0.176, 0.219, 0.218, 0.151, 0.173]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17925193905830383
[2m[36m(func pid=121978)[0m mae:  0.13148930668830872
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2632 | Steps: 2 | Val loss: 0.2887 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.4236 | Steps: 2 | Val loss: 0.3280 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2765 | Steps: 2 | Val loss: 0.3350 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.7671 | Steps: 2 | Val loss: 0.6026 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 10:05:18 (running for 00:40:39.25)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.263 |  0.157 |                   67 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.262 |  0.186 |                   56 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.773 |  0.179 |                   59 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.425 |  0.173 |                   55 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15715982019901276
[2m[36m(func pid=119733)[0m mae:  0.09399548172950745
[2m[36m(func pid=119733)[0m rmse_per_class: [0.082, 0.279, 0.029, 0.296, 0.061, 0.158, 0.219, 0.139, 0.149, 0.16]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17278577387332916
[2m[36m(func pid=123100)[0m mae:  0.12595537304878235
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.258, 0.088, 0.324, 0.089, 0.188, 0.275, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17920012772083282
[2m[36m(func pid=121978)[0m mae:  0.13144293427467346
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.17649969458580017
[2m[36m(func pid=121379)[0m mae:  0.10122807323932648
[2m[36m(func pid=121379)[0m rmse_per_class: [0.157, 0.265, 0.063, 0.317, 0.08, 0.164, 0.216, 0.178, 0.146, 0.179]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2633 | Steps: 2 | Val loss: 0.2875 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4195 | Steps: 2 | Val loss: 0.3263 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.7674 | Steps: 2 | Val loss: 0.6014 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 10:05:23 (running for 00:40:44.59)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.263 |  0.156 |                   68 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.276 |  0.176 |                   57 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.767 |  0.179 |                   60 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.424 |  0.173 |                   56 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15636923909187317
[2m[36m(func pid=119733)[0m mae:  0.09326796978712082
[2m[36m(func pid=119733)[0m rmse_per_class: [0.086, 0.277, 0.027, 0.297, 0.067, 0.158, 0.215, 0.131, 0.149, 0.156]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2591 | Steps: 2 | Val loss: 0.3137 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
[2m[36m(func pid=123100)[0m rmse: 0.17255055904388428
[2m[36m(func pid=123100)[0m mae:  0.1257670521736145
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.258, 0.087, 0.324, 0.089, 0.188, 0.275, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17917928099632263
[2m[36m(func pid=121978)[0m mae:  0.1314193606376648
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.1686810404062271
[2m[36m(func pid=121379)[0m mae:  0.09804372489452362
[2m[36m(func pid=121379)[0m rmse_per_class: [0.142, 0.267, 0.051, 0.314, 0.081, 0.167, 0.215, 0.13, 0.138, 0.184]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2545 | Steps: 2 | Val loss: 0.2872 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4187 | Steps: 2 | Val loss: 0.3252 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7649 | Steps: 2 | Val loss: 0.6001 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 10:05:28 (running for 00:40:49.90)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.254 |  0.156 |                   69 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.259 |  0.169 |                   58 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.767 |  0.179 |                   61 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.42  |  0.173 |                   57 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15607872605323792
[2m[36m(func pid=119733)[0m mae:  0.09254688769578934
[2m[36m(func pid=119733)[0m rmse_per_class: [0.086, 0.278, 0.027, 0.298, 0.074, 0.158, 0.211, 0.126, 0.147, 0.156]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2592 | Steps: 2 | Val loss: 0.3151 | Batch size: 32 | lr: 0.1 | Duration: 3.33s
[2m[36m(func pid=123100)[0m rmse: 0.17243297398090363
[2m[36m(func pid=123100)[0m mae:  0.12568649649620056
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.258, 0.087, 0.324, 0.088, 0.188, 0.274, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17920097708702087
[2m[36m(func pid=121978)[0m mae:  0.13142317533493042
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2658 | Steps: 2 | Val loss: 0.2877 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=121379)[0m rmse: 0.16800931096076965
[2m[36m(func pid=121379)[0m mae:  0.09863205254077911
[2m[36m(func pid=121379)[0m rmse_per_class: [0.136, 0.267, 0.04, 0.312, 0.084, 0.183, 0.218, 0.116, 0.133, 0.191]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4138 | Steps: 2 | Val loss: 0.3238 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.7624 | Steps: 2 | Val loss: 0.5974 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 10:05:34 (running for 00:40:55.34)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.266 |  0.156 |                   70 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.259 |  0.168 |                   59 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.765 |  0.179 |                   62 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.419 |  0.172 |                   58 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.1561768352985382
[2m[36m(func pid=119733)[0m mae:  0.0920589417219162
[2m[36m(func pid=119733)[0m rmse_per_class: [0.085, 0.278, 0.027, 0.3, 0.084, 0.158, 0.207, 0.12, 0.147, 0.155]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2469 | Steps: 2 | Val loss: 0.3178 | Batch size: 32 | lr: 0.1 | Duration: 3.30s
[2m[36m(func pid=123100)[0m rmse: 0.17229844629764557
[2m[36m(func pid=123100)[0m mae:  0.12557759881019592
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.258, 0.087, 0.323, 0.088, 0.188, 0.274, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17920608818531036
[2m[36m(func pid=121978)[0m mae:  0.13142675161361694
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.108]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2532 | Steps: 2 | Val loss: 0.2874 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=121379)[0m rmse: 0.1708013117313385
[2m[36m(func pid=121379)[0m mae:  0.1011781245470047
[2m[36m(func pid=121379)[0m rmse_per_class: [0.135, 0.263, 0.036, 0.307, 0.088, 0.206, 0.225, 0.119, 0.133, 0.195]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.7630 | Steps: 2 | Val loss: 0.5966 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4133 | Steps: 2 | Val loss: 0.3225 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 10:05:39 (running for 00:41:00.40)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.253 |  0.156 |                   71 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.247 |  0.171 |                   60 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.762 |  0.179 |                   63 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.414 |  0.172 |                   59 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15610060095787048
[2m[36m(func pid=119733)[0m mae:  0.09174826741218567
[2m[36m(func pid=119733)[0m rmse_per_class: [0.087, 0.277, 0.026, 0.301, 0.091, 0.158, 0.205, 0.118, 0.143, 0.155]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17920270562171936
[2m[36m(func pid=121978)[0m mae:  0.13142359256744385
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17211197316646576
[2m[36m(func pid=123100)[0m mae:  0.12541550397872925
[2m[36m(func pid=123100)[0m rmse_per_class: [0.116, 0.258, 0.086, 0.323, 0.088, 0.188, 0.274, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2579 | Steps: 2 | Val loss: 0.3168 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2526 | Steps: 2 | Val loss: 0.2884 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=121379)[0m rmse: 0.17178115248680115
[2m[36m(func pid=121379)[0m mae:  0.10198710858821869
[2m[36m(func pid=121379)[0m rmse_per_class: [0.14, 0.261, 0.034, 0.303, 0.09, 0.208, 0.229, 0.12, 0.133, 0.199]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7577 | Steps: 2 | Val loss: 0.5953 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4113 | Steps: 2 | Val loss: 0.3213 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 10:05:44 (running for 00:41:05.71)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.253 |  0.157 |                   72 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.258 |  0.172 |                   61 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.763 |  0.179 |                   64 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.413 |  0.172 |                   60 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.1569422483444214
[2m[36m(func pid=119733)[0m mae:  0.09187094122171402
[2m[36m(func pid=119733)[0m rmse_per_class: [0.089, 0.276, 0.025, 0.303, 0.101, 0.158, 0.204, 0.114, 0.142, 0.157]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17920584976673126
[2m[36m(func pid=121978)[0m mae:  0.13141801953315735
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17196573317050934
[2m[36m(func pid=123100)[0m mae:  0.12528499960899353
[2m[36m(func pid=123100)[0m rmse_per_class: [0.115, 0.258, 0.086, 0.323, 0.088, 0.188, 0.274, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2651 | Steps: 2 | Val loss: 0.3198 | Batch size: 32 | lr: 0.1 | Duration: 3.29s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2532 | Steps: 2 | Val loss: 0.2872 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7578 | Steps: 2 | Val loss: 0.5934 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4086 | Steps: 2 | Val loss: 0.3203 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=121379)[0m rmse: 0.17393521964550018
[2m[36m(func pid=121379)[0m mae:  0.10266735404729843
[2m[36m(func pid=121379)[0m rmse_per_class: [0.145, 0.26, 0.034, 0.301, 0.087, 0.215, 0.235, 0.12, 0.134, 0.208]
[2m[36m(func pid=121379)[0m 
== Status ==
Current time: 2024-01-07 10:05:50 (running for 00:41:11.23)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.253 |  0.156 |                   73 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.265 |  0.174 |                   62 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.758 |  0.179 |                   65 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.411 |  0.172 |                   61 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.156138613820076
[2m[36m(func pid=119733)[0m mae:  0.09127642214298248
[2m[36m(func pid=119733)[0m rmse_per_class: [0.09, 0.273, 0.024, 0.302, 0.105, 0.157, 0.202, 0.112, 0.14, 0.155]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17921583354473114
[2m[36m(func pid=121978)[0m mae:  0.1314283311367035
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17185187339782715
[2m[36m(func pid=123100)[0m mae:  0.1251978874206543
[2m[36m(func pid=123100)[0m rmse_per_class: [0.115, 0.257, 0.086, 0.322, 0.088, 0.188, 0.274, 0.138, 0.142, 0.108]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2593 | Steps: 2 | Val loss: 0.3204 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2500 | Steps: 2 | Val loss: 0.2878 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.7538 | Steps: 2 | Val loss: 0.5914 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4123 | Steps: 2 | Val loss: 0.3202 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=121379)[0m rmse: 0.1752973198890686
[2m[36m(func pid=121379)[0m mae:  0.10314071178436279
[2m[36m(func pid=121379)[0m rmse_per_class: [0.153, 0.26, 0.038, 0.3, 0.084, 0.21, 0.24, 0.119, 0.136, 0.214]
[2m[36m(func pid=121379)[0m 
== Status ==
Current time: 2024-01-07 10:05:55 (running for 00:41:16.48)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.25  |  0.156 |                   74 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.259 |  0.175 |                   63 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.758 |  0.179 |                   66 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.409 |  0.172 |                   62 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119733)[0m rmse: 0.15644407272338867
[2m[36m(func pid=119733)[0m mae:  0.09137585759162903
[2m[36m(func pid=119733)[0m rmse_per_class: [0.091, 0.272, 0.024, 0.305, 0.111, 0.156, 0.202, 0.108, 0.14, 0.156]
[2m[36m(func pid=119733)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17917266488075256
[2m[36m(func pid=121978)[0m mae:  0.13139314949512482
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17180386185646057
[2m[36m(func pid=123100)[0m mae:  0.1251973658800125
[2m[36m(func pid=123100)[0m rmse_per_class: [0.115, 0.257, 0.085, 0.322, 0.087, 0.187, 0.274, 0.138, 0.142, 0.108]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2550 | Steps: 2 | Val loss: 0.3228 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=119733)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2493 | Steps: 2 | Val loss: 0.2875 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.7544 | Steps: 2 | Val loss: 0.5900 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4088 | Steps: 2 | Val loss: 0.3189 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 10:06:00 (running for 00:41:21.76)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: -0.14624999836087227
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00018 | RUNNING    | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.25  |  0.156 |                   74 |
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.255 |  0.177 |                   64 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.754 |  0.179 |                   67 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.412 |  0.172 |                   63 |
| train_32e5a_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121379)[0m rmse: 0.17698544263839722
[2m[36m(func pid=121379)[0m mae:  0.10242533683776855
[2m[36m(func pid=121379)[0m rmse_per_class: [0.17, 0.262, 0.044, 0.304, 0.084, 0.181, 0.239, 0.13, 0.138, 0.218]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=119733)[0m rmse: 0.15656778216362
[2m[36m(func pid=119733)[0m mae:  0.09130173921585083
[2m[36m(func pid=119733)[0m rmse_per_class: [0.092, 0.268, 0.023, 0.305, 0.118, 0.156, 0.201, 0.107, 0.139, 0.155]
[2m[36m(func pid=121978)[0m rmse: 0.17916861176490784
[2m[36m(func pid=121978)[0m mae:  0.1313866376876831
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1715780645608902
[2m[36m(func pid=123100)[0m mae:  0.1250011920928955
[2m[36m(func pid=123100)[0m rmse_per_class: [0.115, 0.257, 0.085, 0.322, 0.087, 0.187, 0.274, 0.138, 0.142, 0.108]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2443 | Steps: 2 | Val loss: 0.3293 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.7518 | Steps: 2 | Val loss: 0.5881 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4081 | Steps: 2 | Val loss: 0.3182 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=121379)[0m rmse: 0.18070310354232788
[2m[36m(func pid=121379)[0m mae:  0.10336413234472275
[2m[36m(func pid=121379)[0m rmse_per_class: [0.19, 0.26, 0.049, 0.309, 0.085, 0.167, 0.236, 0.161, 0.141, 0.209]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17916643619537354
[2m[36m(func pid=121978)[0m mae:  0.13138416409492493
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=123100)[0m rmse: 0.17149195075035095
[2m[36m(func pid=123100)[0m mae:  0.12493272870779037
[2m[36m(func pid=123100)[0m rmse_per_class: [0.115, 0.257, 0.085, 0.322, 0.087, 0.187, 0.274, 0.138, 0.142, 0.108]
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2513 | Steps: 2 | Val loss: 0.3365 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:06:06 (running for 00:41:27.20)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.244 |  0.181 |                   65 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.754 |  0.179 |                   68 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.409 |  0.172 |                   64 |
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=136934)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=136934)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=136934)[0m Configuration completed!
[2m[36m(func pid=136934)[0m New optimizer parameters:
[2m[36m(func pid=136934)[0m SGD (
[2m[36m(func pid=136934)[0m Parameter Group 0
[2m[36m(func pid=136934)[0m     dampening: 0
[2m[36m(func pid=136934)[0m     differentiable: False
[2m[36m(func pid=136934)[0m     foreach: None
[2m[36m(func pid=136934)[0m     lr: 0.01
[2m[36m(func pid=136934)[0m     maximize: False
[2m[36m(func pid=136934)[0m     momentum: 0.9
[2m[36m(func pid=136934)[0m     nesterov: False
[2m[36m(func pid=136934)[0m     weight_decay: 1e-05
[2m[36m(func pid=136934)[0m )
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.1839134395122528
[2m[36m(func pid=121379)[0m mae:  0.10504013299942017
[2m[36m(func pid=121379)[0m rmse_per_class: [0.212, 0.255, 0.051, 0.315, 0.083, 0.164, 0.233, 0.177, 0.15, 0.198]
== Status ==
Current time: 2024-01-07 10:06:11 (running for 00:41:32.61)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.251 |  0.184 |                   66 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.752 |  0.179 |                   69 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.408 |  0.171 |                   65 |
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.7506 | Steps: 2 | Val loss: 0.5871 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4056 | Steps: 2 | Val loss: 0.3179 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8889 | Steps: 2 | Val loss: 0.6873 | Batch size: 32 | lr: 0.01 | Duration: 4.75s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2586 | Steps: 2 | Val loss: 0.3351 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=121978)[0m rmse: 0.1791633665561676
[2m[36m(func pid=121978)[0m mae:  0.13137224316596985
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.108, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1715354472398758
[2m[36m(func pid=123100)[0m mae:  0.12498973309993744
[2m[36m(func pid=123100)[0m rmse_per_class: [0.115, 0.257, 0.085, 0.322, 0.086, 0.187, 0.273, 0.138, 0.142, 0.109]
[2m[36m(func pid=123100)[0m 
== Status ==
Current time: 2024-01-07 10:06:16 (running for 00:41:37.93)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.251 |  0.184 |                   66 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.751 |  0.179 |                   70 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.406 |  0.172 |                   66 |
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.889 |  0.183 |                    1 |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.18258927762508392
[2m[36m(func pid=136934)[0m mae:  0.13432979583740234
[2m[36m(func pid=136934)[0m rmse_per_class: [0.115, 0.266, 0.11, 0.339, 0.11, 0.191, 0.294, 0.145, 0.144, 0.113]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.18559333682060242
[2m[36m(func pid=121379)[0m mae:  0.10650601238012314
[2m[36m(func pid=121379)[0m rmse_per_class: [0.231, 0.247, 0.05, 0.32, 0.082, 0.166, 0.233, 0.194, 0.157, 0.176]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.7469 | Steps: 2 | Val loss: 0.5848 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4037 | Steps: 2 | Val loss: 0.3168 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.8427 | Steps: 2 | Val loss: 0.6364 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2511 | Steps: 2 | Val loss: 0.3295 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=121978)[0m rmse: 0.17910327017307281
[2m[36m(func pid=121978)[0m mae:  0.13130933046340942
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17126230895519257
[2m[36m(func pid=123100)[0m mae:  0.12475452572107315
[2m[36m(func pid=123100)[0m rmse_per_class: [0.115, 0.257, 0.084, 0.322, 0.086, 0.187, 0.273, 0.138, 0.142, 0.108]
[2m[36m(func pid=123100)[0m 
== Status ==
Current time: 2024-01-07 10:06:22 (running for 00:41:43.19)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.259 |  0.186 |                   67 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.747 |  0.179 |                   71 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.404 |  0.171 |                   67 |
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.843 |  0.181 |                    2 |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.18134944140911102
[2m[36m(func pid=136934)[0m mae:  0.13329418003559113
[2m[36m(func pid=136934)[0m rmse_per_class: [0.114, 0.266, 0.109, 0.338, 0.109, 0.191, 0.289, 0.141, 0.143, 0.112]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.18654364347457886
[2m[36m(func pid=121379)[0m mae:  0.10774733871221542
[2m[36m(func pid=121379)[0m rmse_per_class: [0.251, 0.238, 0.051, 0.322, 0.081, 0.168, 0.237, 0.202, 0.16, 0.157]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.7434 | Steps: 2 | Val loss: 0.5833 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4024 | Steps: 2 | Val loss: 0.3160 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.7625 | Steps: 2 | Val loss: 0.5684 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=121978)[0m rmse: 0.17912136018276215
[2m[36m(func pid=121978)[0m mae:  0.1313144564628601
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17108896374702454
[2m[36m(func pid=123100)[0m mae:  0.12460801750421524
[2m[36m(func pid=123100)[0m rmse_per_class: [0.115, 0.257, 0.084, 0.322, 0.086, 0.187, 0.273, 0.137, 0.142, 0.108]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2649 | Steps: 2 | Val loss: 0.3277 | Batch size: 32 | lr: 0.1 | Duration: 3.27s
== Status ==
Current time: 2024-01-07 10:06:27 (running for 00:41:48.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.251 |  0.187 |                   68 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.743 |  0.179 |                   72 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.402 |  0.171 |                   68 |
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.763 |  0.18  |                    3 |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.17994782328605652
[2m[36m(func pid=136934)[0m mae:  0.1318993866443634
[2m[36m(func pid=136934)[0m rmse_per_class: [0.113, 0.265, 0.108, 0.337, 0.106, 0.19, 0.284, 0.141, 0.143, 0.112]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.7427 | Steps: 2 | Val loss: 0.5818 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.4028 | Steps: 2 | Val loss: 0.3150 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=121379)[0m rmse: 0.1846945583820343
[2m[36m(func pid=121379)[0m mae:  0.10653834044933319
[2m[36m(func pid=121379)[0m rmse_per_class: [0.26, 0.232, 0.056, 0.32, 0.084, 0.166, 0.236, 0.182, 0.165, 0.146]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.6735 | Steps: 2 | Val loss: 0.4980 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=121978)[0m rmse: 0.17912814021110535
[2m[36m(func pid=121978)[0m mae:  0.13131514191627502
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17087681591510773
[2m[36m(func pid=123100)[0m mae:  0.12441450357437134
[2m[36m(func pid=123100)[0m rmse_per_class: [0.115, 0.256, 0.084, 0.321, 0.086, 0.187, 0.272, 0.137, 0.142, 0.108]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2491 | Steps: 2 | Val loss: 0.3238 | Batch size: 32 | lr: 0.1 | Duration: 3.31s
== Status ==
Current time: 2024-01-07 10:06:33 (running for 00:41:54.09)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.265 |  0.185 |                   69 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.743 |  0.179 |                   73 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.403 |  0.171 |                   69 |
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.674 |  0.179 |                    4 |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.17887362837791443
[2m[36m(func pid=136934)[0m mae:  0.13090094923973083
[2m[36m(func pid=136934)[0m rmse_per_class: [0.113, 0.264, 0.105, 0.336, 0.104, 0.19, 0.281, 0.14, 0.143, 0.111]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7408 | Steps: 2 | Val loss: 0.5802 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4018 | Steps: 2 | Val loss: 0.3141 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=121379)[0m rmse: 0.18350771069526672
[2m[36m(func pid=121379)[0m mae:  0.10555785894393921
[2m[36m(func pid=121379)[0m rmse_per_class: [0.256, 0.228, 0.059, 0.314, 0.086, 0.163, 0.239, 0.183, 0.168, 0.14]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17911234498023987
[2m[36m(func pid=121978)[0m mae:  0.13129857182502747
[2m[36m(func pid=121978)[0m rmse_per_class: [0.115, 0.26, 0.099, 0.336, 0.107, 0.19, 0.291, 0.14, 0.143, 0.109]
[2m[36m(func pid=121978)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17070657014846802
[2m[36m(func pid=123100)[0m mae:  0.1242683082818985
[2m[36m(func pid=123100)[0m rmse_per_class: [0.114, 0.256, 0.084, 0.321, 0.086, 0.187, 0.271, 0.137, 0.142, 0.108]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.5881 | Steps: 2 | Val loss: 0.4381 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2514 | Steps: 2 | Val loss: 0.3211 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 10:06:38 (running for 00:41:59.44)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.249 |  0.184 |                   70 |
| train_32e5a_00020 | RUNNING    | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.741 |  0.179 |                   74 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.402 |  0.171 |                   70 |
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.588 |  0.178 |                    5 |
| train_32e5a_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1778024584054947
[2m[36m(func pid=136934)[0m mae:  0.12998726963996887
[2m[36m(func pid=136934)[0m rmse_per_class: [0.114, 0.264, 0.102, 0.334, 0.101, 0.19, 0.279, 0.14, 0.143, 0.111]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=121978)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.7384 | Steps: 2 | Val loss: 0.5788 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4005 | Steps: 2 | Val loss: 0.3132 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=121379)[0m rmse: 0.18404024839401245
[2m[36m(func pid=121379)[0m mae:  0.10607907921075821
[2m[36m(func pid=121379)[0m rmse_per_class: [0.257, 0.226, 0.063, 0.314, 0.091, 0.16, 0.242, 0.186, 0.166, 0.135]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=121978)[0m rmse: 0.17914359271526337
[2m[36m(func pid=121978)[0m mae:  0.13133832812309265
[2m[36m(func pid=121978)[0m rmse_per_class: [0.116, 0.261, 0.099, 0.336, 0.107, 0.19, 0.291, 0.141, 0.143, 0.109]
[2m[36m(func pid=123100)[0m rmse: 0.17048358917236328
[2m[36m(func pid=123100)[0m mae:  0.12407209724187851
[2m[36m(func pid=123100)[0m rmse_per_class: [0.115, 0.256, 0.083, 0.321, 0.085, 0.187, 0.271, 0.137, 0.142, 0.108]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.5169 | Steps: 2 | Val loss: 0.3937 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2519 | Steps: 2 | Val loss: 0.3226 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=136934)[0m rmse: 0.17684847116470337
[2m[36m(func pid=136934)[0m mae:  0.12917640805244446
[2m[36m(func pid=136934)[0m rmse_per_class: [0.115, 0.263, 0.099, 0.333, 0.098, 0.19, 0.277, 0.139, 0.143, 0.111]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3993 | Steps: 2 | Val loss: 0.3125 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=121379)[0m rmse: 0.1852843165397644
[2m[36m(func pid=121379)[0m mae:  0.10647197812795639
[2m[36m(func pid=121379)[0m rmse_per_class: [0.253, 0.226, 0.068, 0.318, 0.092, 0.16, 0.242, 0.191, 0.164, 0.139]
[2m[36m(func pid=123100)[0m rmse: 0.17027132213115692
[2m[36m(func pid=123100)[0m mae:  0.12389757484197617
[2m[36m(func pid=123100)[0m rmse_per_class: [0.114, 0.256, 0.083, 0.321, 0.085, 0.187, 0.27, 0.137, 0.142, 0.108]
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4661 | Steps: 2 | Val loss: 0.3608 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=136934)[0m rmse: 0.1757449060678482
[2m[36m(func pid=136934)[0m mae:  0.12823091447353363
[2m[36m(func pid=136934)[0m rmse_per_class: [0.116, 0.263, 0.096, 0.331, 0.094, 0.19, 0.275, 0.138, 0.143, 0.11]
== Status ==
Current time: 2024-01-07 10:06:43 (running for 00:42:04.79)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.14674999937415123
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.251 |  0.184 |                   71 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.401 |  0.17  |                   71 |
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.517 |  0.177 |                    6 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 10:06:50 (running for 00:42:11.61)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.14674999937415123
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.251 |  0.184 |                   71 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.401 |  0.17  |                   71 |
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.466 |  0.176 |                    7 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=138760)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=138760)[0m Configuration completed!
[2m[36m(func pid=138760)[0m New optimizer parameters:
[2m[36m(func pid=138760)[0m SGD (
[2m[36m(func pid=138760)[0m Parameter Group 0
[2m[36m(func pid=138760)[0m     dampening: 0
[2m[36m(func pid=138760)[0m     differentiable: False
[2m[36m(func pid=138760)[0m     foreach: None
[2m[36m(func pid=138760)[0m     lr: 0.1
[2m[36m(func pid=138760)[0m     maximize: False
[2m[36m(func pid=138760)[0m     momentum: 0.9
[2m[36m(func pid=138760)[0m     nesterov: False
[2m[36m(func pid=138760)[0m     weight_decay: 1e-05
[2m[36m(func pid=138760)[0m )
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.4319 | Steps: 2 | Val loss: 0.3373 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4000 | Steps: 2 | Val loss: 0.3125 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2548 | Steps: 2 | Val loss: 0.3241 | Batch size: 32 | lr: 0.1 | Duration: 3.32s
== Status ==
Current time: 2024-01-07 10:06:55 (running for 00:42:16.62)
Memory usage on this node: 26.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.14674999937415123
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.252 |  0.185 |                   72 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.399 |  0.17  |                   72 |
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.466 |  0.176 |                    7 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |        |        |                      |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 0.8362 | Steps: 2 | Val loss: 0.5217 | Batch size: 32 | lr: 0.1 | Duration: 5.16s
[2m[36m(func pid=136934)[0m rmse: 0.17448090016841888
[2m[36m(func pid=136934)[0m mae:  0.1271788775920868
[2m[36m(func pid=136934)[0m rmse_per_class: [0.116, 0.262, 0.093, 0.33, 0.09, 0.191, 0.273, 0.137, 0.143, 0.11]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.17032524943351746
[2m[36m(func pid=123100)[0m mae:  0.1239500418305397
[2m[36m(func pid=123100)[0m rmse_per_class: [0.114, 0.256, 0.083, 0.321, 0.085, 0.187, 0.271, 0.137, 0.142, 0.108]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.1841554045677185
[2m[36m(func pid=121379)[0m mae:  0.10585055500268936
[2m[36m(func pid=121379)[0m rmse_per_class: [0.242, 0.23, 0.071, 0.321, 0.09, 0.162, 0.236, 0.178, 0.165, 0.147]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.18127760291099548
[2m[36m(func pid=138760)[0m mae:  0.1327902227640152
[2m[36m(func pid=138760)[0m rmse_per_class: [0.109, 0.268, 0.118, 0.337, 0.102, 0.192, 0.291, 0.146, 0.142, 0.109]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.4065 | Steps: 2 | Val loss: 0.3217 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3995 | Steps: 2 | Val loss: 0.3125 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2552 | Steps: 2 | Val loss: 0.3263 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 10:07:01 (running for 00:42:22.25)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.14674999937415123
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.255 |  0.184 |                   73 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.4   |  0.17  |                   73 |
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.406 |  0.173 |                    9 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.836 |  0.181 |                    1 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1733255833387375
[2m[36m(func pid=136934)[0m mae:  0.12619902193546295
[2m[36m(func pid=136934)[0m rmse_per_class: [0.117, 0.261, 0.091, 0.328, 0.086, 0.19, 0.27, 0.136, 0.143, 0.11]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 0.5367 | Steps: 2 | Val loss: 0.3511 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=123100)[0m rmse: 0.17034757137298584
[2m[36m(func pid=123100)[0m mae:  0.1239776462316513
[2m[36m(func pid=123100)[0m rmse_per_class: [0.114, 0.256, 0.083, 0.321, 0.085, 0.186, 0.271, 0.137, 0.142, 0.108]
[2m[36m(func pid=123100)[0m 
[2m[36m(func pid=121379)[0m rmse: 0.18309639394283295
[2m[36m(func pid=121379)[0m mae:  0.10555309057235718
[2m[36m(func pid=121379)[0m rmse_per_class: [0.216, 0.239, 0.074, 0.323, 0.089, 0.169, 0.23, 0.157, 0.172, 0.161]
[2m[36m(func pid=121379)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.17715370655059814
[2m[36m(func pid=138760)[0m mae:  0.12962666153907776
[2m[36m(func pid=138760)[0m rmse_per_class: [0.114, 0.265, 0.109, 0.334, 0.089, 0.189, 0.278, 0.139, 0.144, 0.111]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3947 | Steps: 2 | Val loss: 0.3120 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=123100)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3972 | Steps: 2 | Val loss: 0.3118 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=121379)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2699 | Steps: 2 | Val loss: 0.3309 | Batch size: 32 | lr: 0.1 | Duration: 3.34s
== Status ==
Current time: 2024-01-07 10:07:06 (running for 00:42:27.65)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: -0.14674999937415123
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00019 | RUNNING    | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.255 |  0.183 |                   74 |
| train_32e5a_00021 | RUNNING    | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.399 |  0.17  |                   74 |
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.395 |  0.172 |                   10 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.537 |  0.177 |                    2 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.17201486229896545
[2m[36m(func pid=136934)[0m mae:  0.1251244843006134
[2m[36m(func pid=136934)[0m rmse_per_class: [0.117, 0.26, 0.088, 0.327, 0.082, 0.19, 0.267, 0.136, 0.143, 0.109]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=123100)[0m rmse: 0.1702292263507843
[2m[36m(func pid=123100)[0m mae:  0.1239057406783104
[2m[36m(func pid=123100)[0m rmse_per_class: [0.115, 0.256, 0.083, 0.32, 0.084, 0.186, 0.271, 0.137, 0.142, 0.108]
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 0.4025 | Steps: 2 | Val loss: 0.3131 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=121379)[0m rmse: 0.18290701508522034
[2m[36m(func pid=121379)[0m mae:  0.10576416552066803
[2m[36m(func pid=121379)[0m rmse_per_class: [0.189, 0.255, 0.071, 0.328, 0.09, 0.177, 0.225, 0.142, 0.181, 0.173]
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3906 | Steps: 2 | Val loss: 0.3068 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=138760)[0m rmse: 0.172389954328537
[2m[36m(func pid=138760)[0m mae:  0.12579286098480225
[2m[36m(func pid=138760)[0m rmse_per_class: [0.118, 0.264, 0.095, 0.328, 0.076, 0.189, 0.269, 0.133, 0.144, 0.109]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:07:11 (running for 00:42:32.99)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.391 |  0.171 |                   11 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.403 |  0.172 |                    3 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1705860197544098
[2m[36m(func pid=136934)[0m mae:  0.1239849328994751
[2m[36m(func pid=136934)[0m rmse_per_class: [0.117, 0.259, 0.085, 0.326, 0.079, 0.189, 0.265, 0.135, 0.143, 0.109]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 0.4245 | Steps: 2 | Val loss: 0.3262 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=138760)[0m rmse: 0.1673450917005539
[2m[36m(func pid=138760)[0m mae:  0.12167119979858398
[2m[36m(func pid=138760)[0m rmse_per_class: [0.123, 0.258, 0.08, 0.321, 0.065, 0.194, 0.259, 0.125, 0.145, 0.104]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3870 | Steps: 2 | Val loss: 0.3042 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 10:07:17 (running for 00:42:38.65)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.387 |  0.169 |                   12 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.424 |  0.167 |                    4 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.16902537643909454
[2m[36m(func pid=136934)[0m mae:  0.12277175486087799
[2m[36m(func pid=136934)[0m rmse_per_class: [0.117, 0.257, 0.081, 0.324, 0.075, 0.188, 0.263, 0.135, 0.143, 0.107]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 0.4526 | Steps: 2 | Val loss: 0.3332 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=138760)[0m rmse: 0.1612667739391327
[2m[36m(func pid=138760)[0m mae:  0.11623549461364746
[2m[36m(func pid=138760)[0m rmse_per_class: [0.128, 0.252, 0.058, 0.302, 0.057, 0.193, 0.252, 0.126, 0.147, 0.098]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.3871 | Steps: 2 | Val loss: 0.3028 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=136934)[0m rmse: 0.16744793951511383
[2m[36m(func pid=136934)[0m mae:  0.12152848392724991
[2m[36m(func pid=136934)[0m rmse_per_class: [0.116, 0.255, 0.077, 0.323, 0.073, 0.187, 0.261, 0.134, 0.143, 0.106]
[2m[36m(func pid=136934)[0m 
== Status ==
Current time: 2024-01-07 10:07:23 (running for 00:42:44.13)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.387 |  0.167 |                   13 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.453 |  0.161 |                    5 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 0.4427 | Steps: 2 | Val loss: 0.3337 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.3829 | Steps: 2 | Val loss: 0.3023 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=138760)[0m rmse: 0.15160158276557922
[2m[36m(func pid=138760)[0m mae:  0.1067119836807251
[2m[36m(func pid=138760)[0m rmse_per_class: [0.117, 0.246, 0.04, 0.273, 0.054, 0.192, 0.235, 0.125, 0.143, 0.089]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:07:28 (running for 00:42:49.50)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.383 |  0.166 |                   14 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.443 |  0.152 |                    6 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1660701185464859
[2m[36m(func pid=136934)[0m mae:  0.12042295932769775
[2m[36m(func pid=136934)[0m rmse_per_class: [0.116, 0.253, 0.074, 0.321, 0.071, 0.186, 0.259, 0.133, 0.142, 0.105]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 0.4023 | Steps: 2 | Val loss: 0.3229 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.3782 | Steps: 2 | Val loss: 0.3016 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=138760)[0m rmse: 0.14654143154621124
[2m[36m(func pid=138760)[0m mae:  0.10053908824920654
[2m[36m(func pid=138760)[0m rmse_per_class: [0.104, 0.243, 0.035, 0.271, 0.055, 0.196, 0.224, 0.116, 0.139, 0.084]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:07:33 (running for 00:42:54.82)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.378 |  0.165 |                   15 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.402 |  0.147 |                    7 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.16471710801124573
[2m[36m(func pid=136934)[0m mae:  0.11931405961513519
[2m[36m(func pid=136934)[0m rmse_per_class: [0.115, 0.251, 0.07, 0.32, 0.07, 0.185, 0.258, 0.133, 0.142, 0.104]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 0.3629 | Steps: 2 | Val loss: 0.3029 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.3753 | Steps: 2 | Val loss: 0.3012 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=138760)[0m rmse: 0.1470164954662323
[2m[36m(func pid=138760)[0m mae:  0.10017164796590805
[2m[36m(func pid=138760)[0m rmse_per_class: [0.1, 0.243, 0.033, 0.281, 0.055, 0.195, 0.226, 0.113, 0.138, 0.086]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:07:39 (running for 00:43:00.20)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.375 |  0.164 |                   16 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.363 |  0.147 |                    8 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.16363689303398132
[2m[36m(func pid=136934)[0m mae:  0.11841776221990585
[2m[36m(func pid=136934)[0m rmse_per_class: [0.114, 0.249, 0.068, 0.319, 0.068, 0.184, 0.258, 0.132, 0.142, 0.102]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 0.3245 | Steps: 2 | Val loss: 0.2834 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3745 | Steps: 2 | Val loss: 0.3002 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=138760)[0m rmse: 0.14734874665737152
[2m[36m(func pid=138760)[0m mae:  0.1005014032125473
[2m[36m(func pid=138760)[0m rmse_per_class: [0.099, 0.243, 0.029, 0.293, 0.055, 0.187, 0.224, 0.112, 0.141, 0.092]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:07:44 (running for 00:43:05.84)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.374 |  0.162 |                   17 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.324 |  0.147 |                    9 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1624622792005539
[2m[36m(func pid=136934)[0m mae:  0.11741785705089569
[2m[36m(func pid=136934)[0m rmse_per_class: [0.113, 0.248, 0.065, 0.317, 0.067, 0.184, 0.256, 0.131, 0.142, 0.101]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.3053 | Steps: 2 | Val loss: 0.2748 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.3756 | Steps: 2 | Val loss: 0.2983 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=138760)[0m rmse: 0.14789021015167236
[2m[36m(func pid=138760)[0m mae:  0.10071047395467758
[2m[36m(func pid=138760)[0m rmse_per_class: [0.103, 0.243, 0.026, 0.301, 0.055, 0.178, 0.221, 0.112, 0.144, 0.096]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:07:50 (running for 00:43:11.18)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.376 |  0.161 |                   18 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.305 |  0.148 |                   10 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.16104048490524292
[2m[36m(func pid=136934)[0m mae:  0.11617467552423477
[2m[36m(func pid=136934)[0m rmse_per_class: [0.112, 0.247, 0.063, 0.315, 0.066, 0.183, 0.253, 0.13, 0.141, 0.1]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.3134 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.3710 | Steps: 2 | Val loss: 0.2965 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=138760)[0m rmse: 0.14796292781829834
[2m[36m(func pid=138760)[0m mae:  0.10030929744243622
[2m[36m(func pid=138760)[0m rmse_per_class: [0.107, 0.242, 0.026, 0.306, 0.055, 0.166, 0.222, 0.114, 0.145, 0.096]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:07:55 (running for 00:43:16.52)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.371 |  0.16  |                   19 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.313 |  0.148 |                   11 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.15985430777072906
[2m[36m(func pid=136934)[0m mae:  0.11512885987758636
[2m[36m(func pid=136934)[0m rmse_per_class: [0.111, 0.246, 0.061, 0.313, 0.065, 0.183, 0.251, 0.129, 0.141, 0.1]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.3105 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.1 | Duration: 3.28s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.3630 | Steps: 2 | Val loss: 0.2941 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=138760)[0m rmse: 0.14892424643039703
[2m[36m(func pid=138760)[0m mae:  0.10058702528476715
[2m[36m(func pid=138760)[0m rmse_per_class: [0.111, 0.245, 0.027, 0.306, 0.054, 0.161, 0.228, 0.114, 0.145, 0.098]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:08:01 (running for 00:43:22.06)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.363 |  0.158 |                   20 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.31  |  0.149 |                   12 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.15848396718502045
[2m[36m(func pid=136934)[0m mae:  0.11388697475194931
[2m[36m(func pid=136934)[0m rmse_per_class: [0.11, 0.245, 0.059, 0.311, 0.064, 0.182, 0.247, 0.128, 0.141, 0.099]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.2903 | Steps: 2 | Val loss: 0.2709 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.3583 | Steps: 2 | Val loss: 0.2920 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=138760)[0m rmse: 0.14845916628837585
[2m[36m(func pid=138760)[0m mae:  0.09894799441099167
[2m[36m(func pid=138760)[0m rmse_per_class: [0.106, 0.248, 0.028, 0.295, 0.053, 0.158, 0.23, 0.118, 0.144, 0.104]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:08:06 (running for 00:43:27.22)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.358 |  0.157 |                   21 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.29  |  0.148 |                   13 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1574651002883911
[2m[36m(func pid=136934)[0m mae:  0.11298443377017975
[2m[36m(func pid=136934)[0m rmse_per_class: [0.109, 0.245, 0.058, 0.309, 0.063, 0.181, 0.244, 0.127, 0.14, 0.098]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.2883 | Steps: 2 | Val loss: 0.2710 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.3551 | Steps: 2 | Val loss: 0.2897 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=138760)[0m rmse: 0.14996957778930664
[2m[36m(func pid=138760)[0m mae:  0.09832596778869629
[2m[36m(func pid=138760)[0m rmse_per_class: [0.098, 0.253, 0.032, 0.287, 0.052, 0.158, 0.232, 0.128, 0.143, 0.116]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:08:11 (running for 00:43:32.59)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.355 |  0.156 |                   22 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.288 |  0.15  |                   14 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.15642575919628143
[2m[36m(func pid=136934)[0m mae:  0.11209283024072647
[2m[36m(func pid=136934)[0m rmse_per_class: [0.108, 0.244, 0.056, 0.307, 0.063, 0.181, 0.243, 0.126, 0.14, 0.098]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.2741 | Steps: 2 | Val loss: 0.2745 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.3500 | Steps: 2 | Val loss: 0.2879 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=138760)[0m rmse: 0.15217196941375732
[2m[36m(func pid=138760)[0m mae:  0.0992349311709404
[2m[36m(func pid=138760)[0m rmse_per_class: [0.095, 0.257, 0.034, 0.286, 0.052, 0.161, 0.232, 0.131, 0.143, 0.129]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:08:17 (running for 00:43:38.07)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.35  |  0.156 |                   23 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.274 |  0.152 |                   15 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.15550532937049866
[2m[36m(func pid=136934)[0m mae:  0.1112474799156189
[2m[36m(func pid=136934)[0m rmse_per_class: [0.107, 0.243, 0.054, 0.306, 0.062, 0.18, 0.241, 0.125, 0.139, 0.097]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.2719 | Steps: 2 | Val loss: 0.2738 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.3546 | Steps: 2 | Val loss: 0.2861 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=138760)[0m rmse: 0.15098440647125244
[2m[36m(func pid=138760)[0m mae:  0.09728231281042099
[2m[36m(func pid=138760)[0m rmse_per_class: [0.091, 0.263, 0.031, 0.285, 0.053, 0.16, 0.226, 0.124, 0.141, 0.136]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:08:22 (running for 00:43:43.29)
Memory usage on this node: 19.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.355 |  0.155 |                   24 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.272 |  0.151 |                   16 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.15480852127075195
[2m[36m(func pid=136934)[0m mae:  0.11061640083789825
[2m[36m(func pid=136934)[0m rmse_per_class: [0.107, 0.243, 0.053, 0.306, 0.061, 0.179, 0.24, 0.123, 0.139, 0.096]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.2594 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.3465 | Steps: 2 | Val loss: 0.2852 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 10:08:27 (running for 00:43:48.32)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.355 |  0.155 |                   24 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.272 |  0.151 |                   16 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.14947636425495148
[2m[36m(func pid=138760)[0m mae:  0.09540482610464096
[2m[36m(func pid=138760)[0m rmse_per_class: [0.088, 0.27, 0.028, 0.288, 0.056, 0.157, 0.221, 0.112, 0.138, 0.138]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m rmse: 0.15451116859912872
[2m[36m(func pid=136934)[0m mae:  0.1104099377989769
[2m[36m(func pid=136934)[0m rmse_per_class: [0.106, 0.243, 0.051, 0.306, 0.061, 0.178, 0.242, 0.122, 0.139, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.2534 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.3382 | Steps: 2 | Val loss: 0.2842 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 10:08:32 (running for 00:43:53.64)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.347 |  0.155 |                   25 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.259 |  0.149 |                   17 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.14961494505405426
[2m[36m(func pid=138760)[0m mae:  0.09460940212011337
[2m[36m(func pid=138760)[0m rmse_per_class: [0.086, 0.273, 0.026, 0.292, 0.062, 0.157, 0.215, 0.108, 0.135, 0.141]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m rmse: 0.15415886044502258
[2m[36m(func pid=136934)[0m mae:  0.11009879410266876
[2m[36m(func pid=136934)[0m rmse_per_class: [0.106, 0.243, 0.05, 0.306, 0.061, 0.178, 0.242, 0.122, 0.139, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.3359 | Steps: 2 | Val loss: 0.2828 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.2552 | Steps: 2 | Val loss: 0.2751 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 10:08:37 (running for 00:43:59.01)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.338 |  0.154 |                   26 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.253 |  0.15  |                   18 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1535685807466507
[2m[36m(func pid=136934)[0m mae:  0.10959754139184952
[2m[36m(func pid=136934)[0m rmse_per_class: [0.106, 0.243, 0.049, 0.304, 0.061, 0.177, 0.241, 0.121, 0.139, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.1501491516828537
[2m[36m(func pid=138760)[0m mae:  0.09404591470956802
[2m[36m(func pid=138760)[0m rmse_per_class: [0.086, 0.278, 0.025, 0.295, 0.071, 0.158, 0.21, 0.107, 0.136, 0.135]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.3348 | Steps: 2 | Val loss: 0.2819 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.2493 | Steps: 2 | Val loss: 0.2745 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 10:08:43 (running for 00:44:04.65)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.336 |  0.154 |                   27 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.255 |  0.15  |                   19 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.15323297679424286
[2m[36m(func pid=136934)[0m mae:  0.10933385789394379
[2m[36m(func pid=136934)[0m rmse_per_class: [0.106, 0.243, 0.048, 0.304, 0.061, 0.176, 0.242, 0.12, 0.139, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.1500040590763092
[2m[36m(func pid=138760)[0m mae:  0.09306623786687851
[2m[36m(func pid=138760)[0m rmse_per_class: [0.088, 0.274, 0.026, 0.293, 0.08, 0.158, 0.206, 0.106, 0.136, 0.132]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.3340 | Steps: 2 | Val loss: 0.2808 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.2491 | Steps: 2 | Val loss: 0.2733 | Batch size: 32 | lr: 0.1 | Duration: 3.26s
== Status ==
Current time: 2024-01-07 10:08:49 (running for 00:44:10.20)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.335 |  0.153 |                   28 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.249 |  0.15  |                   20 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.15280000865459442
[2m[36m(func pid=136934)[0m mae:  0.10902930796146393
[2m[36m(func pid=136934)[0m rmse_per_class: [0.105, 0.243, 0.047, 0.303, 0.061, 0.175, 0.243, 0.119, 0.139, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14904305338859558
[2m[36m(func pid=138760)[0m mae:  0.09188176691532135
[2m[36m(func pid=138760)[0m rmse_per_class: [0.088, 0.272, 0.026, 0.294, 0.081, 0.156, 0.203, 0.105, 0.135, 0.13]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.3308 | Steps: 2 | Val loss: 0.2797 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.2452 | Steps: 2 | Val loss: 0.2726 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 10:08:54 (running for 00:44:15.93)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.331 |  0.152 |                   30 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.249 |  0.149 |                   21 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1522439569234848
[2m[36m(func pid=136934)[0m mae:  0.10853578895330429
[2m[36m(func pid=136934)[0m rmse_per_class: [0.104, 0.243, 0.046, 0.301, 0.061, 0.174, 0.242, 0.118, 0.138, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14844493567943573
[2m[36m(func pid=138760)[0m mae:  0.09116911143064499
[2m[36m(func pid=138760)[0m rmse_per_class: [0.089, 0.268, 0.024, 0.296, 0.079, 0.155, 0.2, 0.107, 0.136, 0.13]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.3259 | Steps: 2 | Val loss: 0.2789 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.2404 | Steps: 2 | Val loss: 0.2731 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 10:09:00 (running for 00:44:21.35)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.326 |  0.152 |                   31 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.245 |  0.148 |                   22 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.15182140469551086
[2m[36m(func pid=136934)[0m mae:  0.10814204066991806
[2m[36m(func pid=136934)[0m rmse_per_class: [0.103, 0.243, 0.046, 0.3, 0.061, 0.173, 0.242, 0.117, 0.138, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14846357703208923
[2m[36m(func pid=138760)[0m mae:  0.0915244072675705
[2m[36m(func pid=138760)[0m rmse_per_class: [0.089, 0.267, 0.024, 0.301, 0.083, 0.152, 0.202, 0.107, 0.137, 0.123]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.3269 | Steps: 2 | Val loss: 0.2779 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.2452 | Steps: 2 | Val loss: 0.2724 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 10:09:05 (running for 00:44:26.91)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.327 |  0.151 |                   32 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.24  |  0.148 |                   23 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.15122947096824646
[2m[36m(func pid=136934)[0m mae:  0.10760433971881866
[2m[36m(func pid=136934)[0m rmse_per_class: [0.102, 0.243, 0.045, 0.298, 0.061, 0.171, 0.241, 0.117, 0.138, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14779770374298096
[2m[36m(func pid=138760)[0m mae:  0.09095190465450287
[2m[36m(func pid=138760)[0m rmse_per_class: [0.086, 0.265, 0.024, 0.302, 0.081, 0.151, 0.203, 0.108, 0.139, 0.12]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.3223 | Steps: 2 | Val loss: 0.2777 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.2391 | Steps: 2 | Val loss: 0.2716 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 10:09:11 (running for 00:44:32.13)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.322 |  0.151 |                   33 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.245 |  0.148 |                   24 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1511852890253067
[2m[36m(func pid=136934)[0m mae:  0.10764648020267487
[2m[36m(func pid=136934)[0m rmse_per_class: [0.103, 0.244, 0.044, 0.297, 0.061, 0.17, 0.243, 0.116, 0.138, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14759033918380737
[2m[36m(func pid=138760)[0m mae:  0.09058757871389389
[2m[36m(func pid=138760)[0m rmse_per_class: [0.084, 0.261, 0.023, 0.299, 0.08, 0.15, 0.206, 0.109, 0.143, 0.12]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3205 | Steps: 2 | Val loss: 0.2776 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.2425 | Steps: 2 | Val loss: 0.2728 | Batch size: 32 | lr: 0.1 | Duration: 3.28s
== Status ==
Current time: 2024-01-07 10:09:16 (running for 00:44:37.69)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.32  |  0.151 |                   34 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.239 |  0.148 |                   25 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1511821746826172
[2m[36m(func pid=136934)[0m mae:  0.10765925794839859
[2m[36m(func pid=136934)[0m rmse_per_class: [0.103, 0.244, 0.044, 0.297, 0.061, 0.169, 0.244, 0.115, 0.139, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14864438772201538
[2m[36m(func pid=138760)[0m mae:  0.09104333817958832
[2m[36m(func pid=138760)[0m rmse_per_class: [0.083, 0.26, 0.023, 0.299, 0.076, 0.151, 0.208, 0.113, 0.152, 0.121]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3231 | Steps: 2 | Val loss: 0.2768 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2411 | Steps: 2 | Val loss: 0.2721 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 10:09:22 (running for 00:44:43.09)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.323 |  0.151 |                   35 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.242 |  0.149 |                   26 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.15057365596294403
[2m[36m(func pid=136934)[0m mae:  0.10706053674221039
[2m[36m(func pid=136934)[0m rmse_per_class: [0.101, 0.244, 0.043, 0.296, 0.061, 0.169, 0.244, 0.115, 0.138, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14805996417999268
[2m[36m(func pid=138760)[0m mae:  0.09074801206588745
[2m[36m(func pid=138760)[0m rmse_per_class: [0.08, 0.259, 0.023, 0.297, 0.075, 0.15, 0.21, 0.112, 0.151, 0.124]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.3217 | Steps: 2 | Val loss: 0.2761 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.2323 | Steps: 2 | Val loss: 0.2692 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 10:09:27 (running for 00:44:48.65)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.322 |  0.15  |                   36 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.241 |  0.148 |                   27 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1501404047012329
[2m[36m(func pid=136934)[0m mae:  0.1067013368010521
[2m[36m(func pid=136934)[0m rmse_per_class: [0.101, 0.244, 0.043, 0.293, 0.061, 0.167, 0.244, 0.114, 0.138, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14608068764209747
[2m[36m(func pid=138760)[0m mae:  0.08953149616718292
[2m[36m(func pid=138760)[0m rmse_per_class: [0.076, 0.254, 0.023, 0.292, 0.074, 0.149, 0.208, 0.109, 0.144, 0.132]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.3185 | Steps: 2 | Val loss: 0.2753 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 10:09:33 (running for 00:44:54.16)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.318 |  0.15  |                   37 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.232 |  0.146 |                   28 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1496356874704361
[2m[36m(func pid=136934)[0m mae:  0.10626630485057831
[2m[36m(func pid=136934)[0m rmse_per_class: [0.1, 0.244, 0.042, 0.292, 0.061, 0.166, 0.243, 0.114, 0.138, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.2379 | Steps: 2 | Val loss: 0.2683 | Batch size: 32 | lr: 0.1 | Duration: 3.45s
[2m[36m(func pid=138760)[0m rmse: 0.145036518573761
[2m[36m(func pid=138760)[0m mae:  0.08905013650655746
[2m[36m(func pid=138760)[0m rmse_per_class: [0.075, 0.252, 0.023, 0.289, 0.075, 0.149, 0.206, 0.106, 0.136, 0.139]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.3095 | Steps: 2 | Val loss: 0.2747 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 10:09:38 (running for 00:44:59.83)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.309 |  0.149 |                   38 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.238 |  0.145 |                   29 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1491413116455078
[2m[36m(func pid=136934)[0m mae:  0.1057606115937233
[2m[36m(func pid=136934)[0m rmse_per_class: [0.1, 0.244, 0.042, 0.291, 0.062, 0.166, 0.242, 0.113, 0.138, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.2417 | Steps: 2 | Val loss: 0.2688 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
[2m[36m(func pid=138760)[0m rmse: 0.14533255994319916
[2m[36m(func pid=138760)[0m mae:  0.0894063413143158
[2m[36m(func pid=138760)[0m rmse_per_class: [0.074, 0.249, 0.024, 0.287, 0.073, 0.151, 0.206, 0.108, 0.136, 0.145]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.3176 | Steps: 2 | Val loss: 0.2744 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:09:44 (running for 00:45:05.26)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.318 |  0.149 |                   39 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.242 |  0.145 |                   30 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14888617396354675
[2m[36m(func pid=136934)[0m mae:  0.10552263259887695
[2m[36m(func pid=136934)[0m rmse_per_class: [0.099, 0.244, 0.041, 0.29, 0.062, 0.165, 0.243, 0.113, 0.138, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2414 | Steps: 2 | Val loss: 0.2694 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=138760)[0m rmse: 0.14595012366771698
[2m[36m(func pid=138760)[0m mae:  0.09004516899585724
[2m[36m(func pid=138760)[0m rmse_per_class: [0.074, 0.247, 0.023, 0.287, 0.072, 0.154, 0.207, 0.109, 0.135, 0.151]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.3084 | Steps: 2 | Val loss: 0.2739 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 10:09:49 (running for 00:45:10.77)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.308 |  0.149 |                   40 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.241 |  0.146 |                   31 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1485307514667511
[2m[36m(func pid=136934)[0m mae:  0.10512182861566544
[2m[36m(func pid=136934)[0m rmse_per_class: [0.099, 0.244, 0.04, 0.29, 0.062, 0.165, 0.24, 0.112, 0.137, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.2483 | Steps: 2 | Val loss: 0.2680 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.3073 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=138760)[0m rmse: 0.1447179615497589
[2m[36m(func pid=138760)[0m mae:  0.0889451876282692
[2m[36m(func pid=138760)[0m rmse_per_class: [0.073, 0.245, 0.023, 0.288, 0.067, 0.15, 0.206, 0.104, 0.131, 0.159]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:09:55 (running for 00:45:16.21)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.307 |  0.148 |                   41 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.248 |  0.145 |                   32 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14830543100833893
[2m[36m(func pid=136934)[0m mae:  0.10475975275039673
[2m[36m(func pid=136934)[0m rmse_per_class: [0.099, 0.244, 0.04, 0.291, 0.062, 0.165, 0.239, 0.112, 0.137, 0.095]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.2358 | Steps: 2 | Val loss: 0.2707 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3094 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=138760)[0m rmse: 0.14638100564479828
[2m[36m(func pid=138760)[0m mae:  0.09006460756063461
[2m[36m(func pid=138760)[0m rmse_per_class: [0.081, 0.242, 0.024, 0.288, 0.061, 0.15, 0.208, 0.105, 0.133, 0.173]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:10:00 (running for 00:45:21.70)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.309 |  0.148 |                   42 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.236 |  0.146 |                   33 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14828281104564667
[2m[36m(func pid=136934)[0m mae:  0.1046658530831337
[2m[36m(func pid=136934)[0m rmse_per_class: [0.098, 0.245, 0.039, 0.291, 0.063, 0.165, 0.239, 0.111, 0.137, 0.096]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.2448 | Steps: 2 | Val loss: 0.2703 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.3083 | Steps: 2 | Val loss: 0.2738 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=138760)[0m rmse: 0.14695397019386292
[2m[36m(func pid=138760)[0m mae:  0.0899648666381836
[2m[36m(func pid=138760)[0m rmse_per_class: [0.085, 0.239, 0.025, 0.28, 0.062, 0.151, 0.21, 0.106, 0.135, 0.177]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m rmse: 0.1482377052307129
[2m[36m(func pid=136934)[0m mae:  0.10449723899364471
[2m[36m(func pid=136934)[0m rmse_per_class: [0.098, 0.245, 0.039, 0.29, 0.063, 0.165, 0.239, 0.11, 0.137, 0.096]
[2m[36m(func pid=136934)[0m 
== Status ==
Current time: 2024-01-07 10:10:06 (running for 00:45:27.11)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.308 |  0.148 |                   43 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.245 |  0.147 |                   34 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.2456 | Steps: 2 | Val loss: 0.2682 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.3001 | Steps: 2 | Val loss: 0.2739 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=138760)[0m rmse: 0.14599904417991638
[2m[36m(func pid=138760)[0m mae:  0.08886553347110748
[2m[36m(func pid=138760)[0m rmse_per_class: [0.086, 0.237, 0.027, 0.275, 0.063, 0.15, 0.208, 0.104, 0.135, 0.173]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:10:11 (running for 00:45:32.25)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.3   |  0.148 |                   44 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.246 |  0.146 |                   35 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1482928842306137
[2m[36m(func pid=136934)[0m mae:  0.1044619232416153
[2m[36m(func pid=136934)[0m rmse_per_class: [0.098, 0.245, 0.039, 0.29, 0.063, 0.166, 0.24, 0.11, 0.137, 0.096]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.2352 | Steps: 2 | Val loss: 0.2677 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.3046 | Steps: 2 | Val loss: 0.2733 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=138760)[0m rmse: 0.14580798149108887
[2m[36m(func pid=138760)[0m mae:  0.08902893215417862
[2m[36m(func pid=138760)[0m rmse_per_class: [0.086, 0.237, 0.027, 0.275, 0.067, 0.149, 0.209, 0.102, 0.134, 0.171]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:10:16 (running for 00:45:37.84)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.305 |  0.148 |                   45 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.235 |  0.146 |                   36 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14793241024017334
[2m[36m(func pid=136934)[0m mae:  0.10410110652446747
[2m[36m(func pid=136934)[0m rmse_per_class: [0.097, 0.245, 0.038, 0.288, 0.064, 0.165, 0.24, 0.11, 0.137, 0.096]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.2356 | Steps: 2 | Val loss: 0.2701 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.3030 | Steps: 2 | Val loss: 0.2736 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=138760)[0m rmse: 0.14752885699272156
[2m[36m(func pid=138760)[0m mae:  0.09049221128225327
[2m[36m(func pid=138760)[0m rmse_per_class: [0.086, 0.238, 0.027, 0.281, 0.072, 0.15, 0.211, 0.104, 0.133, 0.173]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:10:22 (running for 00:45:43.28)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.303 |  0.148 |                   46 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.236 |  0.148 |                   37 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14807406067848206
[2m[36m(func pid=136934)[0m mae:  0.10414371639490128
[2m[36m(func pid=136934)[0m rmse_per_class: [0.096, 0.245, 0.038, 0.288, 0.064, 0.165, 0.241, 0.109, 0.137, 0.098]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.2414 | Steps: 2 | Val loss: 0.2721 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2978 | Steps: 2 | Val loss: 0.2732 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=138760)[0m rmse: 0.14852622151374817
[2m[36m(func pid=138760)[0m mae:  0.09129001200199127
[2m[36m(func pid=138760)[0m rmse_per_class: [0.086, 0.242, 0.026, 0.288, 0.07, 0.15, 0.212, 0.105, 0.133, 0.173]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:10:27 (running for 00:45:48.48)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.298 |  0.148 |                   47 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.241 |  0.149 |                   38 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1477583348751068
[2m[36m(func pid=136934)[0m mae:  0.10377074778079987
[2m[36m(func pid=136934)[0m rmse_per_class: [0.096, 0.245, 0.038, 0.287, 0.064, 0.164, 0.24, 0.109, 0.137, 0.098]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.2347 | Steps: 2 | Val loss: 0.2726 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.3026 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 10:10:32 (running for 00:45:53.49)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.298 |  0.148 |                   47 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.241 |  0.149 |                   38 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.1485334038734436
[2m[36m(func pid=138760)[0m mae:  0.0913282111287117
[2m[36m(func pid=138760)[0m rmse_per_class: [0.089, 0.247, 0.027, 0.294, 0.064, 0.15, 0.211, 0.104, 0.137, 0.163]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m rmse: 0.14749014377593994
[2m[36m(func pid=136934)[0m mae:  0.10340704023838043
[2m[36m(func pid=136934)[0m rmse_per_class: [0.095, 0.245, 0.038, 0.286, 0.064, 0.163, 0.24, 0.109, 0.136, 0.098]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.3022 | Steps: 2 | Val loss: 0.2733 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.2359 | Steps: 2 | Val loss: 0.2731 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 10:10:37 (running for 00:45:58.68)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.303 |  0.147 |                   48 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.235 |  0.149 |                   39 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14768695831298828
[2m[36m(func pid=136934)[0m mae:  0.10342945903539658
[2m[36m(func pid=136934)[0m rmse_per_class: [0.095, 0.246, 0.037, 0.287, 0.065, 0.163, 0.241, 0.109, 0.136, 0.099]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14881379902362823
[2m[36m(func pid=138760)[0m mae:  0.09109868109226227
[2m[36m(func pid=138760)[0m rmse_per_class: [0.088, 0.25, 0.027, 0.295, 0.058, 0.151, 0.211, 0.105, 0.147, 0.157]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2948 | Steps: 2 | Val loss: 0.2733 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.2353 | Steps: 2 | Val loss: 0.2737 | Batch size: 32 | lr: 0.1 | Duration: 3.37s
== Status ==
Current time: 2024-01-07 10:10:43 (running for 00:46:04.31)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.302 |  0.148 |                   49 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.236 |  0.149 |                   40 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14763939380645752
[2m[36m(func pid=136934)[0m mae:  0.10332570970058441
[2m[36m(func pid=136934)[0m rmse_per_class: [0.096, 0.245, 0.037, 0.286, 0.065, 0.162, 0.241, 0.109, 0.136, 0.099]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.1490810215473175
[2m[36m(func pid=138760)[0m mae:  0.09056263417005539
[2m[36m(func pid=138760)[0m rmse_per_class: [0.089, 0.252, 0.026, 0.293, 0.056, 0.152, 0.212, 0.106, 0.154, 0.151]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2984 | Steps: 2 | Val loss: 0.2730 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.2406 | Steps: 2 | Val loss: 0.2729 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 10:10:48 (running for 00:46:09.91)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.298 |  0.147 |                   51 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.235 |  0.149 |                   41 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14744751155376434
[2m[36m(func pid=136934)[0m mae:  0.10306979715824127
[2m[36m(func pid=136934)[0m rmse_per_class: [0.095, 0.245, 0.036, 0.286, 0.065, 0.162, 0.24, 0.109, 0.136, 0.099]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14837804436683655
[2m[36m(func pid=138760)[0m mae:  0.08931465446949005
[2m[36m(func pid=138760)[0m rmse_per_class: [0.093, 0.25, 0.025, 0.287, 0.055, 0.153, 0.211, 0.107, 0.151, 0.153]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2993 | Steps: 2 | Val loss: 0.2733 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.2437 | Steps: 2 | Val loss: 0.2730 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 10:10:54 (running for 00:46:15.31)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.299 |  0.148 |                   52 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.241 |  0.148 |                   42 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14752189815044403
[2m[36m(func pid=136934)[0m mae:  0.10306964069604874
[2m[36m(func pid=136934)[0m rmse_per_class: [0.095, 0.245, 0.036, 0.287, 0.066, 0.161, 0.241, 0.11, 0.136, 0.1]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.1481325775384903
[2m[36m(func pid=138760)[0m mae:  0.08894912898540497
[2m[36m(func pid=138760)[0m rmse_per_class: [0.091, 0.253, 0.025, 0.287, 0.057, 0.154, 0.209, 0.108, 0.147, 0.15]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2870 | Steps: 2 | Val loss: 0.2730 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2397 | Steps: 2 | Val loss: 0.2717 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 10:10:59 (running for 00:46:20.58)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.287 |  0.147 |                   53 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.244 |  0.148 |                   43 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14726096391677856
[2m[36m(func pid=136934)[0m mae:  0.10271066427230835
[2m[36m(func pid=136934)[0m rmse_per_class: [0.095, 0.245, 0.035, 0.287, 0.066, 0.16, 0.24, 0.11, 0.135, 0.1]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14699648320674896
[2m[36m(func pid=138760)[0m mae:  0.08834093809127808
[2m[36m(func pid=138760)[0m rmse_per_class: [0.085, 0.259, 0.023, 0.289, 0.059, 0.155, 0.206, 0.109, 0.145, 0.14]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2965 | Steps: 2 | Val loss: 0.2728 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2403 | Steps: 2 | Val loss: 0.2712 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 10:11:04 (running for 00:46:25.71)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.297 |  0.147 |                   54 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.24  |  0.147 |                   44 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14721722900867462
[2m[36m(func pid=136934)[0m mae:  0.10255223512649536
[2m[36m(func pid=136934)[0m rmse_per_class: [0.095, 0.246, 0.035, 0.286, 0.066, 0.16, 0.239, 0.109, 0.135, 0.101]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14610302448272705
[2m[36m(func pid=138760)[0m mae:  0.08777330070734024
[2m[36m(func pid=138760)[0m rmse_per_class: [0.079, 0.262, 0.023, 0.291, 0.061, 0.155, 0.203, 0.111, 0.141, 0.135]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2854 | Steps: 2 | Val loss: 0.2723 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.2396 | Steps: 2 | Val loss: 0.2710 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 10:11:10 (running for 00:46:31.09)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.285 |  0.147 |                   55 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.24  |  0.146 |                   45 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1468801200389862
[2m[36m(func pid=136934)[0m mae:  0.10216649621725082
[2m[36m(func pid=136934)[0m rmse_per_class: [0.095, 0.247, 0.035, 0.286, 0.066, 0.16, 0.238, 0.109, 0.135, 0.099]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14646320044994354
[2m[36m(func pid=138760)[0m mae:  0.08797631412744522
[2m[36m(func pid=138760)[0m rmse_per_class: [0.08, 0.258, 0.024, 0.289, 0.067, 0.157, 0.203, 0.114, 0.137, 0.137]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2935 | Steps: 2 | Val loss: 0.2724 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 10:11:15 (running for 00:46:36.38)
Memory usage on this node: 19.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.293 |  0.147 |                   56 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.24  |  0.146 |                   46 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14697524905204773
[2m[36m(func pid=136934)[0m mae:  0.10193789005279541
[2m[36m(func pid=136934)[0m rmse_per_class: [0.094, 0.249, 0.035, 0.287, 0.066, 0.16, 0.236, 0.108, 0.135, 0.1]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2380 | Steps: 2 | Val loss: 0.2712 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=138760)[0m rmse: 0.14690592885017395
[2m[36m(func pid=138760)[0m mae:  0.08873637765645981
[2m[36m(func pid=138760)[0m rmse_per_class: [0.082, 0.26, 0.024, 0.292, 0.069, 0.157, 0.201, 0.113, 0.136, 0.133]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2874 | Steps: 2 | Val loss: 0.2718 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=136934)[0m rmse: 0.14650192856788635
[2m[36m(func pid=136934)[0m mae:  0.10142502933740616
[2m[36m(func pid=136934)[0m rmse_per_class: [0.094, 0.249, 0.034, 0.285, 0.067, 0.161, 0.235, 0.107, 0.134, 0.098]
[2m[36m(func pid=136934)[0m 
== Status ==
Current time: 2024-01-07 10:11:20 (running for 00:46:41.90)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.287 |  0.147 |                   57 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.238 |  0.147 |                   47 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2367 | Steps: 2 | Val loss: 0.2717 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=138760)[0m rmse: 0.1473439782857895
[2m[36m(func pid=138760)[0m mae:  0.08970560878515244
[2m[36m(func pid=138760)[0m rmse_per_class: [0.086, 0.264, 0.023, 0.297, 0.064, 0.155, 0.201, 0.112, 0.138, 0.132]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2803 | Steps: 2 | Val loss: 0.2714 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 10:11:26 (running for 00:46:47.21)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.28  |  0.146 |                   58 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.237 |  0.147 |                   48 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14613410830497742
[2m[36m(func pid=136934)[0m mae:  0.10099282115697861
[2m[36m(func pid=136934)[0m rmse_per_class: [0.093, 0.25, 0.034, 0.285, 0.067, 0.16, 0.234, 0.107, 0.133, 0.098]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.2360 | Steps: 2 | Val loss: 0.2735 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2837 | Steps: 2 | Val loss: 0.2708 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=138760)[0m rmse: 0.1493256390094757
[2m[36m(func pid=138760)[0m mae:  0.09131453931331635
[2m[36m(func pid=138760)[0m rmse_per_class: [0.087, 0.264, 0.023, 0.297, 0.061, 0.154, 0.205, 0.113, 0.152, 0.139]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:11:31 (running for 00:46:52.58)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.284 |  0.146 |                   59 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.236 |  0.149 |                   49 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14569421112537384
[2m[36m(func pid=136934)[0m mae:  0.10056117922067642
[2m[36m(func pid=136934)[0m rmse_per_class: [0.092, 0.249, 0.034, 0.282, 0.067, 0.16, 0.233, 0.106, 0.133, 0.099]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.2414 | Steps: 2 | Val loss: 0.2770 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2861 | Steps: 2 | Val loss: 0.2700 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=138760)[0m rmse: 0.15213389694690704
[2m[36m(func pid=138760)[0m mae:  0.09344200789928436
[2m[36m(func pid=138760)[0m rmse_per_class: [0.09, 0.263, 0.024, 0.299, 0.062, 0.153, 0.211, 0.112, 0.161, 0.146]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:11:36 (running for 00:46:57.78)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.286 |  0.145 |                   60 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.241 |  0.152 |                   50 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14508219063282013
[2m[36m(func pid=136934)[0m mae:  0.10000823438167572
[2m[36m(func pid=136934)[0m rmse_per_class: [0.092, 0.249, 0.034, 0.281, 0.067, 0.159, 0.232, 0.106, 0.133, 0.098]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.2376 | Steps: 2 | Val loss: 0.2778 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2899 | Steps: 2 | Val loss: 0.2697 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=138760)[0m rmse: 0.15309298038482666
[2m[36m(func pid=138760)[0m mae:  0.09408198297023773
[2m[36m(func pid=138760)[0m rmse_per_class: [0.091, 0.262, 0.024, 0.299, 0.062, 0.154, 0.212, 0.113, 0.161, 0.153]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:11:42 (running for 00:47:03.23)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.29  |  0.145 |                   61 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.238 |  0.153 |                   51 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1449175775051117
[2m[36m(func pid=136934)[0m mae:  0.09973303973674774
[2m[36m(func pid=136934)[0m rmse_per_class: [0.091, 0.248, 0.034, 0.279, 0.067, 0.16, 0.231, 0.106, 0.133, 0.1]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.2397 | Steps: 2 | Val loss: 0.2757 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2816 | Steps: 2 | Val loss: 0.2694 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=138760)[0m rmse: 0.15177449584007263
[2m[36m(func pid=138760)[0m mae:  0.09276647865772247
[2m[36m(func pid=138760)[0m rmse_per_class: [0.09, 0.259, 0.024, 0.293, 0.062, 0.154, 0.212, 0.114, 0.153, 0.157]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:11:47 (running for 00:47:08.66)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.282 |  0.145 |                   62 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.24  |  0.152 |                   52 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14469461143016815
[2m[36m(func pid=136934)[0m mae:  0.09953225404024124
[2m[36m(func pid=136934)[0m rmse_per_class: [0.091, 0.247, 0.034, 0.278, 0.067, 0.159, 0.232, 0.107, 0.133, 0.101]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.2361 | Steps: 2 | Val loss: 0.2755 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2814 | Steps: 2 | Val loss: 0.2697 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=138760)[0m rmse: 0.1514650583267212
[2m[36m(func pid=138760)[0m mae:  0.09211032837629318
[2m[36m(func pid=138760)[0m rmse_per_class: [0.093, 0.256, 0.023, 0.29, 0.064, 0.154, 0.211, 0.117, 0.144, 0.163]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m rmse: 0.14481820166110992
[2m[36m(func pid=136934)[0m mae:  0.09960050880908966
[2m[36m(func pid=136934)[0m rmse_per_class: [0.091, 0.246, 0.033, 0.278, 0.067, 0.158, 0.233, 0.107, 0.134, 0.101]
[2m[36m(func pid=136934)[0m 
== Status ==
Current time: 2024-01-07 10:11:52 (running for 00:47:14.00)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.281 |  0.145 |                   63 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.236 |  0.151 |                   53 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.2319 | Steps: 2 | Val loss: 0.2750 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2827 | Steps: 2 | Val loss: 0.2698 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=138760)[0m rmse: 0.15103809535503387
[2m[36m(func pid=138760)[0m mae:  0.09204847365617752
[2m[36m(func pid=138760)[0m rmse_per_class: [0.097, 0.253, 0.024, 0.292, 0.066, 0.153, 0.21, 0.114, 0.138, 0.163]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m rmse: 0.14478157460689545
[2m[36m(func pid=136934)[0m mae:  0.09946812689304352
[2m[36m(func pid=136934)[0m rmse_per_class: [0.091, 0.246, 0.033, 0.279, 0.067, 0.157, 0.232, 0.108, 0.134, 0.102]
== Status ==
Current time: 2024-01-07 10:11:58 (running for 00:47:19.26)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.283 |  0.145 |                   64 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.232 |  0.151 |                   54 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2393 | Steps: 2 | Val loss: 0.2725 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2836 | Steps: 2 | Val loss: 0.2698 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=138760)[0m rmse: 0.14987018704414368
[2m[36m(func pid=138760)[0m mae:  0.09106829762458801
[2m[36m(func pid=138760)[0m rmse_per_class: [0.097, 0.248, 0.025, 0.287, 0.069, 0.152, 0.206, 0.112, 0.135, 0.167]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m rmse: 0.14478404819965363
[2m[36m(func pid=136934)[0m mae:  0.09929244965314865
[2m[36m(func pid=136934)[0m rmse_per_class: [0.091, 0.247, 0.032, 0.281, 0.066, 0.157, 0.231, 0.108, 0.133, 0.101]
[2m[36m(func pid=136934)[0m 
== Status ==
Current time: 2024-01-07 10:12:03 (running for 00:47:24.68)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.284 |  0.145 |                   65 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.239 |  0.15  |                   55 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.2421 | Steps: 2 | Val loss: 0.2720 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2737 | Steps: 2 | Val loss: 0.2695 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=138760)[0m rmse: 0.15002073347568512
[2m[36m(func pid=138760)[0m mae:  0.0908740684390068
[2m[36m(func pid=138760)[0m rmse_per_class: [0.096, 0.245, 0.026, 0.283, 0.073, 0.152, 0.206, 0.108, 0.136, 0.175]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:12:09 (running for 00:47:30.18)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.274 |  0.145 |                   66 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.242 |  0.15  |                   56 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14463791251182556
[2m[36m(func pid=136934)[0m mae:  0.09901239722967148
[2m[36m(func pid=136934)[0m rmse_per_class: [0.091, 0.248, 0.032, 0.281, 0.067, 0.156, 0.23, 0.108, 0.133, 0.101]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2326 | Steps: 2 | Val loss: 0.2703 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2795 | Steps: 2 | Val loss: 0.2690 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 10:12:14 (running for 00:47:35.21)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.274 |  0.145 |                   66 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.242 |  0.15  |                   56 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.14911159873008728
[2m[36m(func pid=138760)[0m mae:  0.09064783900976181
[2m[36m(func pid=138760)[0m rmse_per_class: [0.095, 0.242, 0.025, 0.282, 0.075, 0.152, 0.206, 0.11, 0.136, 0.169]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m rmse: 0.14425446093082428
[2m[36m(func pid=136934)[0m mae:  0.09853135049343109
[2m[36m(func pid=136934)[0m rmse_per_class: [0.091, 0.25, 0.032, 0.282, 0.067, 0.155, 0.227, 0.108, 0.131, 0.099]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.2306 | Steps: 2 | Val loss: 0.2692 | Batch size: 32 | lr: 0.1 | Duration: 3.25s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2786 | Steps: 2 | Val loss: 0.2691 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 10:12:19 (running for 00:47:40.78)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.28  |  0.144 |                   67 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.233 |  0.149 |                   57 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.1481887698173523
[2m[36m(func pid=138760)[0m mae:  0.0899776741862297
[2m[36m(func pid=138760)[0m rmse_per_class: [0.087, 0.245, 0.025, 0.28, 0.081, 0.152, 0.205, 0.112, 0.135, 0.16]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m rmse: 0.14429423213005066
[2m[36m(func pid=136934)[0m mae:  0.09833629429340363
[2m[36m(func pid=136934)[0m rmse_per_class: [0.09, 0.252, 0.032, 0.283, 0.067, 0.155, 0.226, 0.107, 0.131, 0.1]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.2339 | Steps: 2 | Val loss: 0.2680 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2770 | Steps: 2 | Val loss: 0.2685 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 10:12:25 (running for 00:47:46.15)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.279 |  0.144 |                   68 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.231 |  0.148 |                   58 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14384104311466217
[2m[36m(func pid=136934)[0m mae:  0.09782566130161285
[2m[36m(func pid=136934)[0m rmse_per_class: [0.089, 0.251, 0.032, 0.281, 0.067, 0.155, 0.225, 0.106, 0.131, 0.101]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14680896699428558
[2m[36m(func pid=138760)[0m mae:  0.08886309713125229
[2m[36m(func pid=138760)[0m rmse_per_class: [0.084, 0.246, 0.025, 0.278, 0.082, 0.152, 0.203, 0.112, 0.136, 0.15]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2729 | Steps: 2 | Val loss: 0.2688 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.2382 | Steps: 2 | Val loss: 0.2678 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 10:12:30 (running for 00:47:51.50)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.277 |  0.144 |                   69 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.234 |  0.147 |                   59 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14404979348182678
[2m[36m(func pid=136934)[0m mae:  0.0978538915514946
[2m[36m(func pid=136934)[0m rmse_per_class: [0.09, 0.251, 0.032, 0.28, 0.067, 0.156, 0.225, 0.106, 0.132, 0.104]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.1462470293045044
[2m[36m(func pid=138760)[0m mae:  0.08782123774290085
[2m[36m(func pid=138760)[0m rmse_per_class: [0.079, 0.247, 0.025, 0.276, 0.084, 0.152, 0.201, 0.112, 0.137, 0.148]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2737 | Steps: 2 | Val loss: 0.2689 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2329 | Steps: 2 | Val loss: 0.2682 | Batch size: 32 | lr: 0.1 | Duration: 3.27s
== Status ==
Current time: 2024-01-07 10:12:36 (running for 00:47:57.33)
Memory usage on this node: 19.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.273 |  0.144 |                   70 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.238 |  0.146 |                   60 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1441580355167389
[2m[36m(func pid=136934)[0m mae:  0.09789717942476273
[2m[36m(func pid=136934)[0m rmse_per_class: [0.09, 0.25, 0.032, 0.279, 0.068, 0.156, 0.225, 0.106, 0.132, 0.105]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14600779116153717
[2m[36m(func pid=138760)[0m mae:  0.08744124323129654
[2m[36m(func pid=138760)[0m rmse_per_class: [0.079, 0.248, 0.025, 0.276, 0.08, 0.152, 0.201, 0.113, 0.135, 0.151]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2839 | Steps: 2 | Val loss: 0.2686 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.2344 | Steps: 2 | Val loss: 0.2693 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=136934)[0m rmse: 0.14392487704753876
[2m[36m(func pid=136934)[0m mae:  0.09762200713157654
[2m[36m(func pid=136934)[0m rmse_per_class: [0.09, 0.249, 0.032, 0.279, 0.067, 0.156, 0.223, 0.106, 0.133, 0.105]
== Status ==
Current time: 2024-01-07 10:12:41 (running for 00:48:02.84)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.284 |  0.144 |                   72 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.233 |  0.146 |                   61 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.1462215930223465
[2m[36m(func pid=138760)[0m mae:  0.08749823272228241
[2m[36m(func pid=138760)[0m rmse_per_class: [0.079, 0.249, 0.026, 0.278, 0.076, 0.152, 0.201, 0.113, 0.134, 0.155]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2695 | Steps: 2 | Val loss: 0.2689 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.2317 | Steps: 2 | Val loss: 0.2706 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 10:12:47 (running for 00:48:08.11)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.27  |  0.144 |                   73 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.234 |  0.146 |                   62 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14406706392765045
[2m[36m(func pid=136934)[0m mae:  0.09766284376382828
[2m[36m(func pid=136934)[0m rmse_per_class: [0.09, 0.249, 0.031, 0.28, 0.067, 0.155, 0.224, 0.106, 0.133, 0.106]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.1463364213705063
[2m[36m(func pid=138760)[0m mae:  0.08782627433538437
[2m[36m(func pid=138760)[0m rmse_per_class: [0.08, 0.253, 0.026, 0.283, 0.07, 0.15, 0.201, 0.113, 0.133, 0.154]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2733 | Steps: 2 | Val loss: 0.2691 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.2356 | Steps: 2 | Val loss: 0.2722 | Batch size: 32 | lr: 0.1 | Duration: 3.27s
== Status ==
Current time: 2024-01-07 10:12:52 (running for 00:48:13.58)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1472500003874302
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.273 |  0.144 |                   74 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.232 |  0.146 |                   63 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14422889053821564
[2m[36m(func pid=136934)[0m mae:  0.09770822525024414
[2m[36m(func pid=136934)[0m rmse_per_class: [0.089, 0.248, 0.031, 0.28, 0.068, 0.154, 0.226, 0.106, 0.134, 0.107]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14651250839233398
[2m[36m(func pid=138760)[0m mae:  0.08841604739427567
[2m[36m(func pid=138760)[0m rmse_per_class: [0.082, 0.26, 0.025, 0.291, 0.065, 0.148, 0.202, 0.109, 0.133, 0.151]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2740 | Steps: 2 | Val loss: 0.2697 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.2288 | Steps: 2 | Val loss: 0.2745 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 10:12:58 (running for 00:48:19.25)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.274 |  0.145 |                   75 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.236 |  0.147 |                   64 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14463281631469727
[2m[36m(func pid=136934)[0m mae:  0.09798093140125275
[2m[36m(func pid=136934)[0m rmse_per_class: [0.09, 0.247, 0.031, 0.281, 0.068, 0.154, 0.228, 0.107, 0.134, 0.107]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14755763113498688
[2m[36m(func pid=138760)[0m mae:  0.08969446271657944
[2m[36m(func pid=138760)[0m rmse_per_class: [0.084, 0.267, 0.024, 0.301, 0.064, 0.147, 0.202, 0.107, 0.132, 0.148]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2673 | Steps: 2 | Val loss: 0.2700 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2413 | Steps: 2 | Val loss: 0.2739 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 10:13:03 (running for 00:48:24.55)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.267 |  0.145 |                   76 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.229 |  0.148 |                   65 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1448073536157608
[2m[36m(func pid=136934)[0m mae:  0.09809562563896179
[2m[36m(func pid=136934)[0m rmse_per_class: [0.09, 0.246, 0.03, 0.282, 0.068, 0.153, 0.228, 0.107, 0.134, 0.108]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14725717902183533
[2m[36m(func pid=138760)[0m mae:  0.08950705826282501
[2m[36m(func pid=138760)[0m rmse_per_class: [0.083, 0.268, 0.024, 0.301, 0.063, 0.149, 0.201, 0.108, 0.135, 0.141]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2610 | Steps: 2 | Val loss: 0.2700 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.2332 | Steps: 2 | Val loss: 0.2700 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=136934)[0m rmse: 0.1447438895702362
[2m[36m(func pid=136934)[0m mae:  0.09806330502033234
[2m[36m(func pid=136934)[0m rmse_per_class: [0.091, 0.246, 0.03, 0.283, 0.069, 0.153, 0.229, 0.108, 0.134, 0.106]
[2m[36m(func pid=136934)[0m 
== Status ==
Current time: 2024-01-07 10:13:08 (running for 00:48:29.93)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.261 |  0.145 |                   77 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.241 |  0.147 |                   66 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.1452532708644867
[2m[36m(func pid=138760)[0m mae:  0.08773627877235413
[2m[36m(func pid=138760)[0m rmse_per_class: [0.079, 0.261, 0.023, 0.292, 0.065, 0.15, 0.2, 0.11, 0.136, 0.137]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2678 | Steps: 2 | Val loss: 0.2701 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 10:13:14 (running for 00:48:35.22)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.268 |  0.145 |                   78 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.233 |  0.145 |                   67 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1449057161808014
[2m[36m(func pid=136934)[0m mae:  0.09809369593858719
[2m[36m(func pid=136934)[0m rmse_per_class: [0.091, 0.246, 0.03, 0.283, 0.069, 0.153, 0.229, 0.108, 0.133, 0.107]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.2274 | Steps: 2 | Val loss: 0.2677 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=138760)[0m rmse: 0.1442975103855133
[2m[36m(func pid=138760)[0m mae:  0.08682554960250854
[2m[36m(func pid=138760)[0m rmse_per_class: [0.079, 0.255, 0.023, 0.286, 0.066, 0.151, 0.2, 0.111, 0.135, 0.136]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2690 | Steps: 2 | Val loss: 0.2695 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 10:13:19 (running for 00:48:40.62)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.269 |  0.145 |                   79 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.227 |  0.144 |                   68 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1445239633321762
[2m[36m(func pid=136934)[0m mae:  0.09755422919988632
[2m[36m(func pid=136934)[0m rmse_per_class: [0.09, 0.247, 0.03, 0.282, 0.069, 0.153, 0.227, 0.108, 0.132, 0.107]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.2477 | Steps: 2 | Val loss: 0.2651 | Batch size: 32 | lr: 0.1 | Duration: 3.27s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2626 | Steps: 2 | Val loss: 0.2690 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=138760)[0m rmse: 0.14335641264915466
[2m[36m(func pid=138760)[0m mae:  0.08582352101802826
[2m[36m(func pid=138760)[0m rmse_per_class: [0.079, 0.247, 0.024, 0.278, 0.066, 0.151, 0.201, 0.11, 0.136, 0.14]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:13:25 (running for 00:48:46.18)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.263 |  0.144 |                   80 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.248 |  0.143 |                   69 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14411941170692444
[2m[36m(func pid=136934)[0m mae:  0.09709332883358002
[2m[36m(func pid=136934)[0m rmse_per_class: [0.09, 0.247, 0.03, 0.281, 0.068, 0.153, 0.225, 0.108, 0.131, 0.107]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.2373 | Steps: 2 | Val loss: 0.2644 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=138760)[0m rmse: 0.14318031072616577
[2m[36m(func pid=138760)[0m mae:  0.08587318658828735
[2m[36m(func pid=138760)[0m rmse_per_class: [0.08, 0.244, 0.025, 0.276, 0.067, 0.151, 0.203, 0.111, 0.137, 0.138]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2665 | Steps: 2 | Val loss: 0.2687 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 10:13:30 (running for 00:48:51.87)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.266 |  0.144 |                   81 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.237 |  0.143 |                   70 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14392083883285522
[2m[36m(func pid=136934)[0m mae:  0.09678353369235992
[2m[36m(func pid=136934)[0m rmse_per_class: [0.09, 0.247, 0.03, 0.28, 0.068, 0.154, 0.224, 0.107, 0.131, 0.108]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2309 | Steps: 2 | Val loss: 0.2646 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=138760)[0m rmse: 0.14377965033054352
[2m[36m(func pid=138760)[0m mae:  0.08697046339511871
[2m[36m(func pid=138760)[0m rmse_per_class: [0.083, 0.243, 0.025, 0.278, 0.066, 0.151, 0.206, 0.112, 0.136, 0.137]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2684 | Steps: 2 | Val loss: 0.2684 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 10:13:36 (running for 00:48:57.21)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.268 |  0.144 |                   82 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.231 |  0.144 |                   71 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14362208545207977
[2m[36m(func pid=136934)[0m mae:  0.09641765058040619
[2m[36m(func pid=136934)[0m rmse_per_class: [0.088, 0.248, 0.03, 0.279, 0.068, 0.154, 0.224, 0.106, 0.131, 0.107]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2388 | Steps: 2 | Val loss: 0.2649 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2617 | Steps: 2 | Val loss: 0.2680 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=138760)[0m rmse: 0.14370495080947876
[2m[36m(func pid=138760)[0m mae:  0.08765225112438202
[2m[36m(func pid=138760)[0m rmse_per_class: [0.084, 0.243, 0.024, 0.283, 0.063, 0.15, 0.208, 0.112, 0.137, 0.133]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:13:41 (running for 00:49:02.85)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.262 |  0.143 |                   83 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.239 |  0.144 |                   72 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14327958226203918
[2m[36m(func pid=136934)[0m mae:  0.09605176746845245
[2m[36m(func pid=136934)[0m rmse_per_class: [0.088, 0.248, 0.03, 0.278, 0.067, 0.154, 0.223, 0.106, 0.131, 0.107]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.2392 | Steps: 2 | Val loss: 0.2654 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2686 | Steps: 2 | Val loss: 0.2681 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=138760)[0m rmse: 0.14343546330928802
[2m[36m(func pid=138760)[0m mae:  0.08763153851032257
[2m[36m(func pid=138760)[0m rmse_per_class: [0.084, 0.243, 0.024, 0.286, 0.061, 0.149, 0.206, 0.11, 0.138, 0.133]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:13:47 (running for 00:49:08.30)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.269 |  0.143 |                   84 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.239 |  0.143 |                   73 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1433647722005844
[2m[36m(func pid=136934)[0m mae:  0.09611259400844574
[2m[36m(func pid=136934)[0m rmse_per_class: [0.088, 0.248, 0.03, 0.278, 0.068, 0.154, 0.225, 0.106, 0.132, 0.106]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2322 | Steps: 2 | Val loss: 0.2676 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2647 | Steps: 2 | Val loss: 0.2683 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=138760)[0m rmse: 0.14454753696918488
[2m[36m(func pid=138760)[0m mae:  0.08829657733440399
[2m[36m(func pid=138760)[0m rmse_per_class: [0.089, 0.244, 0.024, 0.291, 0.062, 0.149, 0.205, 0.11, 0.138, 0.135]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:13:52 (running for 00:49:13.71)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.14649999886751175
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.265 |  0.143 |                   85 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.232 |  0.145 |                   74 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14348745346069336
[2m[36m(func pid=136934)[0m mae:  0.09605099260807037
[2m[36m(func pid=136934)[0m rmse_per_class: [0.087, 0.248, 0.03, 0.278, 0.067, 0.155, 0.224, 0.106, 0.133, 0.108]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.2300 | Steps: 2 | Val loss: 0.2685 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2596 | Steps: 2 | Val loss: 0.2683 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=138760)[0m rmse: 0.1451595574617386
[2m[36m(func pid=138760)[0m mae:  0.08830489218235016
[2m[36m(func pid=138760)[0m rmse_per_class: [0.092, 0.245, 0.024, 0.289, 0.061, 0.15, 0.206, 0.112, 0.138, 0.135]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:13:58 (running for 00:49:19.17)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.26  |  0.143 |                   86 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.23  |  0.145 |                   75 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14343005418777466
[2m[36m(func pid=136934)[0m mae:  0.0960010290145874
[2m[36m(func pid=136934)[0m rmse_per_class: [0.087, 0.248, 0.029, 0.278, 0.067, 0.154, 0.225, 0.106, 0.133, 0.107]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.2313 | Steps: 2 | Val loss: 0.2700 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2649 | Steps: 2 | Val loss: 0.2686 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=138760)[0m rmse: 0.14638657867908478
[2m[36m(func pid=138760)[0m mae:  0.08866630494594574
[2m[36m(func pid=138760)[0m rmse_per_class: [0.094, 0.246, 0.024, 0.287, 0.062, 0.151, 0.207, 0.115, 0.137, 0.14]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:14:03 (running for 00:49:24.52)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.265 |  0.144 |                   87 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.231 |  0.146 |                   76 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14355167746543884
[2m[36m(func pid=136934)[0m mae:  0.09600051492452621
[2m[36m(func pid=136934)[0m rmse_per_class: [0.086, 0.247, 0.029, 0.28, 0.068, 0.153, 0.225, 0.107, 0.133, 0.108]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.2351 | Steps: 2 | Val loss: 0.2711 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2704 | Steps: 2 | Val loss: 0.2687 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=138760)[0m rmse: 0.14717963337898254
[2m[36m(func pid=138760)[0m mae:  0.08885545283555984
[2m[36m(func pid=138760)[0m rmse_per_class: [0.094, 0.248, 0.024, 0.287, 0.065, 0.152, 0.208, 0.118, 0.137, 0.14]
[2m[36m(func pid=138760)[0m 
== Status ==
Current time: 2024-01-07 10:14:08 (running for 00:49:29.82)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.27  |  0.143 |                   88 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.235 |  0.147 |                   77 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14343944191932678
[2m[36m(func pid=136934)[0m mae:  0.09586039930582047
[2m[36m(func pid=136934)[0m rmse_per_class: [0.086, 0.247, 0.029, 0.28, 0.068, 0.152, 0.225, 0.107, 0.132, 0.108]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2371 | Steps: 2 | Val loss: 0.2714 | Batch size: 32 | lr: 0.1 | Duration: 3.36s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2600 | Steps: 2 | Val loss: 0.2686 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=138760)[0m rmse: 0.14780844748020172
[2m[36m(func pid=138760)[0m mae:  0.08895913511514664
[2m[36m(func pid=138760)[0m rmse_per_class: [0.098, 0.249, 0.024, 0.285, 0.064, 0.153, 0.208, 0.119, 0.136, 0.143]
== Status ==
Current time: 2024-01-07 10:14:13 (running for 00:49:34.85)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.27  |  0.143 |                   88 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.235 |  0.147 |                   77 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m rmse: 0.14322176575660706
[2m[36m(func pid=136934)[0m mae:  0.09574662148952484
[2m[36m(func pid=136934)[0m rmse_per_class: [0.087, 0.247, 0.029, 0.28, 0.067, 0.151, 0.225, 0.107, 0.132, 0.107]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.2332 | Steps: 2 | Val loss: 0.2700 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2630 | Steps: 2 | Val loss: 0.2685 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 10:14:19 (running for 00:49:40.28)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.26  |  0.143 |                   89 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.237 |  0.148 |                   78 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14312639832496643
[2m[36m(func pid=136934)[0m mae:  0.09555862843990326
[2m[36m(func pid=136934)[0m rmse_per_class: [0.087, 0.247, 0.029, 0.281, 0.067, 0.15, 0.225, 0.107, 0.131, 0.107]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14671523869037628
[2m[36m(func pid=138760)[0m mae:  0.08829422295093536
[2m[36m(func pid=138760)[0m rmse_per_class: [0.099, 0.247, 0.024, 0.281, 0.064, 0.152, 0.206, 0.114, 0.137, 0.143]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2569 | Steps: 2 | Val loss: 0.2687 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 0.2330 | Steps: 2 | Val loss: 0.2699 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 10:14:24 (running for 00:49:45.55)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.263 |  0.143 |                   90 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.233 |  0.147 |                   79 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14316585659980774
[2m[36m(func pid=136934)[0m mae:  0.09553541243076324
[2m[36m(func pid=136934)[0m rmse_per_class: [0.088, 0.247, 0.029, 0.282, 0.067, 0.151, 0.224, 0.107, 0.132, 0.107]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14629748463630676
[2m[36m(func pid=138760)[0m mae:  0.08836307376623154
[2m[36m(func pid=138760)[0m rmse_per_class: [0.098, 0.248, 0.024, 0.284, 0.066, 0.151, 0.205, 0.11, 0.137, 0.14]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2635 | Steps: 2 | Val loss: 0.2684 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 0.2338 | Steps: 2 | Val loss: 0.2683 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 10:14:30 (running for 00:49:51.26)
Memory usage on this node: 19.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.257 |  0.143 |                   91 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.233 |  0.146 |                   80 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.143106147646904
[2m[36m(func pid=136934)[0m mae:  0.09520252794027328
[2m[36m(func pid=136934)[0m rmse_per_class: [0.087, 0.247, 0.029, 0.282, 0.067, 0.151, 0.223, 0.107, 0.131, 0.109]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.1450735628604889
[2m[36m(func pid=138760)[0m mae:  0.08783949911594391
[2m[36m(func pid=138760)[0m rmse_per_class: [0.091, 0.249, 0.024, 0.284, 0.068, 0.151, 0.203, 0.109, 0.136, 0.137]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2593 | Steps: 2 | Val loss: 0.2686 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.2363 | Steps: 2 | Val loss: 0.2669 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 10:14:35 (running for 00:49:56.85)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.263 |  0.143 |                   92 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.234 |  0.145 |                   81 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14317429065704346
[2m[36m(func pid=136934)[0m mae:  0.09516972303390503
[2m[36m(func pid=136934)[0m rmse_per_class: [0.085, 0.247, 0.029, 0.283, 0.067, 0.15, 0.223, 0.107, 0.131, 0.11]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14429602026939392
[2m[36m(func pid=138760)[0m mae:  0.08739493042230606
[2m[36m(func pid=138760)[0m rmse_per_class: [0.086, 0.247, 0.024, 0.28, 0.07, 0.151, 0.202, 0.109, 0.136, 0.137]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2567 | Steps: 2 | Val loss: 0.2687 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.2338 | Steps: 2 | Val loss: 0.2660 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 10:14:41 (running for 00:50:02.38)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.259 |  0.143 |                   93 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.236 |  0.144 |                   82 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14315477013587952
[2m[36m(func pid=136934)[0m mae:  0.0950499027967453
[2m[36m(func pid=136934)[0m rmse_per_class: [0.084, 0.247, 0.028, 0.282, 0.066, 0.151, 0.223, 0.107, 0.132, 0.111]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14416426420211792
[2m[36m(func pid=138760)[0m mae:  0.08739487081766129
[2m[36m(func pid=138760)[0m rmse_per_class: [0.083, 0.245, 0.025, 0.277, 0.074, 0.152, 0.204, 0.109, 0.136, 0.136]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2635 | Steps: 2 | Val loss: 0.2681 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.2310 | Steps: 2 | Val loss: 0.2654 | Batch size: 32 | lr: 0.1 | Duration: 3.31s
== Status ==
Current time: 2024-01-07 10:14:46 (running for 00:50:07.76)
Memory usage on this node: 19.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.257 |  0.143 |                   94 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.234 |  0.144 |                   83 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14291957020759583
[2m[36m(func pid=136934)[0m mae:  0.09457162022590637
[2m[36m(func pid=136934)[0m rmse_per_class: [0.083, 0.247, 0.028, 0.28, 0.066, 0.151, 0.221, 0.107, 0.131, 0.114]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14394685626029968
[2m[36m(func pid=138760)[0m mae:  0.08736992627382278
[2m[36m(func pid=138760)[0m rmse_per_class: [0.081, 0.244, 0.025, 0.276, 0.075, 0.153, 0.206, 0.11, 0.137, 0.132]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2622 | Steps: 2 | Val loss: 0.2682 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.2367 | Steps: 2 | Val loss: 0.2652 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 10:14:52 (running for 00:50:13.49)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.262 |  0.143 |                   96 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.231 |  0.144 |                   84 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14314666390419006
[2m[36m(func pid=136934)[0m mae:  0.0945533812046051
[2m[36m(func pid=136934)[0m rmse_per_class: [0.084, 0.248, 0.028, 0.281, 0.067, 0.151, 0.219, 0.106, 0.131, 0.116]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14354680478572845
[2m[36m(func pid=138760)[0m mae:  0.08720772713422775
[2m[36m(func pid=138760)[0m rmse_per_class: [0.08, 0.245, 0.025, 0.279, 0.073, 0.152, 0.207, 0.11, 0.136, 0.131]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2568 | Steps: 2 | Val loss: 0.2679 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.2318 | Steps: 2 | Val loss: 0.2654 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 10:14:57 (running for 00:50:18.93)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.257 |  0.143 |                   97 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.237 |  0.144 |                   85 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.14294449985027313
[2m[36m(func pid=136934)[0m mae:  0.0942707434296608
[2m[36m(func pid=136934)[0m rmse_per_class: [0.084, 0.25, 0.028, 0.281, 0.067, 0.151, 0.217, 0.106, 0.13, 0.115]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14275702834129333
[2m[36m(func pid=138760)[0m mae:  0.08714865148067474
[2m[36m(func pid=138760)[0m rmse_per_class: [0.082, 0.246, 0.024, 0.284, 0.068, 0.15, 0.206, 0.109, 0.134, 0.126]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2550 | Steps: 2 | Val loss: 0.2680 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.2336 | Steps: 2 | Val loss: 0.2678 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 10:15:03 (running for 00:50:24.46)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.255 |  0.143 |                   98 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.232 |  0.143 |                   86 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=136934)[0m rmse: 0.1431635320186615
[2m[36m(func pid=136934)[0m mae:  0.09428513050079346
[2m[36m(func pid=136934)[0m rmse_per_class: [0.085, 0.251, 0.028, 0.281, 0.067, 0.152, 0.217, 0.105, 0.129, 0.116]
[2m[36m(func pid=136934)[0m 
[2m[36m(func pid=138760)[0m rmse: 0.14353428781032562
[2m[36m(func pid=138760)[0m mae:  0.0880669504404068
[2m[36m(func pid=138760)[0m rmse_per_class: [0.083, 0.25, 0.023, 0.293, 0.066, 0.149, 0.204, 0.108, 0.134, 0.126]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2602 | Steps: 2 | Val loss: 0.2679 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.2271 | Steps: 2 | Val loss: 0.2697 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=136934)[0m rmse: 0.14305657148361206
[2m[36m(func pid=136934)[0m mae:  0.09404533356428146
[2m[36m(func pid=136934)[0m rmse_per_class: [0.084, 0.252, 0.028, 0.282, 0.067, 0.152, 0.215, 0.105, 0.129, 0.115]
[2m[36m(func pid=136934)[0m 
== Status ==
Current time: 2024-01-07 10:15:08 (running for 00:50:29.99)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00022 | RUNNING    | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.26  |  0.143 |                   99 |
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.234 |  0.144 |                   87 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.14423857629299164
[2m[36m(func pid=138760)[0m mae:  0.08855454623699188
[2m[36m(func pid=138760)[0m rmse_per_class: [0.084, 0.253, 0.023, 0.298, 0.067, 0.148, 0.202, 0.106, 0.133, 0.127]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=136934)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2576 | Steps: 2 | Val loss: 0.2679 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.2358 | Steps: 2 | Val loss: 0.2714 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=136934)[0m rmse: 0.14308828115463257
[2m[36m(func pid=136934)[0m mae:  0.09404262900352478
[2m[36m(func pid=136934)[0m rmse_per_class: [0.084, 0.25, 0.028, 0.282, 0.067, 0.152, 0.216, 0.106, 0.13, 0.116]
== Status ==
Current time: 2024-01-07 10:15:14 (running for 00:50:35.16)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.227 |  0.144 |                   88 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
| train_32e5a_00018 | TERMINATED | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.249 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.14534536004066467
[2m[36m(func pid=138760)[0m mae:  0.08874009549617767
[2m[36m(func pid=138760)[0m rmse_per_class: [0.087, 0.254, 0.024, 0.298, 0.066, 0.149, 0.201, 0.105, 0.135, 0.134]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.2315 | Steps: 2 | Val loss: 0.2715 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 10:15:20 (running for 00:50:41.44)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.236 |  0.145 |                   89 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
| train_32e5a_00018 | TERMINATED | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.249 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.14608418941497803
[2m[36m(func pid=138760)[0m mae:  0.08861654251813889
[2m[36m(func pid=138760)[0m rmse_per_class: [0.087, 0.253, 0.024, 0.294, 0.068, 0.15, 0.201, 0.107, 0.135, 0.14]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.2293 | Steps: 2 | Val loss: 0.2718 | Batch size: 32 | lr: 0.1 | Duration: 3.28s
== Status ==
Current time: 2024-01-07 10:15:25 (running for 00:50:47.05)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.231 |  0.146 |                   90 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
| train_32e5a_00018 | TERMINATED | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.249 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.14719097316265106
[2m[36m(func pid=138760)[0m mae:  0.088730089366436
[2m[36m(func pid=138760)[0m rmse_per_class: [0.092, 0.251, 0.024, 0.29, 0.068, 0.152, 0.201, 0.113, 0.135, 0.145]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2325 | Steps: 2 | Val loss: 0.2728 | Batch size: 32 | lr: 0.1 | Duration: 3.25s
== Status ==
Current time: 2024-01-07 10:15:32 (running for 00:50:53.10)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.229 |  0.147 |                   91 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
| train_32e5a_00018 | TERMINATED | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.249 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.1486317366361618
[2m[36m(func pid=138760)[0m mae:  0.0892053097486496
[2m[36m(func pid=138760)[0m rmse_per_class: [0.095, 0.25, 0.024, 0.289, 0.071, 0.153, 0.202, 0.115, 0.137, 0.15]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2273 | Steps: 2 | Val loss: 0.2705 | Batch size: 32 | lr: 0.1 | Duration: 3.28s
== Status ==
Current time: 2024-01-07 10:15:37 (running for 00:50:58.82)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.232 |  0.149 |                   92 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
| train_32e5a_00018 | TERMINATED | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.249 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.14715521037578583
[2m[36m(func pid=138760)[0m mae:  0.08822020888328552
[2m[36m(func pid=138760)[0m rmse_per_class: [0.093, 0.247, 0.025, 0.283, 0.071, 0.153, 0.203, 0.113, 0.136, 0.149]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.2277 | Steps: 2 | Val loss: 0.2710 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 10:15:43 (running for 00:51:04.66)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.227 |  0.147 |                   93 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
| train_32e5a_00018 | TERMINATED | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.249 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.14773690700531006
[2m[36m(func pid=138760)[0m mae:  0.08860112726688385
[2m[36m(func pid=138760)[0m rmse_per_class: [0.096, 0.246, 0.026, 0.283, 0.07, 0.152, 0.205, 0.112, 0.135, 0.153]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2330 | Steps: 2 | Val loss: 0.2697 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 10:15:49 (running for 00:51:10.30)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.228 |  0.148 |                   94 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
| train_32e5a_00018 | TERMINATED | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.249 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.14696185290813446
[2m[36m(func pid=138760)[0m mae:  0.08796261250972748
[2m[36m(func pid=138760)[0m rmse_per_class: [0.097, 0.244, 0.026, 0.28, 0.067, 0.151, 0.205, 0.111, 0.135, 0.155]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2291 | Steps: 2 | Val loss: 0.2702 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
== Status ==
Current time: 2024-01-07 10:15:54 (running for 00:51:15.98)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.233 |  0.147 |                   95 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
| train_32e5a_00018 | TERMINATED | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.249 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.1476483792066574
[2m[36m(func pid=138760)[0m mae:  0.08852840214967728
[2m[36m(func pid=138760)[0m rmse_per_class: [0.098, 0.244, 0.026, 0.281, 0.069, 0.152, 0.206, 0.11, 0.135, 0.157]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.2371 | Steps: 2 | Val loss: 0.2694 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 10:16:00 (running for 00:51:21.73)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.229 |  0.148 |                   96 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
| train_32e5a_00018 | TERMINATED | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.249 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.1471228301525116
[2m[36m(func pid=138760)[0m mae:  0.08852903544902802
[2m[36m(func pid=138760)[0m rmse_per_class: [0.097, 0.245, 0.025, 0.282, 0.067, 0.151, 0.205, 0.108, 0.136, 0.154]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.2292 | Steps: 2 | Val loss: 0.2698 | Batch size: 32 | lr: 0.1 | Duration: 3.25s
== Status ==
Current time: 2024-01-07 10:16:06 (running for 00:51:27.45)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.237 |  0.147 |                   97 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
| train_32e5a_00018 | TERMINATED | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.249 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.14724045991897583
[2m[36m(func pid=138760)[0m mae:  0.08921657502651215
[2m[36m(func pid=138760)[0m rmse_per_class: [0.098, 0.248, 0.024, 0.288, 0.065, 0.15, 0.204, 0.108, 0.137, 0.15]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.2345 | Steps: 2 | Val loss: 0.2705 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
== Status ==
Current time: 2024-01-07 10:16:12 (running for 00:51:33.45)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.229 |  0.147 |                   98 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
| train_32e5a_00018 | TERMINATED | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.249 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=138760)[0m rmse: 0.14753180742263794
[2m[36m(func pid=138760)[0m mae:  0.08979053795337677
[2m[36m(func pid=138760)[0m rmse_per_class: [0.095, 0.252, 0.024, 0.292, 0.066, 0.15, 0.204, 0.108, 0.137, 0.148]
[2m[36m(func pid=138760)[0m 
[2m[36m(func pid=138760)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.2293 | Steps: 2 | Val loss: 0.2690 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 10:16:18 (running for 00:51:39.38)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00023 | RUNNING    | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.235 |  0.148 |                   99 |
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
| train_32e5a_00018 | TERMINATED | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.249 |  0.157 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 10:16:19 (running for 00:51:40.07)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: -0.1457499973475933
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/120.04 GiB heap, 0.0/55.44 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   rmse |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------|
| train_32e5a_00000 | TERMINATED | 192.168.7.53:34042  | 0.0001 |       0.99 |         0      |  0.436 |  0.175 |                   75 |
| train_32e5a_00001 | TERMINATED | 192.168.7.53:34415  | 0.001  |       0.99 |         0      |  0.308 |  0.142 |                  100 |
| train_32e5a_00002 | TERMINATED | 192.168.7.53:34838  | 0.01   |       0.99 |         0      |  0.254 |  0.17  |                   75 |
| train_32e5a_00003 | TERMINATED | 192.168.7.53:35262  | 0.1    |       0.99 |         0      |  0.252 |  0.162 |                  100 |
| train_32e5a_00004 | TERMINATED | 192.168.7.53:51709  | 0.0001 |       0.9  |         0      |  0.737 |  0.179 |                   75 |
| train_32e5a_00005 | TERMINATED | 192.168.7.53:51774  | 0.001  |       0.9  |         0      |  0.398 |  0.17  |                   75 |
| train_32e5a_00006 | TERMINATED | 192.168.7.53:57346  | 0.01   |       0.9  |         0      |  0.258 |  0.14  |                  100 |
| train_32e5a_00007 | TERMINATED | 192.168.7.53:57869  | 0.1    |       0.9  |         0      |  0.238 |  0.141 |                  100 |
| train_32e5a_00008 | TERMINATED | 192.168.7.53:69319  | 0.0001 |       0.99 |         0.0001 |  0.435 |  0.175 |                   75 |
| train_32e5a_00009 | TERMINATED | 192.168.7.53:69905  | 0.001  |       0.99 |         0.0001 |  0.379 |  0.148 |                   75 |
| train_32e5a_00010 | TERMINATED | 192.168.7.53:79815  | 0.01   |       0.99 |         0.0001 |  0.262 |  0.157 |                   75 |
| train_32e5a_00011 | TERMINATED | 192.168.7.53:80805  | 0.1    |       0.99 |         0.0001 |  0.24  |  0.171 |                   75 |
| train_32e5a_00012 | TERMINATED | 192.168.7.53:86819  | 0.0001 |       0.9  |         0.0001 |  0.74  |  0.179 |                   75 |
| train_32e5a_00013 | TERMINATED | 192.168.7.53:87355  | 0.001  |       0.9  |         0.0001 |  0.395 |  0.17  |                   75 |
| train_32e5a_00014 | TERMINATED | 192.168.7.53:97073  | 0.01   |       0.9  |         0.0001 |  0.252 |  0.142 |                  100 |
| train_32e5a_00015 | TERMINATED | 192.168.7.53:98661  | 0.1    |       0.9  |         0.0001 |  0.226 |  0.143 |                  100 |
| train_32e5a_00016 | TERMINATED | 192.168.7.53:104571 | 0.0001 |       0.99 |         1e-05  |  0.435 |  0.175 |                   75 |
| train_32e5a_00017 | TERMINATED | 192.168.7.53:105127 | 0.001  |       0.99 |         1e-05  |  0.369 |  0.147 |                   75 |
| train_32e5a_00018 | TERMINATED | 192.168.7.53:119733 | 0.01   |       0.99 |         1e-05  |  0.249 |  0.157 |                   75 |
| train_32e5a_00019 | TERMINATED | 192.168.7.53:121379 | 0.1    |       0.99 |         1e-05  |  0.27  |  0.183 |                   75 |
| train_32e5a_00020 | TERMINATED | 192.168.7.53:121978 | 0.0001 |       0.9  |         1e-05  |  0.738 |  0.179 |                   75 |
| train_32e5a_00021 | TERMINATED | 192.168.7.53:123100 | 0.001  |       0.9  |         1e-05  |  0.397 |  0.17  |                   75 |
| train_32e5a_00022 | TERMINATED | 192.168.7.53:136934 | 0.01   |       0.9  |         1e-05  |  0.258 |  0.143 |                  100 |
| train_32e5a_00023 | TERMINATED | 192.168.7.53:138760 | 0.1    |       0.9  |         1e-05  |  0.229 |  0.146 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+--------+----------------------+


2024-01-07 10:16:19,025	INFO tune.py:798 -- Total run time: 3101.17 seconds (3100.05 seconds for the tuning loop).
[2m[36m(func pid=138760)[0m rmse: 0.14640089869499207
[2m[36m(func pid=138760)[0m mae:  0.08909884840250015
[2m[36m(func pid=138760)[0m rmse_per_class: [0.091, 0.25, 0.024, 0.289, 0.066, 0.15, 0.204, 0.108, 0.137, 0.145]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341344.1 ON aap04 CANCELLED AT 2024-01-07T10:16:26 ***
srun: error: aap04: task 0: Exited with exit code 1

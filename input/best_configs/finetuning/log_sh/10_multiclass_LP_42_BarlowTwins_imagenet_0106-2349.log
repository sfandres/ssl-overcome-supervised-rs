IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 11:57:06,056	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 11:57:06,057	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 11:57:08,594	SUCC scripts.py:747 -- --------------------
2024-01-07 11:57:08,594	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 11:57:08,595	SUCC scripts.py:749 -- --------------------
2024-01-07 11:57:08,595	INFO scripts.py:751 -- Next steps
2024-01-07 11:57:08,595	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 11:57:08,595	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 11:57:08,595	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 11:57:08,595	INFO scripts.py:773 -- import ray
2024-01-07 11:57:08,595	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 11:57:08,595	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 11:57:08,595	INFO scripts.py:791 --   ray status
2024-01-07 11:57:08,595	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 11:57:08,595	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 11:57:08,595	INFO scripts.py:810 --   ray stop
2024-01-07 11:57:08,596	INFO scripts.py:891 -- --block
2024-01-07 11:57:08,596	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 11:57:08,596	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              16234530356162647100
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7fd2969a70d0>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          BarlowTwins
task_name:           multiclass
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          10
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         random
seed:                42
dropout:             None
transfer_learning:   LP
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1

Initial imbalanced dataset:
Diff. classes --> [ 1 21 22 23 31 35 41 42 47 51]
Samples/class --> [10 10 10 10 10 10 10 10 10 10]

Creating the sample distribution plot...
Sample distribution computation in train dataset (s): 1.87
Resulting balanced dataloader:
Diff. classes     --> [0 1 2 3 4 5 6 7 8 9]
New samples/class --> [10 10 10 10 10 10 10 10 10 10]
Done!

Model resnet18 with pretrained weights using BarlowTwins SSL
Model loaded from snapshot_BarlowTwins_resnet18_bd=False_iw=random.pt
Model name:        BarlowTwins
Backbone name:     resnet18
Hidden layer dim.: 256
Output layer dim.: 128
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Linear probing adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 11:57:52,893	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 11:57:52,906	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 11:58:12,096	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-07 11:58:12 (running for 00:00:18.18)
Memory usage on this node: 13.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |
| train_9b9e8_00001 | PENDING  |                    | 0.001  |       0.99 |         0      |
| train_9b9e8_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_9b9e8_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21163)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=21163)[0m Configuration completed!
[2m[36m(func pid=21163)[0m New optimizer parameters:
[2m[36m(func pid=21163)[0m SGD (
[2m[36m(func pid=21163)[0m Parameter Group 0
[2m[36m(func pid=21163)[0m     dampening: 0
[2m[36m(func pid=21163)[0m     differentiable: False
[2m[36m(func pid=21163)[0m     foreach: None
[2m[36m(func pid=21163)[0m     lr: 0.0001
[2m[36m(func pid=21163)[0m     maximize: False
[2m[36m(func pid=21163)[0m     momentum: 0.99
[2m[36m(func pid=21163)[0m     nesterov: False
[2m[36m(func pid=21163)[0m     weight_decay: 0
[2m[36m(func pid=21163)[0m )
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9943 | Steps: 4 | Val loss: 2.3239 | Batch size: 32 | lr: 0.0001 | Duration: 4.51s
[2m[36m(func pid=21163)[0m top1: 0.18097014925373134
[2m[36m(func pid=21163)[0m top5: 0.5321828358208955
[2m[36m(func pid=21163)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=21163)[0m f1_macro: 0.10693067494332138
[2m[36m(func pid=21163)[0m f1_weighted: 0.12987990603984279
[2m[36m(func pid=21163)[0m f1_per_class: [0.2, 0.318, 0.0, 0.117, 0.01, 0.282, 0.012, 0.024, 0.0, 0.107]
== Status ==
Current time: 2024-01-07 11:58:22 (running for 00:00:28.30)
Memory usage on this node: 15.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |
| train_9b9e8_00002 | PENDING  |                    | 0.01   |       0.99 |         0      |
| train_9b9e8_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21543)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=21543)[0m Configuration completed!
[2m[36m(func pid=21543)[0m New optimizer parameters:
[2m[36m(func pid=21543)[0m SGD (
[2m[36m(func pid=21543)[0m Parameter Group 0
[2m[36m(func pid=21543)[0m     dampening: 0
[2m[36m(func pid=21543)[0m     differentiable: False
[2m[36m(func pid=21543)[0m     foreach: None
[2m[36m(func pid=21543)[0m     lr: 0.001
[2m[36m(func pid=21543)[0m     maximize: False
[2m[36m(func pid=21543)[0m     momentum: 0.99
[2m[36m(func pid=21543)[0m     nesterov: False
[2m[36m(func pid=21543)[0m     weight_decay: 0
[2m[36m(func pid=21543)[0m )
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0254 | Steps: 4 | Val loss: 2.2954 | Batch size: 32 | lr: 0.001 | Duration: 4.54s
[2m[36m(func pid=21543)[0m top1: 0.1884328358208955
[2m[36m(func pid=21543)[0m top5: 0.554570895522388
[2m[36m(func pid=21543)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=21543)[0m f1_macro: 0.11299766050927144
[2m[36m(func pid=21543)[0m f1_weighted: 0.13790588254788985
[2m[36m(func pid=21543)[0m f1_per_class: [0.244, 0.327, 0.0, 0.129, 0.011, 0.285, 0.021, 0.012, 0.0, 0.102]
== Status ==
Current time: 2024-01-07 11:58:31 (running for 00:00:37.59)
Memory usage on this node: 18.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |
| train_9b9e8_00003 | PENDING  |                    | 0.1    |       0.99 |         0      |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21969)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21969)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=21969)[0m Configuration completed!
[2m[36m(func pid=21969)[0m New optimizer parameters:
[2m[36m(func pid=21969)[0m SGD (
[2m[36m(func pid=21969)[0m Parameter Group 0
[2m[36m(func pid=21969)[0m     dampening: 0
[2m[36m(func pid=21969)[0m     differentiable: False
[2m[36m(func pid=21969)[0m     foreach: None
[2m[36m(func pid=21969)[0m     lr: 0.01
[2m[36m(func pid=21969)[0m     maximize: False
[2m[36m(func pid=21969)[0m     momentum: 0.99
[2m[36m(func pid=21969)[0m     nesterov: False
[2m[36m(func pid=21969)[0m     weight_decay: 0
[2m[36m(func pid=21969)[0m )
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9884 | Steps: 4 | Val loss: 2.2837 | Batch size: 32 | lr: 0.01 | Duration: 4.94s
[2m[36m(func pid=21969)[0m top1: 0.18563432835820895
[2m[36m(func pid=21969)[0m top5: 0.5573694029850746
[2m[36m(func pid=21969)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=21969)[0m f1_macro: 0.13217165427003902
[2m[36m(func pid=21969)[0m f1_weighted: 0.15892512142951323
[2m[36m(func pid=21969)[0m f1_per_class: [0.146, 0.314, 0.049, 0.211, 0.036, 0.193, 0.021, 0.22, 0.0, 0.132]
== Status ==
Current time: 2024-01-07 11:58:40 (running for 00:00:46.25)
Memory usage on this node: 20.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |
|-------------------+----------+--------------------+--------+------------+----------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+--------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 11:58:48 (running for 00:00:54.71)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.994 |      0.107 |                    1 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |        |            |                      |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |        |            |                      |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |        |            |                      |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=22392)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=22392)[0m Configuration completed!
[2m[36m(func pid=22392)[0m New optimizer parameters:
[2m[36m(func pid=22392)[0m SGD (
[2m[36m(func pid=22392)[0m Parameter Group 0
[2m[36m(func pid=22392)[0m     dampening: 0
[2m[36m(func pid=22392)[0m     differentiable: False
[2m[36m(func pid=22392)[0m     foreach: None
[2m[36m(func pid=22392)[0m     lr: 0.1
[2m[36m(func pid=22392)[0m     maximize: False
[2m[36m(func pid=22392)[0m     momentum: 0.99
[2m[36m(func pid=22392)[0m     nesterov: False
[2m[36m(func pid=22392)[0m     weight_decay: 0
[2m[36m(func pid=22392)[0m )
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9715 | Steps: 4 | Val loss: 2.2804 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.6988 | Steps: 4 | Val loss: 2.0952 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9868 | Steps: 4 | Val loss: 2.3405 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9387 | Steps: 4 | Val loss: 2.7685 | Batch size: 32 | lr: 0.1 | Duration: 4.65s
== Status ==
Current time: 2024-01-07 11:58:53 (running for 00:00:59.75)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.994 |      0.107 |                    1 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  3.025 |      0.113 |                    1 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  2.988 |      0.132 |                    1 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |        |            |                      |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.17024253731343283
[2m[36m(func pid=21163)[0m top5: 0.5102611940298507
[2m[36m(func pid=21163)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=21163)[0m f1_macro: 0.08949303684275592
[2m[36m(func pid=21163)[0m f1_weighted: 0.1231483032911688
[2m[36m(func pid=21163)[0m f1_per_class: [0.079, 0.291, 0.0, 0.115, 0.009, 0.311, 0.006, 0.019, 0.0, 0.065]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.19402985074626866
[2m[36m(func pid=21543)[0m top5: 0.5760261194029851
[2m[36m(func pid=21543)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=21543)[0m f1_macro: 0.12020188324457066
[2m[36m(func pid=21543)[0m f1_weighted: 0.1528358835125522
[2m[36m(func pid=21543)[0m f1_per_class: [0.289, 0.313, 0.0, 0.149, 0.0, 0.317, 0.046, 0.011, 0.0, 0.077]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m top1: 0.22014925373134328
[2m[36m(func pid=21969)[0m top5: 0.7952425373134329
[2m[36m(func pid=21969)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=21969)[0m f1_macro: 0.17256705660700505
[2m[36m(func pid=21969)[0m f1_weighted: 0.17879845521187793
[2m[36m(func pid=21969)[0m f1_per_class: [0.12, 0.011, 0.417, 0.37, 0.105, 0.024, 0.162, 0.236, 0.0, 0.281]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.09701492537313433
[2m[36m(func pid=22392)[0m top5: 0.4771455223880597
[2m[36m(func pid=22392)[0m f1_micro: 0.09701492537313433
[2m[36m(func pid=22392)[0m f1_macro: 0.15852376152794306
[2m[36m(func pid=22392)[0m f1_weighted: 0.07666587370687283
[2m[36m(func pid=22392)[0m f1_per_class: [0.058, 0.0, 0.741, 0.166, 0.0, 0.0, 0.0, 0.377, 0.0, 0.244]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.3984 | Steps: 4 | Val loss: 1.8616 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8736 | Steps: 4 | Val loss: 2.2486 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9620 | Steps: 4 | Val loss: 2.3452 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.4567 | Steps: 4 | Val loss: 2.9521 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 11:58:59 (running for 00:01:05.33)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.987 |      0.089 |                    2 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  2.972 |      0.12  |                    2 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  2.398 |      0.284 |                    3 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  2.939 |      0.159 |                    1 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21969)[0m top1: 0.3931902985074627
[2m[36m(func pid=21969)[0m top5: 0.8227611940298507
[2m[36m(func pid=21969)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=21969)[0m f1_macro: 0.2843514109386413
[2m[36m(func pid=21969)[0m f1_weighted: 0.3260440756245367
[2m[36m(func pid=21969)[0m f1_per_class: [0.378, 0.0, 0.558, 0.553, 0.121, 0.239, 0.375, 0.292, 0.0, 0.328]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21163)[0m top1: 0.15904850746268656
[2m[36m(func pid=21163)[0m top5: 0.503731343283582
[2m[36m(func pid=21163)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=21163)[0m f1_macro: 0.08459602594464109
[2m[36m(func pid=21163)[0m f1_weighted: 0.12376875066398918
[2m[36m(func pid=21163)[0m f1_per_class: [0.052, 0.268, 0.0, 0.119, 0.007, 0.307, 0.018, 0.042, 0.0, 0.032]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.2196828358208955
[2m[36m(func pid=21543)[0m top5: 0.6152052238805971
[2m[36m(func pid=21543)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=21543)[0m f1_macro: 0.14725505516387857
[2m[36m(func pid=21543)[0m f1_weighted: 0.19226191927997563
[2m[36m(func pid=21543)[0m f1_per_class: [0.255, 0.324, 0.143, 0.199, 0.0, 0.344, 0.109, 0.033, 0.028, 0.038]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m top1: 0.20848880597014927
[2m[36m(func pid=22392)[0m top5: 0.6100746268656716
[2m[36m(func pid=22392)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=22392)[0m f1_macro: 0.20433011692367736
[2m[36m(func pid=22392)[0m f1_weighted: 0.14235789651559
[2m[36m(func pid=22392)[0m f1_per_class: [0.201, 0.349, 0.471, 0.007, 0.086, 0.379, 0.024, 0.336, 0.028, 0.164]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7912 | Steps: 4 | Val loss: 2.1989 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9552 | Steps: 4 | Val loss: 2.3410 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.0001 | Steps: 4 | Val loss: 1.9422 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.1971 | Steps: 4 | Val loss: 3.9697 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:59:04 (running for 00:01:10.76)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.962 |      0.085 |                    3 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  2.791 |      0.177 |                    4 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  2.398 |      0.284 |                    3 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  2.457 |      0.204 |                    2 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m top1: 0.24580223880597016
[2m[36m(func pid=21543)[0m top5: 0.6893656716417911
[2m[36m(func pid=21543)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=21543)[0m f1_macro: 0.17729498018020906
[2m[36m(func pid=21543)[0m f1_weighted: 0.23001521126805555
[2m[36m(func pid=21543)[0m f1_per_class: [0.326, 0.323, 0.209, 0.258, 0.0, 0.376, 0.151, 0.087, 0.043, 0.0]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m top1: 0.15811567164179105
[2m[36m(func pid=21163)[0m top5: 0.4976679104477612
[2m[36m(func pid=21163)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=21163)[0m f1_macro: 0.09085705898587539
[2m[36m(func pid=21163)[0m f1_weighted: 0.13354751509606982
[2m[36m(func pid=21163)[0m f1_per_class: [0.04, 0.27, 0.047, 0.144, 0.007, 0.295, 0.026, 0.064, 0.015, 0.0]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.23740671641791045
[2m[36m(func pid=21969)[0m top5: 0.8073694029850746
[2m[36m(func pid=21969)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=21969)[0m f1_macro: 0.20313508094825705
[2m[36m(func pid=21969)[0m f1_weighted: 0.23626841384837985
[2m[36m(func pid=21969)[0m f1_per_class: [0.163, 0.044, 0.312, 0.424, 0.088, 0.326, 0.159, 0.278, 0.051, 0.187]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.2677238805970149
[2m[36m(func pid=22392)[0m top5: 0.6870335820895522
[2m[36m(func pid=22392)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=22392)[0m f1_macro: 0.19830994670382335
[2m[36m(func pid=22392)[0m f1_weighted: 0.2818704632579687
[2m[36m(func pid=22392)[0m f1_per_class: [0.318, 0.082, 0.082, 0.148, 0.056, 0.411, 0.534, 0.179, 0.027, 0.146]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7295 | Steps: 4 | Val loss: 2.1612 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9608 | Steps: 4 | Val loss: 2.3388 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.8123 | Steps: 4 | Val loss: 2.2365 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7307 | Steps: 4 | Val loss: 5.8116 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 11:59:09 (running for 00:01:15.90)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.955 |      0.091 |                    4 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  2.729 |      0.177 |                    5 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  2     |      0.203 |                    4 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  2.197 |      0.198 |                    3 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m top1: 0.25466417910447764
[2m[36m(func pid=21543)[0m top5: 0.7332089552238806
[2m[36m(func pid=21543)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=21543)[0m f1_macro: 0.1772392058643639
[2m[36m(func pid=21543)[0m f1_weighted: 0.24515036799622697
[2m[36m(func pid=21543)[0m f1_per_class: [0.267, 0.328, 0.137, 0.313, 0.043, 0.382, 0.145, 0.114, 0.045, 0.0]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m top1: 0.14692164179104478
[2m[36m(func pid=21163)[0m top5: 0.507929104477612
[2m[36m(func pid=21163)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=21163)[0m f1_macro: 0.0982259814367411
[2m[36m(func pid=21163)[0m f1_weighted: 0.13361485856278804
[2m[36m(func pid=21163)[0m f1_per_class: [0.065, 0.24, 0.093, 0.157, 0.007, 0.272, 0.035, 0.078, 0.013, 0.022]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.18470149253731344
[2m[36m(func pid=21969)[0m top5: 0.7094216417910447
[2m[36m(func pid=21969)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=21969)[0m f1_macro: 0.19704045345225926
[2m[36m(func pid=21969)[0m f1_weighted: 0.15425926029889256
[2m[36m(func pid=21969)[0m f1_per_class: [0.202, 0.209, 0.304, 0.054, 0.078, 0.42, 0.076, 0.343, 0.136, 0.149]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.251865671641791
[2m[36m(func pid=22392)[0m top5: 0.7378731343283582
[2m[36m(func pid=22392)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=22392)[0m f1_macro: 0.22285540932459175
[2m[36m(func pid=22392)[0m f1_weighted: 0.2760523649194668
[2m[36m(func pid=22392)[0m f1_per_class: [0.148, 0.011, 0.074, 0.33, 0.143, 0.436, 0.337, 0.351, 0.108, 0.29]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6955 | Steps: 4 | Val loss: 2.1210 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9327 | Steps: 4 | Val loss: 2.3213 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.3112 | Steps: 4 | Val loss: 2.3258 | Batch size: 32 | lr: 0.01 | Duration: 2.72s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.4330 | Steps: 4 | Val loss: 5.0899 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=21543)[0m top1: 0.2733208955223881
[2m[36m(func pid=21543)[0m top5: 0.7527985074626866
[2m[36m(func pid=21543)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=21543)[0m f1_macro: 0.2282017118193349
[2m[36m(func pid=21543)[0m f1_weighted: 0.26095269097520357
[2m[36m(func pid=21543)[0m f1_per_class: [0.327, 0.369, 0.124, 0.323, 0.047, 0.396, 0.118, 0.234, 0.069, 0.275]
[2m[36m(func pid=21543)[0m 
== Status ==
Current time: 2024-01-07 11:59:15 (running for 00:01:21.14)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.961 |      0.098 |                    5 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  2.696 |      0.228 |                    6 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.812 |      0.197 |                    5 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  2.731 |      0.223 |                    4 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.14225746268656717
[2m[36m(func pid=21163)[0m top5: 0.5307835820895522
[2m[36m(func pid=21163)[0m f1_micro: 0.14225746268656717
[2m[36m(func pid=21163)[0m f1_macro: 0.09386063485706005
[2m[36m(func pid=21163)[0m f1_weighted: 0.13641673308950994
[2m[36m(func pid=21163)[0m f1_per_class: [0.06, 0.213, 0.073, 0.159, 0.007, 0.268, 0.059, 0.088, 0.012, 0.0]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.22434701492537312
[2m[36m(func pid=21969)[0m top5: 0.7430037313432836
[2m[36m(func pid=21969)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=21969)[0m f1_macro: 0.2340300356239391
[2m[36m(func pid=21969)[0m f1_weighted: 0.19404319952210763
[2m[36m(func pid=21969)[0m f1_per_class: [0.411, 0.289, 0.238, 0.142, 0.087, 0.407, 0.06, 0.394, 0.162, 0.151]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.34095149253731344
[2m[36m(func pid=22392)[0m top5: 0.8222947761194029
[2m[36m(func pid=22392)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=22392)[0m f1_macro: 0.2659729811258932
[2m[36m(func pid=22392)[0m f1_weighted: 0.3552984000707431
[2m[36m(func pid=22392)[0m f1_per_class: [0.204, 0.193, 0.224, 0.409, 0.137, 0.188, 0.51, 0.355, 0.11, 0.329]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5300 | Steps: 4 | Val loss: 2.0734 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9259 | Steps: 4 | Val loss: 2.3144 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.1006 | Steps: 4 | Val loss: 2.1782 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6927 | Steps: 4 | Val loss: 5.4561 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=21543)[0m top1: 0.3003731343283582
[2m[36m(func pid=21543)[0m top5: 0.7901119402985075
[2m[36m(func pid=21543)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=21543)[0m f1_macro: 0.2515584980720139
[2m[36m(func pid=21543)[0m f1_weighted: 0.2939046983201883
[2m[36m(func pid=21543)[0m f1_per_class: [0.379, 0.402, 0.087, 0.391, 0.036, 0.379, 0.139, 0.264, 0.09, 0.348]
== Status ==
Current time: 2024-01-07 11:59:20 (running for 00:01:26.21)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.933 |      0.094 |                    6 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  2.53  |      0.252 |                    7 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.311 |      0.234 |                    6 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  3.433 |      0.266 |                    5 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m top1: 0.14505597014925373
[2m[36m(func pid=21163)[0m top5: 0.5433768656716418
[2m[36m(func pid=21163)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=21163)[0m f1_macro: 0.09726238648793394
[2m[36m(func pid=21163)[0m f1_weighted: 0.14293495891526398
[2m[36m(func pid=21163)[0m f1_per_class: [0.063, 0.232, 0.057, 0.165, 0.013, 0.271, 0.061, 0.097, 0.013, 0.0]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.30597014925373134
[2m[36m(func pid=21969)[0m top5: 0.8208955223880597
[2m[36m(func pid=21969)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=21969)[0m f1_macro: 0.2870720407987642
[2m[36m(func pid=21969)[0m f1_weighted: 0.29651924221303044
[2m[36m(func pid=21969)[0m f1_per_class: [0.519, 0.308, 0.226, 0.448, 0.12, 0.425, 0.09, 0.382, 0.184, 0.168]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.41091417910447764
[2m[36m(func pid=22392)[0m top5: 0.8992537313432836
[2m[36m(func pid=22392)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=22392)[0m f1_macro: 0.31234035072109995
[2m[36m(func pid=22392)[0m f1_weighted: 0.4164537236999376
[2m[36m(func pid=22392)[0m f1_per_class: [0.375, 0.465, 0.453, 0.382, 0.074, 0.302, 0.549, 0.29, 0.053, 0.18]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.4426 | Steps: 4 | Val loss: 2.0443 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8996 | Steps: 4 | Val loss: 2.3055 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.2654 | Steps: 4 | Val loss: 1.9626 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 4.3254 | Steps: 4 | Val loss: 9.8338 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 11:59:25 (running for 00:01:31.26)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.926 |      0.097 |                    7 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  2.443 |      0.249 |                    8 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.101 |      0.287 |                    7 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  2.693 |      0.312 |                    6 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m top1: 0.2966417910447761
[2m[36m(func pid=21543)[0m top5: 0.8013059701492538
[2m[36m(func pid=21543)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=21543)[0m f1_macro: 0.24905897593274826
[2m[36m(func pid=21543)[0m f1_weighted: 0.3068595915076049
[2m[36m(func pid=21543)[0m f1_per_class: [0.397, 0.403, 0.071, 0.415, 0.042, 0.335, 0.176, 0.275, 0.089, 0.288]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m top1: 0.1417910447761194
[2m[36m(func pid=21163)[0m top5: 0.5680970149253731
[2m[36m(func pid=21163)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=21163)[0m f1_macro: 0.09722329396594627
[2m[36m(func pid=21163)[0m f1_weighted: 0.15112342001680154
[2m[36m(func pid=21163)[0m f1_per_class: [0.055, 0.199, 0.06, 0.2, 0.018, 0.247, 0.084, 0.11, 0.0, 0.0]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.38619402985074625
[2m[36m(func pid=21969)[0m top5: 0.8824626865671642
[2m[36m(func pid=21969)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=21969)[0m f1_macro: 0.3334885988107577
[2m[36m(func pid=21969)[0m f1_weighted: 0.3702411905973649
[2m[36m(func pid=21969)[0m f1_per_class: [0.519, 0.305, 0.324, 0.572, 0.145, 0.44, 0.211, 0.379, 0.203, 0.237]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.2728544776119403
[2m[36m(func pid=22392)[0m top5: 0.7555970149253731
[2m[36m(func pid=22392)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=22392)[0m f1_macro: 0.2760784045936041
[2m[36m(func pid=22392)[0m f1_weighted: 0.26503449372370447
[2m[36m(func pid=22392)[0m f1_per_class: [0.538, 0.374, 0.462, 0.395, 0.07, 0.34, 0.048, 0.296, 0.132, 0.105]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.3230 | Steps: 4 | Val loss: 2.0155 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8458 | Steps: 4 | Val loss: 2.2950 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.2174 | Steps: 4 | Val loss: 1.8821 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.3139 | Steps: 4 | Val loss: 13.0139 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:59:30 (running for 00:01:36.55)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.9   |      0.097 |                    8 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  2.323 |      0.246 |                    9 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.265 |      0.333 |                    8 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  4.325 |      0.276 |                    7 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m top1: 0.29990671641791045
[2m[36m(func pid=21543)[0m top5: 0.8022388059701493
[2m[36m(func pid=21543)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=21543)[0m f1_macro: 0.24615526379754074
[2m[36m(func pid=21543)[0m f1_weighted: 0.31953481058845057
[2m[36m(func pid=21543)[0m f1_per_class: [0.335, 0.369, 0.066, 0.453, 0.03, 0.313, 0.21, 0.299, 0.085, 0.299]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m top1: 0.1455223880597015
[2m[36m(func pid=21163)[0m top5: 0.5867537313432836
[2m[36m(func pid=21163)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=21163)[0m f1_macro: 0.10522852449236779
[2m[36m(func pid=21163)[0m f1_weighted: 0.1579241194518897
[2m[36m(func pid=21163)[0m f1_per_class: [0.081, 0.186, 0.09, 0.236, 0.012, 0.253, 0.073, 0.121, 0.0, 0.0]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.425839552238806
[2m[36m(func pid=21969)[0m top5: 0.9071828358208955
[2m[36m(func pid=21969)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=21969)[0m f1_macro: 0.3680108085949804
[2m[36m(func pid=21969)[0m f1_weighted: 0.42664523081263866
[2m[36m(func pid=21969)[0m f1_per_class: [0.578, 0.432, 0.3, 0.561, 0.172, 0.428, 0.334, 0.399, 0.187, 0.29]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.2756529850746269
[2m[36m(func pid=22392)[0m top5: 0.6487873134328358
[2m[36m(func pid=22392)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=22392)[0m f1_macro: 0.24578296418297016
[2m[36m(func pid=22392)[0m f1_weighted: 0.25925300132157086
[2m[36m(func pid=22392)[0m f1_per_class: [0.553, 0.253, 0.104, 0.496, 0.086, 0.333, 0.003, 0.332, 0.141, 0.157]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.2454 | Steps: 4 | Val loss: 1.9701 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9072 | Steps: 4 | Val loss: 2.2859 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8560 | Steps: 4 | Val loss: 2.0184 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 4.7505 | Steps: 4 | Val loss: 16.5452 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 11:59:35 (running for 00:01:41.87)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.846 |      0.105 |                    9 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  2.245 |      0.229 |                   10 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.217 |      0.368 |                    9 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  2.314 |      0.246 |                    8 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m top1: 0.30130597014925375
[2m[36m(func pid=21543)[0m top5: 0.8138992537313433
[2m[36m(func pid=21543)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=21543)[0m f1_macro: 0.22943076588392852
[2m[36m(func pid=21543)[0m f1_weighted: 0.32193197077458247
[2m[36m(func pid=21543)[0m f1_per_class: [0.29, 0.304, 0.073, 0.493, 0.024, 0.215, 0.263, 0.285, 0.09, 0.257]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m top1: 0.14319029850746268
[2m[36m(func pid=21163)[0m top5: 0.5970149253731343
[2m[36m(func pid=21163)[0m f1_micro: 0.14319029850746268
[2m[36m(func pid=21163)[0m f1_macro: 0.10593649367320901
[2m[36m(func pid=21163)[0m f1_weighted: 0.1598023764192892
[2m[36m(func pid=21163)[0m f1_per_class: [0.083, 0.155, 0.086, 0.244, 0.028, 0.239, 0.093, 0.131, 0.0, 0.0]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.43236940298507465
[2m[36m(func pid=21969)[0m top5: 0.9039179104477612
[2m[36m(func pid=21969)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=21969)[0m f1_macro: 0.35613393796026244
[2m[36m(func pid=21969)[0m f1_weighted: 0.453112584032425
[2m[36m(func pid=21969)[0m f1_per_class: [0.531, 0.505, 0.138, 0.531, 0.147, 0.386, 0.438, 0.345, 0.202, 0.339]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.19916044776119404
[2m[36m(func pid=22392)[0m top5: 0.6357276119402985
[2m[36m(func pid=22392)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=22392)[0m f1_macro: 0.2111760941953628
[2m[36m(func pid=22392)[0m f1_weighted: 0.20036872599686242
[2m[36m(func pid=22392)[0m f1_per_class: [0.539, 0.091, 0.036, 0.428, 0.08, 0.231, 0.0, 0.347, 0.128, 0.231]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.1485 | Steps: 4 | Val loss: 1.9682 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8816 | Steps: 4 | Val loss: 2.2711 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.8309 | Steps: 4 | Val loss: 2.2485 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.3639 | Steps: 4 | Val loss: 13.1964 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 11:59:41 (running for 00:01:47.29)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.907 |      0.106 |                   10 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  2.149 |      0.222 |                   11 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.856 |      0.356 |                   10 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  4.75  |      0.211 |                    9 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m top1: 0.2798507462686567
[2m[36m(func pid=21543)[0m top5: 0.8017723880597015
[2m[36m(func pid=21543)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=21543)[0m f1_macro: 0.22167010553811503
[2m[36m(func pid=21543)[0m f1_weighted: 0.30699259702929377
[2m[36m(func pid=21543)[0m f1_per_class: [0.298, 0.242, 0.068, 0.446, 0.061, 0.17, 0.309, 0.28, 0.099, 0.243]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m top1: 0.1501865671641791
[2m[36m(func pid=21163)[0m top5: 0.6277985074626866
[2m[36m(func pid=21163)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=21163)[0m f1_macro: 0.11371157167435597
[2m[36m(func pid=21163)[0m f1_weighted: 0.17148924631306517
[2m[36m(func pid=21163)[0m f1_per_class: [0.101, 0.176, 0.076, 0.245, 0.026, 0.218, 0.12, 0.145, 0.031, 0.0]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.404384328358209
[2m[36m(func pid=21969)[0m top5: 0.902518656716418
[2m[36m(func pid=21969)[0m f1_micro: 0.404384328358209
[2m[36m(func pid=21969)[0m f1_macro: 0.33761471929195813
[2m[36m(func pid=21969)[0m f1_weighted: 0.44078761196564004
[2m[36m(func pid=21969)[0m f1_per_class: [0.523, 0.516, 0.073, 0.425, 0.103, 0.391, 0.499, 0.303, 0.187, 0.354]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.269589552238806
[2m[36m(func pid=22392)[0m top5: 0.7621268656716418
[2m[36m(func pid=22392)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=22392)[0m f1_macro: 0.23311175878308013
[2m[36m(func pid=22392)[0m f1_weighted: 0.25732917388814636
[2m[36m(func pid=22392)[0m f1_per_class: [0.423, 0.27, 0.1, 0.488, 0.091, 0.193, 0.06, 0.3, 0.139, 0.268]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8580 | Steps: 4 | Val loss: 2.2535 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.9332 | Steps: 4 | Val loss: 1.9604 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.0980 | Steps: 4 | Val loss: 2.5865 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.5179 | Steps: 4 | Val loss: 11.0758 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=21163)[0m top1: 0.15485074626865672
[2m[36m(func pid=21163)[0m top5: 0.6581156716417911
[2m[36m(func pid=21163)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=21163)[0m f1_macro: 0.11781163589492868
[2m[36m(func pid=21163)[0m f1_weighted: 0.17555654470190013
[2m[36m(func pid=21163)[0m f1_per_class: [0.12, 0.174, 0.09, 0.247, 0.026, 0.195, 0.138, 0.157, 0.031, 0.0]
[2m[36m(func pid=21163)[0m 
== Status ==
Current time: 2024-01-07 11:59:46 (running for 00:01:52.64)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.858 |      0.118 |                   12 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  2.149 |      0.222 |                   11 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.831 |      0.338 |                   11 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  3.364 |      0.233 |                   10 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m top1: 0.2737873134328358
[2m[36m(func pid=21543)[0m top5: 0.7952425373134329
[2m[36m(func pid=21543)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=21543)[0m f1_macro: 0.22645567565065655
[2m[36m(func pid=21543)[0m f1_weighted: 0.3001479149278936
[2m[36m(func pid=21543)[0m f1_per_class: [0.337, 0.247, 0.078, 0.424, 0.081, 0.177, 0.296, 0.285, 0.117, 0.223]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m top1: 0.36800373134328357
[2m[36m(func pid=21969)[0m top5: 0.8843283582089553
[2m[36m(func pid=21969)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=21969)[0m f1_macro: 0.32975872417132995
[2m[36m(func pid=21969)[0m f1_weighted: 0.40119350425576533
[2m[36m(func pid=21969)[0m f1_per_class: [0.528, 0.492, 0.068, 0.316, 0.16, 0.435, 0.457, 0.366, 0.153, 0.322]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.31576492537313433
[2m[36m(func pid=22392)[0m top5: 0.8596082089552238
[2m[36m(func pid=22392)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=22392)[0m f1_macro: 0.3012770444641179
[2m[36m(func pid=22392)[0m f1_weighted: 0.3412763932640163
[2m[36m(func pid=22392)[0m f1_per_class: [0.319, 0.37, 0.6, 0.404, 0.146, 0.159, 0.385, 0.225, 0.133, 0.272]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8428 | Steps: 4 | Val loss: 2.2434 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.9171 | Steps: 4 | Val loss: 1.9885 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.2598 | Steps: 4 | Val loss: 2.9063 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.4780 | Steps: 4 | Val loss: 11.1174 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 11:59:51 (running for 00:01:57.72)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.843 |      0.119 |                   13 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.933 |      0.226 |                   12 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.098 |      0.33  |                   12 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  2.518 |      0.301 |                   11 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.15951492537313433
[2m[36m(func pid=21163)[0m top5: 0.6735074626865671
[2m[36m(func pid=21163)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=21163)[0m f1_macro: 0.11895220941461408
[2m[36m(func pid=21163)[0m f1_weighted: 0.18154662106995018
[2m[36m(func pid=21163)[0m f1_per_class: [0.163, 0.14, 0.08, 0.267, 0.035, 0.157, 0.173, 0.146, 0.028, 0.0]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.261660447761194
[2m[36m(func pid=21543)[0m top5: 0.7798507462686567
[2m[36m(func pid=21543)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=21543)[0m f1_macro: 0.2178721900601336
[2m[36m(func pid=21543)[0m f1_weighted: 0.2895500747959672
[2m[36m(func pid=21543)[0m f1_per_class: [0.261, 0.215, 0.092, 0.399, 0.072, 0.214, 0.289, 0.299, 0.146, 0.193]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m top1: 0.3316231343283582
[2m[36m(func pid=21969)[0m top5: 0.8740671641791045
[2m[36m(func pid=21969)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=21969)[0m f1_macro: 0.3064632739915075
[2m[36m(func pid=21969)[0m f1_weighted: 0.37582052536447297
[2m[36m(func pid=21969)[0m f1_per_class: [0.505, 0.429, 0.06, 0.354, 0.11, 0.402, 0.39, 0.347, 0.18, 0.288]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.386660447761194
[2m[36m(func pid=22392)[0m top5: 0.8847947761194029
[2m[36m(func pid=22392)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=22392)[0m f1_macro: 0.3483342629066659
[2m[36m(func pid=22392)[0m f1_weighted: 0.37685314550368915
[2m[36m(func pid=22392)[0m f1_per_class: [0.335, 0.514, 0.815, 0.276, 0.152, 0.182, 0.536, 0.165, 0.12, 0.389]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7776 | Steps: 4 | Val loss: 2.2346 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.0681 | Steps: 4 | Val loss: 1.9908 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.8787 | Steps: 4 | Val loss: 3.0114 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 11:59:56 (running for 00:02:02.90)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.778 |      0.135 |                   14 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.917 |      0.218 |                   13 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.26  |      0.306 |                   13 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  2.478 |      0.348 |                   12 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.17117537313432835
[2m[36m(func pid=21163)[0m top5: 0.6823694029850746
[2m[36m(func pid=21163)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=21163)[0m f1_macro: 0.13457562288495328
[2m[36m(func pid=21163)[0m f1_weighted: 0.19186734590956006
[2m[36m(func pid=21163)[0m f1_per_class: [0.185, 0.167, 0.113, 0.289, 0.04, 0.172, 0.16, 0.158, 0.029, 0.033]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 3.8854 | Steps: 4 | Val loss: 12.6571 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=21543)[0m top1: 0.2490671641791045
[2m[36m(func pid=21543)[0m top5: 0.769589552238806
[2m[36m(func pid=21543)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=21543)[0m f1_macro: 0.21513918343031602
[2m[36m(func pid=21543)[0m f1_weighted: 0.2759347715891236
[2m[36m(func pid=21543)[0m f1_per_class: [0.261, 0.203, 0.096, 0.305, 0.088, 0.229, 0.331, 0.3, 0.149, 0.19]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m top1: 0.3474813432835821
[2m[36m(func pid=21969)[0m top5: 0.8680037313432836
[2m[36m(func pid=21969)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=21969)[0m f1_macro: 0.3023350913858272
[2m[36m(func pid=21969)[0m f1_weighted: 0.3841888725960924
[2m[36m(func pid=21969)[0m f1_per_class: [0.484, 0.309, 0.096, 0.481, 0.128, 0.404, 0.37, 0.348, 0.2, 0.204]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.3530783582089552
[2m[36m(func pid=22392)[0m top5: 0.878731343283582
[2m[36m(func pid=22392)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=22392)[0m f1_macro: 0.3482542358650212
[2m[36m(func pid=22392)[0m f1_weighted: 0.3442412689039562
[2m[36m(func pid=22392)[0m f1_per_class: [0.365, 0.513, 0.72, 0.235, 0.125, 0.284, 0.407, 0.223, 0.179, 0.432]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7095 | Steps: 4 | Val loss: 2.2215 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.7034 | Steps: 4 | Val loss: 1.9454 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8878 | Steps: 4 | Val loss: 3.3813 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 12:00:02 (running for 00:02:08.16)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.71  |      0.138 |                   15 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  2.068 |      0.215 |                   14 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.879 |      0.302 |                   14 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  3.885 |      0.348 |                   13 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.17444029850746268
[2m[36m(func pid=21163)[0m top5: 0.6963619402985075
[2m[36m(func pid=21163)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=21163)[0m f1_macro: 0.13776645417238345
[2m[36m(func pid=21163)[0m f1_weighted: 0.19361253936983044
[2m[36m(func pid=21163)[0m f1_per_class: [0.181, 0.159, 0.136, 0.304, 0.038, 0.171, 0.156, 0.158, 0.027, 0.048]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 7.0579 | Steps: 4 | Val loss: 14.1177 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=21543)[0m top1: 0.283115671641791
[2m[36m(func pid=21543)[0m top5: 0.7910447761194029
[2m[36m(func pid=21543)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=21543)[0m f1_macro: 0.24780862154751743
[2m[36m(func pid=21543)[0m f1_weighted: 0.31752728462172325
[2m[36m(func pid=21543)[0m f1_per_class: [0.309, 0.277, 0.16, 0.337, 0.077, 0.32, 0.354, 0.33, 0.144, 0.171]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m top1: 0.34421641791044777
[2m[36m(func pid=21969)[0m top5: 0.8498134328358209
[2m[36m(func pid=21969)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=21969)[0m f1_macro: 0.29596997450669543
[2m[36m(func pid=21969)[0m f1_weighted: 0.36646893776633377
[2m[36m(func pid=21969)[0m f1_per_class: [0.5, 0.157, 0.194, 0.523, 0.131, 0.395, 0.358, 0.36, 0.208, 0.133]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7771 | Steps: 4 | Val loss: 2.2124 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=22392)[0m top1: 0.3362873134328358
[2m[36m(func pid=22392)[0m top5: 0.863339552238806
[2m[36m(func pid=22392)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=22392)[0m f1_macro: 0.3440508857069933
[2m[36m(func pid=22392)[0m f1_weighted: 0.3278747771300826
[2m[36m(func pid=22392)[0m f1_per_class: [0.369, 0.459, 0.72, 0.39, 0.204, 0.372, 0.192, 0.3, 0.222, 0.213]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.7207 | Steps: 4 | Val loss: 1.9420 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.5337 | Steps: 4 | Val loss: 3.7418 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 12:00:07 (running for 00:02:13.35)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.777 |      0.14  |                   16 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.703 |      0.248 |                   15 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.888 |      0.296 |                   15 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  7.058 |      0.344 |                   14 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.18190298507462688
[2m[36m(func pid=21163)[0m top5: 0.7000932835820896
[2m[36m(func pid=21163)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=21163)[0m f1_macro: 0.13970281986069388
[2m[36m(func pid=21163)[0m f1_weighted: 0.19810288871701837
[2m[36m(func pid=21163)[0m f1_per_class: [0.205, 0.182, 0.108, 0.319, 0.034, 0.179, 0.138, 0.17, 0.026, 0.036]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.28451492537313433
[2m[36m(func pid=21543)[0m top5: 0.8041044776119403
[2m[36m(func pid=21543)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=21543)[0m f1_macro: 0.2539199409887306
[2m[36m(func pid=21543)[0m f1_weighted: 0.3172754485426593
[2m[36m(func pid=21543)[0m f1_per_class: [0.318, 0.284, 0.174, 0.274, 0.067, 0.353, 0.391, 0.352, 0.138, 0.189]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 4.4938 | Steps: 4 | Val loss: 17.5103 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=21969)[0m top1: 0.32369402985074625
[2m[36m(func pid=21969)[0m top5: 0.8456156716417911
[2m[36m(func pid=21969)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=21969)[0m f1_macro: 0.29345392876990173
[2m[36m(func pid=21969)[0m f1_weighted: 0.3393099294543537
[2m[36m(func pid=21969)[0m f1_per_class: [0.496, 0.12, 0.353, 0.524, 0.107, 0.354, 0.302, 0.36, 0.207, 0.112]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6932 | Steps: 4 | Val loss: 2.1959 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=22392)[0m top1: 0.31156716417910446
[2m[36m(func pid=22392)[0m top5: 0.8381529850746269
[2m[36m(func pid=22392)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=22392)[0m f1_macro: 0.3234146903230207
[2m[36m(func pid=22392)[0m f1_weighted: 0.30796798220296434
[2m[36m(func pid=22392)[0m f1_per_class: [0.403, 0.244, 0.714, 0.475, 0.213, 0.395, 0.162, 0.293, 0.238, 0.098]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.5387 | Steps: 4 | Val loss: 1.8449 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.4183 | Steps: 4 | Val loss: 3.9738 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 12:00:12 (running for 00:02:18.52)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.693 |      0.143 |                   17 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.721 |      0.254 |                   16 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.534 |      0.293 |                   16 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  4.494 |      0.323 |                   15 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.18423507462686567
[2m[36m(func pid=21163)[0m top5: 0.7192164179104478
[2m[36m(func pid=21163)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=21163)[0m f1_macro: 0.1433958332496989
[2m[36m(func pid=21163)[0m f1_weighted: 0.19987976297127508
[2m[36m(func pid=21163)[0m f1_per_class: [0.195, 0.167, 0.11, 0.326, 0.038, 0.168, 0.149, 0.17, 0.028, 0.084]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3204291044776119
[2m[36m(func pid=21543)[0m top5: 0.84375
[2m[36m(func pid=21543)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=21543)[0m f1_macro: 0.2829003964973628
[2m[36m(func pid=21543)[0m f1_weighted: 0.35795534361473363
[2m[36m(func pid=21543)[0m f1_per_class: [0.377, 0.33, 0.205, 0.365, 0.067, 0.381, 0.398, 0.348, 0.151, 0.206]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 6.8627 | Steps: 4 | Val loss: 20.9197 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=21969)[0m top1: 0.29850746268656714
[2m[36m(func pid=21969)[0m top5: 0.8339552238805971
[2m[36m(func pid=21969)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=21969)[0m f1_macro: 0.29223819394141715
[2m[36m(func pid=21969)[0m f1_weighted: 0.3290759791374592
[2m[36m(func pid=21969)[0m f1_per_class: [0.45, 0.17, 0.462, 0.444, 0.073, 0.334, 0.328, 0.337, 0.2, 0.125]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.6819 | Steps: 4 | Val loss: 2.1784 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=22392)[0m top1: 0.31763059701492535
[2m[36m(func pid=22392)[0m top5: 0.8274253731343284
[2m[36m(func pid=22392)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=22392)[0m f1_macro: 0.30719231128633606
[2m[36m(func pid=22392)[0m f1_weighted: 0.29436128588228644
[2m[36m(func pid=22392)[0m f1_per_class: [0.495, 0.112, 0.629, 0.52, 0.2, 0.396, 0.158, 0.217, 0.259, 0.086]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.3464 | Steps: 4 | Val loss: 1.7792 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.7107 | Steps: 4 | Val loss: 4.1684 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 12:00:17 (running for 00:02:23.78)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.682 |      0.164 |                   18 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.539 |      0.283 |                   17 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.418 |      0.292 |                   17 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  6.863 |      0.307 |                   16 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.2042910447761194
[2m[36m(func pid=21163)[0m top5: 0.7341417910447762
[2m[36m(func pid=21163)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=21163)[0m f1_macro: 0.16404191585147881
[2m[36m(func pid=21163)[0m f1_weighted: 0.21709512999086059
[2m[36m(func pid=21163)[0m f1_per_class: [0.225, 0.172, 0.182, 0.357, 0.036, 0.168, 0.167, 0.181, 0.026, 0.126]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.34794776119402987
[2m[36m(func pid=21543)[0m top5: 0.8656716417910447
[2m[36m(func pid=21543)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=21543)[0m f1_macro: 0.3120604125483372
[2m[36m(func pid=21543)[0m f1_weighted: 0.3810242723654437
[2m[36m(func pid=21543)[0m f1_per_class: [0.405, 0.347, 0.258, 0.401, 0.069, 0.42, 0.402, 0.382, 0.191, 0.245]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 6.0357 | Steps: 4 | Val loss: 22.2714 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=21969)[0m top1: 0.2751865671641791
[2m[36m(func pid=21969)[0m top5: 0.8316231343283582
[2m[36m(func pid=21969)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=21969)[0m f1_macro: 0.30962885725378836
[2m[36m(func pid=21969)[0m f1_weighted: 0.29856727590818866
[2m[36m(func pid=21969)[0m f1_per_class: [0.493, 0.292, 0.667, 0.274, 0.074, 0.274, 0.331, 0.324, 0.2, 0.169]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.5970 | Steps: 4 | Val loss: 2.1704 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.1467 | Steps: 4 | Val loss: 1.7393 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=22392)[0m top1: 0.2737873134328358
[2m[36m(func pid=22392)[0m top5: 0.7709888059701493
[2m[36m(func pid=22392)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=22392)[0m f1_macro: 0.24652410486346715
[2m[36m(func pid=22392)[0m f1_weighted: 0.3068774217983984
[2m[36m(func pid=22392)[0m f1_per_class: [0.397, 0.219, 0.213, 0.442, 0.081, 0.317, 0.262, 0.228, 0.224, 0.082]
[2m[36m(func pid=22392)[0m 
== Status ==
Current time: 2024-01-07 12:00:23 (running for 00:02:28.93)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.597 |      0.168 |                   19 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.346 |      0.312 |                   18 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.711 |      0.31  |                   18 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  6.036 |      0.247 |                   17 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.21222014925373134
[2m[36m(func pid=21163)[0m top5: 0.7374067164179104
[2m[36m(func pid=21163)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=21163)[0m f1_macro: 0.1682168839620007
[2m[36m(func pid=21163)[0m f1_weighted: 0.2178584385047501
[2m[36m(func pid=21163)[0m f1_per_class: [0.244, 0.136, 0.198, 0.408, 0.039, 0.135, 0.152, 0.188, 0.027, 0.157]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.2389 | Steps: 4 | Val loss: 4.3040 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=21543)[0m top1: 0.3619402985074627
[2m[36m(func pid=21543)[0m top5: 0.8810634328358209
[2m[36m(func pid=21543)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=21543)[0m f1_macro: 0.32653180269695337
[2m[36m(func pid=21543)[0m f1_weighted: 0.39429347797623576
[2m[36m(func pid=21543)[0m f1_per_class: [0.464, 0.39, 0.255, 0.424, 0.066, 0.427, 0.394, 0.377, 0.178, 0.289]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 3.6506 | Steps: 4 | Val loss: 28.7945 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=21969)[0m top1: 0.28638059701492535
[2m[36m(func pid=21969)[0m top5: 0.8367537313432836
[2m[36m(func pid=21969)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=21969)[0m f1_macro: 0.3181580215579903
[2m[36m(func pid=21969)[0m f1_weighted: 0.3023492884688043
[2m[36m(func pid=21969)[0m f1_per_class: [0.525, 0.356, 0.686, 0.192, 0.085, 0.258, 0.391, 0.299, 0.194, 0.196]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.6585 | Steps: 4 | Val loss: 2.1529 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.2100 | Steps: 4 | Val loss: 1.6877 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=22392)[0m top1: 0.19309701492537312
[2m[36m(func pid=22392)[0m top5: 0.7402052238805971
[2m[36m(func pid=22392)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=22392)[0m f1_macro: 0.2054777329904527
[2m[36m(func pid=22392)[0m f1_weighted: 0.22762111970638582
[2m[36m(func pid=22392)[0m f1_per_class: [0.394, 0.325, 0.09, 0.195, 0.032, 0.052, 0.255, 0.314, 0.168, 0.23]
[2m[36m(func pid=22392)[0m 
== Status ==
Current time: 2024-01-07 12:00:28 (running for 00:02:34.05)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.658 |      0.188 |                   20 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.147 |      0.327 |                   19 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.239 |      0.318 |                   19 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  3.651 |      0.205 |                   18 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.2332089552238806
[2m[36m(func pid=21163)[0m top5: 0.7467350746268657
[2m[36m(func pid=21163)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=21163)[0m f1_macro: 0.18845780124227007
[2m[36m(func pid=21163)[0m f1_weighted: 0.23576674606330558
[2m[36m(func pid=21163)[0m f1_per_class: [0.242, 0.13, 0.235, 0.45, 0.052, 0.158, 0.159, 0.19, 0.06, 0.207]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.9783 | Steps: 4 | Val loss: 3.8612 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=21543)[0m top1: 0.3908582089552239
[2m[36m(func pid=21543)[0m top5: 0.9029850746268657
[2m[36m(func pid=21543)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=21543)[0m f1_macro: 0.3465481311609
[2m[36m(func pid=21543)[0m f1_weighted: 0.4179046391374764
[2m[36m(func pid=21543)[0m f1_per_class: [0.495, 0.428, 0.273, 0.486, 0.081, 0.43, 0.385, 0.38, 0.216, 0.291]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 8.1211 | Steps: 4 | Val loss: 36.1543 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.5702 | Steps: 4 | Val loss: 2.1557 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=21969)[0m top1: 0.3278917910447761
[2m[36m(func pid=21969)[0m top5: 0.8731343283582089
[2m[36m(func pid=21969)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=21969)[0m f1_macro: 0.34034867584082173
[2m[36m(func pid=21969)[0m f1_weighted: 0.34105292780209256
[2m[36m(func pid=21969)[0m f1_per_class: [0.514, 0.4, 0.649, 0.239, 0.105, 0.331, 0.422, 0.291, 0.206, 0.245]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.2950 | Steps: 4 | Val loss: 1.6768 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=22392)[0m top1: 0.18516791044776118
[2m[36m(func pid=22392)[0m top5: 0.6935634328358209
[2m[36m(func pid=22392)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=22392)[0m f1_macro: 0.21634665235257705
[2m[36m(func pid=22392)[0m f1_weighted: 0.20276618346992098
[2m[36m(func pid=22392)[0m f1_per_class: [0.417, 0.399, 0.045, 0.041, 0.044, 0.008, 0.28, 0.322, 0.15, 0.456]
[2m[36m(func pid=22392)[0m 
== Status ==
Current time: 2024-01-07 12:00:33 (running for 00:02:39.35)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.57  |      0.178 |                   21 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.21  |      0.347 |                   20 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.978 |      0.34  |                   20 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  8.121 |      0.216 |                   19 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.2294776119402985
[2m[36m(func pid=21163)[0m top5: 0.7406716417910447
[2m[36m(func pid=21163)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=21163)[0m f1_macro: 0.1784514871295597
[2m[36m(func pid=21163)[0m f1_weighted: 0.22520183164934435
[2m[36m(func pid=21163)[0m f1_per_class: [0.246, 0.122, 0.19, 0.439, 0.045, 0.132, 0.145, 0.219, 0.061, 0.185]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.7556 | Steps: 4 | Val loss: 3.5524 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=21543)[0m top1: 0.3941231343283582
[2m[36m(func pid=21543)[0m top5: 0.9067164179104478
[2m[36m(func pid=21543)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=21543)[0m f1_macro: 0.347310477669208
[2m[36m(func pid=21543)[0m f1_weighted: 0.41736279223397343
[2m[36m(func pid=21543)[0m f1_per_class: [0.486, 0.434, 0.289, 0.496, 0.088, 0.428, 0.377, 0.349, 0.201, 0.324]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 8.7866 | Steps: 4 | Val loss: 38.2874 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.6247 | Steps: 4 | Val loss: 2.1440 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=21969)[0m top1: 0.3619402985074627
[2m[36m(func pid=21969)[0m top5: 0.8931902985074627
[2m[36m(func pid=21969)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=21969)[0m f1_macro: 0.3536352618591558
[2m[36m(func pid=21969)[0m f1_weighted: 0.3720896430512412
[2m[36m(func pid=21969)[0m f1_per_class: [0.504, 0.42, 0.558, 0.308, 0.135, 0.335, 0.45, 0.277, 0.208, 0.342]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.3661 | Steps: 4 | Val loss: 1.6680 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=22392)[0m top1: 0.21455223880597016
[2m[36m(func pid=22392)[0m top5: 0.6637126865671642
[2m[36m(func pid=22392)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=22392)[0m f1_macro: 0.19610798939738527
[2m[36m(func pid=22392)[0m f1_weighted: 0.2161096764549959
[2m[36m(func pid=22392)[0m f1_per_class: [0.407, 0.47, 0.041, 0.023, 0.061, 0.016, 0.315, 0.296, 0.14, 0.194]
[2m[36m(func pid=22392)[0m 
== Status ==
Current time: 2024-01-07 12:00:38 (running for 00:02:44.54)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.625 |      0.187 |                   22 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.295 |      0.347 |                   21 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.756 |      0.354 |                   21 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  8.787 |      0.196 |                   20 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.23787313432835822
[2m[36m(func pid=21163)[0m top5: 0.7486007462686567
[2m[36m(func pid=21163)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=21163)[0m f1_macro: 0.1867408878573051
[2m[36m(func pid=21163)[0m f1_weighted: 0.23270357395800462
[2m[36m(func pid=21163)[0m f1_per_class: [0.247, 0.136, 0.186, 0.447, 0.061, 0.166, 0.141, 0.232, 0.034, 0.218]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.39972014925373134
[2m[36m(func pid=21543)[0m top5: 0.9132462686567164
[2m[36m(func pid=21543)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=21543)[0m f1_macro: 0.35672052155337103
[2m[36m(func pid=21543)[0m f1_weighted: 0.42488394564604187
[2m[36m(func pid=21543)[0m f1_per_class: [0.526, 0.427, 0.353, 0.506, 0.084, 0.427, 0.396, 0.338, 0.206, 0.304]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6899 | Steps: 4 | Val loss: 3.4530 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 9.3992 | Steps: 4 | Val loss: 36.0487 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.5579 | Steps: 4 | Val loss: 2.1351 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=21969)[0m top1: 0.3763992537313433
[2m[36m(func pid=21969)[0m top5: 0.9235074626865671
[2m[36m(func pid=21969)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=21969)[0m f1_macro: 0.35923984150430316
[2m[36m(func pid=21969)[0m f1_weighted: 0.3881398833937385
[2m[36m(func pid=21969)[0m f1_per_class: [0.504, 0.394, 0.511, 0.391, 0.178, 0.336, 0.437, 0.277, 0.257, 0.309]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.1185 | Steps: 4 | Val loss: 1.6717 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=22392)[0m top1: 0.22574626865671643
[2m[36m(func pid=22392)[0m top5: 0.7145522388059702
[2m[36m(func pid=22392)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=22392)[0m f1_macro: 0.20059063208777972
[2m[36m(func pid=22392)[0m f1_weighted: 0.22429153132761037
[2m[36m(func pid=22392)[0m f1_per_class: [0.345, 0.499, 0.053, 0.033, 0.091, 0.068, 0.303, 0.289, 0.118, 0.207]
[2m[36m(func pid=22392)[0m 
== Status ==
Current time: 2024-01-07 12:00:43 (running for 00:02:49.80)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.558 |      0.188 |                   23 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.366 |      0.357 |                   22 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.69  |      0.359 |                   22 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  9.399 |      0.201 |                   21 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.23367537313432835
[2m[36m(func pid=21163)[0m top5: 0.7532649253731343
[2m[36m(func pid=21163)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=21163)[0m f1_macro: 0.1882891759905296
[2m[36m(func pid=21163)[0m f1_weighted: 0.22809008522623767
[2m[36m(func pid=21163)[0m f1_per_class: [0.242, 0.127, 0.247, 0.444, 0.058, 0.159, 0.136, 0.23, 0.033, 0.208]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3969216417910448
[2m[36m(func pid=21543)[0m top5: 0.9155783582089553
[2m[36m(func pid=21543)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=21543)[0m f1_macro: 0.34775341997696296
[2m[36m(func pid=21543)[0m f1_weighted: 0.4215703502461825
[2m[36m(func pid=21543)[0m f1_per_class: [0.532, 0.399, 0.308, 0.515, 0.086, 0.417, 0.398, 0.344, 0.19, 0.288]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.3003 | Steps: 4 | Val loss: 3.4443 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 14.0299 | Steps: 4 | Val loss: 28.2298 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.5079 | Steps: 4 | Val loss: 2.1190 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=21969)[0m top1: 0.38759328358208955
[2m[36m(func pid=21969)[0m top5: 0.9309701492537313
[2m[36m(func pid=21969)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=21969)[0m f1_macro: 0.350660886378421
[2m[36m(func pid=21969)[0m f1_weighted: 0.4002059623880984
[2m[36m(func pid=21969)[0m f1_per_class: [0.519, 0.379, 0.453, 0.443, 0.164, 0.346, 0.439, 0.279, 0.213, 0.272]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.3396 | Steps: 4 | Val loss: 1.6764 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=22392)[0m top1: 0.2789179104477612
[2m[36m(func pid=22392)[0m top5: 0.8064365671641791
[2m[36m(func pid=22392)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=22392)[0m f1_macro: 0.247833784607243
[2m[36m(func pid=22392)[0m f1_weighted: 0.30008006733693515
[2m[36m(func pid=22392)[0m f1_per_class: [0.311, 0.508, 0.102, 0.215, 0.143, 0.232, 0.312, 0.323, 0.126, 0.207]
[2m[36m(func pid=22392)[0m 
== Status ==
Current time: 2024-01-07 12:00:49 (running for 00:02:55.16)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.508 |      0.208 |                   24 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.119 |      0.348 |                   23 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.3   |      0.351 |                   23 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 14.03  |      0.248 |                   22 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.23041044776119404
[2m[36m(func pid=21163)[0m top5: 0.7602611940298507
[2m[36m(func pid=21163)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=21163)[0m f1_macro: 0.2080165002575411
[2m[36m(func pid=21163)[0m f1_weighted: 0.23031701805529936
[2m[36m(func pid=21163)[0m f1_per_class: [0.293, 0.147, 0.204, 0.424, 0.042, 0.167, 0.13, 0.254, 0.09, 0.33]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.40158582089552236
[2m[36m(func pid=21543)[0m top5: 0.9146455223880597
[2m[36m(func pid=21543)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=21543)[0m f1_macro: 0.3505191817310276
[2m[36m(func pid=21543)[0m f1_weighted: 0.4249081860056601
[2m[36m(func pid=21543)[0m f1_per_class: [0.545, 0.384, 0.316, 0.532, 0.091, 0.428, 0.403, 0.305, 0.193, 0.308]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.6005 | Steps: 4 | Val loss: 3.2934 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 8.2222 | Steps: 4 | Val loss: 22.1029 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.5809 | Steps: 4 | Val loss: 2.1110 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=21969)[0m top1: 0.4123134328358209
[2m[36m(func pid=21969)[0m top5: 0.9375
[2m[36m(func pid=21969)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=21969)[0m f1_macro: 0.36235605649279484
[2m[36m(func pid=21969)[0m f1_weighted: 0.42316122548003116
[2m[36m(func pid=21969)[0m f1_per_class: [0.491, 0.335, 0.462, 0.514, 0.254, 0.346, 0.475, 0.279, 0.211, 0.256]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.1343 | Steps: 4 | Val loss: 1.7525 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=22392)[0m top1: 0.3572761194029851
[2m[36m(func pid=22392)[0m top5: 0.8847947761194029
[2m[36m(func pid=22392)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=22392)[0m f1_macro: 0.29747063672662055
[2m[36m(func pid=22392)[0m f1_weighted: 0.3738030283356093
[2m[36m(func pid=22392)[0m f1_per_class: [0.326, 0.439, 0.255, 0.476, 0.129, 0.331, 0.303, 0.342, 0.18, 0.194]
[2m[36m(func pid=22392)[0m 
== Status ==
Current time: 2024-01-07 12:00:54 (running for 00:03:00.44)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.581 |      0.207 |                   25 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.34  |      0.351 |                   24 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.601 |      0.362 |                   24 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  8.222 |      0.297 |                   23 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.240205223880597
[2m[36m(func pid=21163)[0m top5: 0.7555970149253731
[2m[36m(func pid=21163)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=21163)[0m f1_macro: 0.20661612134655946
[2m[36m(func pid=21163)[0m f1_weighted: 0.23134056981294154
[2m[36m(func pid=21163)[0m f1_per_class: [0.277, 0.118, 0.268, 0.458, 0.045, 0.157, 0.126, 0.251, 0.067, 0.299]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.38759328358208955
[2m[36m(func pid=21543)[0m top5: 0.9109141791044776
[2m[36m(func pid=21543)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=21543)[0m f1_macro: 0.33706150951428576
[2m[36m(func pid=21543)[0m f1_weighted: 0.41457851263013135
[2m[36m(func pid=21543)[0m f1_per_class: [0.539, 0.405, 0.253, 0.511, 0.073, 0.412, 0.392, 0.251, 0.216, 0.319]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.6156 | Steps: 4 | Val loss: 3.5099 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 15.4919 | Steps: 4 | Val loss: 23.3180 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.3928 | Steps: 4 | Val loss: 2.1125 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=21969)[0m top1: 0.41697761194029853
[2m[36m(func pid=21969)[0m top5: 0.9286380597014925
[2m[36m(func pid=21969)[0m f1_micro: 0.41697761194029853
[2m[36m(func pid=21969)[0m f1_macro: 0.3681464988186339
[2m[36m(func pid=21969)[0m f1_weighted: 0.4183136423795442
[2m[36m(func pid=21969)[0m f1_per_class: [0.509, 0.306, 0.49, 0.553, 0.273, 0.377, 0.421, 0.311, 0.21, 0.233]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.4815 | Steps: 4 | Val loss: 1.7722 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=22392)[0m top1: 0.3824626865671642
[2m[36m(func pid=22392)[0m top5: 0.9253731343283582
[2m[36m(func pid=22392)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=22392)[0m f1_macro: 0.33587891220160926
[2m[36m(func pid=22392)[0m f1_weighted: 0.37878773433023405
[2m[36m(func pid=22392)[0m f1_per_class: [0.406, 0.376, 0.595, 0.549, 0.083, 0.331, 0.28, 0.3, 0.21, 0.229]
[2m[36m(func pid=22392)[0m 
== Status ==
Current time: 2024-01-07 12:00:59 (running for 00:03:05.62)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.393 |      0.21  |                   26 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.134 |      0.337 |                   25 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.616 |      0.368 |                   25 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 15.492 |      0.336 |                   24 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.24440298507462688
[2m[36m(func pid=21163)[0m top5: 0.7448694029850746
[2m[36m(func pid=21163)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=21163)[0m f1_macro: 0.21029608542730466
[2m[36m(func pid=21163)[0m f1_weighted: 0.23157446795660416
[2m[36m(func pid=21163)[0m f1_per_class: [0.281, 0.115, 0.227, 0.468, 0.044, 0.17, 0.107, 0.271, 0.088, 0.333]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3894589552238806
[2m[36m(func pid=21543)[0m top5: 0.9067164179104478
[2m[36m(func pid=21543)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=21543)[0m f1_macro: 0.3395109259699664
[2m[36m(func pid=21543)[0m f1_weighted: 0.41927712325380784
[2m[36m(func pid=21543)[0m f1_per_class: [0.552, 0.44, 0.261, 0.505, 0.073, 0.411, 0.392, 0.267, 0.204, 0.291]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7948 | Steps: 4 | Val loss: 3.9563 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 11.1630 | Steps: 4 | Val loss: 22.8407 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.3925 | Steps: 4 | Val loss: 2.1074 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=21969)[0m top1: 0.38386194029850745
[2m[36m(func pid=21969)[0m top5: 0.9137126865671642
[2m[36m(func pid=21969)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=21969)[0m f1_macro: 0.3353889240903759
[2m[36m(func pid=21969)[0m f1_weighted: 0.3858084544411201
[2m[36m(func pid=21969)[0m f1_per_class: [0.509, 0.305, 0.364, 0.534, 0.228, 0.337, 0.35, 0.316, 0.193, 0.219]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.9047 | Steps: 4 | Val loss: 1.8001 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=22392)[0m top1: 0.42257462686567165
[2m[36m(func pid=22392)[0m top5: 0.9351679104477612
[2m[36m(func pid=22392)[0m f1_micro: 0.42257462686567165
[2m[36m(func pid=22392)[0m f1_macro: 0.3749424215348422
[2m[36m(func pid=22392)[0m f1_weighted: 0.40899779491088184
[2m[36m(func pid=22392)[0m f1_per_class: [0.458, 0.477, 0.759, 0.572, 0.069, 0.36, 0.29, 0.251, 0.221, 0.293]
[2m[36m(func pid=22392)[0m 
== Status ==
Current time: 2024-01-07 12:01:04 (running for 00:03:10.83)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.392 |      0.21  |                   27 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.482 |      0.34  |                   26 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.795 |      0.335 |                   26 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.163 |      0.375 |                   25 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.24673507462686567
[2m[36m(func pid=21163)[0m top5: 0.75
[2m[36m(func pid=21163)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=21163)[0m f1_macro: 0.20985615135806404
[2m[36m(func pid=21163)[0m f1_weighted: 0.23676260794442858
[2m[36m(func pid=21163)[0m f1_per_class: [0.265, 0.127, 0.259, 0.464, 0.046, 0.192, 0.117, 0.27, 0.07, 0.288]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3885261194029851
[2m[36m(func pid=21543)[0m top5: 0.9081156716417911
[2m[36m(func pid=21543)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=21543)[0m f1_macro: 0.34542100427383204
[2m[36m(func pid=21543)[0m f1_weighted: 0.41855629783027803
[2m[36m(func pid=21543)[0m f1_per_class: [0.584, 0.435, 0.258, 0.492, 0.071, 0.424, 0.392, 0.278, 0.22, 0.299]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.3152 | Steps: 4 | Val loss: 4.6598 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 7.7143 | Steps: 4 | Val loss: 22.9175 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.3235 | Steps: 4 | Val loss: 2.0892 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=21969)[0m top1: 0.3306902985074627
[2m[36m(func pid=21969)[0m top5: 0.8908582089552238
[2m[36m(func pid=21969)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=21969)[0m f1_macro: 0.2900802851553439
[2m[36m(func pid=21969)[0m f1_weighted: 0.3427929573057946
[2m[36m(func pid=21969)[0m f1_per_class: [0.464, 0.316, 0.217, 0.488, 0.177, 0.286, 0.27, 0.342, 0.161, 0.181]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.1558 | Steps: 4 | Val loss: 1.7840 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 12:01:10 (running for 00:03:16.06)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.392 |      0.21  |                   27 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.905 |      0.345 |                   27 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  2.315 |      0.29  |                   27 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  7.714 |      0.438 |                   26 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=22392)[0m top1: 0.43796641791044777
[2m[36m(func pid=22392)[0m top5: 0.9118470149253731
[2m[36m(func pid=22392)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=22392)[0m f1_macro: 0.4381748502468583
[2m[36m(func pid=22392)[0m f1_weighted: 0.4284397324013424
[2m[36m(func pid=22392)[0m f1_per_class: [0.525, 0.53, 0.75, 0.539, 0.19, 0.363, 0.319, 0.323, 0.243, 0.6]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m top1: 0.25093283582089554
[2m[36m(func pid=21163)[0m top5: 0.7649253731343284
[2m[36m(func pid=21163)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=21163)[0m f1_macro: 0.2155533255868562
[2m[36m(func pid=21163)[0m f1_weighted: 0.24311305593980678
[2m[36m(func pid=21163)[0m f1_per_class: [0.274, 0.131, 0.259, 0.467, 0.049, 0.213, 0.126, 0.258, 0.065, 0.313]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3903917910447761
[2m[36m(func pid=21543)[0m top5: 0.9090485074626866
[2m[36m(func pid=21543)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=21543)[0m f1_macro: 0.34020166081146685
[2m[36m(func pid=21543)[0m f1_weighted: 0.4205232878611267
[2m[36m(func pid=21543)[0m f1_per_class: [0.559, 0.444, 0.195, 0.481, 0.079, 0.415, 0.408, 0.289, 0.219, 0.313]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.9606 | Steps: 4 | Val loss: 5.5994 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 7.4750 | Steps: 4 | Val loss: 26.3707 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.3873 | Steps: 4 | Val loss: 2.0668 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.0814 | Steps: 4 | Val loss: 1.8867 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=21969)[0m top1: 0.26865671641791045
[2m[36m(func pid=21969)[0m top5: 0.8404850746268657
[2m[36m(func pid=21969)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=21969)[0m f1_macro: 0.25511011517193755
[2m[36m(func pid=21969)[0m f1_weighted: 0.2889374281021535
[2m[36m(func pid=21969)[0m f1_per_class: [0.482, 0.338, 0.113, 0.408, 0.148, 0.231, 0.176, 0.345, 0.137, 0.174]
[2m[36m(func pid=21969)[0m 
== Status ==
Current time: 2024-01-07 12:01:15 (running for 00:03:21.26)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.387 |      0.231 |                   29 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.156 |      0.34  |                   28 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.961 |      0.255 |                   28 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  7.714 |      0.438 |                   26 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.2658582089552239
[2m[36m(func pid=21163)[0m top5: 0.7653917910447762
[2m[36m(func pid=21163)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=21163)[0m f1_macro: 0.23114465453806923
[2m[36m(func pid=21163)[0m f1_weighted: 0.25012093946706493
[2m[36m(func pid=21163)[0m f1_per_class: [0.293, 0.126, 0.333, 0.495, 0.049, 0.228, 0.114, 0.267, 0.09, 0.317]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=22392)[0m top1: 0.38992537313432835
[2m[36m(func pid=22392)[0m top5: 0.886660447761194
[2m[36m(func pid=22392)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=22392)[0m f1_macro: 0.3894548561003355
[2m[36m(func pid=22392)[0m f1_weighted: 0.384015963719495
[2m[36m(func pid=22392)[0m f1_per_class: [0.53, 0.508, 0.75, 0.403, 0.135, 0.286, 0.348, 0.332, 0.225, 0.376]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3736007462686567
[2m[36m(func pid=21543)[0m top5: 0.8913246268656716
[2m[36m(func pid=21543)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=21543)[0m f1_macro: 0.32602923382796395
[2m[36m(func pid=21543)[0m f1_weighted: 0.4096625422447732
[2m[36m(func pid=21543)[0m f1_per_class: [0.532, 0.421, 0.168, 0.465, 0.072, 0.412, 0.405, 0.29, 0.223, 0.272]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.0756 | Steps: 4 | Val loss: 6.6739 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.2547 | Steps: 4 | Val loss: 2.0503 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 15.1730 | Steps: 4 | Val loss: 31.4918 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=21969)[0m top1: 0.21548507462686567
[2m[36m(func pid=21969)[0m top5: 0.7952425373134329
[2m[36m(func pid=21969)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=21969)[0m f1_macro: 0.22183205846223789
[2m[36m(func pid=21969)[0m f1_weighted: 0.22578379067350407
[2m[36m(func pid=21969)[0m f1_per_class: [0.403, 0.328, 0.091, 0.26, 0.18, 0.195, 0.132, 0.329, 0.116, 0.184]
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.6984 | Steps: 4 | Val loss: 1.8909 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=21969)[0m 
== Status ==
Current time: 2024-01-07 12:01:20 (running for 00:03:26.34)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.255 |      0.239 |                   30 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.081 |      0.326 |                   29 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  2.076 |      0.222 |                   29 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  7.475 |      0.389 |                   27 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.2756529850746269
[2m[36m(func pid=21163)[0m top5: 0.7700559701492538
[2m[36m(func pid=21163)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=21163)[0m f1_macro: 0.2393407306517088
[2m[36m(func pid=21163)[0m f1_weighted: 0.26374018268757243
[2m[36m(func pid=21163)[0m f1_per_class: [0.312, 0.148, 0.301, 0.499, 0.048, 0.249, 0.13, 0.284, 0.092, 0.33]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=22392)[0m top1: 0.31203358208955223
[2m[36m(func pid=22392)[0m top5: 0.8465485074626866
[2m[36m(func pid=22392)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=22392)[0m f1_macro: 0.32990681987321246
[2m[36m(func pid=22392)[0m f1_weighted: 0.3208511335429392
[2m[36m(func pid=22392)[0m f1_per_class: [0.494, 0.426, 0.75, 0.309, 0.138, 0.175, 0.334, 0.294, 0.209, 0.169]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3843283582089552
[2m[36m(func pid=21543)[0m top5: 0.8936567164179104
[2m[36m(func pid=21543)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=21543)[0m f1_macro: 0.3320264645360568
[2m[36m(func pid=21543)[0m f1_weighted: 0.41391810233566845
[2m[36m(func pid=21543)[0m f1_per_class: [0.522, 0.459, 0.176, 0.44, 0.09, 0.417, 0.415, 0.297, 0.245, 0.258]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.0324 | Steps: 4 | Val loss: 7.2458 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.3739 | Steps: 4 | Val loss: 2.0558 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 16.6396 | Steps: 4 | Val loss: 38.2379 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.9031 | Steps: 4 | Val loss: 1.9360 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=21969)[0m top1: 0.19962686567164178
[2m[36m(func pid=21969)[0m top5: 0.7635261194029851
[2m[36m(func pid=21969)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=21969)[0m f1_macro: 0.2097778390999611
[2m[36m(func pid=21969)[0m f1_weighted: 0.20642308415111785
[2m[36m(func pid=21969)[0m f1_per_class: [0.416, 0.33, 0.066, 0.166, 0.14, 0.215, 0.148, 0.315, 0.128, 0.172]
[2m[36m(func pid=21969)[0m 
== Status ==
Current time: 2024-01-07 12:01:25 (running for 00:03:31.49)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.374 |      0.233 |                   31 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.698 |      0.332 |                   30 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.032 |      0.21  |                   30 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 15.173 |      0.33  |                   28 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.2681902985074627
[2m[36m(func pid=21163)[0m top5: 0.7728544776119403
[2m[36m(func pid=21163)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=21163)[0m f1_macro: 0.23312662985428245
[2m[36m(func pid=21163)[0m f1_weighted: 0.26851273022258226
[2m[36m(func pid=21163)[0m f1_per_class: [0.286, 0.193, 0.214, 0.472, 0.044, 0.264, 0.145, 0.271, 0.102, 0.342]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=22392)[0m top1: 0.27611940298507465
[2m[36m(func pid=22392)[0m top5: 0.8129664179104478
[2m[36m(func pid=22392)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=22392)[0m f1_macro: 0.30154704476688776
[2m[36m(func pid=22392)[0m f1_weighted: 0.29695030198730277
[2m[36m(func pid=22392)[0m f1_per_class: [0.484, 0.34, 0.75, 0.259, 0.088, 0.07, 0.393, 0.296, 0.227, 0.108]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3763992537313433
[2m[36m(func pid=21543)[0m top5: 0.8903917910447762
[2m[36m(func pid=21543)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=21543)[0m f1_macro: 0.32646871808112654
[2m[36m(func pid=21543)[0m f1_weighted: 0.401896241224916
[2m[36m(func pid=21543)[0m f1_per_class: [0.492, 0.458, 0.162, 0.419, 0.094, 0.419, 0.395, 0.291, 0.266, 0.267]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.8579 | Steps: 4 | Val loss: 7.5039 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.3678 | Steps: 4 | Val loss: 2.0487 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 5.9207 | Steps: 4 | Val loss: 42.7284 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.2700 | Steps: 4 | Val loss: 1.9445 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=21969)[0m top1: 0.20942164179104478
[2m[36m(func pid=21969)[0m top5: 0.7444029850746269
[2m[36m(func pid=21969)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=21969)[0m f1_macro: 0.21155794223607666
[2m[36m(func pid=21969)[0m f1_weighted: 0.21695124401389285
[2m[36m(func pid=21969)[0m f1_per_class: [0.373, 0.327, 0.06, 0.118, 0.131, 0.232, 0.223, 0.325, 0.142, 0.183]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21163)[0m top1: 0.261660447761194
[2m[36m(func pid=21163)[0m top5: 0.7854477611940298
[2m[36m(func pid=21163)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=21163)[0m f1_macro: 0.2301387459705583
[2m[36m(func pid=21163)[0m f1_weighted: 0.2667090848264398
[2m[36m(func pid=21163)[0m f1_per_class: [0.276, 0.178, 0.233, 0.459, 0.05, 0.236, 0.169, 0.273, 0.108, 0.318]
[2m[36m(func pid=21163)[0m 
== Status ==
Current time: 2024-01-07 12:01:30 (running for 00:03:36.83)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.368 |      0.23  |                   32 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.903 |      0.326 |                   31 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.858 |      0.212 |                   31 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 16.64  |      0.302 |                   29 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=22392)[0m top1: 0.2728544776119403
[2m[36m(func pid=22392)[0m top5: 0.7863805970149254
[2m[36m(func pid=22392)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=22392)[0m f1_macro: 0.2870206517393171
[2m[36m(func pid=22392)[0m f1_weighted: 0.2973307130926496
[2m[36m(func pid=22392)[0m f1_per_class: [0.361, 0.256, 0.8, 0.215, 0.071, 0.058, 0.497, 0.297, 0.229, 0.087]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m top1: 0.37173507462686567
[2m[36m(func pid=21543)[0m top5: 0.8847947761194029
[2m[36m(func pid=21543)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=21543)[0m f1_macro: 0.32279046211307993
[2m[36m(func pid=21543)[0m f1_weighted: 0.4044454753758194
[2m[36m(func pid=21543)[0m f1_per_class: [0.492, 0.424, 0.169, 0.409, 0.103, 0.421, 0.429, 0.333, 0.226, 0.22]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.0567 | Steps: 4 | Val loss: 8.3660 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.2200 | Steps: 4 | Val loss: 2.0412 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 6.3313 | Steps: 4 | Val loss: 48.1122 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.2453 | Steps: 4 | Val loss: 1.9659 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=21969)[0m top1: 0.19962686567164178
[2m[36m(func pid=21969)[0m top5: 0.6875
[2m[36m(func pid=21969)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=21969)[0m f1_macro: 0.20153260251326813
[2m[36m(func pid=21969)[0m f1_weighted: 0.2099879955631461
[2m[36m(func pid=21969)[0m f1_per_class: [0.282, 0.27, 0.051, 0.053, 0.074, 0.259, 0.284, 0.33, 0.189, 0.224]
[2m[36m(func pid=21969)[0m 
== Status ==
Current time: 2024-01-07 12:01:36 (running for 00:03:41.93)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.22  |      0.234 |                   33 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.27  |      0.323 |                   32 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.057 |      0.202 |                   32 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  5.921 |      0.287 |                   30 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.26259328358208955
[2m[36m(func pid=21163)[0m top5: 0.7793843283582089
[2m[36m(func pid=21163)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=21163)[0m f1_macro: 0.2339153103202965
[2m[36m(func pid=21163)[0m f1_weighted: 0.26011611083237685
[2m[36m(func pid=21163)[0m f1_per_class: [0.287, 0.16, 0.22, 0.469, 0.051, 0.255, 0.134, 0.289, 0.135, 0.34]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=22392)[0m top1: 0.269589552238806
[2m[36m(func pid=22392)[0m top5: 0.7481343283582089
[2m[36m(func pid=22392)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=22392)[0m f1_macro: 0.2582816972581937
[2m[36m(func pid=22392)[0m f1_weighted: 0.2730257142396275
[2m[36m(func pid=22392)[0m f1_per_class: [0.244, 0.205, 0.8, 0.128, 0.066, 0.072, 0.548, 0.202, 0.217, 0.101]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m top1: 0.37593283582089554
[2m[36m(func pid=21543)[0m top5: 0.8745335820895522
[2m[36m(func pid=21543)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=21543)[0m f1_macro: 0.32179941403004564
[2m[36m(func pid=21543)[0m f1_weighted: 0.408192951812546
[2m[36m(func pid=21543)[0m f1_per_class: [0.467, 0.452, 0.174, 0.409, 0.109, 0.422, 0.427, 0.352, 0.206, 0.2]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.3742 | Steps: 4 | Val loss: 8.1683 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.2313 | Steps: 4 | Val loss: 2.0462 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 17.7315 | Steps: 4 | Val loss: 56.0232 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.9704 | Steps: 4 | Val loss: 1.9806 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:01:41 (running for 00:03:47.12)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.231 |      0.235 |                   34 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.245 |      0.322 |                   33 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.057 |      0.202 |                   32 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  6.331 |      0.258 |                   31 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.259794776119403
[2m[36m(func pid=21163)[0m top5: 0.7747201492537313
[2m[36m(func pid=21163)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=21163)[0m f1_macro: 0.2353045740541349
[2m[36m(func pid=21163)[0m f1_weighted: 0.2628679474879233
[2m[36m(func pid=21163)[0m f1_per_class: [0.281, 0.149, 0.2, 0.456, 0.05, 0.292, 0.148, 0.296, 0.12, 0.362]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.208955223880597
[2m[36m(func pid=21969)[0m top5: 0.6861007462686567
[2m[36m(func pid=21969)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=21969)[0m f1_macro: 0.20283119744486272
[2m[36m(func pid=21969)[0m f1_weighted: 0.22219315193672076
[2m[36m(func pid=21969)[0m f1_per_class: [0.23, 0.228, 0.062, 0.059, 0.08, 0.303, 0.328, 0.335, 0.204, 0.2]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.24300373134328357
[2m[36m(func pid=22392)[0m top5: 0.71875
[2m[36m(func pid=22392)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=22392)[0m f1_macro: 0.23602714283529994
[2m[36m(func pid=22392)[0m f1_weighted: 0.2448526998239821
[2m[36m(func pid=22392)[0m f1_per_class: [0.118, 0.165, 0.769, 0.057, 0.082, 0.114, 0.544, 0.152, 0.215, 0.143]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m top1: 0.376865671641791
[2m[36m(func pid=21543)[0m top5: 0.8754664179104478
[2m[36m(func pid=21543)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=21543)[0m f1_macro: 0.3223825412428618
[2m[36m(func pid=21543)[0m f1_weighted: 0.40890066287779503
[2m[36m(func pid=21543)[0m f1_per_class: [0.489, 0.452, 0.169, 0.415, 0.112, 0.395, 0.435, 0.322, 0.221, 0.213]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.2542 | Steps: 4 | Val loss: 2.0169 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.3978 | Steps: 4 | Val loss: 7.4969 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 25.7344 | Steps: 4 | Val loss: 52.6146 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7110 | Steps: 4 | Val loss: 2.0055 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=21163)[0m top1: 0.279384328358209
[2m[36m(func pid=21163)[0m top5: 0.7924440298507462
[2m[36m(func pid=21163)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=21163)[0m f1_macro: 0.24758118814177607
[2m[36m(func pid=21163)[0m f1_weighted: 0.2885882390742303
[2m[36m(func pid=21163)[0m f1_per_class: [0.299, 0.207, 0.229, 0.477, 0.054, 0.288, 0.182, 0.299, 0.119, 0.324]
== Status ==
Current time: 2024-01-07 12:01:46 (running for 00:03:52.30)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.254 |      0.248 |                   35 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.97  |      0.322 |                   34 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  2.374 |      0.203 |                   33 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 17.732 |      0.236 |                   32 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.22994402985074627
[2m[36m(func pid=21969)[0m top5: 0.7248134328358209
[2m[36m(func pid=21969)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=21969)[0m f1_macro: 0.21411260020169434
[2m[36m(func pid=21969)[0m f1_weighted: 0.24667788381792982
[2m[36m(func pid=21969)[0m f1_per_class: [0.243, 0.2, 0.094, 0.121, 0.072, 0.311, 0.364, 0.316, 0.235, 0.185]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.25513059701492535
[2m[36m(func pid=22392)[0m top5: 0.7336753731343284
[2m[36m(func pid=22392)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=22392)[0m f1_macro: 0.228662141200706
[2m[36m(func pid=22392)[0m f1_weighted: 0.25534658484072215
[2m[36m(func pid=22392)[0m f1_per_class: [0.125, 0.17, 0.645, 0.105, 0.115, 0.135, 0.537, 0.101, 0.182, 0.171]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m top1: 0.386660447761194
[2m[36m(func pid=21543)[0m top5: 0.875
[2m[36m(func pid=21543)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=21543)[0m f1_macro: 0.3326139561630917
[2m[36m(func pid=21543)[0m f1_weighted: 0.41619720084289735
[2m[36m(func pid=21543)[0m f1_per_class: [0.473, 0.444, 0.181, 0.43, 0.133, 0.43, 0.429, 0.37, 0.223, 0.215]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.1791 | Steps: 4 | Val loss: 2.0072 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.6736 | Steps: 4 | Val loss: 7.0447 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 16.0396 | Steps: 4 | Val loss: 49.4817 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.7624 | Steps: 4 | Val loss: 2.0654 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 12:01:51 (running for 00:03:57.42)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.179 |      0.243 |                   36 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.711 |      0.333 |                   35 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.398 |      0.214 |                   34 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 25.734 |      0.229 |                   33 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.2681902985074627
[2m[36m(func pid=21163)[0m top5: 0.7975746268656716
[2m[36m(func pid=21163)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=21163)[0m f1_macro: 0.24343701915153332
[2m[36m(func pid=21163)[0m f1_weighted: 0.27330088442920486
[2m[36m(func pid=21163)[0m f1_per_class: [0.297, 0.204, 0.233, 0.457, 0.057, 0.293, 0.149, 0.294, 0.117, 0.333]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.24813432835820895
[2m[36m(func pid=21969)[0m top5: 0.7663246268656716
[2m[36m(func pid=21969)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=21969)[0m f1_macro: 0.22800022196624797
[2m[36m(func pid=21969)[0m f1_weighted: 0.270262507089444
[2m[36m(func pid=21969)[0m f1_per_class: [0.273, 0.155, 0.116, 0.202, 0.065, 0.352, 0.373, 0.329, 0.234, 0.18]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m top1: 0.2719216417910448
[2m[36m(func pid=22392)[0m top5: 0.742070895522388
[2m[36m(func pid=22392)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=22392)[0m f1_macro: 0.2226933195962139
[2m[36m(func pid=22392)[0m f1_weighted: 0.278119858920684
[2m[36m(func pid=22392)[0m f1_per_class: [0.27, 0.181, 0.313, 0.19, 0.137, 0.155, 0.516, 0.119, 0.153, 0.194]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m top1: 0.37080223880597013
[2m[36m(func pid=21543)[0m top5: 0.8698694029850746
[2m[36m(func pid=21543)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=21543)[0m f1_macro: 0.316999715725072
[2m[36m(func pid=21543)[0m f1_weighted: 0.39980276857988245
[2m[36m(func pid=21543)[0m f1_per_class: [0.435, 0.426, 0.17, 0.432, 0.113, 0.429, 0.388, 0.377, 0.191, 0.209]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.1661 | Steps: 4 | Val loss: 2.0092 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.0928 | Steps: 4 | Val loss: 6.5539 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.8385 | Steps: 4 | Val loss: 2.0639 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.7183 | Steps: 4 | Val loss: 49.0541 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 12:01:56 (running for 00:04:02.74)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.166 |      0.248 |                   37 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.762 |      0.317 |                   36 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.674 |      0.228 |                   35 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 16.04  |      0.223 |                   34 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.2462686567164179
[2m[36m(func pid=21163)[0m top5: 0.8027052238805971
[2m[36m(func pid=21163)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=21163)[0m f1_macro: 0.24798219141493072
[2m[36m(func pid=21163)[0m f1_weighted: 0.2615977069486581
[2m[36m(func pid=21163)[0m f1_per_class: [0.378, 0.23, 0.238, 0.402, 0.041, 0.259, 0.154, 0.283, 0.115, 0.379]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.26725746268656714
[2m[36m(func pid=21969)[0m top5: 0.8041044776119403
[2m[36m(func pid=21969)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=21969)[0m f1_macro: 0.24340258166584344
[2m[36m(func pid=21969)[0m f1_weighted: 0.290746528617739
[2m[36m(func pid=21969)[0m f1_per_class: [0.303, 0.14, 0.195, 0.277, 0.06, 0.349, 0.382, 0.312, 0.228, 0.188]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.375
[2m[36m(func pid=21543)[0m top5: 0.8722014925373134
[2m[36m(func pid=21543)[0m f1_micro: 0.375
[2m[36m(func pid=21543)[0m f1_macro: 0.3199838058917826
[2m[36m(func pid=21543)[0m f1_weighted: 0.4042462381254965
[2m[36m(func pid=21543)[0m f1_per_class: [0.4, 0.422, 0.172, 0.429, 0.147, 0.433, 0.404, 0.399, 0.192, 0.203]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m top1: 0.2523320895522388
[2m[36m(func pid=22392)[0m top5: 0.7756529850746269
[2m[36m(func pid=22392)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=22392)[0m f1_macro: 0.21994146555364025
[2m[36m(func pid=22392)[0m f1_weighted: 0.2861721395107139
[2m[36m(func pid=22392)[0m f1_per_class: [0.254, 0.229, 0.136, 0.222, 0.139, 0.219, 0.452, 0.193, 0.123, 0.232]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.1070 | Steps: 4 | Val loss: 1.9889 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5689 | Steps: 4 | Val loss: 5.8664 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6169 | Steps: 4 | Val loss: 2.0761 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 7.3370 | Steps: 4 | Val loss: 45.0275 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:02:01 (running for 00:04:07.91)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.107 |      0.255 |                   38 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.838 |      0.32  |                   37 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.093 |      0.243 |                   36 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  2.718 |      0.22  |                   35 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.27052238805970147
[2m[36m(func pid=21163)[0m top5: 0.8138992537313433
[2m[36m(func pid=21163)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=21163)[0m f1_macro: 0.25496478089678215
[2m[36m(func pid=21163)[0m f1_weighted: 0.2890580204688074
[2m[36m(func pid=21163)[0m f1_per_class: [0.369, 0.263, 0.198, 0.421, 0.05, 0.307, 0.192, 0.291, 0.124, 0.333]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.3069029850746269
[2m[36m(func pid=21969)[0m top5: 0.8456156716417911
[2m[36m(func pid=21969)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=21969)[0m f1_macro: 0.27637411439519355
[2m[36m(func pid=21969)[0m f1_weighted: 0.3276970598118781
[2m[36m(func pid=21969)[0m f1_per_class: [0.336, 0.165, 0.283, 0.371, 0.074, 0.358, 0.39, 0.315, 0.265, 0.206]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3773320895522388
[2m[36m(func pid=21543)[0m top5: 0.8666044776119403
[2m[36m(func pid=21543)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=21543)[0m f1_macro: 0.3184984149836621
[2m[36m(func pid=21543)[0m f1_weighted: 0.40421320861121496
[2m[36m(func pid=21543)[0m f1_per_class: [0.381, 0.435, 0.187, 0.429, 0.153, 0.426, 0.406, 0.357, 0.204, 0.207]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m top1: 0.26632462686567165
[2m[36m(func pid=22392)[0m top5: 0.8166977611940298
[2m[36m(func pid=22392)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=22392)[0m f1_macro: 0.24140399408179802
[2m[36m(func pid=22392)[0m f1_weighted: 0.3132992593753268
[2m[36m(func pid=22392)[0m f1_per_class: [0.192, 0.298, 0.068, 0.364, 0.156, 0.257, 0.343, 0.255, 0.141, 0.341]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.0489 | Steps: 4 | Val loss: 1.9657 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.5008 | Steps: 4 | Val loss: 5.1888 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.9113 | Steps: 4 | Val loss: 2.0597 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 16.7030 | Steps: 4 | Val loss: 48.4307 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 12:02:07 (running for 00:04:13.13)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.049 |      0.271 |                   39 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.617 |      0.318 |                   38 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.569 |      0.276 |                   37 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  7.337 |      0.241 |                   36 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.2947761194029851
[2m[36m(func pid=21163)[0m top5: 0.8190298507462687
[2m[36m(func pid=21163)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=21163)[0m f1_macro: 0.27051518672600655
[2m[36m(func pid=21163)[0m f1_weighted: 0.3128252416607154
[2m[36m(func pid=21163)[0m f1_per_class: [0.375, 0.275, 0.224, 0.451, 0.061, 0.355, 0.214, 0.304, 0.142, 0.304]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.36007462686567165
[2m[36m(func pid=21969)[0m top5: 0.8824626865671642
[2m[36m(func pid=21969)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=21969)[0m f1_macro: 0.31524570229306337
[2m[36m(func pid=21969)[0m f1_weighted: 0.3680070023122551
[2m[36m(func pid=21969)[0m f1_per_class: [0.447, 0.207, 0.351, 0.494, 0.112, 0.378, 0.37, 0.31, 0.25, 0.232]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3824626865671642
[2m[36m(func pid=21543)[0m top5: 0.8689365671641791
[2m[36m(func pid=21543)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=21543)[0m f1_macro: 0.3226711126179159
[2m[36m(func pid=21543)[0m f1_weighted: 0.4122844924844554
[2m[36m(func pid=21543)[0m f1_per_class: [0.378, 0.434, 0.171, 0.43, 0.171, 0.433, 0.428, 0.367, 0.207, 0.207]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m top1: 0.2593283582089552
[2m[36m(func pid=22392)[0m top5: 0.8190298507462687
[2m[36m(func pid=22392)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=22392)[0m f1_macro: 0.2513199633150368
[2m[36m(func pid=22392)[0m f1_weighted: 0.3122116246976527
[2m[36m(func pid=22392)[0m f1_per_class: [0.192, 0.349, 0.035, 0.391, 0.037, 0.286, 0.256, 0.318, 0.181, 0.469]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.0733 | Steps: 4 | Val loss: 1.9648 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4090 | Steps: 4 | Val loss: 5.0643 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.2762 | Steps: 4 | Val loss: 2.0648 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 12:02:12 (running for 00:04:18.17)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.073 |      0.273 |                   40 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.911 |      0.323 |                   39 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.501 |      0.315 |                   38 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 16.703 |      0.251 |                   37 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.28218283582089554
[2m[36m(func pid=21163)[0m top5: 0.8208955223880597
[2m[36m(func pid=21163)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=21163)[0m f1_macro: 0.2727178006167313
[2m[36m(func pid=21163)[0m f1_weighted: 0.2955643311764274
[2m[36m(func pid=21163)[0m f1_per_class: [0.452, 0.248, 0.232, 0.423, 0.059, 0.337, 0.199, 0.304, 0.132, 0.341]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 15.0114 | Steps: 4 | Val loss: 46.6760 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=21969)[0m top1: 0.3689365671641791
[2m[36m(func pid=21969)[0m top5: 0.8894589552238806
[2m[36m(func pid=21969)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=21969)[0m f1_macro: 0.33717200428917116
[2m[36m(func pid=21969)[0m f1_weighted: 0.367920897129607
[2m[36m(func pid=21969)[0m f1_per_class: [0.448, 0.249, 0.464, 0.501, 0.141, 0.371, 0.329, 0.345, 0.273, 0.251]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3903917910447761
[2m[36m(func pid=21543)[0m top5: 0.8703358208955224
[2m[36m(func pid=21543)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=21543)[0m f1_macro: 0.32464841732315636
[2m[36m(func pid=21543)[0m f1_weighted: 0.41737672048751556
[2m[36m(func pid=21543)[0m f1_per_class: [0.348, 0.459, 0.184, 0.466, 0.144, 0.427, 0.401, 0.369, 0.198, 0.25]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m top1: 0.3045708955223881
[2m[36m(func pid=22392)[0m top5: 0.8404850746268657
[2m[36m(func pid=22392)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=22392)[0m f1_macro: 0.30285439343949394
[2m[36m(func pid=22392)[0m f1_weighted: 0.3376248458724639
[2m[36m(func pid=22392)[0m f1_per_class: [0.455, 0.427, 0.041, 0.484, 0.08, 0.31, 0.174, 0.333, 0.195, 0.531]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.9467 | Steps: 4 | Val loss: 1.9729 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.2536 | Steps: 4 | Val loss: 4.9745 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 12:02:17 (running for 00:04:23.36)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.947 |      0.261 |                   41 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.276 |      0.325 |                   40 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.409 |      0.337 |                   39 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 15.011 |      0.303 |                   38 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.279384328358209
[2m[36m(func pid=21163)[0m top5: 0.8143656716417911
[2m[36m(func pid=21163)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=21163)[0m f1_macro: 0.26140080109467806
[2m[36m(func pid=21163)[0m f1_weighted: 0.2965724191575354
[2m[36m(func pid=21163)[0m f1_per_class: [0.345, 0.267, 0.19, 0.405, 0.06, 0.354, 0.21, 0.308, 0.13, 0.346]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.0039 | Steps: 4 | Val loss: 2.1448 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 21.2910 | Steps: 4 | Val loss: 42.4270 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=21969)[0m top1: 0.3931902985074627
[2m[36m(func pid=21969)[0m top5: 0.8992537313432836
[2m[36m(func pid=21969)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=21969)[0m f1_macro: 0.366706181130807
[2m[36m(func pid=21969)[0m f1_weighted: 0.3859080373535229
[2m[36m(func pid=21969)[0m f1_per_class: [0.486, 0.363, 0.558, 0.531, 0.148, 0.387, 0.287, 0.332, 0.245, 0.331]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.37919776119402987
[2m[36m(func pid=21543)[0m top5: 0.8647388059701493
[2m[36m(func pid=21543)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=21543)[0m f1_macro: 0.3149722992489387
[2m[36m(func pid=21543)[0m f1_weighted: 0.4107993677697541
[2m[36m(func pid=21543)[0m f1_per_class: [0.338, 0.439, 0.156, 0.465, 0.135, 0.425, 0.393, 0.37, 0.203, 0.224]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.0121 | Steps: 4 | Val loss: 1.9628 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=22392)[0m top1: 0.3568097014925373
[2m[36m(func pid=22392)[0m top5: 0.8484141791044776
[2m[36m(func pid=22392)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=22392)[0m f1_macro: 0.3364315687409575
[2m[36m(func pid=22392)[0m f1_weighted: 0.3595077176285427
[2m[36m(func pid=22392)[0m f1_per_class: [0.557, 0.455, 0.077, 0.501, 0.159, 0.33, 0.193, 0.327, 0.254, 0.513]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6105 | Steps: 4 | Val loss: 5.2382 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 12:02:22 (running for 00:04:28.67)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.012 |      0.262 |                   42 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.004 |      0.315 |                   41 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.254 |      0.367 |                   40 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 21.291 |      0.336 |                   39 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.28638059701492535
[2m[36m(func pid=21163)[0m top5: 0.8176305970149254
[2m[36m(func pid=21163)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=21163)[0m f1_macro: 0.2619256300994385
[2m[36m(func pid=21163)[0m f1_weighted: 0.30486343345576017
[2m[36m(func pid=21163)[0m f1_per_class: [0.358, 0.295, 0.169, 0.399, 0.065, 0.363, 0.222, 0.315, 0.142, 0.291]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6335 | Steps: 4 | Val loss: 2.1452 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 8.2195 | Steps: 4 | Val loss: 40.8868 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=21969)[0m top1: 0.39225746268656714
[2m[36m(func pid=21969)[0m top5: 0.8913246268656716
[2m[36m(func pid=21969)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=21969)[0m f1_macro: 0.3781646424446058
[2m[36m(func pid=21969)[0m f1_weighted: 0.377109534367346
[2m[36m(func pid=21969)[0m f1_per_class: [0.515, 0.456, 0.667, 0.527, 0.187, 0.377, 0.214, 0.305, 0.22, 0.314]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3810634328358209
[2m[36m(func pid=21543)[0m top5: 0.871268656716418
[2m[36m(func pid=21543)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=21543)[0m f1_macro: 0.31710161027651473
[2m[36m(func pid=21543)[0m f1_weighted: 0.4155900772306708
[2m[36m(func pid=21543)[0m f1_per_class: [0.398, 0.438, 0.145, 0.47, 0.137, 0.423, 0.408, 0.347, 0.204, 0.201]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.8172 | Steps: 4 | Val loss: 1.9412 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=22392)[0m top1: 0.3805970149253731
[2m[36m(func pid=22392)[0m top5: 0.8591417910447762
[2m[36m(func pid=22392)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=22392)[0m f1_macro: 0.34084432332533804
[2m[36m(func pid=22392)[0m f1_weighted: 0.3659201753377672
[2m[36m(func pid=22392)[0m f1_per_class: [0.568, 0.466, 0.176, 0.505, 0.132, 0.332, 0.202, 0.327, 0.267, 0.432]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.2452 | Steps: 4 | Val loss: 5.5012 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 12:02:27 (running for 00:04:33.91)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.817 |      0.269 |                   43 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.634 |      0.317 |                   42 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.61  |      0.378 |                   41 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  8.22  |      0.341 |                   40 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.29757462686567165
[2m[36m(func pid=21163)[0m top5: 0.8222947761194029
[2m[36m(func pid=21163)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=21163)[0m f1_macro: 0.2687500135946128
[2m[36m(func pid=21163)[0m f1_weighted: 0.31665418856728983
[2m[36m(func pid=21163)[0m f1_per_class: [0.358, 0.305, 0.186, 0.402, 0.072, 0.362, 0.252, 0.319, 0.15, 0.282]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.7542 | Steps: 4 | Val loss: 2.1820 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 3.6487 | Steps: 4 | Val loss: 40.1005 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=21969)[0m top1: 0.38152985074626866
[2m[36m(func pid=21969)[0m top5: 0.8847947761194029
[2m[36m(func pid=21969)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=21969)[0m f1_macro: 0.3716012365284014
[2m[36m(func pid=21969)[0m f1_weighted: 0.37103716339288795
[2m[36m(func pid=21969)[0m f1_per_class: [0.471, 0.491, 0.615, 0.506, 0.212, 0.371, 0.201, 0.304, 0.2, 0.345]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.373134328358209
[2m[36m(func pid=21543)[0m top5: 0.8731343283582089
[2m[36m(func pid=21543)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=21543)[0m f1_macro: 0.310614564624196
[2m[36m(func pid=21543)[0m f1_weighted: 0.40837862286420706
[2m[36m(func pid=21543)[0m f1_per_class: [0.372, 0.421, 0.152, 0.475, 0.144, 0.424, 0.39, 0.346, 0.206, 0.175]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.8709 | Steps: 4 | Val loss: 1.9255 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=22392)[0m top1: 0.3880597014925373
[2m[36m(func pid=22392)[0m top5: 0.8666044776119403
[2m[36m(func pid=22392)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=22392)[0m f1_macro: 0.3721103865613659
[2m[36m(func pid=22392)[0m f1_weighted: 0.3796899916686379
[2m[36m(func pid=22392)[0m f1_per_class: [0.547, 0.475, 0.382, 0.468, 0.16, 0.325, 0.277, 0.325, 0.249, 0.513]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.0460 | Steps: 4 | Val loss: 5.9054 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=21163)[0m top1: 0.30597014925373134
[2m[36m(func pid=21163)[0m top5: 0.8278917910447762
[2m[36m(func pid=21163)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=21163)[0m f1_macro: 0.27176550255041715
[2m[36m(func pid=21163)[0m f1_weighted: 0.32730253872711457
[2m[36m(func pid=21163)[0m f1_per_class: [0.366, 0.293, 0.183, 0.4, 0.078, 0.39, 0.285, 0.329, 0.131, 0.261]
[2m[36m(func pid=21163)[0m 
== Status ==
Current time: 2024-01-07 12:02:33 (running for 00:04:39.14)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.871 |      0.272 |                   44 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.754 |      0.311 |                   43 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.245 |      0.372 |                   42 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  3.649 |      0.372 |                   41 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8779 | Steps: 4 | Val loss: 2.1923 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 5.7219 | Steps: 4 | Val loss: 39.7992 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=21969)[0m top1: 0.35774253731343286
[2m[36m(func pid=21969)[0m top5: 0.8675373134328358
[2m[36m(func pid=21969)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=21969)[0m f1_macro: 0.36270390514162815
[2m[36m(func pid=21969)[0m f1_weighted: 0.3523646861907198
[2m[36m(func pid=21969)[0m f1_per_class: [0.39, 0.49, 0.667, 0.478, 0.184, 0.367, 0.172, 0.304, 0.17, 0.404]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.363339552238806
[2m[36m(func pid=21543)[0m top5: 0.8763992537313433
[2m[36m(func pid=21543)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=21543)[0m f1_macro: 0.3096094738068793
[2m[36m(func pid=21543)[0m f1_weighted: 0.3983660059691237
[2m[36m(func pid=21543)[0m f1_per_class: [0.414, 0.413, 0.162, 0.445, 0.15, 0.417, 0.393, 0.336, 0.191, 0.174]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.9232 | Steps: 4 | Val loss: 1.9155 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=22392)[0m top1: 0.39365671641791045
[2m[36m(func pid=22392)[0m top5: 0.867070895522388
[2m[36m(func pid=22392)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=22392)[0m f1_macro: 0.4014786239573872
[2m[36m(func pid=22392)[0m f1_weighted: 0.3927419695946298
[2m[36m(func pid=22392)[0m f1_per_class: [0.469, 0.491, 0.65, 0.427, 0.163, 0.324, 0.347, 0.328, 0.244, 0.571]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.5708 | Steps: 4 | Val loss: 6.1867 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 12:02:38 (running for 00:04:44.31)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.923 |      0.279 |                   45 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.878 |      0.31  |                   44 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.046 |      0.363 |                   43 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  5.722 |      0.401 |                   42 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.31576492537313433
[2m[36m(func pid=21163)[0m top5: 0.8353544776119403
[2m[36m(func pid=21163)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=21163)[0m f1_macro: 0.2794859961674065
[2m[36m(func pid=21163)[0m f1_weighted: 0.33862527548424537
[2m[36m(func pid=21163)[0m f1_per_class: [0.4, 0.304, 0.195, 0.408, 0.081, 0.402, 0.304, 0.316, 0.142, 0.242]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5685 | Steps: 4 | Val loss: 2.2244 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 6.0496 | Steps: 4 | Val loss: 38.8690 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=21969)[0m top1: 0.332089552238806
[2m[36m(func pid=21969)[0m top5: 0.8596082089552238
[2m[36m(func pid=21969)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=21969)[0m f1_macro: 0.3481902558060542
[2m[36m(func pid=21969)[0m f1_weighted: 0.3326106046628813
[2m[36m(func pid=21969)[0m f1_per_class: [0.395, 0.488, 0.686, 0.428, 0.148, 0.326, 0.173, 0.3, 0.164, 0.375]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3572761194029851
[2m[36m(func pid=21543)[0m top5: 0.875
[2m[36m(func pid=21543)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=21543)[0m f1_macro: 0.3046486971639463
[2m[36m(func pid=21543)[0m f1_weighted: 0.39595322757730655
[2m[36m(func pid=21543)[0m f1_per_class: [0.4, 0.408, 0.145, 0.437, 0.165, 0.405, 0.404, 0.316, 0.197, 0.168]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.9248 | Steps: 4 | Val loss: 1.9046 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=22392)[0m top1: 0.39552238805970147
[2m[36m(func pid=22392)[0m top5: 0.875
[2m[36m(func pid=22392)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=22392)[0m f1_macro: 0.4126836360157478
[2m[36m(func pid=22392)[0m f1_weighted: 0.39941030188333976
[2m[36m(func pid=22392)[0m f1_per_class: [0.435, 0.482, 0.828, 0.373, 0.2, 0.318, 0.435, 0.302, 0.224, 0.531]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.9599 | Steps: 4 | Val loss: 6.4739 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 12:02:43 (running for 00:04:49.58)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.925 |      0.286 |                   46 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.569 |      0.305 |                   45 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.571 |      0.348 |                   44 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  6.05  |      0.413 |                   43 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3199626865671642
[2m[36m(func pid=21163)[0m top5: 0.8386194029850746
[2m[36m(func pid=21163)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=21163)[0m f1_macro: 0.2859505187071227
[2m[36m(func pid=21163)[0m f1_weighted: 0.3422761542087997
[2m[36m(func pid=21163)[0m f1_per_class: [0.444, 0.322, 0.164, 0.404, 0.083, 0.424, 0.296, 0.322, 0.149, 0.249]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.0864 | Steps: 4 | Val loss: 2.1750 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=21969)[0m top1: 0.3148320895522388
[2m[36m(func pid=21969)[0m top5: 0.8549440298507462
[2m[36m(func pid=21969)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=21969)[0m f1_macro: 0.3345397412986459
[2m[36m(func pid=21969)[0m f1_weighted: 0.318657796657098
[2m[36m(func pid=21969)[0m f1_per_class: [0.354, 0.495, 0.686, 0.372, 0.127, 0.311, 0.185, 0.299, 0.153, 0.364]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 6.0487 | Steps: 4 | Val loss: 42.9642 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
[2m[36m(func pid=21543)[0m top1: 0.3614738805970149
[2m[36m(func pid=21543)[0m top5: 0.8833955223880597
[2m[36m(func pid=21543)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=21543)[0m f1_macro: 0.30948617429308767
[2m[36m(func pid=21543)[0m f1_weighted: 0.40100365269898575
[2m[36m(func pid=21543)[0m f1_per_class: [0.422, 0.349, 0.167, 0.465, 0.195, 0.422, 0.422, 0.321, 0.165, 0.167]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.9070 | Steps: 4 | Val loss: 1.9146 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=22392)[0m top1: 0.37173507462686567
[2m[36m(func pid=22392)[0m top5: 0.8535447761194029
[2m[36m(func pid=22392)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=22392)[0m f1_macro: 0.3746600997441597
[2m[36m(func pid=22392)[0m f1_weighted: 0.367601085633682
[2m[36m(func pid=22392)[0m f1_per_class: [0.345, 0.462, 0.8, 0.24, 0.168, 0.307, 0.489, 0.256, 0.217, 0.464]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.5993 | Steps: 4 | Val loss: 6.7173 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=21163)[0m top1: 0.3003731343283582
[2m[36m(func pid=21163)[0m top5: 0.8311567164179104
[2m[36m(func pid=21163)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=21163)[0m f1_macro: 0.28130807187450124
[2m[36m(func pid=21163)[0m f1_weighted: 0.32504075399023274
[2m[36m(func pid=21163)[0m f1_per_class: [0.449, 0.317, 0.127, 0.339, 0.065, 0.421, 0.297, 0.333, 0.191, 0.274]
== Status ==
Current time: 2024-01-07 12:02:48 (running for 00:04:54.82)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.907 |      0.281 |                   47 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.086 |      0.309 |                   46 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.96  |      0.335 |                   45 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  6.049 |      0.375 |                   44 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.8767 | Steps: 4 | Val loss: 2.2020 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 3.8258 | Steps: 4 | Val loss: 47.1698 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=21969)[0m top1: 0.3045708955223881
[2m[36m(func pid=21969)[0m top5: 0.8460820895522388
[2m[36m(func pid=21969)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=21969)[0m f1_macro: 0.32590936282608735
[2m[36m(func pid=21969)[0m f1_weighted: 0.3125312661589117
[2m[36m(func pid=21969)[0m f1_per_class: [0.37, 0.455, 0.667, 0.296, 0.086, 0.314, 0.255, 0.306, 0.18, 0.33]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3614738805970149
[2m[36m(func pid=21543)[0m top5: 0.878731343283582
[2m[36m(func pid=21543)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=21543)[0m f1_macro: 0.30890332551375493
[2m[36m(func pid=21543)[0m f1_weighted: 0.4024151991340692
[2m[36m(func pid=21543)[0m f1_per_class: [0.4, 0.371, 0.151, 0.439, 0.21, 0.422, 0.439, 0.327, 0.168, 0.162]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.1093 | Steps: 4 | Val loss: 1.9002 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=22392)[0m top1: 0.36986940298507465
[2m[36m(func pid=22392)[0m top5: 0.8292910447761194
[2m[36m(func pid=22392)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=22392)[0m f1_macro: 0.35585163169127554
[2m[36m(func pid=22392)[0m f1_weighted: 0.3520286529500544
[2m[36m(func pid=22392)[0m f1_per_class: [0.343, 0.441, 0.8, 0.17, 0.146, 0.287, 0.535, 0.195, 0.221, 0.421]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.7557 | Steps: 4 | Val loss: 7.0787 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:02:54 (running for 00:05:00.13)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  2.109 |      0.274 |                   48 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.877 |      0.309 |                   47 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.599 |      0.326 |                   46 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  3.826 |      0.356 |                   45 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3101679104477612
[2m[36m(func pid=21163)[0m top5: 0.8344216417910447
[2m[36m(func pid=21163)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=21163)[0m f1_macro: 0.274451467572981
[2m[36m(func pid=21163)[0m f1_weighted: 0.33497045885626886
[2m[36m(func pid=21163)[0m f1_per_class: [0.398, 0.326, 0.127, 0.34, 0.085, 0.441, 0.328, 0.317, 0.166, 0.217]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7403 | Steps: 4 | Val loss: 2.2000 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 27.0135 | Steps: 4 | Val loss: 53.2485 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=21969)[0m top1: 0.30177238805970147
[2m[36m(func pid=21969)[0m top5: 0.832089552238806
[2m[36m(func pid=21969)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=21969)[0m f1_macro: 0.3151953366226079
[2m[36m(func pid=21969)[0m f1_weighted: 0.3153145851984823
[2m[36m(func pid=21969)[0m f1_per_class: [0.34, 0.427, 0.649, 0.223, 0.071, 0.316, 0.355, 0.288, 0.185, 0.299]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3656716417910448
[2m[36m(func pid=21543)[0m top5: 0.8931902985074627
[2m[36m(func pid=21543)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=21543)[0m f1_macro: 0.31459141057191925
[2m[36m(func pid=21543)[0m f1_weighted: 0.40434029061392635
[2m[36m(func pid=21543)[0m f1_per_class: [0.425, 0.406, 0.152, 0.434, 0.196, 0.416, 0.43, 0.327, 0.171, 0.189]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.8316 | Steps: 4 | Val loss: 1.8993 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=22392)[0m top1: 0.34468283582089554
[2m[36m(func pid=22392)[0m top5: 0.7882462686567164
[2m[36m(func pid=22392)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=22392)[0m f1_macro: 0.3329899250042135
[2m[36m(func pid=22392)[0m f1_weighted: 0.3181732677694798
[2m[36m(func pid=22392)[0m f1_per_class: [0.355, 0.415, 0.8, 0.09, 0.112, 0.244, 0.536, 0.162, 0.216, 0.4]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.1428 | Steps: 4 | Val loss: 7.7744 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:02:59 (running for 00:05:05.41)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.832 |      0.285 |                   49 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.74  |      0.315 |                   48 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.756 |      0.315 |                   47 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 27.014 |      0.333 |                   46 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3162313432835821
[2m[36m(func pid=21163)[0m top5: 0.8302238805970149
[2m[36m(func pid=21163)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=21163)[0m f1_macro: 0.28516250548571936
[2m[36m(func pid=21163)[0m f1_weighted: 0.3428545198669809
[2m[36m(func pid=21163)[0m f1_per_class: [0.453, 0.316, 0.141, 0.365, 0.078, 0.435, 0.332, 0.324, 0.174, 0.232]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.7683 | Steps: 4 | Val loss: 2.2787 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=21969)[0m top1: 0.2775186567164179
[2m[36m(func pid=21969)[0m top5: 0.8143656716417911
[2m[36m(func pid=21969)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=21969)[0m f1_macro: 0.299124601907478
[2m[36m(func pid=21969)[0m f1_weighted: 0.3013800639304311
[2m[36m(func pid=21969)[0m f1_per_class: [0.304, 0.376, 0.686, 0.182, 0.055, 0.301, 0.391, 0.245, 0.21, 0.241]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 12.1612 | Steps: 4 | Val loss: 57.6022 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=21543)[0m top1: 0.35261194029850745
[2m[36m(func pid=21543)[0m top5: 0.8885261194029851
[2m[36m(func pid=21543)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=21543)[0m f1_macro: 0.30607966851686697
[2m[36m(func pid=21543)[0m f1_weighted: 0.38972120745446875
[2m[36m(func pid=21543)[0m f1_per_class: [0.437, 0.428, 0.109, 0.412, 0.147, 0.409, 0.39, 0.332, 0.171, 0.227]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.8261 | Steps: 4 | Val loss: 1.8908 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=22392)[0m top1: 0.3180970149253731
[2m[36m(func pid=22392)[0m top5: 0.7653917910447762
[2m[36m(func pid=22392)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=22392)[0m f1_macro: 0.3133030121823851
[2m[36m(func pid=22392)[0m f1_weighted: 0.2958879662592631
[2m[36m(func pid=22392)[0m f1_per_class: [0.351, 0.321, 0.8, 0.075, 0.099, 0.24, 0.53, 0.187, 0.207, 0.323]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5384 | Steps: 4 | Val loss: 8.1281 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.9422 | Steps: 4 | Val loss: 2.3608 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 12:03:04 (running for 00:05:10.75)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.826 |      0.286 |                   50 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.768 |      0.306 |                   49 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.143 |      0.299 |                   48 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 12.161 |      0.313 |                   47 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.31902985074626866
[2m[36m(func pid=21163)[0m top5: 0.832089552238806
[2m[36m(func pid=21163)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=21163)[0m f1_macro: 0.2859306024989841
[2m[36m(func pid=21163)[0m f1_weighted: 0.3434917114076672
[2m[36m(func pid=21163)[0m f1_per_class: [0.446, 0.34, 0.15, 0.371, 0.081, 0.431, 0.318, 0.323, 0.168, 0.232]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.2593283582089552
[2m[36m(func pid=21969)[0m top5: 0.8050373134328358
[2m[36m(func pid=21969)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=21969)[0m f1_macro: 0.28985939890206747
[2m[36m(func pid=21969)[0m f1_weighted: 0.29081318062253214
[2m[36m(func pid=21969)[0m f1_per_class: [0.264, 0.335, 0.706, 0.186, 0.045, 0.286, 0.388, 0.199, 0.278, 0.213]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 6.8109 | Steps: 4 | Val loss: 57.3349 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=21543)[0m top1: 0.33955223880597013
[2m[36m(func pid=21543)[0m top5: 0.8768656716417911
[2m[36m(func pid=21543)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=21543)[0m f1_macro: 0.29952861847858675
[2m[36m(func pid=21543)[0m f1_weighted: 0.3742430716624
[2m[36m(func pid=21543)[0m f1_per_class: [0.467, 0.422, 0.093, 0.353, 0.109, 0.414, 0.397, 0.299, 0.195, 0.247]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.8176 | Steps: 4 | Val loss: 1.8900 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=22392)[0m top1: 0.31576492537313433
[2m[36m(func pid=22392)[0m top5: 0.7625932835820896
[2m[36m(func pid=22392)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=22392)[0m f1_macro: 0.31496996416004597
[2m[36m(func pid=22392)[0m f1_weighted: 0.3090407681640676
[2m[36m(func pid=22392)[0m f1_per_class: [0.407, 0.31, 0.8, 0.131, 0.098, 0.223, 0.533, 0.209, 0.181, 0.258]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.0315 | Steps: 4 | Val loss: 8.3649 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.0404 | Steps: 4 | Val loss: 2.4124 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 12:03:10 (running for 00:05:16.19)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.818 |      0.284 |                   51 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.942 |      0.3   |                   50 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.538 |      0.29  |                   49 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  6.811 |      0.315 |                   48 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.31669776119402987
[2m[36m(func pid=21163)[0m top5: 0.8367537313432836
[2m[36m(func pid=21163)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=21163)[0m f1_macro: 0.28393229827104477
[2m[36m(func pid=21163)[0m f1_weighted: 0.3364975948411941
[2m[36m(func pid=21163)[0m f1_per_class: [0.453, 0.36, 0.168, 0.35, 0.082, 0.426, 0.308, 0.307, 0.162, 0.224]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.24813432835820895
[2m[36m(func pid=21969)[0m top5: 0.8041044776119403
[2m[36m(func pid=21969)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=21969)[0m f1_macro: 0.28254510892162676
[2m[36m(func pid=21969)[0m f1_weighted: 0.28570349047635707
[2m[36m(func pid=21969)[0m f1_per_class: [0.222, 0.285, 0.706, 0.214, 0.046, 0.278, 0.372, 0.243, 0.272, 0.188]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 4.0203 | Steps: 4 | Val loss: 57.0363 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=21543)[0m top1: 0.33488805970149255
[2m[36m(func pid=21543)[0m top5: 0.867070895522388
[2m[36m(func pid=21543)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=21543)[0m f1_macro: 0.2989636111480617
[2m[36m(func pid=21543)[0m f1_weighted: 0.36972670108710975
[2m[36m(func pid=21543)[0m f1_per_class: [0.487, 0.442, 0.086, 0.315, 0.109, 0.397, 0.41, 0.308, 0.187, 0.249]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.8904 | Steps: 4 | Val loss: 1.8887 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=22392)[0m top1: 0.30923507462686567
[2m[36m(func pid=22392)[0m top5: 0.769589552238806
[2m[36m(func pid=22392)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=22392)[0m f1_macro: 0.31752822358457766
[2m[36m(func pid=22392)[0m f1_weighted: 0.31651948992526124
[2m[36m(func pid=22392)[0m f1_per_class: [0.447, 0.285, 0.8, 0.179, 0.088, 0.228, 0.519, 0.234, 0.191, 0.204]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5888 | Steps: 4 | Val loss: 8.1898 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 12:03:15 (running for 00:05:21.42)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.89  |      0.279 |                   52 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.04  |      0.299 |                   51 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.032 |      0.283 |                   50 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  4.02  |      0.318 |                   49 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.31203358208955223
[2m[36m(func pid=21163)[0m top5: 0.8362873134328358
[2m[36m(func pid=21163)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=21163)[0m f1_macro: 0.2792435889568424
[2m[36m(func pid=21163)[0m f1_weighted: 0.32862507605941416
[2m[36m(func pid=21163)[0m f1_per_class: [0.438, 0.348, 0.157, 0.342, 0.092, 0.429, 0.292, 0.327, 0.163, 0.204]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7612 | Steps: 4 | Val loss: 2.3477 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=21969)[0m top1: 0.2681902985074627
[2m[36m(func pid=21969)[0m top5: 0.8069029850746269
[2m[36m(func pid=21969)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=21969)[0m f1_macro: 0.2880051377641684
[2m[36m(func pid=21969)[0m f1_weighted: 0.30602552131859795
[2m[36m(func pid=21969)[0m f1_per_class: [0.182, 0.323, 0.688, 0.273, 0.065, 0.301, 0.356, 0.253, 0.26, 0.179]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 4.1218 | Steps: 4 | Val loss: 57.2636 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=21543)[0m top1: 0.34421641791044777
[2m[36m(func pid=21543)[0m top5: 0.875
[2m[36m(func pid=21543)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=21543)[0m f1_macro: 0.3042601985145233
[2m[36m(func pid=21543)[0m f1_weighted: 0.3771993104463796
[2m[36m(func pid=21543)[0m f1_per_class: [0.483, 0.444, 0.094, 0.318, 0.105, 0.4, 0.427, 0.318, 0.191, 0.263]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.8051 | Steps: 4 | Val loss: 1.8724 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4448 | Steps: 4 | Val loss: 7.8766 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=22392)[0m top1: 0.3064365671641791
[2m[36m(func pid=22392)[0m top5: 0.773320895522388
[2m[36m(func pid=22392)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=22392)[0m f1_macro: 0.31738160397264376
[2m[36m(func pid=22392)[0m f1_weighted: 0.32785861941160127
[2m[36m(func pid=22392)[0m f1_per_class: [0.446, 0.25, 0.759, 0.254, 0.077, 0.23, 0.499, 0.263, 0.218, 0.177]
[2m[36m(func pid=22392)[0m 
== Status ==
Current time: 2024-01-07 12:03:20 (running for 00:05:26.63)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.805 |      0.286 |                   53 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.761 |      0.304 |                   52 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.589 |      0.288 |                   51 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  4.122 |      0.317 |                   50 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3180970149253731
[2m[36m(func pid=21163)[0m top5: 0.8367537313432836
[2m[36m(func pid=21163)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=21163)[0m f1_macro: 0.28601187149375795
[2m[36m(func pid=21163)[0m f1_weighted: 0.3307460017178234
[2m[36m(func pid=21163)[0m f1_per_class: [0.483, 0.376, 0.146, 0.367, 0.099, 0.438, 0.253, 0.323, 0.16, 0.214]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.7490 | Steps: 4 | Val loss: 2.3582 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=21969)[0m top1: 0.28451492537313433
[2m[36m(func pid=21969)[0m top5: 0.8050373134328358
[2m[36m(func pid=21969)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=21969)[0m f1_macro: 0.2885224937752991
[2m[36m(func pid=21969)[0m f1_weighted: 0.32078828721933694
[2m[36m(func pid=21969)[0m f1_per_class: [0.18, 0.358, 0.595, 0.335, 0.067, 0.285, 0.324, 0.327, 0.249, 0.167]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 4.3129 | Steps: 4 | Val loss: 56.8548 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=21543)[0m top1: 0.33861940298507465
[2m[36m(func pid=21543)[0m top5: 0.8791977611940298
[2m[36m(func pid=21543)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=21543)[0m f1_macro: 0.3019054935735215
[2m[36m(func pid=21543)[0m f1_weighted: 0.3668284238053571
[2m[36m(func pid=21543)[0m f1_per_class: [0.476, 0.437, 0.115, 0.31, 0.127, 0.391, 0.407, 0.327, 0.18, 0.25]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.7924 | Steps: 4 | Val loss: 1.8863 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4456 | Steps: 4 | Val loss: 7.6794 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=22392)[0m top1: 0.30550373134328357
[2m[36m(func pid=22392)[0m top5: 0.7803171641791045
[2m[36m(func pid=22392)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=22392)[0m f1_macro: 0.32158705112321034
[2m[36m(func pid=22392)[0m f1_weighted: 0.3348661732838855
[2m[36m(func pid=22392)[0m f1_per_class: [0.459, 0.257, 0.8, 0.294, 0.065, 0.231, 0.482, 0.263, 0.198, 0.166]
[2m[36m(func pid=22392)[0m 
== Status ==
Current time: 2024-01-07 12:03:25 (running for 00:05:31.89)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.792 |      0.283 |                   54 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.749 |      0.302 |                   53 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.445 |      0.289 |                   52 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  4.313 |      0.322 |                   51 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.30830223880597013
[2m[36m(func pid=21163)[0m top5: 0.8283582089552238
[2m[36m(func pid=21163)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=21163)[0m f1_macro: 0.2833408374036546
[2m[36m(func pid=21163)[0m f1_weighted: 0.3297648193588538
[2m[36m(func pid=21163)[0m f1_per_class: [0.492, 0.348, 0.118, 0.334, 0.087, 0.439, 0.295, 0.316, 0.191, 0.212]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.6832 | Steps: 4 | Val loss: 2.3450 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=21969)[0m top1: 0.29850746268656714
[2m[36m(func pid=21969)[0m top5: 0.8050373134328358
[2m[36m(func pid=21969)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=21969)[0m f1_macro: 0.29541549830003805
[2m[36m(func pid=21969)[0m f1_weighted: 0.32859284759142876
[2m[36m(func pid=21969)[0m f1_per_class: [0.169, 0.388, 0.611, 0.376, 0.076, 0.321, 0.279, 0.336, 0.24, 0.158]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 3.2995 | Steps: 4 | Val loss: 57.3769 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=21543)[0m top1: 0.3521455223880597
[2m[36m(func pid=21543)[0m top5: 0.882929104477612
[2m[36m(func pid=21543)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=21543)[0m f1_macro: 0.31268715657646023
[2m[36m(func pid=21543)[0m f1_weighted: 0.3832846105811105
[2m[36m(func pid=21543)[0m f1_per_class: [0.482, 0.455, 0.107, 0.355, 0.129, 0.407, 0.4, 0.338, 0.182, 0.272]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.5896 | Steps: 4 | Val loss: 1.8599 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.2699 | Steps: 4 | Val loss: 7.6752 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=22392)[0m top1: 0.3111007462686567
[2m[36m(func pid=22392)[0m top5: 0.7798507462686567
[2m[36m(func pid=22392)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=22392)[0m f1_macro: 0.3218615089258081
[2m[36m(func pid=22392)[0m f1_weighted: 0.34332412628896264
[2m[36m(func pid=22392)[0m f1_per_class: [0.473, 0.246, 0.8, 0.33, 0.074, 0.232, 0.487, 0.249, 0.193, 0.135]
[2m[36m(func pid=22392)[0m 
== Status ==
Current time: 2024-01-07 12:03:31 (running for 00:05:37.08)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.59  |      0.291 |                   55 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.683 |      0.313 |                   54 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.446 |      0.295 |                   53 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  3.299 |      0.322 |                   52 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3218283582089552
[2m[36m(func pid=21163)[0m top5: 0.8386194029850746
[2m[36m(func pid=21163)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=21163)[0m f1_macro: 0.290717686082
[2m[36m(func pid=21163)[0m f1_weighted: 0.34264919899765667
[2m[36m(func pid=21163)[0m f1_per_class: [0.5, 0.374, 0.111, 0.359, 0.096, 0.435, 0.299, 0.325, 0.185, 0.223]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.6714 | Steps: 4 | Val loss: 2.3040 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=21969)[0m top1: 0.29990671641791045
[2m[36m(func pid=21969)[0m top5: 0.8003731343283582
[2m[36m(func pid=21969)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=21969)[0m f1_macro: 0.2934338048416477
[2m[36m(func pid=21969)[0m f1_weighted: 0.3206833321920681
[2m[36m(func pid=21969)[0m f1_per_class: [0.165, 0.394, 0.579, 0.41, 0.091, 0.313, 0.218, 0.335, 0.273, 0.157]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3605410447761194
[2m[36m(func pid=21543)[0m top5: 0.8833955223880597
[2m[36m(func pid=21543)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=21543)[0m f1_macro: 0.30845316544610457
[2m[36m(func pid=21543)[0m f1_weighted: 0.3958594537852667
[2m[36m(func pid=21543)[0m f1_per_class: [0.403, 0.462, 0.119, 0.381, 0.14, 0.403, 0.423, 0.337, 0.161, 0.255]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 8.5997 | Steps: 4 | Val loss: 58.0950 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.7533 | Steps: 4 | Val loss: 1.8548 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.2637 | Steps: 4 | Val loss: 7.3248 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=22392)[0m top1: 0.31716417910447764
[2m[36m(func pid=22392)[0m top5: 0.7793843283582089
[2m[36m(func pid=22392)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=22392)[0m f1_macro: 0.32022010953612795
[2m[36m(func pid=22392)[0m f1_weighted: 0.35331920798208166
[2m[36m(func pid=22392)[0m f1_per_class: [0.554, 0.243, 0.632, 0.378, 0.091, 0.239, 0.468, 0.253, 0.215, 0.129]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5809 | Steps: 4 | Val loss: 2.2968 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=21163)[0m top1: 0.324160447761194
[2m[36m(func pid=21163)[0m top5: 0.8381529850746269
[2m[36m(func pid=21163)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=21163)[0m f1_macro: 0.29920423503554117
[2m[36m(func pid=21163)[0m f1_weighted: 0.3441545261269084
[2m[36m(func pid=21163)[0m f1_per_class: [0.559, 0.376, 0.108, 0.387, 0.091, 0.426, 0.271, 0.343, 0.199, 0.233]
[2m[36m(func pid=21163)[0m 
== Status ==
Current time: 2024-01-07 12:03:36 (running for 00:05:42.39)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.753 |      0.299 |                   56 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.671 |      0.308 |                   55 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.27  |      0.293 |                   54 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  8.6   |      0.32  |                   53 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21969)[0m top1: 0.31763059701492535
[2m[36m(func pid=21969)[0m top5: 0.8157649253731343
[2m[36m(func pid=21969)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=21969)[0m f1_macro: 0.30476748917295854
[2m[36m(func pid=21969)[0m f1_weighted: 0.3356449692651585
[2m[36m(func pid=21969)[0m f1_per_class: [0.199, 0.401, 0.579, 0.428, 0.102, 0.324, 0.236, 0.367, 0.258, 0.153]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.365205223880597
[2m[36m(func pid=21543)[0m top5: 0.8843283582089553
[2m[36m(func pid=21543)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=21543)[0m f1_macro: 0.3091096320212782
[2m[36m(func pid=21543)[0m f1_weighted: 0.40392338589426896
[2m[36m(func pid=21543)[0m f1_per_class: [0.382, 0.452, 0.117, 0.424, 0.13, 0.394, 0.424, 0.317, 0.165, 0.288]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 12.0443 | Steps: 4 | Val loss: 57.6772 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.5925 | Steps: 4 | Val loss: 1.8435 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.1028 | Steps: 4 | Val loss: 7.0639 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=22392)[0m top1: 0.30830223880597013
[2m[36m(func pid=22392)[0m top5: 0.7901119402985075
[2m[36m(func pid=22392)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=22392)[0m f1_macro: 0.3170200105074932
[2m[36m(func pid=22392)[0m f1_weighted: 0.34573758014208744
[2m[36m(func pid=22392)[0m f1_per_class: [0.544, 0.246, 0.6, 0.405, 0.102, 0.253, 0.408, 0.268, 0.231, 0.114]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.9682 | Steps: 4 | Val loss: 2.2282 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 12:03:41 (running for 00:05:47.58)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.592 |      0.295 |                   57 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.581 |      0.309 |                   56 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.264 |      0.305 |                   55 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 12.044 |      0.317 |                   54 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3302238805970149
[2m[36m(func pid=21163)[0m top5: 0.8442164179104478
[2m[36m(func pid=21163)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=21163)[0m f1_macro: 0.29487990563427857
[2m[36m(func pid=21163)[0m f1_weighted: 0.3494675029173652
[2m[36m(func pid=21163)[0m f1_per_class: [0.463, 0.371, 0.124, 0.412, 0.089, 0.419, 0.275, 0.348, 0.205, 0.242]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m top1: 0.32742537313432835
[2m[36m(func pid=21969)[0m top5: 0.8274253731343284
[2m[36m(func pid=21969)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=21969)[0m f1_macro: 0.30705008052593646
[2m[36m(func pid=21969)[0m f1_weighted: 0.34315850564248657
[2m[36m(func pid=21969)[0m f1_per_class: [0.24, 0.408, 0.542, 0.44, 0.127, 0.331, 0.247, 0.352, 0.228, 0.157]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3689365671641791
[2m[36m(func pid=21543)[0m top5: 0.8894589552238806
[2m[36m(func pid=21543)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=21543)[0m f1_macro: 0.30932086460529795
[2m[36m(func pid=21543)[0m f1_weighted: 0.4040250798594277
[2m[36m(func pid=21543)[0m f1_per_class: [0.371, 0.444, 0.137, 0.427, 0.139, 0.393, 0.428, 0.308, 0.167, 0.28]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 7.3694 | Steps: 4 | Val loss: 57.9387 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.9047 | Steps: 4 | Val loss: 1.8300 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.7934 | Steps: 4 | Val loss: 7.5161 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=22392)[0m top1: 0.30736940298507465
[2m[36m(func pid=22392)[0m top5: 0.7905783582089553
[2m[36m(func pid=22392)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=22392)[0m f1_macro: 0.30435720622678036
[2m[36m(func pid=22392)[0m f1_weighted: 0.3382778533195977
[2m[36m(func pid=22392)[0m f1_per_class: [0.514, 0.32, 0.453, 0.37, 0.126, 0.244, 0.378, 0.287, 0.211, 0.14]
[2m[36m(func pid=22392)[0m 
== Status ==
Current time: 2024-01-07 12:03:47 (running for 00:05:52.97)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.905 |      0.299 |                   58 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.968 |      0.309 |                   57 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.103 |      0.307 |                   56 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  7.369 |      0.304 |                   55 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3423507462686567
[2m[36m(func pid=21163)[0m top5: 0.8521455223880597
[2m[36m(func pid=21163)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=21163)[0m f1_macro: 0.2990003520084241
[2m[36m(func pid=21163)[0m f1_weighted: 0.3604603682448575
[2m[36m(func pid=21163)[0m f1_per_class: [0.437, 0.377, 0.125, 0.414, 0.109, 0.446, 0.298, 0.358, 0.194, 0.232]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4081 | Steps: 4 | Val loss: 2.1671 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=21969)[0m top1: 0.30223880597014924
[2m[36m(func pid=21969)[0m top5: 0.8129664179104478
[2m[36m(func pid=21969)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=21969)[0m f1_macro: 0.27492524973786125
[2m[36m(func pid=21969)[0m f1_weighted: 0.3176170529745045
[2m[36m(func pid=21969)[0m f1_per_class: [0.237, 0.384, 0.366, 0.374, 0.112, 0.339, 0.248, 0.301, 0.211, 0.176]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3885261194029851
[2m[36m(func pid=21543)[0m top5: 0.894589552238806
[2m[36m(func pid=21543)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=21543)[0m f1_macro: 0.326162738749368
[2m[36m(func pid=21543)[0m f1_weighted: 0.41896043350205686
[2m[36m(func pid=21543)[0m f1_per_class: [0.407, 0.455, 0.174, 0.48, 0.135, 0.403, 0.41, 0.323, 0.175, 0.299]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.1149 | Steps: 4 | Val loss: 59.4838 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.6533 | Steps: 4 | Val loss: 1.8192 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.3515 | Steps: 4 | Val loss: 7.3467 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 12:03:52 (running for 00:05:58.08)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.905 |      0.299 |                   58 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.408 |      0.326 |                   58 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.793 |      0.275 |                   57 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  2.115 |      0.287 |                   56 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=22392)[0m top1: 0.29990671641791045
[2m[36m(func pid=22392)[0m top5: 0.7728544776119403
[2m[36m(func pid=22392)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=22392)[0m f1_macro: 0.2871214771158677
[2m[36m(func pid=22392)[0m f1_weighted: 0.32527323845886275
[2m[36m(func pid=22392)[0m f1_per_class: [0.505, 0.332, 0.257, 0.362, 0.176, 0.257, 0.336, 0.273, 0.191, 0.18]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m top1: 0.34841417910447764
[2m[36m(func pid=21163)[0m top5: 0.8502798507462687
[2m[36m(func pid=21163)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=21163)[0m f1_macro: 0.3060466299202788
[2m[36m(func pid=21163)[0m f1_weighted: 0.3680255814402027
[2m[36m(func pid=21163)[0m f1_per_class: [0.481, 0.386, 0.13, 0.417, 0.104, 0.435, 0.315, 0.364, 0.197, 0.231]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7627 | Steps: 4 | Val loss: 2.2804 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=21969)[0m top1: 0.30783582089552236
[2m[36m(func pid=21969)[0m top5: 0.8194962686567164
[2m[36m(func pid=21969)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=21969)[0m f1_macro: 0.28622669902857584
[2m[36m(func pid=21969)[0m f1_weighted: 0.324611431049629
[2m[36m(func pid=21969)[0m f1_per_class: [0.283, 0.378, 0.413, 0.381, 0.13, 0.335, 0.268, 0.288, 0.214, 0.172]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.37080223880597013
[2m[36m(func pid=21543)[0m top5: 0.8810634328358209
[2m[36m(func pid=21543)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=21543)[0m f1_macro: 0.31385799616898913
[2m[36m(func pid=21543)[0m f1_weighted: 0.4026152775616015
[2m[36m(func pid=21543)[0m f1_per_class: [0.341, 0.397, 0.152, 0.471, 0.115, 0.397, 0.398, 0.336, 0.209, 0.323]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.6578 | Steps: 4 | Val loss: 1.8242 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 4.3764 | Steps: 4 | Val loss: 59.9753 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.9286 | Steps: 4 | Val loss: 7.2367 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 12:03:57 (running for 00:06:03.36)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.658 |      0.302 |                   60 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.763 |      0.314 |                   59 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.352 |      0.286 |                   58 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  2.115 |      0.287 |                   56 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.34654850746268656
[2m[36m(func pid=21163)[0m top5: 0.8470149253731343
[2m[36m(func pid=21163)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=21163)[0m f1_macro: 0.30179801447597143
[2m[36m(func pid=21163)[0m f1_weighted: 0.36981866216141523
[2m[36m(func pid=21163)[0m f1_per_class: [0.427, 0.389, 0.122, 0.423, 0.084, 0.423, 0.319, 0.367, 0.227, 0.238]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=22392)[0m top1: 0.300839552238806
[2m[36m(func pid=22392)[0m top5: 0.7747201492537313
[2m[36m(func pid=22392)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=22392)[0m f1_macro: 0.2832053110350707
[2m[36m(func pid=22392)[0m f1_weighted: 0.3285044449738259
[2m[36m(func pid=22392)[0m f1_per_class: [0.527, 0.354, 0.171, 0.397, 0.167, 0.267, 0.303, 0.253, 0.179, 0.214]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5186 | Steps: 4 | Val loss: 2.2463 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=21969)[0m top1: 0.30223880597014924
[2m[36m(func pid=21969)[0m top5: 0.8283582089552238
[2m[36m(func pid=21969)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=21969)[0m f1_macro: 0.28775123360021226
[2m[36m(func pid=21969)[0m f1_weighted: 0.3232253427042166
[2m[36m(func pid=21969)[0m f1_per_class: [0.335, 0.353, 0.413, 0.351, 0.128, 0.327, 0.306, 0.301, 0.185, 0.178]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.38619402985074625
[2m[36m(func pid=21543)[0m top5: 0.8852611940298507
[2m[36m(func pid=21543)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=21543)[0m f1_macro: 0.32703968704063324
[2m[36m(func pid=21543)[0m f1_weighted: 0.41398867302263004
[2m[36m(func pid=21543)[0m f1_per_class: [0.38, 0.436, 0.171, 0.516, 0.102, 0.384, 0.37, 0.344, 0.227, 0.341]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.5905 | Steps: 4 | Val loss: 1.8009 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 5.7614 | Steps: 4 | Val loss: 63.4730 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.0916 | Steps: 4 | Val loss: 7.2939 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 12:04:02 (running for 00:06:08.68)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.59  |      0.311 |                   61 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.519 |      0.327 |                   60 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.929 |      0.288 |                   59 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  4.376 |      0.283 |                   57 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3596082089552239
[2m[36m(func pid=21163)[0m top5: 0.8549440298507462
[2m[36m(func pid=21163)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=21163)[0m f1_macro: 0.31094217833837023
[2m[36m(func pid=21163)[0m f1_weighted: 0.38185856610042107
[2m[36m(func pid=21163)[0m f1_per_class: [0.443, 0.416, 0.126, 0.439, 0.083, 0.424, 0.326, 0.374, 0.215, 0.264]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.4806 | Steps: 4 | Val loss: 2.3078 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=22392)[0m top1: 0.28591417910447764
[2m[36m(func pid=22392)[0m top5: 0.7625932835820896
[2m[36m(func pid=22392)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=22392)[0m f1_macro: 0.2723959252122699
[2m[36m(func pid=22392)[0m f1_weighted: 0.3094196880842034
[2m[36m(func pid=22392)[0m f1_per_class: [0.485, 0.369, 0.116, 0.389, 0.154, 0.3, 0.228, 0.264, 0.152, 0.268]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.2966417910447761
[2m[36m(func pid=21969)[0m top5: 0.8288246268656716
[2m[36m(func pid=21969)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=21969)[0m f1_macro: 0.2891115574003419
[2m[36m(func pid=21969)[0m f1_weighted: 0.3232607330886264
[2m[36m(func pid=21969)[0m f1_per_class: [0.397, 0.283, 0.4, 0.336, 0.118, 0.323, 0.362, 0.282, 0.168, 0.221]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.36613805970149255
[2m[36m(func pid=21543)[0m top5: 0.8759328358208955
[2m[36m(func pid=21543)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=21543)[0m f1_macro: 0.3130662784617742
[2m[36m(func pid=21543)[0m f1_weighted: 0.389847298960207
[2m[36m(func pid=21543)[0m f1_per_class: [0.34, 0.427, 0.173, 0.49, 0.096, 0.374, 0.326, 0.342, 0.218, 0.344]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.6917 | Steps: 4 | Val loss: 1.7915 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 6.6871 | Steps: 4 | Val loss: 70.4974 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.1657 | Steps: 4 | Val loss: 7.5882 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=21163)[0m top1: 0.35401119402985076
[2m[36m(func pid=21163)[0m top5: 0.8568097014925373
[2m[36m(func pid=21163)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=21163)[0m f1_macro: 0.3081153759431632
[2m[36m(func pid=21163)[0m f1_weighted: 0.37857423634610354
[2m[36m(func pid=21163)[0m f1_per_class: [0.463, 0.41, 0.117, 0.42, 0.085, 0.423, 0.338, 0.366, 0.212, 0.249]
[2m[36m(func pid=21163)[0m 
== Status ==
Current time: 2024-01-07 12:04:08 (running for 00:06:13.97)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.692 |      0.308 |                   62 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.481 |      0.313 |                   61 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.092 |      0.289 |                   60 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  5.761 |      0.272 |                   58 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.3651 | Steps: 4 | Val loss: 2.3515 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=22392)[0m top1: 0.269589552238806
[2m[36m(func pid=22392)[0m top5: 0.7397388059701493
[2m[36m(func pid=22392)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=22392)[0m f1_macro: 0.2646533484112957
[2m[36m(func pid=22392)[0m f1_weighted: 0.3000158560572978
[2m[36m(func pid=22392)[0m f1_per_class: [0.44, 0.382, 0.061, 0.383, 0.123, 0.298, 0.2, 0.256, 0.13, 0.374]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.2873134328358209
[2m[36m(func pid=21969)[0m top5: 0.8222947761194029
[2m[36m(func pid=21969)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=21969)[0m f1_macro: 0.27675123650867145
[2m[36m(func pid=21969)[0m f1_weighted: 0.30901743809022064
[2m[36m(func pid=21969)[0m f1_per_class: [0.345, 0.244, 0.377, 0.244, 0.118, 0.333, 0.424, 0.282, 0.153, 0.247]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.36427238805970147
[2m[36m(func pid=21543)[0m top5: 0.8717350746268657
[2m[36m(func pid=21543)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=21543)[0m f1_macro: 0.315564527489219
[2m[36m(func pid=21543)[0m f1_weighted: 0.38364690643299476
[2m[36m(func pid=21543)[0m f1_per_class: [0.329, 0.438, 0.206, 0.493, 0.098, 0.371, 0.297, 0.334, 0.23, 0.359]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.7674 | Steps: 4 | Val loss: 1.8163 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 8.2336 | Steps: 4 | Val loss: 76.4522 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.1406 | Steps: 4 | Val loss: 7.8340 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 12:04:13 (running for 00:06:19.18)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.767 |      0.299 |                   63 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.365 |      0.316 |                   62 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.166 |      0.277 |                   61 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  6.687 |      0.265 |                   59 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.341884328358209
[2m[36m(func pid=21163)[0m top5: 0.8502798507462687
[2m[36m(func pid=21163)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=21163)[0m f1_macro: 0.29871668217693614
[2m[36m(func pid=21163)[0m f1_weighted: 0.36835538712462124
[2m[36m(func pid=21163)[0m f1_per_class: [0.408, 0.39, 0.111, 0.418, 0.083, 0.418, 0.321, 0.369, 0.221, 0.249]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.6075 | Steps: 4 | Val loss: 2.2987 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=22392)[0m top1: 0.23507462686567165
[2m[36m(func pid=22392)[0m top5: 0.7290111940298507
[2m[36m(func pid=22392)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=22392)[0m f1_macro: 0.24614758378192483
[2m[36m(func pid=22392)[0m f1_weighted: 0.26387958150248897
[2m[36m(func pid=22392)[0m f1_per_class: [0.396, 0.355, 0.054, 0.338, 0.091, 0.279, 0.144, 0.273, 0.118, 0.414]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.279384328358209
[2m[36m(func pid=21969)[0m top5: 0.8185634328358209
[2m[36m(func pid=21969)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=21969)[0m f1_macro: 0.26953450084392505
[2m[36m(func pid=21969)[0m f1_weighted: 0.29794003366442146
[2m[36m(func pid=21969)[0m f1_per_class: [0.365, 0.231, 0.338, 0.213, 0.136, 0.311, 0.434, 0.266, 0.147, 0.254]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.36427238805970147
[2m[36m(func pid=21543)[0m top5: 0.8838619402985075
[2m[36m(func pid=21543)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=21543)[0m f1_macro: 0.31994954449439866
[2m[36m(func pid=21543)[0m f1_weighted: 0.38559116284527245
[2m[36m(func pid=21543)[0m f1_per_class: [0.341, 0.412, 0.26, 0.478, 0.107, 0.384, 0.328, 0.334, 0.211, 0.344]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.6417 | Steps: 4 | Val loss: 1.8424 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 10.7882 | Steps: 4 | Val loss: 79.1816 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.7167 | Steps: 4 | Val loss: 7.8869 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=21163)[0m top1: 0.322294776119403
[2m[36m(func pid=21163)[0m top5: 0.8558768656716418
[2m[36m(func pid=21163)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=21163)[0m f1_macro: 0.29121198047995683
[2m[36m(func pid=21163)[0m f1_weighted: 0.354854108067201
[2m[36m(func pid=21163)[0m f1_per_class: [0.441, 0.379, 0.103, 0.35, 0.068, 0.411, 0.352, 0.352, 0.201, 0.254]
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.0231 | Steps: 4 | Val loss: 2.4227 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 12:04:18 (running for 00:06:24.72)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.642 |      0.291 |                   64 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.607 |      0.32  |                   63 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.141 |      0.27  |                   62 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  8.234 |      0.246 |                   60 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=22392)[0m top1: 0.22761194029850745
[2m[36m(func pid=22392)[0m top5: 0.7019589552238806
[2m[36m(func pid=22392)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=22392)[0m f1_macro: 0.23575076455804772
[2m[36m(func pid=22392)[0m f1_weighted: 0.24085878319547416
[2m[36m(func pid=22392)[0m f1_per_class: [0.333, 0.393, 0.06, 0.264, 0.093, 0.279, 0.119, 0.267, 0.124, 0.427]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.2947761194029851
[2m[36m(func pid=21969)[0m top5: 0.7971082089552238
[2m[36m(func pid=21969)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=21969)[0m f1_macro: 0.2602693018412677
[2m[36m(func pid=21969)[0m f1_weighted: 0.30655338557280504
[2m[36m(func pid=21969)[0m f1_per_class: [0.324, 0.179, 0.277, 0.222, 0.123, 0.314, 0.488, 0.257, 0.165, 0.253]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.35634328358208955
[2m[36m(func pid=21543)[0m top5: 0.8656716417910447
[2m[36m(func pid=21543)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=21543)[0m f1_macro: 0.3193013750405176
[2m[36m(func pid=21543)[0m f1_weighted: 0.3776561907359325
[2m[36m(func pid=21543)[0m f1_per_class: [0.301, 0.442, 0.242, 0.459, 0.092, 0.365, 0.304, 0.356, 0.228, 0.404]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.5294 | Steps: 4 | Val loss: 1.8303 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 24.9858 | Steps: 4 | Val loss: 81.6622 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5120 | Steps: 4 | Val loss: 7.8528 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 12:04:23 (running for 00:06:29.76)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.529 |      0.29  |                   65 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.023 |      0.319 |                   64 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.717 |      0.26  |                   63 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 10.788 |      0.236 |                   61 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3269589552238806
[2m[36m(func pid=21163)[0m top5: 0.8582089552238806
[2m[36m(func pid=21163)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=21163)[0m f1_macro: 0.28987244565699083
[2m[36m(func pid=21163)[0m f1_weighted: 0.36004363849549487
[2m[36m(func pid=21163)[0m f1_per_class: [0.421, 0.373, 0.105, 0.379, 0.071, 0.417, 0.348, 0.345, 0.186, 0.254]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7975 | Steps: 4 | Val loss: 2.4429 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=22392)[0m top1: 0.2248134328358209
[2m[36m(func pid=22392)[0m top5: 0.6865671641791045
[2m[36m(func pid=22392)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=22392)[0m f1_macro: 0.22945367677406367
[2m[36m(func pid=22392)[0m f1_weighted: 0.23895567047216343
[2m[36m(func pid=22392)[0m f1_per_class: [0.221, 0.374, 0.061, 0.227, 0.121, 0.3, 0.153, 0.278, 0.148, 0.412]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.29244402985074625
[2m[36m(func pid=21969)[0m top5: 0.7966417910447762
[2m[36m(func pid=21969)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=21969)[0m f1_macro: 0.26532699679095384
[2m[36m(func pid=21969)[0m f1_weighted: 0.3060927711820577
[2m[36m(func pid=21969)[0m f1_per_class: [0.348, 0.165, 0.321, 0.252, 0.105, 0.302, 0.466, 0.26, 0.182, 0.251]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3512126865671642
[2m[36m(func pid=21543)[0m top5: 0.8666044776119403
[2m[36m(func pid=21543)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=21543)[0m f1_macro: 0.3152238541967584
[2m[36m(func pid=21543)[0m f1_weighted: 0.3725623044461528
[2m[36m(func pid=21543)[0m f1_per_class: [0.308, 0.454, 0.237, 0.439, 0.086, 0.359, 0.304, 0.343, 0.219, 0.404]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.7293 | Steps: 4 | Val loss: 1.7982 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 13.0204 | Steps: 4 | Val loss: 82.2319 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=21163)[0m top1: 0.3451492537313433
[2m[36m(func pid=21163)[0m top5: 0.8619402985074627
[2m[36m(func pid=21163)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=21163)[0m f1_macro: 0.2970141288026569
[2m[36m(func pid=21163)[0m f1_weighted: 0.37582946484721347
[2m[36m(func pid=21163)[0m f1_per_class: [0.408, 0.399, 0.113, 0.397, 0.085, 0.425, 0.367, 0.352, 0.174, 0.251]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5026 | Steps: 4 | Val loss: 8.0125 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 12:04:28 (running for 00:06:34.92)
Memory usage on this node: 24.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.729 |      0.297 |                   66 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.798 |      0.315 |                   65 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.512 |      0.265 |                   64 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 24.986 |      0.229 |                   62 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.7112 | Steps: 4 | Val loss: 2.4680 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=22392)[0m top1: 0.2248134328358209
[2m[36m(func pid=22392)[0m top5: 0.6833022388059702
[2m[36m(func pid=22392)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=22392)[0m f1_macro: 0.22862943978222067
[2m[36m(func pid=22392)[0m f1_weighted: 0.24365153968939268
[2m[36m(func pid=22392)[0m f1_per_class: [0.181, 0.325, 0.063, 0.25, 0.105, 0.3, 0.174, 0.293, 0.155, 0.441]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.2933768656716418
[2m[36m(func pid=21969)[0m top5: 0.7831156716417911
[2m[36m(func pid=21969)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=21969)[0m f1_macro: 0.27428572370760984
[2m[36m(func pid=21969)[0m f1_weighted: 0.30801511827265693
[2m[36m(func pid=21969)[0m f1_per_class: [0.34, 0.168, 0.329, 0.267, 0.086, 0.295, 0.449, 0.298, 0.196, 0.313]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.355410447761194
[2m[36m(func pid=21543)[0m top5: 0.8647388059701493
[2m[36m(func pid=21543)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=21543)[0m f1_macro: 0.32465069136157026
[2m[36m(func pid=21543)[0m f1_weighted: 0.37500281909560274
[2m[36m(func pid=21543)[0m f1_per_class: [0.309, 0.459, 0.264, 0.442, 0.086, 0.357, 0.304, 0.347, 0.224, 0.455]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.5229 | Steps: 4 | Val loss: 1.8105 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 17.5837 | Steps: 4 | Val loss: 77.7127 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.1226 | Steps: 4 | Val loss: 7.8304 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.8481 | Steps: 4 | Val loss: 2.4939 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 12:04:34 (running for 00:06:40.35)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.523 |      0.299 |                   67 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.711 |      0.325 |                   66 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.503 |      0.274 |                   65 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 13.02  |      0.229 |                   63 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3381529850746269
[2m[36m(func pid=21163)[0m top5: 0.8605410447761194
[2m[36m(func pid=21163)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=21163)[0m f1_macro: 0.29923883156989256
[2m[36m(func pid=21163)[0m f1_weighted: 0.3694644460086064
[2m[36m(func pid=21163)[0m f1_per_class: [0.43, 0.367, 0.135, 0.4, 0.07, 0.423, 0.357, 0.347, 0.211, 0.253]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=22392)[0m top1: 0.2453358208955224
[2m[36m(func pid=22392)[0m top5: 0.7047574626865671
[2m[36m(func pid=22392)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=22392)[0m f1_macro: 0.24381025268598133
[2m[36m(func pid=22392)[0m f1_weighted: 0.2724115431951531
[2m[36m(func pid=22392)[0m f1_per_class: [0.166, 0.332, 0.074, 0.287, 0.118, 0.296, 0.231, 0.293, 0.166, 0.476]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.2957089552238806
[2m[36m(func pid=21969)[0m top5: 0.7961753731343284
[2m[36m(func pid=21969)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=21969)[0m f1_macro: 0.2836357851023129
[2m[36m(func pid=21969)[0m f1_weighted: 0.3146838612545436
[2m[36m(func pid=21969)[0m f1_per_class: [0.352, 0.198, 0.333, 0.31, 0.093, 0.306, 0.41, 0.285, 0.193, 0.356]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.34888059701492535
[2m[36m(func pid=21543)[0m top5: 0.8624067164179104
[2m[36m(func pid=21543)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=21543)[0m f1_macro: 0.3200901463874808
[2m[36m(func pid=21543)[0m f1_weighted: 0.36888293361542857
[2m[36m(func pid=21543)[0m f1_per_class: [0.316, 0.449, 0.255, 0.435, 0.083, 0.353, 0.297, 0.355, 0.213, 0.444]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.5604 | Steps: 4 | Val loss: 1.7811 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 21.9831 | Steps: 4 | Val loss: 75.7859 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6544 | Steps: 4 | Val loss: 7.7794 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 12:04:39 (running for 00:06:45.68)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.56  |      0.31  |                   68 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.848 |      0.32  |                   67 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.123 |      0.284 |                   66 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 17.584 |      0.244 |                   64 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.35447761194029853
[2m[36m(func pid=21163)[0m top5: 0.8642723880597015
[2m[36m(func pid=21163)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=21163)[0m f1_macro: 0.30977361429470973
[2m[36m(func pid=21163)[0m f1_weighted: 0.3802145757370485
[2m[36m(func pid=21163)[0m f1_per_class: [0.4, 0.407, 0.139, 0.416, 0.084, 0.426, 0.349, 0.359, 0.236, 0.282]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.4073 | Steps: 4 | Val loss: 2.4770 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=22392)[0m top1: 0.25093283582089554
[2m[36m(func pid=22392)[0m top5: 0.707089552238806
[2m[36m(func pid=22392)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=22392)[0m f1_macro: 0.24568497318479016
[2m[36m(func pid=22392)[0m f1_weighted: 0.2821961219047161
[2m[36m(func pid=22392)[0m f1_per_class: [0.144, 0.336, 0.098, 0.267, 0.107, 0.287, 0.284, 0.298, 0.168, 0.469]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.3045708955223881
[2m[36m(func pid=21969)[0m top5: 0.8064365671641791
[2m[36m(func pid=21969)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=21969)[0m f1_macro: 0.30121022229588734
[2m[36m(func pid=21969)[0m f1_weighted: 0.32573505148694887
[2m[36m(func pid=21969)[0m f1_per_class: [0.358, 0.268, 0.448, 0.339, 0.109, 0.313, 0.376, 0.266, 0.214, 0.319]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3460820895522388
[2m[36m(func pid=21543)[0m top5: 0.8647388059701493
[2m[36m(func pid=21543)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=21543)[0m f1_macro: 0.3166104270811031
[2m[36m(func pid=21543)[0m f1_weighted: 0.36234304631069864
[2m[36m(func pid=21543)[0m f1_per_class: [0.361, 0.447, 0.267, 0.418, 0.084, 0.352, 0.291, 0.348, 0.227, 0.37]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.5662 | Steps: 4 | Val loss: 1.7559 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 13.4221 | Steps: 4 | Val loss: 70.7340 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.4387 | Steps: 4 | Val loss: 7.6736 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 12:04:45 (running for 00:06:50.96)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.566 |      0.317 |                   69 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.407 |      0.317 |                   68 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.654 |      0.301 |                   67 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 21.983 |      0.246 |                   65 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3670708955223881
[2m[36m(func pid=21163)[0m top5: 0.8675373134328358
[2m[36m(func pid=21163)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=21163)[0m f1_macro: 0.3166210900721089
[2m[36m(func pid=21163)[0m f1_weighted: 0.39412480363733804
[2m[36m(func pid=21163)[0m f1_per_class: [0.413, 0.406, 0.149, 0.455, 0.084, 0.425, 0.358, 0.363, 0.238, 0.275]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.9449 | Steps: 4 | Val loss: 2.5381 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=22392)[0m top1: 0.2677238805970149
[2m[36m(func pid=22392)[0m top5: 0.7346082089552238
[2m[36m(func pid=22392)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=22392)[0m f1_macro: 0.25407997904748597
[2m[36m(func pid=22392)[0m f1_weighted: 0.2947504169539825
[2m[36m(func pid=22392)[0m f1_per_class: [0.158, 0.336, 0.126, 0.293, 0.103, 0.297, 0.3, 0.267, 0.182, 0.478]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.31529850746268656
[2m[36m(func pid=21969)[0m top5: 0.8143656716417911
[2m[36m(func pid=21969)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=21969)[0m f1_macro: 0.31296012347272145
[2m[36m(func pid=21969)[0m f1_weighted: 0.3340314059315053
[2m[36m(func pid=21969)[0m f1_per_class: [0.342, 0.276, 0.481, 0.382, 0.116, 0.34, 0.345, 0.276, 0.233, 0.339]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.332089552238806
[2m[36m(func pid=21543)[0m top5: 0.8558768656716418
[2m[36m(func pid=21543)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=21543)[0m f1_macro: 0.30630280807803156
[2m[36m(func pid=21543)[0m f1_weighted: 0.34891132112618156
[2m[36m(func pid=21543)[0m f1_per_class: [0.324, 0.458, 0.293, 0.377, 0.084, 0.333, 0.294, 0.321, 0.216, 0.362]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.4655 | Steps: 4 | Val loss: 1.7332 | Batch size: 32 | lr: 0.0001 | Duration: 2.72s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.4294 | Steps: 4 | Val loss: 63.5540 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.3096 | Steps: 4 | Val loss: 7.3447 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=21163)[0m top1: 0.37593283582089554
[2m[36m(func pid=21163)[0m top5: 0.8731343283582089
[2m[36m(func pid=21163)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=21163)[0m f1_macro: 0.32410779710781973
[2m[36m(func pid=21163)[0m f1_weighted: 0.4000553961374442
[2m[36m(func pid=21163)[0m f1_per_class: [0.454, 0.421, 0.166, 0.452, 0.096, 0.43, 0.368, 0.368, 0.222, 0.264]
[2m[36m(func pid=21163)[0m 
== Status ==
Current time: 2024-01-07 12:04:50 (running for 00:06:56.07)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.466 |      0.324 |                   70 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.945 |      0.306 |                   69 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  2.439 |      0.313 |                   68 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 13.422 |      0.254 |                   66 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.5717 | Steps: 4 | Val loss: 2.4408 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=22392)[0m top1: 0.3041044776119403
[2m[36m(func pid=22392)[0m top5: 0.7723880597014925
[2m[36m(func pid=22392)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=22392)[0m f1_macro: 0.2809541557928397
[2m[36m(func pid=22392)[0m f1_weighted: 0.3238470374893995
[2m[36m(func pid=22392)[0m f1_per_class: [0.238, 0.391, 0.203, 0.312, 0.083, 0.297, 0.346, 0.226, 0.207, 0.507]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.3353544776119403
[2m[36m(func pid=21969)[0m top5: 0.8306902985074627
[2m[36m(func pid=21969)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=21969)[0m f1_macro: 0.3228560889766161
[2m[36m(func pid=21969)[0m f1_weighted: 0.3563583096595394
[2m[36m(func pid=21969)[0m f1_per_class: [0.347, 0.309, 0.464, 0.431, 0.116, 0.345, 0.348, 0.284, 0.271, 0.314]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.35401119402985076
[2m[36m(func pid=21543)[0m top5: 0.8666044776119403
[2m[36m(func pid=21543)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=21543)[0m f1_macro: 0.32598304006322365
[2m[36m(func pid=21543)[0m f1_weighted: 0.36881910767832077
[2m[36m(func pid=21543)[0m f1_per_class: [0.405, 0.474, 0.338, 0.418, 0.09, 0.346, 0.302, 0.327, 0.219, 0.341]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.5770 | Steps: 4 | Val loss: 1.7324 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 22.9849 | Steps: 4 | Val loss: 63.0517 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.2408 | Steps: 4 | Val loss: 7.0793 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 12:04:55 (running for 00:07:01.27)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.577 |      0.324 |                   71 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.572 |      0.326 |                   70 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.31  |      0.323 |                   69 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  1.429 |      0.281 |                   67 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.37453358208955223
[2m[36m(func pid=21163)[0m top5: 0.8717350746268657
[2m[36m(func pid=21163)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=21163)[0m f1_macro: 0.32407241714526025
[2m[36m(func pid=21163)[0m f1_weighted: 0.39927643354587866
[2m[36m(func pid=21163)[0m f1_per_class: [0.474, 0.413, 0.149, 0.456, 0.084, 0.424, 0.367, 0.357, 0.243, 0.274]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6288 | Steps: 4 | Val loss: 2.4064 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=22392)[0m top1: 0.31949626865671643
[2m[36m(func pid=22392)[0m top5: 0.7901119402985075
[2m[36m(func pid=22392)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=22392)[0m f1_macro: 0.3077675586736702
[2m[36m(func pid=22392)[0m f1_weighted: 0.3309541094440525
[2m[36m(func pid=22392)[0m f1_per_class: [0.362, 0.428, 0.31, 0.303, 0.079, 0.258, 0.357, 0.243, 0.195, 0.542]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.35261194029850745
[2m[36m(func pid=21969)[0m top5: 0.8376865671641791
[2m[36m(func pid=21969)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=21969)[0m f1_macro: 0.33240018253935993
[2m[36m(func pid=21969)[0m f1_weighted: 0.37011737269178013
[2m[36m(func pid=21969)[0m f1_per_class: [0.342, 0.34, 0.5, 0.489, 0.119, 0.351, 0.316, 0.31, 0.258, 0.299]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.35074626865671643
[2m[36m(func pid=21543)[0m top5: 0.8791977611940298
[2m[36m(func pid=21543)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=21543)[0m f1_macro: 0.3171216111401058
[2m[36m(func pid=21543)[0m f1_weighted: 0.37107186561085914
[2m[36m(func pid=21543)[0m f1_per_class: [0.387, 0.444, 0.289, 0.414, 0.107, 0.348, 0.335, 0.319, 0.205, 0.323]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.4304 | Steps: 4 | Val loss: 1.7436 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 12.6576 | Steps: 4 | Val loss: 63.0031 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4402 | Steps: 4 | Val loss: 6.9455 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 12:05:00 (running for 00:07:06.55)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.43  |      0.317 |                   72 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.629 |      0.317 |                   71 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  1.241 |      0.332 |                   70 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 22.985 |      0.308 |                   68 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3712686567164179
[2m[36m(func pid=21163)[0m top5: 0.8628731343283582
[2m[36m(func pid=21163)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=21163)[0m f1_macro: 0.31726751142078435
[2m[36m(func pid=21163)[0m f1_weighted: 0.3922304152395177
[2m[36m(func pid=21163)[0m f1_per_class: [0.452, 0.407, 0.146, 0.47, 0.085, 0.423, 0.342, 0.322, 0.237, 0.288]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.6208 | Steps: 4 | Val loss: 2.4228 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=22392)[0m top1: 0.3208955223880597
[2m[36m(func pid=22392)[0m top5: 0.7999067164179104
[2m[36m(func pid=22392)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=22392)[0m f1_macro: 0.32679212660277035
[2m[36m(func pid=22392)[0m f1_weighted: 0.33244496806332713
[2m[36m(func pid=22392)[0m f1_per_class: [0.545, 0.448, 0.4, 0.312, 0.067, 0.245, 0.339, 0.213, 0.213, 0.485]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.3666044776119403
[2m[36m(func pid=21969)[0m top5: 0.8544776119402985
[2m[36m(func pid=21969)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=21969)[0m f1_macro: 0.3321563154364662
[2m[36m(func pid=21969)[0m f1_weighted: 0.38113000766297944
[2m[36m(func pid=21969)[0m f1_per_class: [0.303, 0.365, 0.423, 0.512, 0.126, 0.335, 0.318, 0.356, 0.245, 0.339]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3512126865671642
[2m[36m(func pid=21543)[0m top5: 0.8805970149253731
[2m[36m(func pid=21543)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=21543)[0m f1_macro: 0.3185479345822742
[2m[36m(func pid=21543)[0m f1_weighted: 0.37332320014294285
[2m[36m(func pid=21543)[0m f1_per_class: [0.405, 0.433, 0.289, 0.411, 0.104, 0.358, 0.348, 0.316, 0.205, 0.316]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.6438 | Steps: 4 | Val loss: 1.7517 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.8642 | Steps: 4 | Val loss: 64.3676 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.1287 | Steps: 4 | Val loss: 7.0779 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 12:05:05 (running for 00:07:11.91)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.644 |      0.322 |                   73 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.621 |      0.319 |                   72 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.44  |      0.332 |                   71 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 12.658 |      0.327 |                   69 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.36800373134328357
[2m[36m(func pid=21163)[0m top5: 0.8614738805970149
[2m[36m(func pid=21163)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=21163)[0m f1_macro: 0.3218572157401277
[2m[36m(func pid=21163)[0m f1_weighted: 0.3914425946266076
[2m[36m(func pid=21163)[0m f1_per_class: [0.496, 0.403, 0.137, 0.467, 0.078, 0.423, 0.338, 0.329, 0.261, 0.286]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.6283 | Steps: 4 | Val loss: 2.4422 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=22392)[0m top1: 0.3064365671641791
[2m[36m(func pid=22392)[0m top5: 0.8041044776119403
[2m[36m(func pid=22392)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=22392)[0m f1_macro: 0.3342635561896633
[2m[36m(func pid=22392)[0m f1_weighted: 0.3203701309767601
[2m[36m(func pid=22392)[0m f1_per_class: [0.635, 0.446, 0.471, 0.306, 0.077, 0.176, 0.327, 0.192, 0.239, 0.475]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.363339552238806
[2m[36m(func pid=21969)[0m top5: 0.8530783582089553
[2m[36m(func pid=21969)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=21969)[0m f1_macro: 0.31982577086962627
[2m[36m(func pid=21969)[0m f1_weighted: 0.38322412269803885
[2m[36m(func pid=21969)[0m f1_per_class: [0.276, 0.374, 0.367, 0.51, 0.102, 0.316, 0.338, 0.335, 0.237, 0.345]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21543)[0m top1: 0.34888059701492535
[2m[36m(func pid=21543)[0m top5: 0.8801305970149254
[2m[36m(func pid=21543)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=21543)[0m f1_macro: 0.31454239849422105
[2m[36m(func pid=21543)[0m f1_weighted: 0.37520746649161824
[2m[36m(func pid=21543)[0m f1_per_class: [0.383, 0.42, 0.304, 0.395, 0.121, 0.345, 0.384, 0.32, 0.191, 0.282]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.4081 | Steps: 4 | Val loss: 1.7366 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 3.9362 | Steps: 4 | Val loss: 68.7824 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=21163)[0m top1: 0.376865671641791
[2m[36m(func pid=21163)[0m top5: 0.8619402985074627
[2m[36m(func pid=21163)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=21163)[0m f1_macro: 0.3264142591589302
[2m[36m(func pid=21163)[0m f1_weighted: 0.3987452913467534
[2m[36m(func pid=21163)[0m f1_per_class: [0.484, 0.406, 0.167, 0.485, 0.088, 0.423, 0.342, 0.353, 0.249, 0.267]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3122 | Steps: 4 | Val loss: 7.1425 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 12:05:11 (running for 00:07:17.14)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.408 |      0.326 |                   74 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.628 |      0.315 |                   73 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.129 |      0.32  |                   72 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  2.864 |      0.334 |                   70 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7319 | Steps: 4 | Val loss: 2.3333 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=22392)[0m top1: 0.28218283582089554
[2m[36m(func pid=22392)[0m top5: 0.804570895522388
[2m[36m(func pid=22392)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=22392)[0m f1_macro: 0.3291336561443613
[2m[36m(func pid=22392)[0m f1_weighted: 0.2943273060148404
[2m[36m(func pid=22392)[0m f1_per_class: [0.605, 0.439, 0.6, 0.277, 0.071, 0.154, 0.281, 0.178, 0.244, 0.442]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.36847014925373134
[2m[36m(func pid=21969)[0m top5: 0.8535447761194029
[2m[36m(func pid=21969)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=21969)[0m f1_macro: 0.31951372264976047
[2m[36m(func pid=21969)[0m f1_weighted: 0.3942578480609725
[2m[36m(func pid=21969)[0m f1_per_class: [0.251, 0.389, 0.394, 0.512, 0.112, 0.304, 0.38, 0.294, 0.222, 0.339]
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.5811 | Steps: 4 | Val loss: 1.7443 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=21543)[0m top1: 0.363339552238806
[2m[36m(func pid=21543)[0m top5: 0.8931902985074627
[2m[36m(func pid=21543)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=21543)[0m f1_macro: 0.3302564533329616
[2m[36m(func pid=21543)[0m f1_weighted: 0.39035239966253665
[2m[36m(func pid=21543)[0m f1_per_class: [0.44, 0.436, 0.351, 0.402, 0.13, 0.364, 0.412, 0.295, 0.193, 0.28]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 10.4675 | Steps: 4 | Val loss: 69.5913 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 12:05:16 (running for 00:07:22.40)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.581 |      0.32  |                   75 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.732 |      0.33  |                   74 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.312 |      0.32  |                   73 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  3.936 |      0.329 |                   71 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.36986940298507465
[2m[36m(func pid=21163)[0m top5: 0.8666044776119403
[2m[36m(func pid=21163)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=21163)[0m f1_macro: 0.3196000528969857
[2m[36m(func pid=21163)[0m f1_weighted: 0.394381979293885
[2m[36m(func pid=21163)[0m f1_per_class: [0.466, 0.371, 0.186, 0.478, 0.097, 0.436, 0.353, 0.359, 0.219, 0.231]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.2300 | Steps: 4 | Val loss: 7.2699 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.6992 | Steps: 4 | Val loss: 2.3555 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=22392)[0m top1: 0.28125
[2m[36m(func pid=22392)[0m top5: 0.8031716417910447
[2m[36m(func pid=22392)[0m f1_micro: 0.28125
[2m[36m(func pid=22392)[0m f1_macro: 0.33456820676401267
[2m[36m(func pid=22392)[0m f1_weighted: 0.29296876996631543
[2m[36m(func pid=22392)[0m f1_per_class: [0.622, 0.428, 0.688, 0.309, 0.07, 0.149, 0.252, 0.191, 0.237, 0.4]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21969)[0m top1: 0.36613805970149255
[2m[36m(func pid=21969)[0m top5: 0.8512126865671642
[2m[36m(func pid=21969)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=21969)[0m f1_macro: 0.3162589779553238
[2m[36m(func pid=21969)[0m f1_weighted: 0.39042913055129963
[2m[36m(func pid=21969)[0m f1_per_class: [0.244, 0.383, 0.413, 0.5, 0.092, 0.274, 0.397, 0.273, 0.212, 0.375]
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.6548 | Steps: 4 | Val loss: 1.7318 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=21543)[0m top1: 0.3619402985074627
[2m[36m(func pid=21543)[0m top5: 0.8950559701492538
[2m[36m(func pid=21543)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=21543)[0m f1_macro: 0.33450008740169984
[2m[36m(func pid=21543)[0m f1_weighted: 0.38977188655642314
[2m[36m(func pid=21543)[0m f1_per_class: [0.454, 0.424, 0.406, 0.408, 0.126, 0.345, 0.415, 0.301, 0.194, 0.271]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 30.4227 | Steps: 4 | Val loss: 75.3742 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=21163)[0m top1: 0.37453358208955223
[2m[36m(func pid=21163)[0m top5: 0.8656716417910447
[2m[36m(func pid=21163)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=21163)[0m f1_macro: 0.327675989114988
[2m[36m(func pid=21163)[0m f1_weighted: 0.39533063818595415
[2m[36m(func pid=21163)[0m f1_per_class: [0.496, 0.361, 0.222, 0.49, 0.094, 0.44, 0.344, 0.369, 0.225, 0.236]
[2m[36m(func pid=21163)[0m 
== Status ==
Current time: 2024-01-07 12:05:21 (running for 00:07:27.76)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: 0.33125000000000004
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING  | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.655 |      0.328 |                   76 |
| train_9b9e8_00001 | RUNNING  | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.699 |      0.335 |                   75 |
| train_9b9e8_00002 | RUNNING  | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  2.23  |      0.316 |                   74 |
| train_9b9e8_00003 | RUNNING  | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 10.468 |      0.335 |                   72 |
| train_9b9e8_00004 | PENDING  |                    | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING  |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING  |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING  |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING  |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING  |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING  |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING  |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING  |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING  |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING  |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING  |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING  |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING  |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING  |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING  |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21969)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.9196 | Steps: 4 | Val loss: 7.7161 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 0.7907 | Steps: 4 | Val loss: 2.4122 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=22392)[0m top1: 0.259794776119403
[2m[36m(func pid=22392)[0m top5: 0.7686567164179104
[2m[36m(func pid=22392)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=22392)[0m f1_macro: 0.3173499322919738
[2m[36m(func pid=22392)[0m f1_weighted: 0.2713061332599853
[2m[36m(func pid=22392)[0m f1_per_class: [0.492, 0.377, 0.759, 0.272, 0.074, 0.169, 0.242, 0.203, 0.247, 0.339]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.5127 | Steps: 4 | Val loss: 1.7187 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=21543)[0m top1: 0.3358208955223881
[2m[36m(func pid=21543)[0m top5: 0.8843283582089553
[2m[36m(func pid=21543)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=21543)[0m f1_macro: 0.3058079517434513
[2m[36m(func pid=21543)[0m f1_weighted: 0.36116717644639534
[2m[36m(func pid=21543)[0m f1_per_class: [0.427, 0.389, 0.292, 0.347, 0.127, 0.33, 0.415, 0.263, 0.181, 0.288]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21969)[0m top1: 0.36427238805970147
[2m[36m(func pid=21969)[0m top5: 0.8451492537313433
[2m[36m(func pid=21969)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=21969)[0m f1_macro: 0.3127302382347301
[2m[36m(func pid=21969)[0m f1_weighted: 0.3847902703791642
[2m[36m(func pid=21969)[0m f1_per_class: [0.248, 0.426, 0.348, 0.466, 0.091, 0.282, 0.388, 0.232, 0.226, 0.421]
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 7.4887 | Steps: 4 | Val loss: 73.0069 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=21163)[0m top1: 0.37826492537313433
[2m[36m(func pid=21163)[0m top5: 0.8656716417910447
[2m[36m(func pid=21163)[0m f1_micro: 0.37826492537313433
[2m[36m(func pid=21163)[0m f1_macro: 0.3333018101419929
[2m[36m(func pid=21163)[0m f1_weighted: 0.3967852429829953
[2m[36m(func pid=21163)[0m f1_per_class: [0.554, 0.376, 0.238, 0.501, 0.095, 0.437, 0.333, 0.34, 0.213, 0.246]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 0.8339 | Steps: 4 | Val loss: 2.3195 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=22392)[0m top1: 0.26259328358208955
[2m[36m(func pid=22392)[0m top5: 0.7667910447761194
[2m[36m(func pid=22392)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=22392)[0m f1_macro: 0.32447723759807534
[2m[36m(func pid=22392)[0m f1_weighted: 0.2803546901403906
[2m[36m(func pid=22392)[0m f1_per_class: [0.492, 0.34, 0.786, 0.267, 0.075, 0.276, 0.259, 0.216, 0.218, 0.317]
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.5866 | Steps: 4 | Val loss: 1.7256 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=21543)[0m top1: 0.35774253731343286
[2m[36m(func pid=21543)[0m top5: 0.9029850746268657
[2m[36m(func pid=21543)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=21543)[0m f1_macro: 0.3221648439612862
[2m[36m(func pid=21543)[0m f1_weighted: 0.38432243756229756
[2m[36m(func pid=21543)[0m f1_per_class: [0.444, 0.39, 0.302, 0.38, 0.172, 0.326, 0.456, 0.281, 0.192, 0.278]
[2m[36m(func pid=21163)[0m top1: 0.3763992537313433
[2m[36m(func pid=21163)[0m top5: 0.8689365671641791
[2m[36m(func pid=21163)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=21163)[0m f1_macro: 0.32724390080787613
[2m[36m(func pid=21163)[0m f1_weighted: 0.3941458806886239
[2m[36m(func pid=21163)[0m f1_per_class: [0.504, 0.344, 0.245, 0.507, 0.113, 0.447, 0.336, 0.355, 0.202, 0.22]
== Status ==
Current time: 2024-01-07 12:05:27 (running for 00:07:32.95)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3275
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.513 |      0.333 |                   77 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.791 |      0.306 |                   76 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 30.423 |      0.317 |                   73 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 12:05:33 (running for 00:07:39.50)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3275
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.513 |      0.333 |                   77 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.791 |      0.306 |                   76 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  7.489 |      0.324 |                   74 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=39840)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=39840)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=39840)[0m Configuration completed!
[2m[36m(func pid=39840)[0m New optimizer parameters:
[2m[36m(func pid=39840)[0m SGD (
[2m[36m(func pid=39840)[0m Parameter Group 0
[2m[36m(func pid=39840)[0m     dampening: 0
[2m[36m(func pid=39840)[0m     differentiable: False
[2m[36m(func pid=39840)[0m     foreach: None
[2m[36m(func pid=39840)[0m     lr: 0.0001
[2m[36m(func pid=39840)[0m     maximize: False
[2m[36m(func pid=39840)[0m     momentum: 0.9
[2m[36m(func pid=39840)[0m     nesterov: False
[2m[36m(func pid=39840)[0m     weight_decay: 0
[2m[36m(func pid=39840)[0m )
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.6026 | Steps: 4 | Val loss: 69.8979 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.4643 | Steps: 4 | Val loss: 1.7275 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 0.2921 | Steps: 4 | Val loss: 2.3372 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9834 | Steps: 4 | Val loss: 2.3138 | Batch size: 32 | lr: 0.0001 | Duration: 4.59s
== Status ==
Current time: 2024-01-07 12:05:38 (running for 00:07:44.51)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3275
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.587 |      0.327 |                   78 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.834 |      0.322 |                   77 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  7.489 |      0.324 |                   74 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3726679104477612
[2m[36m(func pid=21163)[0m top5: 0.8680037313432836
[2m[36m(func pid=21163)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=21163)[0m f1_macro: 0.3225116991781838
[2m[36m(func pid=21163)[0m f1_weighted: 0.3904745278196422
[2m[36m(func pid=21163)[0m f1_per_class: [0.481, 0.324, 0.24, 0.518, 0.102, 0.437, 0.329, 0.347, 0.222, 0.224]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.35774253731343286
[2m[36m(func pid=21543)[0m top5: 0.8964552238805971
[2m[36m(func pid=21543)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=21543)[0m f1_macro: 0.32477566256597806
[2m[36m(func pid=21543)[0m f1_weighted: 0.38730961030373345
[2m[36m(func pid=21543)[0m f1_per_class: [0.483, 0.377, 0.333, 0.4, 0.177, 0.318, 0.456, 0.28, 0.194, 0.229]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m top1: 0.27611940298507465
[2m[36m(func pid=22392)[0m top5: 0.7793843283582089
[2m[36m(func pid=22392)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=22392)[0m f1_macro: 0.3286448825998995
[2m[36m(func pid=22392)[0m f1_weighted: 0.29587857598421274
[2m[36m(func pid=22392)[0m f1_per_class: [0.529, 0.345, 0.759, 0.291, 0.082, 0.281, 0.28, 0.242, 0.196, 0.282]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=39840)[0m top1: 0.1828358208955224
[2m[36m(func pid=39840)[0m top5: 0.5424440298507462
[2m[36m(func pid=39840)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=39840)[0m f1_macro: 0.11053902950664392
[2m[36m(func pid=39840)[0m f1_weighted: 0.12770048486404903
[2m[36m(func pid=39840)[0m f1_per_class: [0.225, 0.326, 0.0, 0.102, 0.012, 0.278, 0.012, 0.024, 0.0, 0.125]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.4834 | Steps: 4 | Val loss: 1.7100 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 0.4147 | Steps: 4 | Val loss: 2.4288 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 2.9317 | Steps: 4 | Val loss: 73.1250 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9785 | Steps: 4 | Val loss: 2.3123 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 12:05:44 (running for 00:07:50.13)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.464 |      0.323 |                   79 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.415 |      0.317 |                   79 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  1.603 |      0.329 |                   75 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.983 |      0.111 |                    1 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.37406716417910446
[2m[36m(func pid=21163)[0m top5: 0.8722014925373134
[2m[36m(func pid=21163)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=21163)[0m f1_macro: 0.32711544053736463
[2m[36m(func pid=21163)[0m f1_weighted: 0.3894137190430301
[2m[36m(func pid=21163)[0m f1_per_class: [0.542, 0.301, 0.222, 0.52, 0.098, 0.444, 0.328, 0.361, 0.229, 0.227]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3474813432835821
[2m[36m(func pid=21543)[0m top5: 0.8959888059701493
[2m[36m(func pid=21543)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=21543)[0m f1_macro: 0.3167611138778627
[2m[36m(func pid=21543)[0m f1_weighted: 0.37826841357333485
[2m[36m(func pid=21543)[0m f1_per_class: [0.464, 0.38, 0.325, 0.373, 0.178, 0.318, 0.453, 0.277, 0.187, 0.213]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m top1: 0.2644589552238806
[2m[36m(func pid=22392)[0m top5: 0.7789179104477612
[2m[36m(func pid=22392)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=22392)[0m f1_macro: 0.30710923105495597
[2m[36m(func pid=22392)[0m f1_weighted: 0.2913121910613635
[2m[36m(func pid=22392)[0m f1_per_class: [0.438, 0.315, 0.71, 0.297, 0.104, 0.288, 0.287, 0.238, 0.158, 0.237]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=39840)[0m top1: 0.16837686567164178
[2m[36m(func pid=39840)[0m top5: 0.5415111940298507
[2m[36m(func pid=39840)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=39840)[0m f1_macro: 0.09431109222579652
[2m[36m(func pid=39840)[0m f1_weighted: 0.12139270087002134
[2m[36m(func pid=39840)[0m f1_per_class: [0.135, 0.299, 0.0, 0.112, 0.008, 0.292, 0.003, 0.011, 0.0, 0.083]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.0267 | Steps: 4 | Val loss: 2.4739 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.4051 | Steps: 4 | Val loss: 1.7260 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 2.7505 | Steps: 4 | Val loss: 72.6759 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9505 | Steps: 4 | Val loss: 2.3274 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 12:05:49 (running for 00:07:55.34)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.483 |      0.327 |                   80 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.027 |      0.323 |                   80 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  2.932 |      0.307 |                   76 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.979 |      0.094 |                    2 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m top1: 0.34654850746268656
[2m[36m(func pid=21543)[0m top5: 0.8917910447761194
[2m[36m(func pid=21543)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=21543)[0m f1_macro: 0.3233274314703533
[2m[36m(func pid=21543)[0m f1_weighted: 0.38117456417141204
[2m[36m(func pid=21543)[0m f1_per_class: [0.486, 0.365, 0.325, 0.379, 0.26, 0.304, 0.469, 0.289, 0.168, 0.189]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=21163)[0m top1: 0.37406716417910446
[2m[36m(func pid=21163)[0m top5: 0.8722014925373134
[2m[36m(func pid=21163)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=21163)[0m f1_macro: 0.32653389476371053
[2m[36m(func pid=21163)[0m f1_weighted: 0.39201666024844106
[2m[36m(func pid=21163)[0m f1_per_class: [0.544, 0.317, 0.233, 0.524, 0.104, 0.433, 0.33, 0.358, 0.214, 0.208]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=22392)[0m top1: 0.2658582089552239
[2m[36m(func pid=22392)[0m top5: 0.7756529850746269
[2m[36m(func pid=22392)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=22392)[0m f1_macro: 0.310046614447632
[2m[36m(func pid=22392)[0m f1_weighted: 0.29303816118192694
[2m[36m(func pid=22392)[0m f1_per_class: [0.457, 0.301, 0.727, 0.312, 0.118, 0.299, 0.282, 0.239, 0.154, 0.212]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=39840)[0m top1: 0.16371268656716417
[2m[36m(func pid=39840)[0m top5: 0.523320895522388
[2m[36m(func pid=39840)[0m f1_micro: 0.16371268656716417
[2m[36m(func pid=39840)[0m f1_macro: 0.08566071276149365
[2m[36m(func pid=39840)[0m f1_weighted: 0.12480732332323109
[2m[36m(func pid=39840)[0m f1_per_class: [0.061, 0.283, 0.0, 0.119, 0.023, 0.294, 0.021, 0.024, 0.0, 0.033]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 1.3672 | Steps: 4 | Val loss: 1.7212 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.2494 | Steps: 4 | Val loss: 2.4815 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 3.5808 | Steps: 4 | Val loss: 69.1764 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9726 | Steps: 4 | Val loss: 2.3329 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 12:05:54 (running for 00:08:00.57)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.367 |      0.327 |                   82 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.027 |      0.323 |                   80 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  2.75  |      0.31  |                   77 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.951 |      0.086 |                    3 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3675373134328358
[2m[36m(func pid=21163)[0m top5: 0.8708022388059702
[2m[36m(func pid=21163)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=21163)[0m f1_macro: 0.3268699901363211
[2m[36m(func pid=21163)[0m f1_weighted: 0.38137423389446984
[2m[36m(func pid=21163)[0m f1_per_class: [0.545, 0.304, 0.255, 0.514, 0.096, 0.425, 0.311, 0.357, 0.23, 0.23]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.35261194029850745
[2m[36m(func pid=21543)[0m top5: 0.8931902985074627
[2m[36m(func pid=21543)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=21543)[0m f1_macro: 0.3283968104887461
[2m[36m(func pid=21543)[0m f1_weighted: 0.3876490984109443
[2m[36m(func pid=21543)[0m f1_per_class: [0.476, 0.371, 0.371, 0.385, 0.267, 0.301, 0.483, 0.279, 0.181, 0.17]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m top1: 0.2891791044776119
[2m[36m(func pid=22392)[0m top5: 0.7910447761194029
[2m[36m(func pid=22392)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=22392)[0m f1_macro: 0.31699979104495846
[2m[36m(func pid=22392)[0m f1_weighted: 0.3115244081109241
[2m[36m(func pid=22392)[0m f1_per_class: [0.43, 0.293, 0.727, 0.373, 0.139, 0.311, 0.288, 0.23, 0.171, 0.208]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=39840)[0m top1: 0.15951492537313433
[2m[36m(func pid=39840)[0m top5: 0.5153917910447762
[2m[36m(func pid=39840)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=39840)[0m f1_macro: 0.08695370255406991
[2m[36m(func pid=39840)[0m f1_weighted: 0.12479006277717458
[2m[36m(func pid=39840)[0m f1_per_class: [0.052, 0.25, 0.0, 0.125, 0.017, 0.307, 0.02, 0.073, 0.0, 0.026]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 1.2288 | Steps: 4 | Val loss: 1.7503 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 0.7609 | Steps: 4 | Val loss: 2.5441 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 7.9452 | Steps: 4 | Val loss: 63.6953 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 12:06:00 (running for 00:08:05.96)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.229 |      0.323 |                   83 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.249 |      0.328 |                   81 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  3.581 |      0.317 |                   78 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.973 |      0.087 |                    4 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3558768656716418
[2m[36m(func pid=21163)[0m top5: 0.8647388059701493
[2m[36m(func pid=21163)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=21163)[0m f1_macro: 0.3226674308426958
[2m[36m(func pid=21163)[0m f1_weighted: 0.3737237277886921
[2m[36m(func pid=21163)[0m f1_per_class: [0.564, 0.292, 0.267, 0.502, 0.079, 0.41, 0.311, 0.345, 0.232, 0.225]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9598 | Steps: 4 | Val loss: 2.3228 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=21543)[0m top1: 0.34281716417910446
[2m[36m(func pid=21543)[0m top5: 0.8824626865671642
[2m[36m(func pid=21543)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=21543)[0m f1_macro: 0.3143243483519463
[2m[36m(func pid=21543)[0m f1_weighted: 0.37785020007173803
[2m[36m(func pid=21543)[0m f1_per_class: [0.474, 0.381, 0.306, 0.398, 0.206, 0.303, 0.432, 0.287, 0.189, 0.168]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m top1: 0.3292910447761194
[2m[36m(func pid=22392)[0m top5: 0.7994402985074627
[2m[36m(func pid=22392)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=22392)[0m f1_macro: 0.3218033747255759
[2m[36m(func pid=22392)[0m f1_weighted: 0.34313345255555777
[2m[36m(func pid=22392)[0m f1_per_class: [0.468, 0.32, 0.585, 0.436, 0.163, 0.337, 0.31, 0.202, 0.223, 0.173]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=39840)[0m top1: 0.1525186567164179
[2m[36m(func pid=39840)[0m top5: 0.5284514925373134
[2m[36m(func pid=39840)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=39840)[0m f1_macro: 0.08047009891759531
[2m[36m(func pid=39840)[0m f1_weighted: 0.12934311201051085
[2m[36m(func pid=39840)[0m f1_per_class: [0.02, 0.233, 0.0, 0.142, 0.008, 0.289, 0.04, 0.073, 0.0, 0.0]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 1.6174 | Steps: 4 | Val loss: 1.7761 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 0.4603 | Steps: 4 | Val loss: 2.5175 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 6.6387 | Steps: 4 | Val loss: 63.3718 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 12:06:05 (running for 00:08:11.41)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.229 |      0.323 |                   83 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.46  |      0.305 |                   83 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  7.945 |      0.322 |                   79 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.96  |      0.08  |                    5 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3512126865671642
[2m[36m(func pid=21163)[0m top5: 0.8600746268656716
[2m[36m(func pid=21163)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=21163)[0m f1_macro: 0.3181014517380078
[2m[36m(func pid=21163)[0m f1_weighted: 0.3704767494027177
[2m[36m(func pid=21163)[0m f1_per_class: [0.538, 0.295, 0.224, 0.494, 0.075, 0.418, 0.3, 0.362, 0.255, 0.221]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.34468283582089554
[2m[36m(func pid=21543)[0m top5: 0.8740671641791045
[2m[36m(func pid=21543)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=21543)[0m f1_macro: 0.30527734129706513
[2m[36m(func pid=21543)[0m f1_weighted: 0.37830071869978527
[2m[36m(func pid=21543)[0m f1_per_class: [0.441, 0.348, 0.277, 0.438, 0.182, 0.287, 0.423, 0.286, 0.195, 0.176]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9490 | Steps: 4 | Val loss: 2.3159 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=22392)[0m top1: 0.36847014925373134
[2m[36m(func pid=22392)[0m top5: 0.8059701492537313
[2m[36m(func pid=22392)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=22392)[0m f1_macro: 0.3188315450534188
[2m[36m(func pid=22392)[0m f1_weighted: 0.36577963205996816
[2m[36m(func pid=22392)[0m f1_per_class: [0.476, 0.33, 0.406, 0.506, 0.152, 0.371, 0.299, 0.197, 0.291, 0.16]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=39840)[0m top1: 0.15625
[2m[36m(func pid=39840)[0m top5: 0.53125
[2m[36m(func pid=39840)[0m f1_micro: 0.15625
[2m[36m(func pid=39840)[0m f1_macro: 0.0893206299745459
[2m[36m(func pid=39840)[0m f1_weighted: 0.13948295805844874
[2m[36m(func pid=39840)[0m f1_per_class: [0.017, 0.224, 0.057, 0.161, 0.008, 0.28, 0.062, 0.084, 0.0, 0.0]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.4611 | Steps: 4 | Val loss: 1.7843 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 0.4430 | Steps: 4 | Val loss: 2.4848 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 22.9892 | Steps: 4 | Val loss: 67.6223 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 12:06:10 (running for 00:08:16.59)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.461 |      0.311 |                   85 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.46  |      0.305 |                   83 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  6.639 |      0.319 |                   80 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.949 |      0.089 |                    6 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3460820895522388
[2m[36m(func pid=21163)[0m top5: 0.8656716417910447
[2m[36m(func pid=21163)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=21163)[0m f1_macro: 0.3107043940589337
[2m[36m(func pid=21163)[0m f1_weighted: 0.36657052044477273
[2m[36m(func pid=21163)[0m f1_per_class: [0.512, 0.302, 0.185, 0.489, 0.075, 0.409, 0.292, 0.368, 0.25, 0.225]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9625 | Steps: 4 | Val loss: 2.3206 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=21543)[0m top1: 0.34421641791044777
[2m[36m(func pid=21543)[0m top5: 0.878731343283582
[2m[36m(func pid=21543)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=21543)[0m f1_macro: 0.31095771820636486
[2m[36m(func pid=21543)[0m f1_weighted: 0.3785885314169883
[2m[36m(func pid=21543)[0m f1_per_class: [0.454, 0.341, 0.271, 0.425, 0.235, 0.304, 0.432, 0.291, 0.191, 0.166]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m top1: 0.3670708955223881
[2m[36m(func pid=22392)[0m top5: 0.7821828358208955
[2m[36m(func pid=22392)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=22392)[0m f1_macro: 0.31272376468971763
[2m[36m(func pid=22392)[0m f1_weighted: 0.3515813851892683
[2m[36m(func pid=22392)[0m f1_per_class: [0.487, 0.302, 0.329, 0.506, 0.169, 0.394, 0.252, 0.224, 0.309, 0.155]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=39840)[0m top1: 0.14412313432835822
[2m[36m(func pid=39840)[0m top5: 0.534981343283582
[2m[36m(func pid=39840)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=39840)[0m f1_macro: 0.09375486830900574
[2m[36m(func pid=39840)[0m f1_weighted: 0.1360256947055086
[2m[36m(func pid=39840)[0m f1_per_class: [0.031, 0.201, 0.114, 0.159, 0.008, 0.262, 0.069, 0.081, 0.012, 0.0]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.2393 | Steps: 4 | Val loss: 1.7476 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 0.5102 | Steps: 4 | Val loss: 2.4756 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 8.2248 | Steps: 4 | Val loss: 66.8501 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 12:06:15 (running for 00:08:21.83)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.239 |      0.32  |                   86 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.443 |      0.311 |                   84 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 22.989 |      0.313 |                   81 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.962 |      0.094 |                    7 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3572761194029851
[2m[36m(func pid=21163)[0m top5: 0.8698694029850746
[2m[36m(func pid=21163)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=21163)[0m f1_macro: 0.32022700845906654
[2m[36m(func pid=21163)[0m f1_weighted: 0.37479237961066925
[2m[36m(func pid=21163)[0m f1_per_class: [0.534, 0.319, 0.229, 0.49, 0.092, 0.416, 0.307, 0.355, 0.247, 0.214]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9340 | Steps: 4 | Val loss: 2.3111 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=21543)[0m top1: 0.34841417910447764
[2m[36m(func pid=21543)[0m top5: 0.8805970149253731
[2m[36m(func pid=21543)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=21543)[0m f1_macro: 0.31042939333130676
[2m[36m(func pid=21543)[0m f1_weighted: 0.3839908862615675
[2m[36m(func pid=21543)[0m f1_per_class: [0.466, 0.36, 0.236, 0.429, 0.243, 0.296, 0.44, 0.284, 0.193, 0.158]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m top1: 0.37080223880597013
[2m[36m(func pid=22392)[0m top5: 0.7971082089552238
[2m[36m(func pid=22392)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=22392)[0m f1_macro: 0.299670697552056
[2m[36m(func pid=22392)[0m f1_weighted: 0.355338189639379
[2m[36m(func pid=22392)[0m f1_per_class: [0.408, 0.318, 0.292, 0.51, 0.17, 0.393, 0.268, 0.183, 0.293, 0.161]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 1.3535 | Steps: 4 | Val loss: 1.7711 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=39840)[0m top1: 0.14878731343283583
[2m[36m(func pid=39840)[0m top5: 0.5531716417910447
[2m[36m(func pid=39840)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=39840)[0m f1_macro: 0.09960095286629059
[2m[36m(func pid=39840)[0m f1_weighted: 0.14253675120670095
[2m[36m(func pid=39840)[0m f1_per_class: [0.03, 0.206, 0.125, 0.16, 0.02, 0.273, 0.081, 0.088, 0.012, 0.0]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 0.3595 | Steps: 4 | Val loss: 2.4578 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 28.5172 | Steps: 4 | Val loss: 68.6181 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 12:06:21 (running for 00:08:27.01)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.353 |      0.312 |                   87 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.51  |      0.31  |                   85 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  8.225 |      0.3   |                   82 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.934 |      0.1   |                    8 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.35494402985074625
[2m[36m(func pid=21163)[0m top5: 0.8605410447761194
[2m[36m(func pid=21163)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=21163)[0m f1_macro: 0.3118213269970885
[2m[36m(func pid=21163)[0m f1_weighted: 0.3729778358785749
[2m[36m(func pid=21163)[0m f1_per_class: [0.481, 0.295, 0.218, 0.504, 0.096, 0.414, 0.306, 0.366, 0.222, 0.214]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.34841417910447764
[2m[36m(func pid=21543)[0m top5: 0.8833955223880597
[2m[36m(func pid=21543)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=21543)[0m f1_macro: 0.30086546866673414
[2m[36m(func pid=21543)[0m f1_weighted: 0.38174793445058813
[2m[36m(func pid=21543)[0m f1_per_class: [0.421, 0.345, 0.23, 0.448, 0.179, 0.319, 0.419, 0.283, 0.191, 0.173]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9457 | Steps: 4 | Val loss: 2.2919 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=22392)[0m top1: 0.36613805970149255
[2m[36m(func pid=22392)[0m top5: 0.7957089552238806
[2m[36m(func pid=22392)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=22392)[0m f1_macro: 0.2924267201539999
[2m[36m(func pid=22392)[0m f1_weighted: 0.3530566533720776
[2m[36m(func pid=22392)[0m f1_per_class: [0.362, 0.363, 0.255, 0.492, 0.157, 0.359, 0.257, 0.281, 0.224, 0.175]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 1.2541 | Steps: 4 | Val loss: 1.7748 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.6197 | Steps: 4 | Val loss: 2.2824 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=39840)[0m top1: 0.15671641791044777
[2m[36m(func pid=39840)[0m top5: 0.5783582089552238
[2m[36m(func pid=39840)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=39840)[0m f1_macro: 0.1002723568260399
[2m[36m(func pid=39840)[0m f1_weighted: 0.14959345056557555
[2m[36m(func pid=39840)[0m f1_per_class: [0.027, 0.219, 0.103, 0.167, 0.02, 0.295, 0.086, 0.073, 0.013, 0.0]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 14.6386 | Steps: 4 | Val loss: 70.4722 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=21163)[0m top1: 0.3530783582089552
[2m[36m(func pid=21163)[0m top5: 0.8586753731343284
[2m[36m(func pid=21163)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=21163)[0m f1_macro: 0.3128013332963836
[2m[36m(func pid=21163)[0m f1_weighted: 0.3728350223031585
[2m[36m(func pid=21163)[0m f1_per_class: [0.488, 0.31, 0.229, 0.495, 0.089, 0.418, 0.305, 0.359, 0.229, 0.206]
[2m[36m(func pid=21163)[0m 
== Status ==
Current time: 2024-01-07 12:06:26 (running for 00:08:32.24)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.254 |      0.313 |                   88 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.36  |      0.301 |                   86 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 28.517 |      0.292 |                   83 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.946 |      0.1   |                    9 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m top1: 0.3843283582089552
[2m[36m(func pid=21543)[0m top5: 0.8973880597014925
[2m[36m(func pid=21543)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=21543)[0m f1_macro: 0.32506192960576796
[2m[36m(func pid=21543)[0m f1_weighted: 0.41371911398842637
[2m[36m(func pid=21543)[0m f1_per_class: [0.435, 0.385, 0.241, 0.475, 0.219, 0.368, 0.455, 0.283, 0.204, 0.186]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9255 | Steps: 4 | Val loss: 2.2955 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=22392)[0m top1: 0.35261194029850745
[2m[36m(func pid=22392)[0m top5: 0.7868470149253731
[2m[36m(func pid=22392)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=22392)[0m f1_macro: 0.2856231549192475
[2m[36m(func pid=22392)[0m f1_weighted: 0.3416149932586659
[2m[36m(func pid=22392)[0m f1_per_class: [0.283, 0.422, 0.21, 0.452, 0.175, 0.333, 0.231, 0.31, 0.214, 0.226]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 1.4381 | Steps: 4 | Val loss: 1.7654 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 0.3956 | Steps: 4 | Val loss: 2.3409 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=39840)[0m top1: 0.1571828358208955
[2m[36m(func pid=39840)[0m top5: 0.5699626865671642
[2m[36m(func pid=39840)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=39840)[0m f1_macro: 0.10134396215227341
[2m[36m(func pid=39840)[0m f1_weighted: 0.1507218314096732
[2m[36m(func pid=39840)[0m f1_per_class: [0.026, 0.236, 0.095, 0.163, 0.021, 0.29, 0.084, 0.085, 0.014, 0.0]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 18.9587 | Steps: 4 | Val loss: 73.7527 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 12:06:31 (running for 00:08:37.52)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.438 |      0.319 |                   89 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.62  |      0.325 |                   87 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 14.639 |      0.286 |                   84 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.925 |      0.101 |                   10 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3568097014925373
[2m[36m(func pid=21163)[0m top5: 0.8563432835820896
[2m[36m(func pid=21163)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=21163)[0m f1_macro: 0.31910591353914547
[2m[36m(func pid=21163)[0m f1_weighted: 0.3789362959322441
[2m[36m(func pid=21163)[0m f1_per_class: [0.522, 0.338, 0.238, 0.49, 0.092, 0.421, 0.313, 0.345, 0.226, 0.206]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.37919776119402987
[2m[36m(func pid=21543)[0m top5: 0.8880597014925373
[2m[36m(func pid=21543)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=21543)[0m f1_macro: 0.321208610579969
[2m[36m(func pid=21543)[0m f1_weighted: 0.4090996504505199
[2m[36m(func pid=21543)[0m f1_per_class: [0.426, 0.392, 0.217, 0.471, 0.22, 0.355, 0.443, 0.281, 0.221, 0.186]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9053 | Steps: 4 | Val loss: 2.2862 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=22392)[0m top1: 0.3204291044776119
[2m[36m(func pid=22392)[0m top5: 0.7663246268656716
[2m[36m(func pid=22392)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=22392)[0m f1_macro: 0.25940020418258064
[2m[36m(func pid=22392)[0m f1_weighted: 0.3075527646101654
[2m[36m(func pid=22392)[0m f1_per_class: [0.252, 0.413, 0.206, 0.425, 0.148, 0.286, 0.171, 0.325, 0.167, 0.201]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 1.3114 | Steps: 4 | Val loss: 1.7479 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 0.5944 | Steps: 4 | Val loss: 2.4283 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=39840)[0m top1: 0.15811567164179105
[2m[36m(func pid=39840)[0m top5: 0.5825559701492538
[2m[36m(func pid=39840)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=39840)[0m f1_macro: 0.10480678188617723
[2m[36m(func pid=39840)[0m f1_weighted: 0.15820618419243115
[2m[36m(func pid=39840)[0m f1_per_class: [0.038, 0.228, 0.098, 0.164, 0.027, 0.283, 0.114, 0.083, 0.013, 0.0]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 14.4008 | Steps: 4 | Val loss: 78.8505 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 12:06:36 (running for 00:08:42.89)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.311 |      0.323 |                   90 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.396 |      0.321 |                   88 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 18.959 |      0.259 |                   85 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.905 |      0.105 |                   11 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3628731343283582
[2m[36m(func pid=21163)[0m top5: 0.8647388059701493
[2m[36m(func pid=21163)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=21163)[0m f1_macro: 0.3228762596044444
[2m[36m(func pid=21163)[0m f1_weighted: 0.3839569438398015
[2m[36m(func pid=21163)[0m f1_per_class: [0.544, 0.357, 0.218, 0.5, 0.093, 0.408, 0.312, 0.344, 0.238, 0.215]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3530783582089552
[2m[36m(func pid=21543)[0m top5: 0.8796641791044776
[2m[36m(func pid=21543)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=21543)[0m f1_macro: 0.30115630561803003
[2m[36m(func pid=21543)[0m f1_weighted: 0.3821860569307777
[2m[36m(func pid=21543)[0m f1_per_class: [0.378, 0.353, 0.184, 0.428, 0.214, 0.344, 0.424, 0.289, 0.216, 0.182]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m top1: 0.3111007462686567
[2m[36m(func pid=22392)[0m top5: 0.761660447761194
[2m[36m(func pid=22392)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=22392)[0m f1_macro: 0.2557678992598825
[2m[36m(func pid=22392)[0m f1_weighted: 0.3027559413149976
[2m[36m(func pid=22392)[0m f1_per_class: [0.257, 0.436, 0.131, 0.41, 0.151, 0.294, 0.153, 0.31, 0.194, 0.222]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8646 | Steps: 4 | Val loss: 2.2848 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 1.3183 | Steps: 4 | Val loss: 1.7794 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 0.3906 | Steps: 4 | Val loss: 2.4170 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=39840)[0m top1: 0.15391791044776118
[2m[36m(func pid=39840)[0m top5: 0.5909514925373134
[2m[36m(func pid=39840)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=39840)[0m f1_macro: 0.09881436320313644
[2m[36m(func pid=39840)[0m f1_weighted: 0.15289963157888323
[2m[36m(func pid=39840)[0m f1_per_class: [0.033, 0.2, 0.098, 0.176, 0.014, 0.288, 0.103, 0.077, 0.0, 0.0]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 25.2720 | Steps: 4 | Val loss: 89.1374 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 12:06:42 (running for 00:08:48.07)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.318 |      0.314 |                   91 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.594 |      0.301 |                   89 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 14.401 |      0.256 |                   86 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.865 |      0.099 |                   12 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3512126865671642
[2m[36m(func pid=21163)[0m top5: 0.8596082089552238
[2m[36m(func pid=21163)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=21163)[0m f1_macro: 0.3135723658439749
[2m[36m(func pid=21163)[0m f1_weighted: 0.3761565736931719
[2m[36m(func pid=21163)[0m f1_per_class: [0.508, 0.354, 0.22, 0.486, 0.094, 0.408, 0.305, 0.35, 0.223, 0.187]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.363339552238806
[2m[36m(func pid=21543)[0m top5: 0.8819962686567164
[2m[36m(func pid=21543)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=21543)[0m f1_macro: 0.3088566629907438
[2m[36m(func pid=21543)[0m f1_weighted: 0.39114242442647684
[2m[36m(func pid=21543)[0m f1_per_class: [0.418, 0.382, 0.179, 0.45, 0.178, 0.337, 0.411, 0.301, 0.25, 0.182]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.9083 | Steps: 4 | Val loss: 2.2821 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=22392)[0m top1: 0.2621268656716418
[2m[36m(func pid=22392)[0m top5: 0.7336753731343284
[2m[36m(func pid=22392)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=22392)[0m f1_macro: 0.23136088833633134
[2m[36m(func pid=22392)[0m f1_weighted: 0.25957403584779776
[2m[36m(func pid=22392)[0m f1_per_class: [0.202, 0.409, 0.098, 0.311, 0.185, 0.245, 0.142, 0.283, 0.206, 0.231]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 1.2148 | Steps: 4 | Val loss: 1.7516 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 0.9388 | Steps: 4 | Val loss: 2.4050 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=39840)[0m top1: 0.15858208955223882
[2m[36m(func pid=39840)[0m top5: 0.5918843283582089
[2m[36m(func pid=39840)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=39840)[0m f1_macro: 0.10423821947646657
[2m[36m(func pid=39840)[0m f1_weighted: 0.15924329911824783
[2m[36m(func pid=39840)[0m f1_per_class: [0.022, 0.199, 0.133, 0.188, 0.014, 0.294, 0.111, 0.081, 0.0, 0.0]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 20.8038 | Steps: 4 | Val loss: 89.1925 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=21163)[0m top1: 0.3596082089552239
[2m[36m(func pid=21163)[0m top5: 0.8652052238805971
[2m[36m(func pid=21163)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=21163)[0m f1_macro: 0.32482105491394353
[2m[36m(func pid=21163)[0m f1_weighted: 0.3822092697835053
[2m[36m(func pid=21163)[0m f1_per_class: [0.536, 0.359, 0.258, 0.482, 0.09, 0.422, 0.317, 0.343, 0.227, 0.214]
[2m[36m(func pid=21163)[0m 
== Status ==
Current time: 2024-01-07 12:06:47 (running for 00:08:53.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.215 |      0.325 |                   92 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.391 |      0.309 |                   90 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 25.272 |      0.231 |                   87 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.908 |      0.104 |                   13 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21543)[0m top1: 0.3666044776119403
[2m[36m(func pid=21543)[0m top5: 0.8852611940298507
[2m[36m(func pid=21543)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=21543)[0m f1_macro: 0.3126675295276285
[2m[36m(func pid=21543)[0m f1_weighted: 0.3932715143341542
[2m[36m(func pid=21543)[0m f1_per_class: [0.423, 0.401, 0.183, 0.455, 0.186, 0.357, 0.396, 0.301, 0.243, 0.183]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m top1: 0.26119402985074625
[2m[36m(func pid=22392)[0m top5: 0.730410447761194
[2m[36m(func pid=22392)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=22392)[0m f1_macro: 0.2282877806785501
[2m[36m(func pid=22392)[0m f1_weighted: 0.2682747598109033
[2m[36m(func pid=22392)[0m f1_per_class: [0.186, 0.406, 0.101, 0.291, 0.129, 0.248, 0.192, 0.276, 0.231, 0.223]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.9120 | Steps: 4 | Val loss: 2.2793 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 1.3837 | Steps: 4 | Val loss: 1.7755 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 0.2394 | Steps: 4 | Val loss: 2.4118 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=39840)[0m top1: 0.1609141791044776
[2m[36m(func pid=39840)[0m top5: 0.5942164179104478
[2m[36m(func pid=39840)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=39840)[0m f1_macro: 0.10530264558921817
[2m[36m(func pid=39840)[0m f1_weighted: 0.16119445069623267
[2m[36m(func pid=39840)[0m f1_per_class: [0.023, 0.208, 0.089, 0.187, 0.033, 0.299, 0.108, 0.08, 0.026, 0.0]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 23.5613 | Steps: 4 | Val loss: 82.6291 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 12:06:52 (running for 00:08:58.46)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.384 |      0.316 |                   93 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.939 |      0.313 |                   91 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 20.804 |      0.228 |                   88 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.912 |      0.105 |                   14 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3498134328358209
[2m[36m(func pid=21163)[0m top5: 0.8591417910447762
[2m[36m(func pid=21163)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=21163)[0m f1_macro: 0.3157670088069689
[2m[36m(func pid=21163)[0m f1_weighted: 0.3705235374063663
[2m[36m(func pid=21163)[0m f1_per_class: [0.489, 0.358, 0.261, 0.464, 0.096, 0.414, 0.302, 0.362, 0.198, 0.215]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3670708955223881
[2m[36m(func pid=21543)[0m top5: 0.8833955223880597
[2m[36m(func pid=21543)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=21543)[0m f1_macro: 0.31425956613822126
[2m[36m(func pid=21543)[0m f1_weighted: 0.392265008547699
[2m[36m(func pid=21543)[0m f1_per_class: [0.423, 0.409, 0.178, 0.458, 0.185, 0.355, 0.383, 0.304, 0.261, 0.188]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8928 | Steps: 4 | Val loss: 2.2805 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=22392)[0m top1: 0.28125
[2m[36m(func pid=22392)[0m top5: 0.7411380597014925
[2m[36m(func pid=22392)[0m f1_micro: 0.28125
[2m[36m(func pid=22392)[0m f1_macro: 0.23809715020828487
[2m[36m(func pid=22392)[0m f1_weighted: 0.3079325501366063
[2m[36m(func pid=22392)[0m f1_per_class: [0.185, 0.381, 0.1, 0.284, 0.15, 0.239, 0.35, 0.289, 0.217, 0.186]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 1.3694 | Steps: 4 | Val loss: 1.7704 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 0.2957 | Steps: 4 | Val loss: 2.4801 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=39840)[0m top1: 0.16277985074626866
[2m[36m(func pid=39840)[0m top5: 0.5862873134328358
[2m[36m(func pid=39840)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=39840)[0m f1_macro: 0.11394071601953767
[2m[36m(func pid=39840)[0m f1_weighted: 0.16701647794805016
[2m[36m(func pid=39840)[0m f1_per_class: [0.035, 0.21, 0.167, 0.208, 0.024, 0.289, 0.109, 0.083, 0.013, 0.0]
[2m[36m(func pid=39840)[0m 
== Status ==
Current time: 2024-01-07 12:06:57 (running for 00:09:03.67)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.369 |      0.316 |                   94 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.239 |      0.314 |                   92 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 23.561 |      0.238 |                   89 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.893 |      0.114 |                   15 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.35774253731343286
[2m[36m(func pid=21163)[0m top5: 0.8600746268656716
[2m[36m(func pid=21163)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=21163)[0m f1_macro: 0.31585504148457744
[2m[36m(func pid=21163)[0m f1_weighted: 0.3769686738784378
[2m[36m(func pid=21163)[0m f1_per_class: [0.45, 0.386, 0.267, 0.476, 0.102, 0.409, 0.3, 0.359, 0.202, 0.206]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 17.4593 | Steps: 4 | Val loss: 84.6102 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=21543)[0m top1: 0.3628731343283582
[2m[36m(func pid=21543)[0m top5: 0.8736007462686567
[2m[36m(func pid=21543)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=21543)[0m f1_macro: 0.3073656119500513
[2m[36m(func pid=21543)[0m f1_weighted: 0.38691921070008145
[2m[36m(func pid=21543)[0m f1_per_class: [0.397, 0.412, 0.172, 0.464, 0.148, 0.34, 0.362, 0.319, 0.27, 0.189]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.9101 | Steps: 4 | Val loss: 2.2745 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=22392)[0m top1: 0.2868470149253731
[2m[36m(func pid=22392)[0m top5: 0.7430037313432836
[2m[36m(func pid=22392)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=22392)[0m f1_macro: 0.24063471804099468
[2m[36m(func pid=22392)[0m f1_weighted: 0.31713051742545745
[2m[36m(func pid=22392)[0m f1_per_class: [0.164, 0.289, 0.095, 0.245, 0.142, 0.282, 0.448, 0.325, 0.224, 0.194]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 1.2734 | Steps: 4 | Val loss: 1.7610 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 0.3920 | Steps: 4 | Val loss: 2.4736 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=39840)[0m top1: 0.16417910447761194
[2m[36m(func pid=39840)[0m top5: 0.5960820895522388
[2m[36m(func pid=39840)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=39840)[0m f1_macro: 0.11689412292857652
[2m[36m(func pid=39840)[0m f1_weighted: 0.16970470175381994
[2m[36m(func pid=39840)[0m f1_per_class: [0.053, 0.207, 0.158, 0.229, 0.029, 0.278, 0.1, 0.101, 0.014, 0.0]
[2m[36m(func pid=39840)[0m 
== Status ==
Current time: 2024-01-07 12:07:03 (running for 00:09:09.03)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.273 |      0.324 |                   95 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.296 |      0.307 |                   93 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 17.459 |      0.241 |                   90 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.91  |      0.117 |                   16 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.36240671641791045
[2m[36m(func pid=21163)[0m top5: 0.8619402985074627
[2m[36m(func pid=21163)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=21163)[0m f1_macro: 0.32379645476394125
[2m[36m(func pid=21163)[0m f1_weighted: 0.3840653701223147
[2m[36m(func pid=21163)[0m f1_per_class: [0.5, 0.391, 0.273, 0.477, 0.105, 0.417, 0.317, 0.338, 0.208, 0.212]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 11.6284 | Steps: 4 | Val loss: 95.4568 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=21543)[0m top1: 0.3694029850746269
[2m[36m(func pid=21543)[0m top5: 0.8754664179104478
[2m[36m(func pid=21543)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=21543)[0m f1_macro: 0.3106609514134736
[2m[36m(func pid=21543)[0m f1_weighted: 0.3922865826615859
[2m[36m(func pid=21543)[0m f1_per_class: [0.416, 0.437, 0.157, 0.474, 0.133, 0.352, 0.351, 0.311, 0.273, 0.202]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m top1: 0.27611940298507465
[2m[36m(func pid=22392)[0m top5: 0.6888992537313433
[2m[36m(func pid=22392)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=22392)[0m f1_macro: 0.21345190331057523
[2m[36m(func pid=22392)[0m f1_weighted: 0.2942325094106315
[2m[36m(func pid=22392)[0m f1_per_class: [0.233, 0.162, 0.086, 0.206, 0.082, 0.28, 0.505, 0.205, 0.202, 0.175]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8885 | Steps: 4 | Val loss: 2.2696 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 1.1367 | Steps: 4 | Val loss: 1.7675 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 0.2231 | Steps: 4 | Val loss: 2.5328 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=39840)[0m top1: 0.1707089552238806
[2m[36m(func pid=39840)[0m top5: 0.6063432835820896
[2m[36m(func pid=39840)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=39840)[0m f1_macro: 0.11574087094173043
[2m[36m(func pid=39840)[0m f1_weighted: 0.1745552670049696
[2m[36m(func pid=39840)[0m f1_per_class: [0.057, 0.224, 0.136, 0.203, 0.018, 0.315, 0.125, 0.065, 0.014, 0.0]
[2m[36m(func pid=39840)[0m 
== Status ==
Current time: 2024-01-07 12:07:08 (running for 00:09:14.24)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.137 |      0.317 |                   96 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.392 |      0.311 |                   94 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.628 |      0.213 |                   91 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.888 |      0.116 |                   17 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.35494402985074625
[2m[36m(func pid=21163)[0m top5: 0.8642723880597015
[2m[36m(func pid=21163)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=21163)[0m f1_macro: 0.31700163527152664
[2m[36m(func pid=21163)[0m f1_weighted: 0.3777122772055174
[2m[36m(func pid=21163)[0m f1_per_class: [0.471, 0.381, 0.27, 0.463, 0.108, 0.419, 0.316, 0.342, 0.198, 0.201]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 22.8301 | Steps: 4 | Val loss: 105.3561 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=21543)[0m top1: 0.3614738805970149
[2m[36m(func pid=21543)[0m top5: 0.8703358208955224
[2m[36m(func pid=21543)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=21543)[0m f1_macro: 0.30582316872925885
[2m[36m(func pid=21543)[0m f1_weighted: 0.3801763240589042
[2m[36m(func pid=21543)[0m f1_per_class: [0.42, 0.448, 0.143, 0.448, 0.12, 0.349, 0.332, 0.294, 0.281, 0.223]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8981 | Steps: 4 | Val loss: 2.2718 | Batch size: 32 | lr: 0.0001 | Duration: 3.24s
[2m[36m(func pid=22392)[0m top1: 0.26725746268656714
[2m[36m(func pid=22392)[0m top5: 0.6711753731343284
[2m[36m(func pid=22392)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=22392)[0m f1_macro: 0.20769583895006377
[2m[36m(func pid=22392)[0m f1_weighted: 0.28342793262699867
[2m[36m(func pid=22392)[0m f1_per_class: [0.278, 0.093, 0.118, 0.215, 0.056, 0.292, 0.502, 0.159, 0.196, 0.167]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 1.1206 | Steps: 4 | Val loss: 1.7426 | Batch size: 32 | lr: 0.0001 | Duration: 3.24s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.2006 | Steps: 4 | Val loss: 2.5951 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 12:07:13 (running for 00:09:19.87)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.137 |      0.317 |                   96 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.223 |      0.306 |                   95 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 22.83  |      0.208 |                   92 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.898 |      0.113 |                   18 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3628731343283582
[2m[36m(func pid=21163)[0m top5: 0.8703358208955224
[2m[36m(func pid=21163)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=21163)[0m f1_macro: 0.3269906369961187
[2m[36m(func pid=21163)[0m f1_weighted: 0.38568476227863846
[2m[36m(func pid=21163)[0m f1_per_class: [0.52, 0.391, 0.27, 0.462, 0.11, 0.427, 0.331, 0.342, 0.199, 0.218]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=39840)[0m top1: 0.166044776119403
[2m[36m(func pid=39840)[0m top5: 0.6082089552238806
[2m[36m(func pid=39840)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=39840)[0m f1_macro: 0.11295673928643997
[2m[36m(func pid=39840)[0m f1_weighted: 0.17085969572833926
[2m[36m(func pid=39840)[0m f1_per_class: [0.07, 0.199, 0.13, 0.212, 0.012, 0.308, 0.119, 0.079, 0.0, 0.0]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=21543)[0m top1: 0.35027985074626866
[2m[36m(func pid=21543)[0m top5: 0.8642723880597015
[2m[36m(func pid=21543)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=21543)[0m f1_macro: 0.29599284880971555
[2m[36m(func pid=21543)[0m f1_weighted: 0.37283986531387175
[2m[36m(func pid=21543)[0m f1_per_class: [0.378, 0.429, 0.128, 0.446, 0.101, 0.346, 0.323, 0.3, 0.289, 0.22]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 6.9090 | Steps: 4 | Val loss: 115.8354 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 1.3831 | Steps: 4 | Val loss: 1.7716 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=22392)[0m top1: 0.25419776119402987
[2m[36m(func pid=22392)[0m top5: 0.6352611940298507
[2m[36m(func pid=22392)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=22392)[0m f1_macro: 0.20329484996039474
[2m[36m(func pid=22392)[0m f1_weighted: 0.258996046325812
[2m[36m(func pid=22392)[0m f1_per_class: [0.376, 0.055, 0.203, 0.167, 0.047, 0.296, 0.501, 0.045, 0.176, 0.167]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.8955 | Steps: 4 | Val loss: 2.2610 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 0.5354 | Steps: 4 | Val loss: 2.7703 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 12:07:19 (running for 00:09:25.22)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.383 |      0.306 |                   98 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.201 |      0.296 |                   96 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  6.909 |      0.203 |                   93 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.898 |      0.113 |                   18 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.34421641791044777
[2m[36m(func pid=21163)[0m top5: 0.8591417910447762
[2m[36m(func pid=21163)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=21163)[0m f1_macro: 0.3056528355897437
[2m[36m(func pid=21163)[0m f1_weighted: 0.37041884499281263
[2m[36m(func pid=21163)[0m f1_per_class: [0.446, 0.374, 0.197, 0.423, 0.098, 0.427, 0.336, 0.312, 0.221, 0.222]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.324160447761194
[2m[36m(func pid=21543)[0m top5: 0.8451492537313433
[2m[36m(func pid=21543)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=21543)[0m f1_macro: 0.2780451032785155
[2m[36m(func pid=21543)[0m f1_weighted: 0.34245644003674286
[2m[36m(func pid=21543)[0m f1_per_class: [0.333, 0.422, 0.115, 0.413, 0.103, 0.345, 0.262, 0.295, 0.278, 0.214]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=39840)[0m top1: 0.18050373134328357
[2m[36m(func pid=39840)[0m top5: 0.6357276119402985
[2m[36m(func pid=39840)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=39840)[0m f1_macro: 0.1265281785294345
[2m[36m(func pid=39840)[0m f1_weighted: 0.18820077317288078
[2m[36m(func pid=39840)[0m f1_per_class: [0.097, 0.224, 0.136, 0.237, 0.027, 0.312, 0.133, 0.086, 0.014, 0.0]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 30.6742 | Steps: 4 | Val loss: 128.5514 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 1.2799 | Steps: 4 | Val loss: 1.8216 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=22392)[0m top1: 0.23180970149253732
[2m[36m(func pid=22392)[0m top5: 0.6086753731343284
[2m[36m(func pid=22392)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=22392)[0m f1_macro: 0.20747848058939206
[2m[36m(func pid=22392)[0m f1_weighted: 0.23397926326713997
[2m[36m(func pid=22392)[0m f1_per_class: [0.423, 0.026, 0.28, 0.116, 0.048, 0.324, 0.466, 0.045, 0.171, 0.176]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 1.1256 | Steps: 4 | Val loss: 2.8009 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.8657 | Steps: 4 | Val loss: 2.2739 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 12:07:24 (running for 00:09:30.49)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=1
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 4 RUNNING, 1 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | RUNNING    | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.28  |      0.296 |                   99 |
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.535 |      0.278 |                   97 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 30.674 |      0.207 |                   94 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.896 |      0.127 |                   19 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3306902985074627
[2m[36m(func pid=21163)[0m top5: 0.8498134328358209
[2m[36m(func pid=21163)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=21163)[0m f1_macro: 0.29572741101869326
[2m[36m(func pid=21163)[0m f1_weighted: 0.3578093972219423
[2m[36m(func pid=21163)[0m f1_per_class: [0.424, 0.382, 0.176, 0.403, 0.088, 0.415, 0.314, 0.32, 0.213, 0.221]
[2m[36m(func pid=21163)[0m 
[2m[36m(func pid=21543)[0m top1: 0.3362873134328358
[2m[36m(func pid=21543)[0m top5: 0.8442164179104478
[2m[36m(func pid=21543)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=21543)[0m f1_macro: 0.2862692753590284
[2m[36m(func pid=21543)[0m f1_weighted: 0.3477162566067997
[2m[36m(func pid=21543)[0m f1_per_class: [0.317, 0.429, 0.141, 0.452, 0.108, 0.344, 0.235, 0.315, 0.27, 0.25]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=39840)[0m top1: 0.17490671641791045
[2m[36m(func pid=39840)[0m top5: 0.6058768656716418
[2m[36m(func pid=39840)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=39840)[0m f1_macro: 0.12290394249152434
[2m[36m(func pid=39840)[0m f1_weighted: 0.18155122825842318
[2m[36m(func pid=39840)[0m f1_per_class: [0.086, 0.241, 0.107, 0.2, 0.03, 0.323, 0.13, 0.099, 0.013, 0.0]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 26.3303 | Steps: 4 | Val loss: 143.1897 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=21163)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 1.3277 | Steps: 4 | Val loss: 1.8311 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=22392)[0m top1: 0.20055970149253732
[2m[36m(func pid=22392)[0m top5: 0.59375
[2m[36m(func pid=22392)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=22392)[0m f1_macro: 0.21208682922827019
[2m[36m(func pid=22392)[0m f1_weighted: 0.20834695181385293
[2m[36m(func pid=22392)[0m f1_per_class: [0.427, 0.021, 0.4, 0.078, 0.041, 0.356, 0.401, 0.075, 0.145, 0.177]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 0.7111 | Steps: 4 | Val loss: 2.7094 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.9053 | Steps: 4 | Val loss: 2.2725 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 12:07:29 (running for 00:09:35.70)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.3305
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (19 PENDING, 3 RUNNING, 2 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00001 | RUNNING    | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  1.126 |      0.286 |                   98 |
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 26.33  |      0.212 |                   95 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.866 |      0.123 |                   20 |
| train_9b9e8_00005 | PENDING    |                    | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=21163)[0m top1: 0.3292910447761194
[2m[36m(func pid=21163)[0m top5: 0.8507462686567164
[2m[36m(func pid=21163)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=21163)[0m f1_macro: 0.29738578202718957
[2m[36m(func pid=21163)[0m f1_weighted: 0.3518496768942987
[2m[36m(func pid=21163)[0m f1_per_class: [0.422, 0.397, 0.179, 0.379, 0.092, 0.413, 0.303, 0.336, 0.234, 0.218]
[2m[36m(func pid=21543)[0m top1: 0.3474813432835821
[2m[36m(func pid=21543)[0m top5: 0.8568097014925373
[2m[36m(func pid=21543)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=21543)[0m f1_macro: 0.2978067743172675
[2m[36m(func pid=21543)[0m f1_weighted: 0.36160915612336936
[2m[36m(func pid=21543)[0m f1_per_class: [0.3, 0.426, 0.149, 0.44, 0.137, 0.353, 0.287, 0.331, 0.283, 0.273]
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 38.8628 | Steps: 4 | Val loss: 148.2158 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=39840)[0m top1: 0.15858208955223882
[2m[36m(func pid=39840)[0m top5: 0.6161380597014925
[2m[36m(func pid=39840)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=39840)[0m f1_macro: 0.11380205709345459
[2m[36m(func pid=39840)[0m f1_weighted: 0.16850191767474876
[2m[36m(func pid=39840)[0m f1_per_class: [0.053, 0.202, 0.1, 0.195, 0.026, 0.312, 0.116, 0.108, 0.026, 0.0]
[2m[36m(func pid=21543)[0m 
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=22392)[0m top1: 0.1767723880597015
[2m[36m(func pid=22392)[0m top5: 0.5942164179104478
[2m[36m(func pid=22392)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=22392)[0m f1_macro: 0.2130586448240758
[2m[36m(func pid=22392)[0m f1_weighted: 0.18502890622767448
[2m[36m(func pid=22392)[0m f1_per_class: [0.447, 0.026, 0.429, 0.069, 0.047, 0.343, 0.32, 0.14, 0.13, 0.18]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=21543)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.4222 | Steps: 4 | Val loss: 2.7531 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.8789 | Steps: 4 | Val loss: 2.2793 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=21543)[0m top1: 0.34421641791044777
[2m[36m(func pid=21543)[0m top5: 0.8451492537313433
[2m[36m(func pid=21543)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=21543)[0m f1_macro: 0.29335676719126214
[2m[36m(func pid=21543)[0m f1_weighted: 0.3599112824470726
[2m[36m(func pid=21543)[0m f1_per_class: [0.296, 0.432, 0.158, 0.446, 0.12, 0.347, 0.278, 0.327, 0.258, 0.272]
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 36.8842 | Steps: 4 | Val loss: 135.9969 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=39840)[0m top1: 0.1571828358208955
[2m[36m(func pid=39840)[0m top5: 0.601679104477612
[2m[36m(func pid=39840)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=39840)[0m f1_macro: 0.11342787181471026
[2m[36m(func pid=39840)[0m f1_weighted: 0.1708634590781314
[2m[36m(func pid=39840)[0m f1_per_class: [0.062, 0.208, 0.082, 0.204, 0.025, 0.314, 0.113, 0.101, 0.025, 0.0]
[2m[36m(func pid=22392)[0m top1: 0.18470149253731344
[2m[36m(func pid=22392)[0m top5: 0.617070895522388
[2m[36m(func pid=22392)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=22392)[0m f1_macro: 0.2292342583823901
[2m[36m(func pid=22392)[0m f1_weighted: 0.2015688461497228
[2m[36m(func pid=22392)[0m f1_per_class: [0.447, 0.069, 0.462, 0.122, 0.059, 0.352, 0.287, 0.195, 0.12, 0.179]
== Status ==
Current time: 2024-01-07 12:07:36 (running for 00:09:41.99)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.3305
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (18 PENDING, 3 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 38.863 |      0.213 |                   96 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.905 |      0.114 |                   21 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | PENDING    |                    | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=45540)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=45540)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=45540)[0m Configuration completed!
[2m[36m(func pid=45540)[0m New optimizer parameters:
[2m[36m(func pid=45540)[0m SGD (
[2m[36m(func pid=45540)[0m Parameter Group 0
[2m[36m(func pid=45540)[0m     dampening: 0
[2m[36m(func pid=45540)[0m     differentiable: False
[2m[36m(func pid=45540)[0m     foreach: None
[2m[36m(func pid=45540)[0m     lr: 0.001
[2m[36m(func pid=45540)[0m     maximize: False
[2m[36m(func pid=45540)[0m     momentum: 0.9
[2m[36m(func pid=45540)[0m     nesterov: False
[2m[36m(func pid=45540)[0m     weight_decay: 0
[2m[36m(func pid=45540)[0m )
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 29.8050 | Steps: 4 | Val loss: 115.0468 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.8642 | Steps: 4 | Val loss: 2.2707 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9403 | Steps: 4 | Val loss: 2.3214 | Batch size: 32 | lr: 0.001 | Duration: 4.74s
[2m[36m(func pid=39840)[0m top1: 0.16651119402985073
[2m[36m(func pid=39840)[0m top5: 0.6110074626865671
[2m[36m(func pid=39840)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=39840)[0m f1_macro: 0.12127978925034592
[2m[36m(func pid=39840)[0m f1_weighted: 0.17588473385191927
[2m[36m(func pid=39840)[0m f1_per_class: [0.087, 0.215, 0.104, 0.238, 0.027, 0.303, 0.091, 0.136, 0.013, 0.0]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=22392)[0m top1: 0.2234141791044776
[2m[36m(func pid=22392)[0m top5: 0.6665111940298507
[2m[36m(func pid=22392)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=22392)[0m f1_macro: 0.2671726165379613
[2m[36m(func pid=22392)[0m f1_weighted: 0.24686442460046376
[2m[36m(func pid=22392)[0m f1_per_class: [0.469, 0.098, 0.522, 0.225, 0.069, 0.354, 0.296, 0.321, 0.145, 0.173]
[2m[36m(func pid=45540)[0m top1: 0.18003731343283583
[2m[36m(func pid=45540)[0m top5: 0.534981343283582
[2m[36m(func pid=45540)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=45540)[0m f1_macro: 0.1124076278779674
[2m[36m(func pid=45540)[0m f1_weighted: 0.12788580657336018
[2m[36m(func pid=45540)[0m f1_per_class: [0.308, 0.323, 0.0, 0.113, 0.01, 0.265, 0.006, 0.024, 0.0, 0.075]
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.9118 | Steps: 4 | Val loss: 2.2703 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 12:07:44 (running for 00:09:50.42)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 36.884 |      0.229 |                   97 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.864 |      0.121 |                   23 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |        |            |                      |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=46161)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=46161)[0m Configuration completed!
[2m[36m(func pid=46161)[0m New optimizer parameters:
[2m[36m(func pid=46161)[0m SGD (
[2m[36m(func pid=46161)[0m Parameter Group 0
[2m[36m(func pid=46161)[0m     dampening: 0
[2m[36m(func pid=46161)[0m     differentiable: False
[2m[36m(func pid=46161)[0m     foreach: None
[2m[36m(func pid=46161)[0m     lr: 0.01
[2m[36m(func pid=46161)[0m     maximize: False
[2m[36m(func pid=46161)[0m     momentum: 0.9
[2m[36m(func pid=46161)[0m     nesterov: False
[2m[36m(func pid=46161)[0m     weight_decay: 0
[2m[36m(func pid=46161)[0m )
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=39840)[0m top1: 0.1707089552238806
[2m[36m(func pid=39840)[0m top5: 0.6161380597014925
[2m[36m(func pid=39840)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=39840)[0m f1_macro: 0.12286411531696799
[2m[36m(func pid=39840)[0m f1_weighted: 0.18376876585403135
[2m[36m(func pid=39840)[0m f1_per_class: [0.094, 0.198, 0.098, 0.244, 0.028, 0.313, 0.12, 0.122, 0.013, 0.0]
== Status ==
Current time: 2024-01-07 12:07:49 (running for 00:09:55.86)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 29.805 |      0.267 |                   98 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.912 |      0.123 |                   24 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.94  |      0.112 |                    1 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |        |            |                      |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 6.6982 | Steps: 4 | Val loss: 95.4709 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9454 | Steps: 4 | Val loss: 2.3279 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0147 | Steps: 4 | Val loss: 2.2558 | Batch size: 32 | lr: 0.01 | Duration: 4.57s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.8391 | Steps: 4 | Val loss: 2.2757 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=22392)[0m top1: 0.2719216417910448
[2m[36m(func pid=22392)[0m top5: 0.7182835820895522
[2m[36m(func pid=22392)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=22392)[0m f1_macro: 0.29044078603358015
[2m[36m(func pid=22392)[0m f1_weighted: 0.2892792702496453
[2m[36m(func pid=22392)[0m f1_per_class: [0.495, 0.183, 0.49, 0.25, 0.087, 0.375, 0.352, 0.312, 0.19, 0.17]
[2m[36m(func pid=22392)[0m 
[2m[36m(func pid=45540)[0m top1: 0.17397388059701493
[2m[36m(func pid=45540)[0m top5: 0.5153917910447762
[2m[36m(func pid=45540)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=45540)[0m f1_macro: 0.11043197172899655
[2m[36m(func pid=45540)[0m f1_weighted: 0.12750260310898376
[2m[36m(func pid=45540)[0m f1_per_class: [0.286, 0.286, 0.0, 0.125, 0.023, 0.298, 0.006, 0.01, 0.0, 0.069]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.20055970149253732
[2m[36m(func pid=46161)[0m top5: 0.6058768656716418
[2m[36m(func pid=46161)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=46161)[0m f1_macro: 0.13790442259045674
[2m[36m(func pid=46161)[0m f1_weighted: 0.17106390949460784
[2m[36m(func pid=46161)[0m f1_per_class: [0.297, 0.333, 0.0, 0.212, 0.014, 0.19, 0.061, 0.117, 0.0, 0.156]
[2m[36m(func pid=46161)[0m 
== Status ==
Current time: 2024-01-07 12:07:55 (running for 00:10:01.37)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00003 | RUNNING    | 192.168.7.53:22392 | 0.1    |       0.99 |         0      |  6.698 |      0.29  |                   99 |
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.839 |      0.122 |                   25 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.945 |      0.11  |                    2 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  3.015 |      0.138 |                    1 |
| train_9b9e8_00007 | PENDING    |                    | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=39840)[0m top1: 0.16651119402985073
[2m[36m(func pid=39840)[0m top5: 0.6077425373134329
[2m[36m(func pid=39840)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=39840)[0m f1_macro: 0.12216019659695314
[2m[36m(func pid=39840)[0m f1_weighted: 0.17967927003312442
[2m[36m(func pid=39840)[0m f1_per_class: [0.107, 0.187, 0.075, 0.244, 0.03, 0.305, 0.111, 0.139, 0.011, 0.013]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=22392)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 11.9584 | Steps: 4 | Val loss: 86.3547 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9248 | Steps: 4 | Val loss: 2.2993 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7888 | Steps: 4 | Val loss: 2.1423 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.8852 | Steps: 4 | Val loss: 2.2655 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=22392)[0m top1: 0.2943097014925373
[2m[36m(func pid=22392)[0m top5: 0.7406716417910447
[2m[36m(func pid=22392)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=22392)[0m f1_macro: 0.29515725229753736
[2m[36m(func pid=22392)[0m f1_weighted: 0.3087656073162609
[2m[36m(func pid=22392)[0m f1_per_class: [0.427, 0.279, 0.456, 0.228, 0.068, 0.362, 0.386, 0.331, 0.213, 0.201]
[2m[36m(func pid=45540)[0m top1: 0.1730410447761194
[2m[36m(func pid=45540)[0m top5: 0.5527052238805971
[2m[36m(func pid=45540)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=45540)[0m f1_macro: 0.10726166640838777
[2m[36m(func pid=45540)[0m f1_weighted: 0.1469218442343246
[2m[36m(func pid=45540)[0m f1_per_class: [0.141, 0.252, 0.093, 0.153, 0.013, 0.32, 0.063, 0.038, 0.0, 0.0]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.26632462686567165
[2m[36m(func pid=46161)[0m top5: 0.7280783582089553
[2m[36m(func pid=46161)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=46161)[0m f1_macro: 0.178921275960155
[2m[36m(func pid=46161)[0m f1_weighted: 0.22700925117374093
[2m[36m(func pid=46161)[0m f1_per_class: [0.435, 0.058, 0.128, 0.414, 0.068, 0.175, 0.217, 0.059, 0.027, 0.208]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=39840)[0m top1: 0.1646455223880597
[2m[36m(func pid=39840)[0m top5: 0.6305970149253731
[2m[36m(func pid=39840)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=39840)[0m f1_macro: 0.11856329406538675
[2m[36m(func pid=39840)[0m f1_weighted: 0.17928181596647855
[2m[36m(func pid=39840)[0m f1_per_class: [0.086, 0.187, 0.071, 0.232, 0.028, 0.312, 0.125, 0.107, 0.022, 0.014]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8522 | Steps: 4 | Val loss: 2.2559 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.5381 | Steps: 4 | Val loss: 2.2027 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=45540)[0m top1: 0.1884328358208955
[2m[36m(func pid=45540)[0m top5: 0.6184701492537313
[2m[36m(func pid=45540)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=45540)[0m f1_macro: 0.12604791384771938
[2m[36m(func pid=45540)[0m f1_weighted: 0.17984253697072128
[2m[36m(func pid=45540)[0m f1_per_class: [0.155, 0.234, 0.13, 0.202, 0.019, 0.335, 0.126, 0.06, 0.0, 0.0]
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.8168 | Steps: 4 | Val loss: 2.2630 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=46161)[0m top1: 0.18050373134328357
[2m[36m(func pid=46161)[0m top5: 0.6721082089552238
[2m[36m(func pid=46161)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=46161)[0m f1_macro: 0.17100350759506616
[2m[36m(func pid=46161)[0m f1_weighted: 0.19895527337253774
[2m[36m(func pid=46161)[0m f1_per_class: [0.168, 0.0, 0.032, 0.318, 0.066, 0.389, 0.15, 0.194, 0.06, 0.333]
[2m[36m(func pid=39840)[0m top1: 0.16930970149253732
[2m[36m(func pid=39840)[0m top5: 0.6315298507462687
[2m[36m(func pid=39840)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=39840)[0m f1_macro: 0.12149158001351057
[2m[36m(func pid=39840)[0m f1_weighted: 0.18688050256889183
[2m[36m(func pid=39840)[0m f1_per_class: [0.099, 0.179, 0.083, 0.249, 0.018, 0.303, 0.139, 0.122, 0.022, 0.0]
== Status ==
Current time: 2024-01-07 12:08:01 (running for 00:10:07.05)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.885 |      0.119 |                   26 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.925 |      0.107 |                    3 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  2.789 |      0.179 |                    2 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=47180)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=47180)[0m Configuration completed!
[2m[36m(func pid=47180)[0m New optimizer parameters:
[2m[36m(func pid=47180)[0m SGD (
[2m[36m(func pid=47180)[0m Parameter Group 0
[2m[36m(func pid=47180)[0m     dampening: 0
[2m[36m(func pid=47180)[0m     differentiable: False
[2m[36m(func pid=47180)[0m     foreach: None
[2m[36m(func pid=47180)[0m     lr: 0.1
[2m[36m(func pid=47180)[0m     maximize: False
[2m[36m(func pid=47180)[0m     momentum: 0.9
[2m[36m(func pid=47180)[0m     nesterov: False
[2m[36m(func pid=47180)[0m     weight_decay: 0
[2m[36m(func pid=47180)[0m )
[2m[36m(func pid=47180)[0m 
== Status ==
Current time: 2024-01-07 12:08:07 (running for 00:10:13.14)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.885 |      0.119 |                   26 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.925 |      0.107 |                    3 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  2.538 |      0.171 |                    3 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.3916 | Steps: 4 | Val loss: 2.1250 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.7711 | Steps: 4 | Val loss: 2.2673 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7988 | Steps: 4 | Val loss: 2.2191 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9489 | Steps: 4 | Val loss: 3.0569 | Batch size: 32 | lr: 0.1 | Duration: 4.75s
== Status ==
Current time: 2024-01-07 12:08:12 (running for 00:10:18.16)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.817 |      0.121 |                   27 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.852 |      0.126 |                    4 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  2.538 |      0.171 |                    3 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |        |            |                      |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.21641791044776118
[2m[36m(func pid=46161)[0m top5: 0.7220149253731343
[2m[36m(func pid=46161)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=46161)[0m f1_macro: 0.21686342047768856
[2m[36m(func pid=46161)[0m f1_weighted: 0.2107665753032999
[2m[36m(func pid=46161)[0m f1_per_class: [0.122, 0.041, 0.124, 0.403, 0.074, 0.386, 0.041, 0.329, 0.214, 0.435]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m top1: 0.2196828358208955
[2m[36m(func pid=45540)[0m top5: 0.6730410447761194
[2m[36m(func pid=45540)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=45540)[0m f1_macro: 0.15526098728477872
[2m[36m(func pid=45540)[0m f1_weighted: 0.22049907213703965
[2m[36m(func pid=45540)[0m f1_per_class: [0.209, 0.269, 0.163, 0.252, 0.029, 0.336, 0.185, 0.075, 0.0, 0.034]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m top1: 0.17024253731343283
[2m[36m(func pid=39840)[0m top5: 0.6240671641791045
[2m[36m(func pid=39840)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=39840)[0m f1_macro: 0.12182900214823869
[2m[36m(func pid=39840)[0m f1_weighted: 0.18199771731209297
[2m[36m(func pid=39840)[0m f1_per_class: [0.096, 0.187, 0.071, 0.234, 0.03, 0.326, 0.125, 0.112, 0.024, 0.013]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.03917910447761194
[2m[36m(func pid=47180)[0m top5: 0.37966417910447764
[2m[36m(func pid=47180)[0m f1_micro: 0.03917910447761194
[2m[36m(func pid=47180)[0m f1_macro: 0.04934160807253008
[2m[36m(func pid=47180)[0m f1_weighted: 0.01470710206712332
[2m[36m(func pid=47180)[0m f1_per_class: [0.087, 0.0, 0.047, 0.0, 0.034, 0.0, 0.0, 0.131, 0.118, 0.077]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.9142 | Steps: 4 | Val loss: 1.8768 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7261 | Steps: 4 | Val loss: 2.1753 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.8251 | Steps: 4 | Val loss: 2.2620 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.6661 | Steps: 4 | Val loss: 1.8623 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=46161)[0m top1: 0.35494402985074625
[2m[36m(func pid=46161)[0m top5: 0.7961753731343284
[2m[36m(func pid=46161)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=46161)[0m f1_macro: 0.32892064941419624
[2m[36m(func pid=46161)[0m f1_weighted: 0.2992752126184472
[2m[36m(func pid=46161)[0m f1_per_class: [0.537, 0.223, 0.558, 0.568, 0.074, 0.374, 0.05, 0.346, 0.15, 0.409]
== Status ==
Current time: 2024-01-07 12:08:17 (running for 00:10:23.61)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.771 |      0.122 |                   28 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.799 |      0.155 |                    5 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.914 |      0.329 |                    5 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.949 |      0.049 |                    1 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m top1: 0.2621268656716418
[2m[36m(func pid=45540)[0m top5: 0.726679104477612
[2m[36m(func pid=45540)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=45540)[0m f1_macro: 0.18711322159641972
[2m[36m(func pid=45540)[0m f1_weighted: 0.26175462589750015
[2m[36m(func pid=45540)[0m f1_per_class: [0.226, 0.277, 0.273, 0.33, 0.03, 0.385, 0.227, 0.056, 0.0, 0.069]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m top1: 0.177705223880597
[2m[36m(func pid=39840)[0m top5: 0.632929104477612
[2m[36m(func pid=39840)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=39840)[0m f1_macro: 0.12708826784029034
[2m[36m(func pid=39840)[0m f1_weighted: 0.19520225169838437
[2m[36m(func pid=39840)[0m f1_per_class: [0.124, 0.192, 0.068, 0.247, 0.013, 0.334, 0.151, 0.104, 0.026, 0.011]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.34281716417910446
[2m[36m(func pid=47180)[0m top5: 0.8530783582089553
[2m[36m(func pid=47180)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=47180)[0m f1_macro: 0.29203432768603743
[2m[36m(func pid=47180)[0m f1_weighted: 0.2931191992516692
[2m[36m(func pid=47180)[0m f1_per_class: [0.426, 0.444, 0.233, 0.495, 0.197, 0.343, 0.006, 0.354, 0.028, 0.394]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.9182 | Steps: 4 | Val loss: 1.7843 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7319 | Steps: 4 | Val loss: 2.1327 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.8395 | Steps: 4 | Val loss: 2.2624 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9817 | Steps: 4 | Val loss: 2.2583 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 12:08:22 (running for 00:10:28.71)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.825 |      0.127 |                   29 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.726 |      0.187 |                    6 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.918 |      0.377 |                    6 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.666 |      0.292 |                    2 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.38386194029850745
[2m[36m(func pid=46161)[0m top5: 0.8255597014925373
[2m[36m(func pid=46161)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=46161)[0m f1_macro: 0.3771355219866116
[2m[36m(func pid=46161)[0m f1_weighted: 0.32266183156184464
[2m[36m(func pid=46161)[0m f1_per_class: [0.659, 0.361, 0.774, 0.552, 0.104, 0.391, 0.042, 0.35, 0.18, 0.357]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m top1: 0.2966417910447761
[2m[36m(func pid=45540)[0m top5: 0.7658582089552238
[2m[36m(func pid=45540)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=45540)[0m f1_macro: 0.2055650932655236
[2m[36m(func pid=45540)[0m f1_weighted: 0.3025958400359419
[2m[36m(func pid=45540)[0m f1_per_class: [0.214, 0.262, 0.296, 0.382, 0.039, 0.378, 0.326, 0.051, 0.0, 0.107]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m top1: 0.17350746268656717
[2m[36m(func pid=39840)[0m top5: 0.6296641791044776
[2m[36m(func pid=39840)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=39840)[0m f1_macro: 0.12298258101584636
[2m[36m(func pid=39840)[0m f1_weighted: 0.19225065207111888
[2m[36m(func pid=39840)[0m f1_per_class: [0.095, 0.195, 0.059, 0.258, 0.02, 0.335, 0.133, 0.089, 0.025, 0.02]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.35447761194029853
[2m[36m(func pid=47180)[0m top5: 0.8666044776119403
[2m[36m(func pid=47180)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=47180)[0m f1_macro: 0.2752969408428098
[2m[36m(func pid=47180)[0m f1_weighted: 0.302792670609487
[2m[36m(func pid=47180)[0m f1_per_class: [0.5, 0.046, 0.407, 0.618, 0.154, 0.318, 0.171, 0.273, 0.109, 0.157]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.6749 | Steps: 4 | Val loss: 1.7866 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6879 | Steps: 4 | Val loss: 2.1163 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.8262 | Steps: 4 | Val loss: 2.2605 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.6986 | Steps: 4 | Val loss: 4.2750 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=46161)[0m top1: 0.3204291044776119
[2m[36m(func pid=46161)[0m top5: 0.8791977611940298
[2m[36m(func pid=46161)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=46161)[0m f1_macro: 0.2992129298614851
[2m[36m(func pid=46161)[0m f1_weighted: 0.35876923853203213
[2m[36m(func pid=46161)[0m f1_per_class: [0.43, 0.315, 0.429, 0.436, 0.142, 0.334, 0.38, 0.217, 0.126, 0.183]
[2m[36m(func pid=46161)[0m 
== Status ==
Current time: 2024-01-07 12:08:28 (running for 00:10:34.24)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.84  |      0.123 |                   30 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.732 |      0.206 |                    7 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.675 |      0.299 |                    7 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.982 |      0.275 |                    3 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=45540)[0m top1: 0.3050373134328358
[2m[36m(func pid=45540)[0m top5: 0.7723880597014925
[2m[36m(func pid=45540)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=45540)[0m f1_macro: 0.2154646971849079
[2m[36m(func pid=45540)[0m f1_weighted: 0.30681386136137456
[2m[36m(func pid=45540)[0m f1_per_class: [0.251, 0.201, 0.295, 0.416, 0.048, 0.402, 0.329, 0.053, 0.0, 0.16]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m top1: 0.16324626865671643
[2m[36m(func pid=39840)[0m top5: 0.6399253731343284
[2m[36m(func pid=39840)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=39840)[0m f1_macro: 0.11672105103816863
[2m[36m(func pid=39840)[0m f1_weighted: 0.18411401202448774
[2m[36m(func pid=39840)[0m f1_per_class: [0.098, 0.179, 0.058, 0.252, 0.016, 0.326, 0.127, 0.075, 0.025, 0.01]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.21175373134328357
[2m[36m(func pid=47180)[0m top5: 0.6926305970149254
[2m[36m(func pid=47180)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=47180)[0m f1_macro: 0.22585179951442153
[2m[36m(func pid=47180)[0m f1_weighted: 0.20093597743258831
[2m[36m(func pid=47180)[0m f1_per_class: [0.333, 0.429, 0.041, 0.151, 0.097, 0.337, 0.021, 0.395, 0.185, 0.267]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.3719 | Steps: 4 | Val loss: 1.8576 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6243 | Steps: 4 | Val loss: 2.0972 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.7889 | Steps: 4 | Val loss: 2.2594 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.2906 | Steps: 4 | Val loss: 4.6388 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 12:08:33 (running for 00:10:39.40)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.826 |      0.117 |                   31 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.688 |      0.215 |                    8 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.372 |      0.25  |                    8 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.699 |      0.226 |                    4 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3162313432835821
[2m[36m(func pid=46161)[0m top5: 0.847481343283582
[2m[36m(func pid=46161)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=46161)[0m f1_macro: 0.25005712183631645
[2m[36m(func pid=46161)[0m f1_weighted: 0.32263740476344427
[2m[36m(func pid=46161)[0m f1_per_class: [0.252, 0.329, 0.3, 0.143, 0.146, 0.353, 0.555, 0.09, 0.148, 0.185]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m top1: 0.30783582089552236
[2m[36m(func pid=45540)[0m top5: 0.784981343283582
[2m[36m(func pid=45540)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=45540)[0m f1_macro: 0.23221131679080803
[2m[36m(func pid=45540)[0m f1_weighted: 0.31875498014892667
[2m[36m(func pid=45540)[0m f1_per_class: [0.229, 0.194, 0.373, 0.41, 0.056, 0.402, 0.372, 0.067, 0.025, 0.195]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m top1: 0.166044776119403
[2m[36m(func pid=39840)[0m top5: 0.636660447761194
[2m[36m(func pid=39840)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=39840)[0m f1_macro: 0.12137660175158735
[2m[36m(func pid=39840)[0m f1_weighted: 0.18378114457618863
[2m[36m(func pid=39840)[0m f1_per_class: [0.122, 0.176, 0.049, 0.248, 0.021, 0.334, 0.124, 0.091, 0.028, 0.022]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.22061567164179105
[2m[36m(func pid=47180)[0m top5: 0.7332089552238806
[2m[36m(func pid=47180)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=47180)[0m f1_macro: 0.2220848290919645
[2m[36m(func pid=47180)[0m f1_weighted: 0.2559346185870157
[2m[36m(func pid=47180)[0m f1_per_class: [0.155, 0.308, 0.055, 0.145, 0.072, 0.34, 0.315, 0.258, 0.189, 0.383]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.5374 | Steps: 4 | Val loss: 1.7885 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.5260 | Steps: 4 | Val loss: 2.0745 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.7634 | Steps: 4 | Val loss: 2.2631 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 12:08:38 (running for 00:10:44.60)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.789 |      0.121 |                   32 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.624 |      0.232 |                    9 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.537 |      0.267 |                    9 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.291 |      0.222 |                    5 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3666044776119403
[2m[36m(func pid=46161)[0m top5: 0.8605410447761194
[2m[36m(func pid=46161)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=46161)[0m f1_macro: 0.26712579220814286
[2m[36m(func pid=46161)[0m f1_weighted: 0.38703435261781394
[2m[36m(func pid=46161)[0m f1_per_class: [0.317, 0.252, 0.171, 0.365, 0.11, 0.396, 0.578, 0.128, 0.203, 0.151]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.8366 | Steps: 4 | Val loss: 3.5462 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=45540)[0m top1: 0.31669776119402987
[2m[36m(func pid=45540)[0m top5: 0.800839552238806
[2m[36m(func pid=45540)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=45540)[0m f1_macro: 0.23477801188394137
[2m[36m(func pid=45540)[0m f1_weighted: 0.32899260543701797
[2m[36m(func pid=45540)[0m f1_per_class: [0.245, 0.163, 0.367, 0.442, 0.052, 0.39, 0.399, 0.056, 0.026, 0.208]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m top1: 0.16977611940298507
[2m[36m(func pid=39840)[0m top5: 0.6301305970149254
[2m[36m(func pid=39840)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=39840)[0m f1_macro: 0.1309465588197984
[2m[36m(func pid=39840)[0m f1_weighted: 0.18879606479673908
[2m[36m(func pid=39840)[0m f1_per_class: [0.15, 0.16, 0.052, 0.258, 0.031, 0.334, 0.131, 0.123, 0.025, 0.045]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.3885261194029851
[2m[36m(func pid=47180)[0m top5: 0.8157649253731343
[2m[36m(func pid=47180)[0m f1_micro: 0.3885261194029851
[2m[36m(func pid=47180)[0m f1_macro: 0.30492903536178145
[2m[36m(func pid=47180)[0m f1_weighted: 0.3790822862452254
[2m[36m(func pid=47180)[0m f1_per_class: [0.471, 0.047, 0.222, 0.56, 0.185, 0.36, 0.442, 0.378, 0.149, 0.234]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.2637 | Steps: 4 | Val loss: 1.8837 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.5303 | Steps: 4 | Val loss: 2.0448 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.7512 | Steps: 4 | Val loss: 2.2643 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=46161)[0m top1: 0.32322761194029853
[2m[36m(func pid=46161)[0m top5: 0.8526119402985075
[2m[36m(func pid=46161)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=46161)[0m f1_macro: 0.2833831086157489
[2m[36m(func pid=46161)[0m f1_weighted: 0.3547200235789231
[2m[36m(func pid=46161)[0m f1_per_class: [0.512, 0.169, 0.127, 0.462, 0.085, 0.413, 0.369, 0.311, 0.24, 0.145]
== Status ==
Current time: 2024-01-07 12:08:43 (running for 00:10:49.89)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.763 |      0.131 |                   33 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.526 |      0.235 |                   10 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.264 |      0.283 |                   10 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.837 |      0.305 |                    6 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.0156 | Steps: 4 | Val loss: 3.6405 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=45540)[0m top1: 0.341884328358209
[2m[36m(func pid=45540)[0m top5: 0.8180970149253731
[2m[36m(func pid=45540)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=45540)[0m f1_macro: 0.24045192125648135
[2m[36m(func pid=45540)[0m f1_weighted: 0.345243472268698
[2m[36m(func pid=45540)[0m f1_per_class: [0.299, 0.116, 0.367, 0.49, 0.048, 0.365, 0.445, 0.042, 0.026, 0.207]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m top1: 0.16884328358208955
[2m[36m(func pid=39840)[0m top5: 0.6333955223880597
[2m[36m(func pid=39840)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=39840)[0m f1_macro: 0.13385501499356509
[2m[36m(func pid=39840)[0m f1_weighted: 0.18644303123167869
[2m[36m(func pid=39840)[0m f1_per_class: [0.153, 0.146, 0.075, 0.255, 0.027, 0.335, 0.128, 0.149, 0.026, 0.045]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.32276119402985076
[2m[36m(func pid=47180)[0m top5: 0.8395522388059702
[2m[36m(func pid=47180)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=47180)[0m f1_macro: 0.3486242260847551
[2m[36m(func pid=47180)[0m f1_weighted: 0.3414057778994426
[2m[36m(func pid=47180)[0m f1_per_class: [0.6, 0.228, 0.786, 0.504, 0.222, 0.272, 0.298, 0.279, 0.142, 0.154]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.4925 | Steps: 4 | Val loss: 1.9873 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5040 | Steps: 4 | Val loss: 2.0304 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.7875 | Steps: 4 | Val loss: 2.2588 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=46161)[0m top1: 0.2966417910447761
[2m[36m(func pid=46161)[0m top5: 0.8278917910447762
[2m[36m(func pid=46161)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=46161)[0m f1_macro: 0.2961783196437846
[2m[36m(func pid=46161)[0m f1_weighted: 0.316599317605385
[2m[36m(func pid=46161)[0m f1_per_class: [0.612, 0.316, 0.145, 0.429, 0.062, 0.388, 0.18, 0.342, 0.247, 0.242]
[2m[36m(func pid=46161)[0m 
== Status ==
Current time: 2024-01-07 12:08:49 (running for 00:10:55.17)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.751 |      0.134 |                   34 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.53  |      0.24  |                   11 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.492 |      0.296 |                   11 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  3.016 |      0.349 |                    7 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.1928 | Steps: 4 | Val loss: 4.3711 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=45540)[0m top1: 0.3493470149253731
[2m[36m(func pid=45540)[0m top5: 0.8218283582089553
[2m[36m(func pid=45540)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=45540)[0m f1_macro: 0.2485913775203148
[2m[36m(func pid=45540)[0m f1_weighted: 0.35566960419658666
[2m[36m(func pid=45540)[0m f1_per_class: [0.272, 0.116, 0.355, 0.481, 0.05, 0.362, 0.482, 0.072, 0.025, 0.27]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m top1: 0.17397388059701493
[2m[36m(func pid=39840)[0m top5: 0.6422574626865671
[2m[36m(func pid=39840)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=39840)[0m f1_macro: 0.13929822481932594
[2m[36m(func pid=39840)[0m f1_weighted: 0.19540002793838168
[2m[36m(func pid=39840)[0m f1_per_class: [0.139, 0.155, 0.079, 0.265, 0.033, 0.336, 0.141, 0.159, 0.026, 0.06]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.3675373134328358
[2m[36m(func pid=47180)[0m top5: 0.820429104477612
[2m[36m(func pid=47180)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=47180)[0m f1_macro: 0.32596499032278803
[2m[36m(func pid=47180)[0m f1_weighted: 0.3464670180676089
[2m[36m(func pid=47180)[0m f1_per_class: [0.562, 0.456, 0.476, 0.112, 0.09, 0.336, 0.551, 0.145, 0.213, 0.318]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.3667 | Steps: 4 | Val loss: 1.8858 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.4265 | Steps: 4 | Val loss: 2.0163 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.7884 | Steps: 4 | Val loss: 2.2615 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 12:08:54 (running for 00:11:00.37)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.787 |      0.139 |                   35 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.504 |      0.249 |                   12 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.367 |      0.329 |                   12 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.193 |      0.326 |                    8 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3460820895522388
[2m[36m(func pid=46161)[0m top5: 0.8451492537313433
[2m[36m(func pid=46161)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=46161)[0m f1_macro: 0.3291251979333902
[2m[36m(func pid=46161)[0m f1_weighted: 0.34166186199380005
[2m[36m(func pid=46161)[0m f1_per_class: [0.558, 0.482, 0.162, 0.442, 0.101, 0.392, 0.147, 0.369, 0.224, 0.415]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.9711 | Steps: 4 | Val loss: 4.3539 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=45540)[0m top1: 0.3460820895522388
[2m[36m(func pid=45540)[0m top5: 0.8222947761194029
[2m[36m(func pid=45540)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=45540)[0m f1_macro: 0.25307269602695864
[2m[36m(func pid=45540)[0m f1_weighted: 0.3611736280031178
[2m[36m(func pid=45540)[0m f1_per_class: [0.268, 0.196, 0.349, 0.46, 0.046, 0.373, 0.471, 0.07, 0.025, 0.272]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m top1: 0.1791044776119403
[2m[36m(func pid=39840)[0m top5: 0.6333955223880597
[2m[36m(func pid=39840)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=39840)[0m f1_macro: 0.14518338382845936
[2m[36m(func pid=39840)[0m f1_weighted: 0.1961800646552183
[2m[36m(func pid=39840)[0m f1_per_class: [0.172, 0.169, 0.095, 0.271, 0.036, 0.335, 0.127, 0.167, 0.025, 0.055]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.33348880597014924
[2m[36m(func pid=47180)[0m top5: 0.8059701492537313
[2m[36m(func pid=47180)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=47180)[0m f1_macro: 0.3058945210822479
[2m[36m(func pid=47180)[0m f1_weighted: 0.3189082779144603
[2m[36m(func pid=47180)[0m f1_per_class: [0.5, 0.422, 0.222, 0.477, 0.161, 0.367, 0.093, 0.386, 0.164, 0.267]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.1732 | Steps: 4 | Val loss: 1.8910 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.3976 | Steps: 4 | Val loss: 2.0050 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.7380 | Steps: 4 | Val loss: 2.2574 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 12:08:59 (running for 00:11:05.73)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.788 |      0.145 |                   36 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.426 |      0.253 |                   13 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.173 |      0.316 |                   13 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.971 |      0.306 |                    9 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3558768656716418
[2m[36m(func pid=46161)[0m top5: 0.84375
[2m[36m(func pid=46161)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=46161)[0m f1_macro: 0.31637335047009596
[2m[36m(func pid=46161)[0m f1_weighted: 0.36968873860483037
[2m[36m(func pid=46161)[0m f1_per_class: [0.504, 0.42, 0.09, 0.52, 0.159, 0.411, 0.208, 0.355, 0.222, 0.273]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.7267 | Steps: 4 | Val loss: 3.7428 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=45540)[0m top1: 0.35401119402985076
[2m[36m(func pid=45540)[0m top5: 0.8190298507462687
[2m[36m(func pid=45540)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=45540)[0m f1_macro: 0.26519099068296603
[2m[36m(func pid=45540)[0m f1_weighted: 0.3711765259466439
[2m[36m(func pid=45540)[0m f1_per_class: [0.27, 0.258, 0.306, 0.47, 0.052, 0.401, 0.441, 0.103, 0.025, 0.327]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m top1: 0.18003731343283583
[2m[36m(func pid=39840)[0m top5: 0.6324626865671642
[2m[36m(func pid=39840)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=39840)[0m f1_macro: 0.147634842598476
[2m[36m(func pid=39840)[0m f1_weighted: 0.19656369197948914
[2m[36m(func pid=39840)[0m f1_per_class: [0.168, 0.151, 0.115, 0.271, 0.034, 0.346, 0.135, 0.162, 0.025, 0.07]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.408115671641791
[2m[36m(func pid=47180)[0m top5: 0.8582089552238806
[2m[36m(func pid=47180)[0m f1_micro: 0.408115671641791
[2m[36m(func pid=47180)[0m f1_macro: 0.32469372907317695
[2m[36m(func pid=47180)[0m f1_weighted: 0.4058227648972152
[2m[36m(func pid=47180)[0m f1_per_class: [0.47, 0.341, 0.188, 0.556, 0.168, 0.389, 0.355, 0.369, 0.172, 0.239]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.5846 | Steps: 4 | Val loss: 1.8703 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.4165 | Steps: 4 | Val loss: 2.0176 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 12:09:05 (running for 00:11:11.07)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.738 |      0.148 |                   37 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.398 |      0.265 |                   14 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.585 |      0.306 |                   14 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.727 |      0.325 |                   10 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3694029850746269
[2m[36m(func pid=46161)[0m top5: 0.8521455223880597
[2m[36m(func pid=46161)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=46161)[0m f1_macro: 0.3063307784664383
[2m[36m(func pid=46161)[0m f1_weighted: 0.377177976338728
[2m[36m(func pid=46161)[0m f1_per_class: [0.522, 0.262, 0.078, 0.573, 0.122, 0.413, 0.276, 0.357, 0.209, 0.251]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.7878 | Steps: 4 | Val loss: 2.2500 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7877 | Steps: 4 | Val loss: 5.6933 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=45540)[0m top1: 0.31902985074626866
[2m[36m(func pid=45540)[0m top5: 0.8152985074626866
[2m[36m(func pid=45540)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=45540)[0m f1_macro: 0.2530338973780665
[2m[36m(func pid=45540)[0m f1_weighted: 0.3480986444127432
[2m[36m(func pid=45540)[0m f1_per_class: [0.222, 0.242, 0.286, 0.435, 0.045, 0.361, 0.412, 0.165, 0.025, 0.337]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m top1: 0.17630597014925373
[2m[36m(func pid=39840)[0m top5: 0.6478544776119403
[2m[36m(func pid=39840)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=39840)[0m f1_macro: 0.14907411364883472
[2m[36m(func pid=39840)[0m f1_weighted: 0.19257434124740355
[2m[36m(func pid=39840)[0m f1_per_class: [0.192, 0.161, 0.058, 0.268, 0.037, 0.326, 0.116, 0.197, 0.029, 0.107]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.0330 | Steps: 4 | Val loss: 1.7422 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=47180)[0m top1: 0.31203358208955223
[2m[36m(func pid=47180)[0m top5: 0.7098880597014925
[2m[36m(func pid=47180)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=47180)[0m f1_macro: 0.2527726005930112
[2m[36m(func pid=47180)[0m f1_weighted: 0.29350999058803384
[2m[36m(func pid=47180)[0m f1_per_class: [0.327, 0.409, 0.257, 0.007, 0.095, 0.286, 0.537, 0.205, 0.144, 0.261]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.3637 | Steps: 4 | Val loss: 2.0230 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 12:09:10 (running for 00:11:16.40)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.788 |      0.149 |                   38 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.416 |      0.253 |                   15 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.033 |      0.315 |                   15 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.788 |      0.253 |                   11 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.37593283582089554
[2m[36m(func pid=46161)[0m top5: 0.8768656716417911
[2m[36m(func pid=46161)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=46161)[0m f1_macro: 0.31519543916879944
[2m[36m(func pid=46161)[0m f1_weighted: 0.39929101837666786
[2m[36m(func pid=46161)[0m f1_per_class: [0.512, 0.292, 0.12, 0.539, 0.155, 0.414, 0.374, 0.317, 0.2, 0.229]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.7748 | Steps: 4 | Val loss: 2.2482 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.2781 | Steps: 4 | Val loss: 5.9429 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=45540)[0m top1: 0.2989738805970149
[2m[36m(func pid=45540)[0m top5: 0.8027052238805971
[2m[36m(func pid=45540)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=45540)[0m f1_macro: 0.24936218490563505
[2m[36m(func pid=45540)[0m f1_weighted: 0.3236483169781876
[2m[36m(func pid=45540)[0m f1_per_class: [0.199, 0.2, 0.275, 0.451, 0.048, 0.372, 0.322, 0.213, 0.071, 0.343]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.8253 | Steps: 4 | Val loss: 1.7822 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=39840)[0m top1: 0.17117537313432835
[2m[36m(func pid=39840)[0m top5: 0.6585820895522388
[2m[36m(func pid=39840)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=39840)[0m f1_macro: 0.14253600431048064
[2m[36m(func pid=39840)[0m f1_weighted: 0.19026358000236576
[2m[36m(func pid=39840)[0m f1_per_class: [0.19, 0.174, 0.058, 0.254, 0.027, 0.313, 0.123, 0.192, 0.025, 0.07]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.2798507462686567
[2m[36m(func pid=47180)[0m top5: 0.7047574626865671
[2m[36m(func pid=47180)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=47180)[0m f1_macro: 0.2816151240504112
[2m[36m(func pid=47180)[0m f1_weighted: 0.2660190578406012
[2m[36m(func pid=47180)[0m f1_per_class: [0.388, 0.475, 0.433, 0.03, 0.144, 0.32, 0.361, 0.195, 0.161, 0.31]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.3325 | Steps: 4 | Val loss: 1.9960 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 12:09:15 (running for 00:11:21.51)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.775 |      0.143 |                   39 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.364 |      0.249 |                   16 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.825 |      0.307 |                   16 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.278 |      0.282 |                   12 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3521455223880597
[2m[36m(func pid=46161)[0m top5: 0.8754664179104478
[2m[36m(func pid=46161)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=46161)[0m f1_macro: 0.30749310856748024
[2m[36m(func pid=46161)[0m f1_weighted: 0.38715419182270533
[2m[36m(func pid=46161)[0m f1_per_class: [0.512, 0.351, 0.135, 0.459, 0.13, 0.386, 0.393, 0.275, 0.194, 0.24]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8100 | Steps: 4 | Val loss: 3.5916 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.7752 | Steps: 4 | Val loss: 2.2559 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=45540)[0m top1: 0.314365671641791
[2m[36m(func pid=45540)[0m top5: 0.8003731343283582
[2m[36m(func pid=45540)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=45540)[0m f1_macro: 0.25554409775641934
[2m[36m(func pid=45540)[0m f1_weighted: 0.3112029222085659
[2m[36m(func pid=45540)[0m f1_per_class: [0.232, 0.157, 0.333, 0.501, 0.065, 0.389, 0.242, 0.265, 0.048, 0.325]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.9310 | Steps: 4 | Val loss: 1.8257 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=39840)[0m top1: 0.177705223880597
[2m[36m(func pid=39840)[0m top5: 0.6352611940298507
[2m[36m(func pid=39840)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=39840)[0m f1_macro: 0.1575351746123686
[2m[36m(func pid=39840)[0m f1_weighted: 0.191795579619398
[2m[36m(func pid=39840)[0m f1_per_class: [0.209, 0.193, 0.112, 0.241, 0.037, 0.345, 0.114, 0.18, 0.027, 0.118]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.4351679104477612
[2m[36m(func pid=47180)[0m top5: 0.9081156716417911
[2m[36m(func pid=47180)[0m f1_micro: 0.4351679104477612
[2m[36m(func pid=47180)[0m f1_macro: 0.39733263271461283
[2m[36m(func pid=47180)[0m f1_weighted: 0.42742876708836514
[2m[36m(func pid=47180)[0m f1_per_class: [0.483, 0.505, 0.571, 0.46, 0.176, 0.452, 0.39, 0.319, 0.176, 0.439]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.1815 | Steps: 4 | Val loss: 1.9887 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 12:09:20 (running for 00:11:26.69)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.775 |      0.158 |                   40 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.333 |      0.256 |                   17 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.931 |      0.308 |                   17 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.81  |      0.397 |                   13 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.345615671641791
[2m[36m(func pid=46161)[0m top5: 0.8763992537313433
[2m[36m(func pid=46161)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=46161)[0m f1_macro: 0.3081035177380398
[2m[36m(func pid=46161)[0m f1_weighted: 0.37651981377896315
[2m[36m(func pid=46161)[0m f1_per_class: [0.481, 0.38, 0.169, 0.363, 0.098, 0.404, 0.42, 0.288, 0.214, 0.263]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.7170 | Steps: 4 | Val loss: 2.2503 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.7561 | Steps: 4 | Val loss: 3.8156 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=45540)[0m top1: 0.32509328358208955
[2m[36m(func pid=45540)[0m top5: 0.7947761194029851
[2m[36m(func pid=45540)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=45540)[0m f1_macro: 0.2646493012607379
[2m[36m(func pid=45540)[0m f1_weighted: 0.30890159861970434
[2m[36m(func pid=45540)[0m f1_per_class: [0.302, 0.192, 0.333, 0.53, 0.065, 0.404, 0.173, 0.281, 0.048, 0.318]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.0327 | Steps: 4 | Val loss: 1.7377 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=39840)[0m top1: 0.17723880597014927
[2m[36m(func pid=39840)[0m top5: 0.6422574626865671
[2m[36m(func pid=39840)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=39840)[0m f1_macro: 0.1532759383851202
[2m[36m(func pid=39840)[0m f1_weighted: 0.1950654383607784
[2m[36m(func pid=39840)[0m f1_per_class: [0.204, 0.164, 0.086, 0.265, 0.028, 0.326, 0.125, 0.193, 0.025, 0.116]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.4141791044776119
[2m[36m(func pid=47180)[0m top5: 0.8917910447761194
[2m[36m(func pid=47180)[0m f1_micro: 0.4141791044776119
[2m[36m(func pid=47180)[0m f1_macro: 0.35695464501469865
[2m[36m(func pid=47180)[0m f1_weighted: 0.40257578746758005
[2m[36m(func pid=47180)[0m f1_per_class: [0.397, 0.466, 0.537, 0.561, 0.136, 0.404, 0.267, 0.307, 0.195, 0.299]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.1615 | Steps: 4 | Val loss: 2.0045 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=46161)[0m top1: 0.39132462686567165
[2m[36m(func pid=46161)[0m top5: 0.8969216417910447
[2m[36m(func pid=46161)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=46161)[0m f1_macro: 0.324166936666747
[2m[36m(func pid=46161)[0m f1_weighted: 0.4229722718977384
[2m[36m(func pid=46161)[0m f1_per_class: [0.434, 0.378, 0.218, 0.475, 0.096, 0.401, 0.484, 0.231, 0.232, 0.293]
== Status ==
Current time: 2024-01-07 12:09:25 (running for 00:11:31.90)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.717 |      0.153 |                   41 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.182 |      0.265 |                   18 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.033 |      0.324 |                   18 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.756 |      0.357 |                   14 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.7258 | Steps: 4 | Val loss: 2.2439 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.2331 | Steps: 4 | Val loss: 4.9104 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=45540)[0m top1: 0.3064365671641791
[2m[36m(func pid=45540)[0m top5: 0.7835820895522388
[2m[36m(func pid=45540)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=45540)[0m f1_macro: 0.2581793062809411
[2m[36m(func pid=45540)[0m f1_weighted: 0.29405868623769693
[2m[36m(func pid=45540)[0m f1_per_class: [0.238, 0.191, 0.375, 0.491, 0.068, 0.432, 0.151, 0.285, 0.072, 0.278]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.1311 | Steps: 4 | Val loss: 1.7299 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=39840)[0m top1: 0.17957089552238806
[2m[36m(func pid=39840)[0m top5: 0.6581156716417911
[2m[36m(func pid=39840)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=39840)[0m f1_macro: 0.1531904650356389
[2m[36m(func pid=39840)[0m f1_weighted: 0.19831769896003834
[2m[36m(func pid=39840)[0m f1_per_class: [0.168, 0.163, 0.125, 0.288, 0.031, 0.321, 0.122, 0.183, 0.025, 0.106]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.302705223880597
[2m[36m(func pid=47180)[0m top5: 0.8311567164179104
[2m[36m(func pid=47180)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=47180)[0m f1_macro: 0.2798631826288538
[2m[36m(func pid=47180)[0m f1_weighted: 0.33091108762360794
[2m[36m(func pid=47180)[0m f1_per_class: [0.142, 0.367, 0.314, 0.266, 0.2, 0.4, 0.385, 0.301, 0.209, 0.215]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.1531 | Steps: 4 | Val loss: 1.9939 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 12:09:31 (running for 00:11:37.05)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.726 |      0.153 |                   42 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.162 |      0.258 |                   19 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.131 |      0.322 |                   19 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.233 |      0.28  |                   15 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.40111940298507465
[2m[36m(func pid=46161)[0m top5: 0.8973880597014925
[2m[36m(func pid=46161)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=46161)[0m f1_macro: 0.3219957629839854
[2m[36m(func pid=46161)[0m f1_weighted: 0.42726517840895123
[2m[36m(func pid=46161)[0m f1_per_class: [0.482, 0.357, 0.255, 0.503, 0.102, 0.411, 0.494, 0.153, 0.236, 0.227]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.7321 | Steps: 4 | Val loss: 2.2422 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.1216 | Steps: 4 | Val loss: 5.5682 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=45540)[0m top1: 0.3111007462686567
[2m[36m(func pid=45540)[0m top5: 0.789179104477612
[2m[36m(func pid=45540)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=45540)[0m f1_macro: 0.26410488360111545
[2m[36m(func pid=45540)[0m f1_weighted: 0.30498031835182643
[2m[36m(func pid=45540)[0m f1_per_class: [0.235, 0.253, 0.375, 0.483, 0.065, 0.412, 0.164, 0.304, 0.07, 0.28]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.9551 | Steps: 4 | Val loss: 1.8558 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=39840)[0m top1: 0.17350746268656717
[2m[36m(func pid=39840)[0m top5: 0.6595149253731343
[2m[36m(func pid=39840)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=39840)[0m f1_macro: 0.14681391396198665
[2m[36m(func pid=39840)[0m f1_weighted: 0.19455189257846775
[2m[36m(func pid=39840)[0m f1_per_class: [0.15, 0.163, 0.107, 0.283, 0.027, 0.317, 0.119, 0.173, 0.026, 0.105]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.2957089552238806
[2m[36m(func pid=47180)[0m top5: 0.7919776119402985
[2m[36m(func pid=47180)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=47180)[0m f1_macro: 0.25841721523931416
[2m[36m(func pid=47180)[0m f1_weighted: 0.3336231221493566
[2m[36m(func pid=47180)[0m f1_per_class: [0.215, 0.42, 0.1, 0.283, 0.113, 0.354, 0.389, 0.185, 0.158, 0.367]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.2833 | Steps: 4 | Val loss: 1.9777 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 12:09:36 (running for 00:11:42.28)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.732 |      0.147 |                   43 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.153 |      0.264 |                   20 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.955 |      0.317 |                   20 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.122 |      0.258 |                   16 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.37173507462686567
[2m[36m(func pid=46161)[0m top5: 0.8736007462686567
[2m[36m(func pid=46161)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=46161)[0m f1_macro: 0.3165264335585265
[2m[36m(func pid=46161)[0m f1_weighted: 0.40474377133714556
[2m[36m(func pid=46161)[0m f1_per_class: [0.525, 0.389, 0.181, 0.499, 0.136, 0.386, 0.405, 0.203, 0.185, 0.256]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.7156 | Steps: 4 | Val loss: 2.2483 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.3809 | Steps: 4 | Val loss: 5.5937 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=45540)[0m top1: 0.3064365671641791
[2m[36m(func pid=45540)[0m top5: 0.7952425373134329
[2m[36m(func pid=45540)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=45540)[0m f1_macro: 0.2627980746686559
[2m[36m(func pid=45540)[0m f1_weighted: 0.3100487948896867
[2m[36m(func pid=45540)[0m f1_per_class: [0.216, 0.267, 0.381, 0.447, 0.063, 0.429, 0.205, 0.285, 0.068, 0.266]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 0.8603 | Steps: 4 | Val loss: 1.8780 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=39840)[0m top1: 0.16884328358208955
[2m[36m(func pid=39840)[0m top5: 0.6464552238805971
[2m[36m(func pid=39840)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=39840)[0m f1_macro: 0.14211049296989406
[2m[36m(func pid=39840)[0m f1_weighted: 0.19058655558886925
[2m[36m(func pid=39840)[0m f1_per_class: [0.156, 0.146, 0.107, 0.277, 0.027, 0.305, 0.126, 0.171, 0.024, 0.082]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m top1: 0.34841417910447764
[2m[36m(func pid=47180)[0m top5: 0.8092350746268657
[2m[36m(func pid=47180)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=47180)[0m f1_macro: 0.3222772101892845
[2m[36m(func pid=47180)[0m f1_weighted: 0.3416533910629476
[2m[36m(func pid=47180)[0m f1_per_class: [0.463, 0.462, 0.191, 0.505, 0.12, 0.345, 0.135, 0.314, 0.182, 0.506]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.1898 | Steps: 4 | Val loss: 1.9766 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 12:09:41 (running for 00:11:47.58)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.716 |      0.142 |                   44 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.283 |      0.263 |                   21 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.86  |      0.335 |                   21 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.381 |      0.322 |                   17 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3824626865671642
[2m[36m(func pid=46161)[0m top5: 0.8638059701492538
[2m[36m(func pid=46161)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=46161)[0m f1_macro: 0.33484315300361767
[2m[36m(func pid=46161)[0m f1_weighted: 0.4052494365510963
[2m[36m(func pid=46161)[0m f1_per_class: [0.5, 0.435, 0.187, 0.514, 0.161, 0.37, 0.344, 0.333, 0.198, 0.306]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.8137 | Steps: 4 | Val loss: 5.6829 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6910 | Steps: 4 | Val loss: 2.2441 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=45540)[0m top1: 0.30783582089552236
[2m[36m(func pid=45540)[0m top5: 0.7975746268656716
[2m[36m(func pid=45540)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=45540)[0m f1_macro: 0.2628014634812581
[2m[36m(func pid=45540)[0m f1_weighted: 0.32759163344958786
[2m[36m(func pid=45540)[0m f1_per_class: [0.215, 0.284, 0.264, 0.415, 0.058, 0.422, 0.282, 0.297, 0.118, 0.274]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.0207 | Steps: 4 | Val loss: 1.9765 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=47180)[0m top1: 0.3614738805970149
[2m[36m(func pid=47180)[0m top5: 0.7933768656716418
[2m[36m(func pid=47180)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=47180)[0m f1_macro: 0.3357038826382395
[2m[36m(func pid=47180)[0m f1_weighted: 0.3334718668222918
[2m[36m(func pid=47180)[0m f1_per_class: [0.46, 0.498, 0.283, 0.451, 0.131, 0.36, 0.119, 0.324, 0.273, 0.458]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=39840)[0m top1: 0.17817164179104478
[2m[36m(func pid=39840)[0m top5: 0.6455223880597015
[2m[36m(func pid=39840)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=39840)[0m f1_macro: 0.14899086950181004
[2m[36m(func pid=39840)[0m f1_weighted: 0.1998539523127144
[2m[36m(func pid=39840)[0m f1_per_class: [0.153, 0.146, 0.1, 0.297, 0.029, 0.316, 0.129, 0.196, 0.034, 0.09]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.1694 | Steps: 4 | Val loss: 1.9551 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 12:09:46 (running for 00:11:52.75)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.691 |      0.149 |                   45 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.19  |      0.263 |                   22 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.021 |      0.318 |                   22 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.814 |      0.336 |                   18 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.36986940298507465
[2m[36m(func pid=46161)[0m top5: 0.8376865671641791
[2m[36m(func pid=46161)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=46161)[0m f1_macro: 0.3175627037279847
[2m[36m(func pid=46161)[0m f1_weighted: 0.38266037915209655
[2m[36m(func pid=46161)[0m f1_per_class: [0.366, 0.488, 0.184, 0.432, 0.136, 0.348, 0.331, 0.328, 0.214, 0.349]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.9840 | Steps: 4 | Val loss: 5.3125 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.7470 | Steps: 4 | Val loss: 2.2503 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=45540)[0m top1: 0.3138992537313433
[2m[36m(func pid=45540)[0m top5: 0.8166977611940298
[2m[36m(func pid=45540)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=45540)[0m f1_macro: 0.26926572777904406
[2m[36m(func pid=45540)[0m f1_weighted: 0.3367983047146773
[2m[36m(func pid=45540)[0m f1_per_class: [0.295, 0.307, 0.286, 0.383, 0.058, 0.432, 0.333, 0.226, 0.143, 0.231]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.1206 | Steps: 4 | Val loss: 2.0376 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=47180)[0m top1: 0.33488805970149255
[2m[36m(func pid=47180)[0m top5: 0.816231343283582
[2m[36m(func pid=47180)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=47180)[0m f1_macro: 0.3332087671421429
[2m[36m(func pid=47180)[0m f1_weighted: 0.3453785323886858
[2m[36m(func pid=47180)[0m f1_per_class: [0.318, 0.434, 0.605, 0.369, 0.083, 0.363, 0.284, 0.306, 0.301, 0.269]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=39840)[0m top1: 0.18003731343283583
[2m[36m(func pid=39840)[0m top5: 0.6361940298507462
[2m[36m(func pid=39840)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=39840)[0m f1_macro: 0.1559399648175189
[2m[36m(func pid=39840)[0m f1_weighted: 0.19579012874821078
[2m[36m(func pid=39840)[0m f1_per_class: [0.176, 0.154, 0.115, 0.299, 0.028, 0.336, 0.095, 0.214, 0.02, 0.122]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.1044 | Steps: 4 | Val loss: 1.9545 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=46161)[0m top1: 0.3512126865671642
[2m[36m(func pid=46161)[0m top5: 0.8269589552238806
[2m[36m(func pid=46161)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=46161)[0m f1_macro: 0.3094804810188774
[2m[36m(func pid=46161)[0m f1_weighted: 0.3550140236672386
[2m[36m(func pid=46161)[0m f1_per_class: [0.357, 0.483, 0.2, 0.398, 0.129, 0.334, 0.276, 0.335, 0.219, 0.364]
== Status ==
Current time: 2024-01-07 12:09:52 (running for 00:11:57.98)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.747 |      0.156 |                   46 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.169 |      0.269 |                   23 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.121 |      0.309 |                   23 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.984 |      0.333 |                   19 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.4137 | Steps: 4 | Val loss: 5.7819 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7430 | Steps: 4 | Val loss: 2.2440 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=45540)[0m top1: 0.3087686567164179
[2m[36m(func pid=45540)[0m top5: 0.8120335820895522
[2m[36m(func pid=45540)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=45540)[0m f1_macro: 0.26796878848440797
[2m[36m(func pid=45540)[0m f1_weighted: 0.3396724291911878
[2m[36m(func pid=45540)[0m f1_per_class: [0.335, 0.278, 0.208, 0.379, 0.052, 0.429, 0.361, 0.215, 0.164, 0.259]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.0042 | Steps: 4 | Val loss: 2.0855 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=47180)[0m top1: 0.2989738805970149
[2m[36m(func pid=47180)[0m top5: 0.7961753731343284
[2m[36m(func pid=47180)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=47180)[0m f1_macro: 0.27588477239246983
[2m[36m(func pid=47180)[0m f1_weighted: 0.3204831363207413
[2m[36m(func pid=47180)[0m f1_per_class: [0.265, 0.32, 0.433, 0.194, 0.153, 0.275, 0.487, 0.264, 0.28, 0.088]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=39840)[0m top1: 0.177705223880597
[2m[36m(func pid=39840)[0m top5: 0.652518656716418
[2m[36m(func pid=39840)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=39840)[0m f1_macro: 0.15163325011222323
[2m[36m(func pid=39840)[0m f1_weighted: 0.19545448509887198
[2m[36m(func pid=39840)[0m f1_per_class: [0.161, 0.151, 0.118, 0.285, 0.037, 0.321, 0.124, 0.166, 0.032, 0.121]
[2m[36m(func pid=39840)[0m 
== Status ==
Current time: 2024-01-07 12:09:57 (running for 00:12:03.12)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.743 |      0.152 |                   47 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.104 |      0.268 |                   24 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.004 |      0.277 |                   24 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.414 |      0.276 |                   20 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.9730 | Steps: 4 | Val loss: 1.9355 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=46161)[0m top1: 0.3180970149253731
[2m[36m(func pid=46161)[0m top5: 0.8227611940298507
[2m[36m(func pid=46161)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=46161)[0m f1_macro: 0.27709501390324925
[2m[36m(func pid=46161)[0m f1_weighted: 0.34068744897449565
[2m[36m(func pid=46161)[0m f1_per_class: [0.315, 0.404, 0.193, 0.374, 0.126, 0.311, 0.318, 0.326, 0.208, 0.196]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.4802 | Steps: 4 | Val loss: 7.8797 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.6688 | Steps: 4 | Val loss: 2.2406 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=45540)[0m top1: 0.332089552238806
[2m[36m(func pid=45540)[0m top5: 0.8143656716417911
[2m[36m(func pid=45540)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=45540)[0m f1_macro: 0.28947312734313013
[2m[36m(func pid=45540)[0m f1_weighted: 0.35733815082333886
[2m[36m(func pid=45540)[0m f1_per_class: [0.318, 0.317, 0.282, 0.422, 0.069, 0.435, 0.345, 0.259, 0.174, 0.274]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.8684 | Steps: 4 | Val loss: 2.0871 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=47180)[0m top1: 0.22527985074626866
[2m[36m(func pid=47180)[0m top5: 0.6856343283582089
[2m[36m(func pid=47180)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=47180)[0m f1_macro: 0.21404752932312632
[2m[36m(func pid=47180)[0m f1_weighted: 0.2345696426394422
[2m[36m(func pid=47180)[0m f1_per_class: [0.365, 0.375, 0.048, 0.023, 0.13, 0.238, 0.347, 0.287, 0.18, 0.148]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=39840)[0m top1: 0.1865671641791045
[2m[36m(func pid=39840)[0m top5: 0.6459888059701493
[2m[36m(func pid=39840)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=39840)[0m f1_macro: 0.16104775220795642
[2m[36m(func pid=39840)[0m f1_weighted: 0.20304703649410136
[2m[36m(func pid=39840)[0m f1_per_class: [0.171, 0.183, 0.145, 0.279, 0.033, 0.342, 0.125, 0.184, 0.023, 0.126]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=46161)[0m top1: 0.33255597014925375
[2m[36m(func pid=46161)[0m top5: 0.8297574626865671
[2m[36m(func pid=46161)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=46161)[0m f1_macro: 0.2827440309619434
[2m[36m(func pid=46161)[0m f1_weighted: 0.37618992826264613
[2m[36m(func pid=46161)[0m f1_per_class: [0.372, 0.265, 0.22, 0.425, 0.093, 0.346, 0.458, 0.307, 0.211, 0.132]
[2m[36m(func pid=46161)[0m 
== Status ==
Current time: 2024-01-07 12:10:02 (running for 00:12:08.42)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.669 |      0.161 |                   48 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.973 |      0.289 |                   25 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.868 |      0.283 |                   25 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.48  |      0.214 |                   21 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.0100 | Steps: 4 | Val loss: 1.9369 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.8139 | Steps: 4 | Val loss: 7.4259 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.7180 | Steps: 4 | Val loss: 2.2405 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=45540)[0m top1: 0.3185634328358209
[2m[36m(func pid=45540)[0m top5: 0.8129664179104478
[2m[36m(func pid=45540)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=45540)[0m f1_macro: 0.285689286136308
[2m[36m(func pid=45540)[0m f1_weighted: 0.338074139489509
[2m[36m(func pid=45540)[0m f1_per_class: [0.33, 0.29, 0.312, 0.419, 0.081, 0.438, 0.294, 0.273, 0.169, 0.25]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.7165 | Steps: 4 | Val loss: 1.9625 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=47180)[0m top1: 0.24253731343283583
[2m[36m(func pid=47180)[0m top5: 0.7089552238805971
[2m[36m(func pid=47180)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=47180)[0m f1_macro: 0.23702968391670334
[2m[36m(func pid=47180)[0m f1_weighted: 0.2498978448847408
[2m[36m(func pid=47180)[0m f1_per_class: [0.349, 0.408, 0.058, 0.304, 0.107, 0.348, 0.067, 0.325, 0.156, 0.248]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=39840)[0m top1: 0.18423507462686567
[2m[36m(func pid=39840)[0m top5: 0.6595149253731343
[2m[36m(func pid=39840)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=39840)[0m f1_macro: 0.15719618343669756
[2m[36m(func pid=39840)[0m f1_weighted: 0.20122035399417168
[2m[36m(func pid=39840)[0m f1_per_class: [0.159, 0.164, 0.151, 0.291, 0.026, 0.341, 0.12, 0.182, 0.036, 0.103]
[2m[36m(func pid=39840)[0m 
== Status ==
Current time: 2024-01-07 12:10:07 (running for 00:12:13.76)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.718 |      0.157 |                   49 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.01  |      0.286 |                   26 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.716 |      0.301 |                   26 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.814 |      0.237 |                   22 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.9804 | Steps: 4 | Val loss: 1.9260 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=46161)[0m top1: 0.35027985074626866
[2m[36m(func pid=46161)[0m top5: 0.8610074626865671
[2m[36m(func pid=46161)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=46161)[0m f1_macro: 0.3008374188692081
[2m[36m(func pid=46161)[0m f1_weighted: 0.3940404348162356
[2m[36m(func pid=46161)[0m f1_per_class: [0.464, 0.295, 0.253, 0.422, 0.094, 0.364, 0.492, 0.308, 0.183, 0.134]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.2976 | Steps: 4 | Val loss: 6.3723 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=45540)[0m top1: 0.31763059701492535
[2m[36m(func pid=45540)[0m top5: 0.8194962686567164
[2m[36m(func pid=45540)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=45540)[0m f1_macro: 0.28975038313071705
[2m[36m(func pid=45540)[0m f1_weighted: 0.3379779992211142
[2m[36m(func pid=45540)[0m f1_per_class: [0.391, 0.285, 0.308, 0.432, 0.083, 0.419, 0.289, 0.268, 0.164, 0.258]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.7444 | Steps: 4 | Val loss: 2.2379 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7175 | Steps: 4 | Val loss: 1.9444 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=47180)[0m top1: 0.35774253731343286
[2m[36m(func pid=47180)[0m top5: 0.8236940298507462
[2m[36m(func pid=47180)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=47180)[0m f1_macro: 0.3468588432185537
[2m[36m(func pid=47180)[0m f1_weighted: 0.3195911587295226
[2m[36m(func pid=47180)[0m f1_per_class: [0.512, 0.267, 0.75, 0.551, 0.274, 0.39, 0.128, 0.201, 0.187, 0.21]
[2m[36m(func pid=47180)[0m 
== Status ==
Current time: 2024-01-07 12:10:12 (running for 00:12:18.87)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.744 |      0.163 |                   50 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.98  |      0.29  |                   27 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.716 |      0.301 |                   26 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.298 |      0.347 |                   23 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=39840)[0m top1: 0.18889925373134328
[2m[36m(func pid=39840)[0m top5: 0.6529850746268657
[2m[36m(func pid=39840)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=39840)[0m f1_macro: 0.16292422812591092
[2m[36m(func pid=39840)[0m f1_weighted: 0.2060629266333288
[2m[36m(func pid=39840)[0m f1_per_class: [0.187, 0.17, 0.143, 0.3, 0.027, 0.341, 0.117, 0.213, 0.028, 0.104]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=46161)[0m top1: 0.35774253731343286
[2m[36m(func pid=46161)[0m top5: 0.871268656716418
[2m[36m(func pid=46161)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=46161)[0m f1_macro: 0.3089362291352401
[2m[36m(func pid=46161)[0m f1_weighted: 0.38551738900618227
[2m[36m(func pid=46161)[0m f1_per_class: [0.455, 0.424, 0.211, 0.353, 0.092, 0.405, 0.438, 0.284, 0.206, 0.221]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.0443 | Steps: 4 | Val loss: 1.8990 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.8405 | Steps: 4 | Val loss: 4.7949 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=45540)[0m top1: 0.34281716417910446
[2m[36m(func pid=45540)[0m top5: 0.8255597014925373
[2m[36m(func pid=45540)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=45540)[0m f1_macro: 0.302434363885992
[2m[36m(func pid=45540)[0m f1_weighted: 0.36570696934583524
[2m[36m(func pid=45540)[0m f1_per_class: [0.39, 0.277, 0.369, 0.487, 0.077, 0.416, 0.337, 0.26, 0.168, 0.242]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.0353 | Steps: 4 | Val loss: 1.9400 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.6825 | Steps: 4 | Val loss: 2.2462 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 12:10:18 (running for 00:12:23.99)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.744 |      0.163 |                   50 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  2.044 |      0.302 |                   28 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.717 |      0.309 |                   27 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.84  |      0.34  |                   24 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m top1: 0.40578358208955223
[2m[36m(func pid=47180)[0m top5: 0.8885261194029851
[2m[36m(func pid=47180)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=47180)[0m f1_macro: 0.3400477676540752
[2m[36m(func pid=47180)[0m f1_weighted: 0.41229587950215074
[2m[36m(func pid=47180)[0m f1_per_class: [0.346, 0.39, 0.75, 0.446, 0.154, 0.319, 0.536, 0.016, 0.269, 0.176]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m top1: 0.3614738805970149
[2m[36m(func pid=46161)[0m top5: 0.8763992537313433
[2m[36m(func pid=46161)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=46161)[0m f1_macro: 0.32140076810613477
[2m[36m(func pid=46161)[0m f1_weighted: 0.38277377161844534
[2m[36m(func pid=46161)[0m f1_per_class: [0.462, 0.458, 0.198, 0.353, 0.091, 0.392, 0.41, 0.275, 0.224, 0.352]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=39840)[0m top1: 0.1865671641791045
[2m[36m(func pid=39840)[0m top5: 0.6413246268656716
[2m[36m(func pid=39840)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=39840)[0m f1_macro: 0.16249917789720864
[2m[36m(func pid=39840)[0m f1_weighted: 0.19803169556770292
[2m[36m(func pid=39840)[0m f1_per_class: [0.181, 0.183, 0.148, 0.268, 0.039, 0.379, 0.102, 0.195, 0.023, 0.108]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.8944 | Steps: 4 | Val loss: 1.8793 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7964 | Steps: 4 | Val loss: 5.6770 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=45540)[0m top1: 0.34701492537313433
[2m[36m(func pid=45540)[0m top5: 0.8367537313432836
[2m[36m(func pid=45540)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=45540)[0m f1_macro: 0.29988564700523435
[2m[36m(func pid=45540)[0m f1_weighted: 0.3699800523875201
[2m[36m(func pid=45540)[0m f1_per_class: [0.471, 0.216, 0.312, 0.492, 0.075, 0.4, 0.389, 0.237, 0.162, 0.246]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.9443 | Steps: 4 | Val loss: 1.9086 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.6968 | Steps: 4 | Val loss: 2.2544 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 12:10:23 (running for 00:12:29.40)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.683 |      0.162 |                   51 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.894 |      0.3   |                   29 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.035 |      0.321 |                   28 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.796 |      0.334 |                   25 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m top1: 0.34468283582089554
[2m[36m(func pid=47180)[0m top5: 0.8409514925373134
[2m[36m(func pid=47180)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=47180)[0m f1_macro: 0.3339693842223858
[2m[36m(func pid=47180)[0m f1_weighted: 0.36141310670293564
[2m[36m(func pid=47180)[0m f1_per_class: [0.204, 0.408, 0.75, 0.248, 0.146, 0.344, 0.485, 0.327, 0.218, 0.211]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m top1: 0.38572761194029853
[2m[36m(func pid=46161)[0m top5: 0.875
[2m[36m(func pid=46161)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=46161)[0m f1_macro: 0.34925412370300185
[2m[36m(func pid=46161)[0m f1_weighted: 0.39893388840754856
[2m[36m(func pid=46161)[0m f1_per_class: [0.519, 0.496, 0.22, 0.419, 0.094, 0.402, 0.357, 0.314, 0.259, 0.412]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.9586 | Steps: 4 | Val loss: 1.8815 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=39840)[0m top1: 0.18190298507462688
[2m[36m(func pid=39840)[0m top5: 0.6305970149253731
[2m[36m(func pid=39840)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=39840)[0m f1_macro: 0.16389314400617755
[2m[36m(func pid=39840)[0m f1_weighted: 0.19570128808644952
[2m[36m(func pid=39840)[0m f1_per_class: [0.186, 0.192, 0.154, 0.242, 0.036, 0.375, 0.113, 0.199, 0.024, 0.118]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.9196 | Steps: 4 | Val loss: 6.2500 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=45540)[0m top1: 0.34328358208955223
[2m[36m(func pid=45540)[0m top5: 0.8344216417910447
[2m[36m(func pid=45540)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=45540)[0m f1_macro: 0.28723264485557165
[2m[36m(func pid=45540)[0m f1_weighted: 0.3671861930681164
[2m[36m(func pid=45540)[0m f1_per_class: [0.398, 0.218, 0.273, 0.481, 0.071, 0.384, 0.402, 0.207, 0.189, 0.249]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.8013 | Steps: 4 | Val loss: 1.8893 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.6789 | Steps: 4 | Val loss: 2.2332 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 12:10:28 (running for 00:12:34.75)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.697 |      0.164 |                   52 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.959 |      0.287 |                   30 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.944 |      0.349 |                   29 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.92  |      0.294 |                   26 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m top1: 0.28824626865671643
[2m[36m(func pid=47180)[0m top5: 0.8292910447761194
[2m[36m(func pid=47180)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=47180)[0m f1_macro: 0.29394063911172963
[2m[36m(func pid=47180)[0m f1_weighted: 0.2920722122243353
[2m[36m(func pid=47180)[0m f1_per_class: [0.457, 0.398, 0.356, 0.281, 0.13, 0.362, 0.211, 0.349, 0.17, 0.225]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m top1: 0.39598880597014924
[2m[36m(func pid=46161)[0m top5: 0.8675373134328358
[2m[36m(func pid=46161)[0m f1_micro: 0.39598880597014924
[2m[36m(func pid=46161)[0m f1_macro: 0.3571056941862143
[2m[36m(func pid=46161)[0m f1_weighted: 0.40622915789024694
[2m[36m(func pid=46161)[0m f1_per_class: [0.559, 0.496, 0.27, 0.514, 0.103, 0.381, 0.296, 0.326, 0.267, 0.359]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.8776 | Steps: 4 | Val loss: 1.8552 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=39840)[0m top1: 0.18516791044776118
[2m[36m(func pid=39840)[0m top5: 0.667910447761194
[2m[36m(func pid=39840)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=39840)[0m f1_macro: 0.16329289145798348
[2m[36m(func pid=39840)[0m f1_weighted: 0.20224157834446188
[2m[36m(func pid=39840)[0m f1_per_class: [0.188, 0.196, 0.154, 0.264, 0.03, 0.344, 0.126, 0.183, 0.027, 0.121]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.9503 | Steps: 4 | Val loss: 7.1907 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=45540)[0m top1: 0.35634328358208955
[2m[36m(func pid=45540)[0m top5: 0.8493470149253731
[2m[36m(func pid=45540)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=45540)[0m f1_macro: 0.29663882640390604
[2m[36m(func pid=45540)[0m f1_weighted: 0.3820599593220909
[2m[36m(func pid=45540)[0m f1_per_class: [0.448, 0.215, 0.265, 0.476, 0.07, 0.393, 0.452, 0.223, 0.168, 0.257]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.5618 | Steps: 4 | Val loss: 1.7899 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.6897 | Steps: 4 | Val loss: 2.2314 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 12:10:34 (running for 00:12:40.08)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.679 |      0.163 |                   53 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.878 |      0.297 |                   31 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.801 |      0.357 |                   30 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.95  |      0.26  |                   27 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m top1: 0.27238805970149255
[2m[36m(func pid=47180)[0m top5: 0.8246268656716418
[2m[36m(func pid=47180)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=47180)[0m f1_macro: 0.259608639624696
[2m[36m(func pid=47180)[0m f1_weighted: 0.29969628743711674
[2m[36m(func pid=47180)[0m f1_per_class: [0.29, 0.361, 0.042, 0.385, 0.084, 0.406, 0.154, 0.354, 0.211, 0.309]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m top1: 0.4123134328358209
[2m[36m(func pid=46161)[0m top5: 0.8880597014925373
[2m[36m(func pid=46161)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=46161)[0m f1_macro: 0.3532833530432512
[2m[36m(func pid=46161)[0m f1_weighted: 0.4313534999649522
[2m[36m(func pid=46161)[0m f1_per_class: [0.525, 0.474, 0.26, 0.522, 0.103, 0.334, 0.41, 0.314, 0.25, 0.339]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.9221 | Steps: 4 | Val loss: 1.8619 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=39840)[0m top1: 0.1921641791044776
[2m[36m(func pid=39840)[0m top5: 0.664179104477612
[2m[36m(func pid=39840)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=39840)[0m f1_macro: 0.16816474743787402
[2m[36m(func pid=39840)[0m f1_weighted: 0.20712929805965938
[2m[36m(func pid=39840)[0m f1_per_class: [0.205, 0.223, 0.138, 0.255, 0.037, 0.351, 0.131, 0.188, 0.027, 0.127]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.2009 | Steps: 4 | Val loss: 6.7794 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=45540)[0m top1: 0.35494402985074625
[2m[36m(func pid=45540)[0m top5: 0.8470149253731343
[2m[36m(func pid=45540)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=45540)[0m f1_macro: 0.296704233808979
[2m[36m(func pid=45540)[0m f1_weighted: 0.3821347718608674
[2m[36m(func pid=45540)[0m f1_per_class: [0.449, 0.212, 0.216, 0.483, 0.066, 0.408, 0.435, 0.25, 0.176, 0.271]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7913 | Steps: 4 | Val loss: 1.8127 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.6614 | Steps: 4 | Val loss: 2.2292 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 12:10:39 (running for 00:12:45.51)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.69  |      0.168 |                   54 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.922 |      0.297 |                   32 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.562 |      0.353 |                   31 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.201 |      0.285 |                   28 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m top1: 0.29151119402985076
[2m[36m(func pid=47180)[0m top5: 0.8129664179104478
[2m[36m(func pid=47180)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=47180)[0m f1_macro: 0.28535480917615075
[2m[36m(func pid=47180)[0m f1_weighted: 0.28716122802461325
[2m[36m(func pid=47180)[0m f1_per_class: [0.453, 0.454, 0.065, 0.262, 0.119, 0.392, 0.166, 0.343, 0.194, 0.405]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m top1: 0.40111940298507465
[2m[36m(func pid=46161)[0m top5: 0.8964552238805971
[2m[36m(func pid=46161)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=46161)[0m f1_macro: 0.3268884125619431
[2m[36m(func pid=46161)[0m f1_weighted: 0.42857974818935224
[2m[36m(func pid=46161)[0m f1_per_class: [0.408, 0.448, 0.215, 0.459, 0.119, 0.33, 0.498, 0.287, 0.203, 0.303]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.7635 | Steps: 4 | Val loss: 1.8678 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=39840)[0m top1: 0.19169776119402984
[2m[36m(func pid=39840)[0m top5: 0.6716417910447762
[2m[36m(func pid=39840)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=39840)[0m f1_macro: 0.16921604431431275
[2m[36m(func pid=39840)[0m f1_weighted: 0.20592302691928238
[2m[36m(func pid=39840)[0m f1_per_class: [0.198, 0.208, 0.174, 0.263, 0.038, 0.351, 0.128, 0.189, 0.026, 0.117]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.4525 | Steps: 4 | Val loss: 6.8712 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7210 | Steps: 4 | Val loss: 2.0218 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=45540)[0m top1: 0.3493470149253731
[2m[36m(func pid=45540)[0m top5: 0.8502798507462687
[2m[36m(func pid=45540)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=45540)[0m f1_macro: 0.283008122923173
[2m[36m(func pid=45540)[0m f1_weighted: 0.38075140871472624
[2m[36m(func pid=45540)[0m f1_per_class: [0.38, 0.224, 0.185, 0.44, 0.069, 0.405, 0.473, 0.248, 0.174, 0.233]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.6724 | Steps: 4 | Val loss: 2.2218 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 12:10:44 (running for 00:12:50.76)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.661 |      0.169 |                   55 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.763 |      0.283 |                   33 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.791 |      0.327 |                   32 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.452 |      0.248 |                   29 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.35634328358208955
[2m[36m(func pid=46161)[0m top5: 0.8708022388059702
[2m[36m(func pid=46161)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=46161)[0m f1_macro: 0.30028238863826673
[2m[36m(func pid=46161)[0m f1_weighted: 0.3761473397472099
[2m[36m(func pid=46161)[0m f1_per_class: [0.371, 0.442, 0.203, 0.275, 0.103, 0.322, 0.509, 0.261, 0.194, 0.323]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m top1: 0.26072761194029853
[2m[36m(func pid=47180)[0m top5: 0.7859141791044776
[2m[36m(func pid=47180)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=47180)[0m f1_macro: 0.24767020818671942
[2m[36m(func pid=47180)[0m f1_weighted: 0.25779917565195765
[2m[36m(func pid=47180)[0m f1_per_class: [0.364, 0.268, 0.177, 0.364, 0.18, 0.308, 0.128, 0.331, 0.166, 0.19]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.8567 | Steps: 4 | Val loss: 1.8512 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=39840)[0m top1: 0.19449626865671643
[2m[36m(func pid=39840)[0m top5: 0.6847014925373134
[2m[36m(func pid=39840)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=39840)[0m f1_macro: 0.17103450646401153
[2m[36m(func pid=39840)[0m f1_weighted: 0.20794185372479826
[2m[36m(func pid=39840)[0m f1_per_class: [0.188, 0.214, 0.17, 0.275, 0.049, 0.324, 0.127, 0.204, 0.027, 0.133]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.3508 | Steps: 4 | Val loss: 8.5895 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.9303 | Steps: 4 | Val loss: 2.0155 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=45540)[0m top1: 0.355410447761194
[2m[36m(func pid=45540)[0m top5: 0.8488805970149254
[2m[36m(func pid=45540)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=45540)[0m f1_macro: 0.2815784712525937
[2m[36m(func pid=45540)[0m f1_weighted: 0.38556576147322796
[2m[36m(func pid=45540)[0m f1_per_class: [0.405, 0.25, 0.162, 0.433, 0.074, 0.384, 0.492, 0.232, 0.17, 0.215]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.6591 | Steps: 4 | Val loss: 2.2121 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 12:10:49 (running for 00:12:55.84)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.672 |      0.171 |                   56 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.857 |      0.282 |                   34 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.721 |      0.3   |                   33 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.351 |      0.252 |                   30 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m top1: 0.26632462686567165
[2m[36m(func pid=47180)[0m top5: 0.7112873134328358
[2m[36m(func pid=47180)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=47180)[0m f1_macro: 0.25173505368104837
[2m[36m(func pid=47180)[0m f1_weighted: 0.29055034072029506
[2m[36m(func pid=47180)[0m f1_per_class: [0.194, 0.011, 0.471, 0.442, 0.19, 0.252, 0.338, 0.382, 0.133, 0.106]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m top1: 0.3558768656716418
[2m[36m(func pid=46161)[0m top5: 0.8666044776119403
[2m[36m(func pid=46161)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=46161)[0m f1_macro: 0.30460555829453145
[2m[36m(func pid=46161)[0m f1_weighted: 0.37003617566492375
[2m[36m(func pid=46161)[0m f1_per_class: [0.397, 0.453, 0.197, 0.261, 0.116, 0.372, 0.475, 0.26, 0.198, 0.317]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.8003 | Steps: 4 | Val loss: 1.8588 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=39840)[0m top1: 0.20289179104477612
[2m[36m(func pid=39840)[0m top5: 0.6996268656716418
[2m[36m(func pid=39840)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=39840)[0m f1_macro: 0.17456168789321166
[2m[36m(func pid=39840)[0m f1_weighted: 0.21929780651150274
[2m[36m(func pid=39840)[0m f1_per_class: [0.206, 0.227, 0.18, 0.299, 0.035, 0.326, 0.137, 0.189, 0.026, 0.121]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.6623 | Steps: 4 | Val loss: 7.4766 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8025 | Steps: 4 | Val loss: 1.8904 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=45540)[0m top1: 0.34654850746268656
[2m[36m(func pid=45540)[0m top5: 0.8470149253731343
[2m[36m(func pid=45540)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=45540)[0m f1_macro: 0.27306981175450173
[2m[36m(func pid=45540)[0m f1_weighted: 0.37870542301347915
[2m[36m(func pid=45540)[0m f1_per_class: [0.371, 0.238, 0.174, 0.408, 0.081, 0.386, 0.502, 0.239, 0.155, 0.178]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.6753 | Steps: 4 | Val loss: 2.1977 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=47180)[0m top1: 0.3344216417910448
[2m[36m(func pid=47180)[0m top5: 0.7835820895522388
[2m[36m(func pid=47180)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=47180)[0m f1_macro: 0.2910500477307172
[2m[36m(func pid=47180)[0m f1_weighted: 0.33213317203520887
[2m[36m(func pid=47180)[0m f1_per_class: [0.426, 0.042, 0.579, 0.518, 0.099, 0.173, 0.401, 0.371, 0.15, 0.153]
[2m[36m(func pid=47180)[0m 
== Status ==
Current time: 2024-01-07 12:10:55 (running for 00:13:01.09)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.659 |      0.175 |                   57 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.8   |      0.273 |                   35 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.93  |      0.305 |                   34 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.662 |      0.291 |                   31 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.39132462686567165
[2m[36m(func pid=46161)[0m top5: 0.8833955223880597
[2m[36m(func pid=46161)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=46161)[0m f1_macro: 0.32272700265531734
[2m[36m(func pid=46161)[0m f1_weighted: 0.421111697146775
[2m[36m(func pid=46161)[0m f1_per_class: [0.443, 0.431, 0.164, 0.453, 0.159, 0.403, 0.464, 0.267, 0.21, 0.235]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.8858 | Steps: 4 | Val loss: 1.8206 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=39840)[0m top1: 0.2178171641791045
[2m[36m(func pid=39840)[0m top5: 0.7122201492537313
[2m[36m(func pid=39840)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=39840)[0m f1_macro: 0.18333376006594276
[2m[36m(func pid=39840)[0m f1_weighted: 0.23613315802633683
[2m[36m(func pid=39840)[0m f1_per_class: [0.217, 0.218, 0.176, 0.335, 0.035, 0.351, 0.154, 0.185, 0.037, 0.126]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.4041 | Steps: 4 | Val loss: 5.3373 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6242 | Steps: 4 | Val loss: 1.9851 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=45540)[0m top1: 0.363339552238806
[2m[36m(func pid=45540)[0m top5: 0.8619402985074627
[2m[36m(func pid=45540)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=45540)[0m f1_macro: 0.281222770537262
[2m[36m(func pid=45540)[0m f1_weighted: 0.3908233687434795
[2m[36m(func pid=45540)[0m f1_per_class: [0.428, 0.224, 0.173, 0.456, 0.081, 0.392, 0.504, 0.202, 0.182, 0.172]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.6043 | Steps: 4 | Val loss: 2.2059 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 12:11:00 (running for 00:13:06.22)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.675 |      0.183 |                   58 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.886 |      0.281 |                   36 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.802 |      0.323 |                   35 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.404 |      0.388 |                   32 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m top1: 0.4076492537313433
[2m[36m(func pid=47180)[0m top5: 0.8689365671641791
[2m[36m(func pid=47180)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=47180)[0m f1_macro: 0.38824542978128795
[2m[36m(func pid=47180)[0m f1_weighted: 0.40578469339646683
[2m[36m(func pid=47180)[0m f1_per_class: [0.548, 0.434, 0.629, 0.508, 0.136, 0.237, 0.371, 0.417, 0.207, 0.396]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m top1: 0.37406716417910446
[2m[36m(func pid=46161)[0m top5: 0.8628731343283582
[2m[36m(func pid=46161)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=46161)[0m f1_macro: 0.3117067192692936
[2m[36m(func pid=46161)[0m f1_weighted: 0.40550670235819525
[2m[36m(func pid=46161)[0m f1_per_class: [0.436, 0.364, 0.164, 0.503, 0.146, 0.401, 0.396, 0.315, 0.23, 0.163]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.8433 | Steps: 4 | Val loss: 1.8069 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=39840)[0m top1: 0.21361940298507462
[2m[36m(func pid=39840)[0m top5: 0.7014925373134329
[2m[36m(func pid=39840)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=39840)[0m f1_macro: 0.18017807352634646
[2m[36m(func pid=39840)[0m f1_weighted: 0.2310508886780955
[2m[36m(func pid=39840)[0m f1_per_class: [0.219, 0.2, 0.165, 0.321, 0.039, 0.37, 0.155, 0.182, 0.025, 0.126]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.0906 | Steps: 4 | Val loss: 6.2412 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6372 | Steps: 4 | Val loss: 2.0504 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=45540)[0m top1: 0.3763992537313433
[2m[36m(func pid=45540)[0m top5: 0.8624067164179104
[2m[36m(func pid=45540)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=45540)[0m f1_macro: 0.2911057347780638
[2m[36m(func pid=45540)[0m f1_weighted: 0.40299613948184665
[2m[36m(func pid=45540)[0m f1_per_class: [0.421, 0.232, 0.2, 0.466, 0.09, 0.411, 0.519, 0.218, 0.192, 0.163]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.5918 | Steps: 4 | Val loss: 2.2046 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 12:11:05 (running for 00:13:11.78)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.604 |      0.18  |                   59 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.843 |      0.291 |                   37 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.624 |      0.312 |                   36 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.091 |      0.338 |                   33 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m top1: 0.3568097014925373
[2m[36m(func pid=47180)[0m top5: 0.808768656716418
[2m[36m(func pid=47180)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=47180)[0m f1_macro: 0.33776232322509564
[2m[36m(func pid=47180)[0m f1_weighted: 0.36020765576472824
[2m[36m(func pid=47180)[0m f1_per_class: [0.581, 0.472, 0.277, 0.322, 0.083, 0.319, 0.35, 0.394, 0.225, 0.356]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m top1: 0.3614738805970149
[2m[36m(func pid=46161)[0m top5: 0.851679104477612
[2m[36m(func pid=46161)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=46161)[0m f1_macro: 0.30029202044398096
[2m[36m(func pid=46161)[0m f1_weighted: 0.3898871065024047
[2m[36m(func pid=46161)[0m f1_per_class: [0.398, 0.326, 0.187, 0.513, 0.129, 0.388, 0.364, 0.315, 0.235, 0.148]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.8060 | Steps: 4 | Val loss: 1.7867 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=39840)[0m top1: 0.21222014925373134
[2m[36m(func pid=39840)[0m top5: 0.6972947761194029
[2m[36m(func pid=39840)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=39840)[0m f1_macro: 0.17945248586854298
[2m[36m(func pid=39840)[0m f1_weighted: 0.2319805053956979
[2m[36m(func pid=39840)[0m f1_per_class: [0.226, 0.193, 0.136, 0.327, 0.032, 0.376, 0.151, 0.199, 0.025, 0.129]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3414 | Steps: 4 | Val loss: 6.5270 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.5366 | Steps: 4 | Val loss: 2.1245 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=45540)[0m top1: 0.3894589552238806
[2m[36m(func pid=45540)[0m top5: 0.8642723880597015
[2m[36m(func pid=45540)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=45540)[0m f1_macro: 0.2961990144252432
[2m[36m(func pid=45540)[0m f1_weighted: 0.41240748992046233
[2m[36m(func pid=45540)[0m f1_per_class: [0.391, 0.234, 0.226, 0.503, 0.093, 0.409, 0.514, 0.245, 0.173, 0.174]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.6651 | Steps: 4 | Val loss: 2.1974 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 12:11:11 (running for 00:13:17.28)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.592 |      0.179 |                   60 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.806 |      0.296 |                   38 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.637 |      0.3   |                   37 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.341 |      0.286 |                   34 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m top1: 0.30736940298507465
[2m[36m(func pid=47180)[0m top5: 0.8208955223880597
[2m[36m(func pid=47180)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=47180)[0m f1_macro: 0.2864292681350863
[2m[36m(func pid=47180)[0m f1_weighted: 0.3366341647431145
[2m[36m(func pid=47180)[0m f1_per_class: [0.571, 0.178, 0.125, 0.466, 0.06, 0.318, 0.317, 0.391, 0.219, 0.22]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m top1: 0.34421641791044777
[2m[36m(func pid=46161)[0m top5: 0.8334888059701493
[2m[36m(func pid=46161)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=46161)[0m f1_macro: 0.29095719655350183
[2m[36m(func pid=46161)[0m f1_weighted: 0.3741917024157915
[2m[36m(func pid=46161)[0m f1_per_class: [0.372, 0.322, 0.203, 0.497, 0.137, 0.366, 0.341, 0.315, 0.197, 0.158]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.8675 | Steps: 4 | Val loss: 1.7917 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=39840)[0m top1: 0.21455223880597016
[2m[36m(func pid=39840)[0m top5: 0.7047574626865671
[2m[36m(func pid=39840)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=39840)[0m f1_macro: 0.1824872179092341
[2m[36m(func pid=39840)[0m f1_weighted: 0.23487271851161717
[2m[36m(func pid=39840)[0m f1_per_class: [0.226, 0.187, 0.129, 0.334, 0.031, 0.363, 0.162, 0.194, 0.026, 0.171]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.5239 | Steps: 4 | Val loss: 2.1595 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.7260 | Steps: 4 | Val loss: 6.5011 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=45540)[0m top1: 0.3805970149253731
[2m[36m(func pid=45540)[0m top5: 0.8647388059701493
[2m[36m(func pid=45540)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=45540)[0m f1_macro: 0.2971761929202753
[2m[36m(func pid=45540)[0m f1_weighted: 0.40386024619926636
[2m[36m(func pid=45540)[0m f1_per_class: [0.4, 0.237, 0.232, 0.493, 0.088, 0.404, 0.49, 0.251, 0.183, 0.193]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.6540 | Steps: 4 | Val loss: 2.1970 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 12:11:16 (running for 00:13:22.70)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.665 |      0.182 |                   61 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.867 |      0.297 |                   39 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.524 |      0.293 |                   39 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.341 |      0.286 |                   34 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3344216417910448
[2m[36m(func pid=46161)[0m top5: 0.835820895522388
[2m[36m(func pid=46161)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=46161)[0m f1_macro: 0.29286037283236077
[2m[36m(func pid=46161)[0m f1_weighted: 0.3620030474997641
[2m[36m(func pid=46161)[0m f1_per_class: [0.361, 0.332, 0.265, 0.478, 0.134, 0.378, 0.309, 0.308, 0.194, 0.169]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m top1: 0.3400186567164179
[2m[36m(func pid=47180)[0m top5: 0.8194962686567164
[2m[36m(func pid=47180)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=47180)[0m f1_macro: 0.29764682284268373
[2m[36m(func pid=47180)[0m f1_weighted: 0.35184732925530254
[2m[36m(func pid=47180)[0m f1_per_class: [0.54, 0.184, 0.096, 0.524, 0.122, 0.341, 0.294, 0.433, 0.211, 0.232]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.8911 | Steps: 4 | Val loss: 1.8114 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=39840)[0m top1: 0.2126865671641791
[2m[36m(func pid=39840)[0m top5: 0.7005597014925373
[2m[36m(func pid=39840)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=39840)[0m f1_macro: 0.18236302977459157
[2m[36m(func pid=39840)[0m f1_weighted: 0.23079502293051857
[2m[36m(func pid=39840)[0m f1_per_class: [0.211, 0.165, 0.151, 0.345, 0.033, 0.33, 0.157, 0.226, 0.033, 0.173]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.9938 | Steps: 4 | Val loss: 2.0781 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.4543 | Steps: 4 | Val loss: 7.0070 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=45540)[0m top1: 0.3689365671641791
[2m[36m(func pid=45540)[0m top5: 0.8493470149253731
[2m[36m(func pid=45540)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=45540)[0m f1_macro: 0.2872761423301289
[2m[36m(func pid=45540)[0m f1_weighted: 0.3945869281896067
[2m[36m(func pid=45540)[0m f1_per_class: [0.327, 0.247, 0.186, 0.493, 0.088, 0.411, 0.451, 0.276, 0.189, 0.204]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:11:22 (running for 00:13:27.93)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.654 |      0.182 |                   62 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.891 |      0.287 |                   40 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.994 |      0.31  |                   40 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.726 |      0.298 |                   35 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.6390 | Steps: 4 | Val loss: 2.2096 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=46161)[0m top1: 0.3493470149253731
[2m[36m(func pid=46161)[0m top5: 0.8484141791044776
[2m[36m(func pid=46161)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=46161)[0m f1_macro: 0.3097498072489552
[2m[36m(func pid=46161)[0m f1_weighted: 0.36229967603078594
[2m[36m(func pid=46161)[0m f1_per_class: [0.357, 0.375, 0.299, 0.477, 0.153, 0.386, 0.274, 0.321, 0.222, 0.234]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m top1: 0.3031716417910448
[2m[36m(func pid=47180)[0m top5: 0.8199626865671642
[2m[36m(func pid=47180)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=47180)[0m f1_macro: 0.25994499923959724
[2m[36m(func pid=47180)[0m f1_weighted: 0.3469597492556929
[2m[36m(func pid=47180)[0m f1_per_class: [0.44, 0.257, 0.124, 0.449, 0.1, 0.341, 0.358, 0.253, 0.159, 0.118]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.7113 | Steps: 4 | Val loss: 1.8174 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=39840)[0m top1: 0.2019589552238806
[2m[36m(func pid=39840)[0m top5: 0.6921641791044776
[2m[36m(func pid=39840)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=39840)[0m f1_macro: 0.17665297952762943
[2m[36m(func pid=39840)[0m f1_weighted: 0.2226780096976548
[2m[36m(func pid=39840)[0m f1_per_class: [0.227, 0.189, 0.132, 0.322, 0.03, 0.325, 0.143, 0.209, 0.031, 0.158]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.2274 | Steps: 4 | Val loss: 6.8788 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.7690 | Steps: 4 | Val loss: 1.9832 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=45540)[0m top1: 0.3614738805970149
[2m[36m(func pid=45540)[0m top5: 0.8428171641791045
[2m[36m(func pid=45540)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=45540)[0m f1_macro: 0.28587677268596334
[2m[36m(func pid=45540)[0m f1_weighted: 0.3908613704689344
[2m[36m(func pid=45540)[0m f1_per_class: [0.37, 0.257, 0.153, 0.461, 0.077, 0.442, 0.458, 0.223, 0.197, 0.221]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.36427238805970147
[2m[36m(func pid=46161)[0m top5: 0.8647388059701493
[2m[36m(func pid=46161)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=46161)[0m f1_macro: 0.32040734926053394
[2m[36m(func pid=46161)[0m f1_weighted: 0.3889420013868742
[2m[36m(func pid=46161)[0m f1_per_class: [0.446, 0.414, 0.208, 0.445, 0.156, 0.358, 0.379, 0.309, 0.214, 0.275]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m top1: 0.31296641791044777
[2m[36m(func pid=47180)[0m top5: 0.8362873134328358
[2m[36m(func pid=47180)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=47180)[0m f1_macro: 0.24923347599997073
[2m[36m(func pid=47180)[0m f1_weighted: 0.35702563620968025
[2m[36m(func pid=47180)[0m f1_per_class: [0.409, 0.369, 0.127, 0.367, 0.057, 0.322, 0.44, 0.124, 0.155, 0.123]
== Status ==
Current time: 2024-01-07 12:11:27 (running for 00:13:33.17)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.639 |      0.177 |                   63 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.711 |      0.286 |                   41 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.769 |      0.32  |                   41 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.454 |      0.26  |                   36 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6259 | Steps: 4 | Val loss: 2.2066 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.7628 | Steps: 4 | Val loss: 1.8106 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=39840)[0m top1: 0.20569029850746268
[2m[36m(func pid=39840)[0m top5: 0.6893656716417911
[2m[36m(func pid=39840)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=39840)[0m f1_macro: 0.18143457725259302
[2m[36m(func pid=39840)[0m f1_weighted: 0.22742458171531862
[2m[36m(func pid=39840)[0m f1_per_class: [0.24, 0.188, 0.137, 0.314, 0.035, 0.323, 0.163, 0.23, 0.031, 0.154]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.1303 | Steps: 4 | Val loss: 6.4760 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5230 | Steps: 4 | Val loss: 1.8495 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=45540)[0m top1: 0.3628731343283582
[2m[36m(func pid=45540)[0m top5: 0.8418843283582089
[2m[36m(func pid=45540)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=45540)[0m f1_macro: 0.28881440045781803
[2m[36m(func pid=45540)[0m f1_weighted: 0.3904168535765775
[2m[36m(func pid=45540)[0m f1_per_class: [0.346, 0.271, 0.154, 0.444, 0.094, 0.454, 0.455, 0.253, 0.197, 0.22]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:11:32 (running for 00:13:38.60)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.626 |      0.181 |                   64 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.763 |      0.289 |                   42 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.769 |      0.32  |                   41 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.13  |      0.277 |                   38 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m top1: 0.3414179104477612
[2m[36m(func pid=47180)[0m top5: 0.8017723880597015
[2m[36m(func pid=47180)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=47180)[0m f1_macro: 0.27734426401962076
[2m[36m(func pid=47180)[0m f1_weighted: 0.3562660886564158
[2m[36m(func pid=47180)[0m f1_per_class: [0.311, 0.47, 0.135, 0.436, 0.08, 0.337, 0.261, 0.371, 0.186, 0.186]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m top1: 0.41138059701492535
[2m[36m(func pid=46161)[0m top5: 0.9043843283582089
[2m[36m(func pid=46161)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=46161)[0m f1_macro: 0.3542849869312993
[2m[36m(func pid=46161)[0m f1_weighted: 0.4309964040986488
[2m[36m(func pid=46161)[0m f1_per_class: [0.569, 0.476, 0.218, 0.404, 0.121, 0.384, 0.504, 0.287, 0.238, 0.342]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6204 | Steps: 4 | Val loss: 2.2031 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.6505 | Steps: 4 | Val loss: 1.7879 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=39840)[0m top1: 0.20755597014925373
[2m[36m(func pid=39840)[0m top5: 0.6963619402985075
[2m[36m(func pid=39840)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=39840)[0m f1_macro: 0.18265488395072105
[2m[36m(func pid=39840)[0m f1_weighted: 0.22806546771084144
[2m[36m(func pid=39840)[0m f1_per_class: [0.215, 0.207, 0.15, 0.295, 0.036, 0.363, 0.162, 0.208, 0.024, 0.167]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.5688 | Steps: 4 | Val loss: 1.9110 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.1999 | Steps: 4 | Val loss: 6.4884 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=45540)[0m top1: 0.3726679104477612
[2m[36m(func pid=45540)[0m top5: 0.855410447761194
[2m[36m(func pid=45540)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=45540)[0m f1_macro: 0.30303795499805586
[2m[36m(func pid=45540)[0m f1_weighted: 0.4000566414122851
[2m[36m(func pid=45540)[0m f1_per_class: [0.385, 0.361, 0.148, 0.434, 0.102, 0.454, 0.438, 0.263, 0.203, 0.241]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=47180)[0m top1: 0.363339552238806
[2m[36m(func pid=47180)[0m top5: 0.8083022388059702
[2m[36m(func pid=47180)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=47180)[0m f1_macro: 0.2986423413314496
[2m[36m(func pid=47180)[0m f1_weighted: 0.3576247772700536
[2m[36m(func pid=47180)[0m f1_per_class: [0.458, 0.327, 0.144, 0.547, 0.099, 0.345, 0.225, 0.364, 0.207, 0.27]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m top1: 0.40298507462686567
[2m[36m(func pid=46161)[0m top5: 0.8894589552238806
[2m[36m(func pid=46161)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=46161)[0m f1_macro: 0.34771978111286156
[2m[36m(func pid=46161)[0m f1_weighted: 0.41904045721949335
[2m[36m(func pid=46161)[0m f1_per_class: [0.536, 0.497, 0.188, 0.372, 0.114, 0.38, 0.484, 0.284, 0.244, 0.377]
== Status ==
Current time: 2024-01-07 12:11:38 (running for 00:13:44.02)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.62  |      0.183 |                   65 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.65  |      0.303 |                   43 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.523 |      0.354 |                   42 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.2   |      0.299 |                   39 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.6015 | Steps: 4 | Val loss: 2.2008 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.7744 | Steps: 4 | Val loss: 1.8191 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.8014 | Steps: 4 | Val loss: 7.7865 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=39840)[0m top1: 0.20662313432835822
[2m[36m(func pid=39840)[0m top5: 0.6977611940298507
[2m[36m(func pid=39840)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=39840)[0m f1_macro: 0.1808471993307525
[2m[36m(func pid=39840)[0m f1_weighted: 0.226889143970192
[2m[36m(func pid=39840)[0m f1_per_class: [0.203, 0.223, 0.144, 0.289, 0.038, 0.351, 0.161, 0.204, 0.024, 0.171]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.6702 | Steps: 4 | Val loss: 2.0606 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=45540)[0m top1: 0.3530783582089552
[2m[36m(func pid=45540)[0m top5: 0.8488805970149254
[2m[36m(func pid=45540)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=45540)[0m f1_macro: 0.2920519833500105
[2m[36m(func pid=45540)[0m f1_weighted: 0.37905756412413655
[2m[36m(func pid=45540)[0m f1_per_class: [0.324, 0.396, 0.127, 0.373, 0.122, 0.45, 0.409, 0.272, 0.19, 0.256]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:11:43 (running for 00:13:49.26)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.601 |      0.181 |                   66 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.774 |      0.292 |                   44 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.569 |      0.348 |                   43 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.801 |      0.281 |                   40 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3736007462686567
[2m[36m(func pid=46161)[0m top5: 0.8610074626865671
[2m[36m(func pid=46161)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=46161)[0m f1_macro: 0.32521688033426904
[2m[36m(func pid=46161)[0m f1_weighted: 0.3866816996063023
[2m[36m(func pid=46161)[0m f1_per_class: [0.441, 0.497, 0.16, 0.355, 0.105, 0.355, 0.402, 0.323, 0.238, 0.376]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m top1: 0.27098880597014924
[2m[36m(func pid=47180)[0m top5: 0.8208955223880597
[2m[36m(func pid=47180)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=47180)[0m f1_macro: 0.28117081613629963
[2m[36m(func pid=47180)[0m f1_weighted: 0.3109797659144865
[2m[36m(func pid=47180)[0m f1_per_class: [0.564, 0.174, 0.184, 0.426, 0.037, 0.329, 0.275, 0.351, 0.176, 0.294]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.6922 | Steps: 4 | Val loss: 2.1937 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.5429 | Steps: 4 | Val loss: 1.8219 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6646 | Steps: 4 | Val loss: 2.1850 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.8380 | Steps: 4 | Val loss: 6.0399 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=39840)[0m top1: 0.2126865671641791
[2m[36m(func pid=39840)[0m top5: 0.710820895522388
[2m[36m(func pid=39840)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=39840)[0m f1_macro: 0.18689135037203472
[2m[36m(func pid=39840)[0m f1_weighted: 0.23023187434526404
[2m[36m(func pid=39840)[0m f1_per_class: [0.207, 0.224, 0.167, 0.303, 0.033, 0.345, 0.155, 0.227, 0.024, 0.184]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=45540)[0m top1: 0.353544776119403
[2m[36m(func pid=45540)[0m top5: 0.8409514925373134
[2m[36m(func pid=45540)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=45540)[0m f1_macro: 0.29950821779021963
[2m[36m(func pid=45540)[0m f1_weighted: 0.37610853055098076
[2m[36m(func pid=45540)[0m f1_per_class: [0.358, 0.408, 0.143, 0.386, 0.112, 0.444, 0.373, 0.313, 0.181, 0.277]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:11:48 (running for 00:13:54.55)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.692 |      0.187 |                   67 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.543 |      0.3   |                   45 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.665 |      0.313 |                   45 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.801 |      0.281 |                   40 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3516791044776119
[2m[36m(func pid=46161)[0m top5: 0.8428171641791045
[2m[36m(func pid=46161)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=46161)[0m f1_macro: 0.31254580020385786
[2m[36m(func pid=46161)[0m f1_weighted: 0.366982649086754
[2m[36m(func pid=46161)[0m f1_per_class: [0.441, 0.486, 0.167, 0.398, 0.117, 0.337, 0.312, 0.33, 0.214, 0.323]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m top1: 0.34095149253731344
[2m[36m(func pid=47180)[0m top5: 0.8624067164179104
[2m[36m(func pid=47180)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=47180)[0m f1_macro: 0.3003461898809539
[2m[36m(func pid=47180)[0m f1_weighted: 0.36739698409741406
[2m[36m(func pid=47180)[0m f1_per_class: [0.532, 0.291, 0.211, 0.375, 0.124, 0.369, 0.444, 0.268, 0.237, 0.151]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6017 | Steps: 4 | Val loss: 2.1940 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.8181 | Steps: 4 | Val loss: 1.8641 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.8003 | Steps: 4 | Val loss: 2.2945 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.8589 | Steps: 4 | Val loss: 5.5017 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=39840)[0m top1: 0.20708955223880596
[2m[36m(func pid=39840)[0m top5: 0.710820895522388
[2m[36m(func pid=39840)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=39840)[0m f1_macro: 0.19232909061483047
[2m[36m(func pid=39840)[0m f1_weighted: 0.22563683070427407
[2m[36m(func pid=39840)[0m f1_per_class: [0.208, 0.189, 0.22, 0.287, 0.048, 0.339, 0.174, 0.219, 0.045, 0.194]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=45540)[0m top1: 0.3283582089552239
[2m[36m(func pid=45540)[0m top5: 0.8190298507462687
[2m[36m(func pid=45540)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=45540)[0m f1_macro: 0.28518873639915193
[2m[36m(func pid=45540)[0m f1_weighted: 0.35058731822351497
[2m[36m(func pid=45540)[0m f1_per_class: [0.317, 0.409, 0.129, 0.363, 0.111, 0.411, 0.32, 0.326, 0.195, 0.272]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.322294776119403
[2m[36m(func pid=46161)[0m top5: 0.8362873134328358
[2m[36m(func pid=46161)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=46161)[0m f1_macro: 0.29484735166232845
[2m[36m(func pid=46161)[0m f1_weighted: 0.3394001595734801
[2m[36m(func pid=46161)[0m f1_per_class: [0.468, 0.417, 0.16, 0.42, 0.126, 0.362, 0.234, 0.325, 0.193, 0.244]
== Status ==
Current time: 2024-01-07 12:11:53 (running for 00:13:59.74)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.602 |      0.192 |                   68 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.818 |      0.285 |                   46 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.8   |      0.295 |                   46 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.838 |      0.3   |                   41 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m top1: 0.40625
[2m[36m(func pid=47180)[0m top5: 0.8931902985074627
[2m[36m(func pid=47180)[0m f1_micro: 0.40625
[2m[36m(func pid=47180)[0m f1_macro: 0.3193904283186358
[2m[36m(func pid=47180)[0m f1_weighted: 0.40806169704143286
[2m[36m(func pid=47180)[0m f1_per_class: [0.48, 0.434, 0.292, 0.345, 0.14, 0.383, 0.548, 0.141, 0.209, 0.222]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.6024 | Steps: 4 | Val loss: 2.1952 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.5659 | Steps: 4 | Val loss: 1.8482 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.6231 | Steps: 4 | Val loss: 2.3234 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.2652 | Steps: 4 | Val loss: 5.5575 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=39840)[0m top1: 0.21128731343283583
[2m[36m(func pid=39840)[0m top5: 0.7080223880597015
[2m[36m(func pid=39840)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=39840)[0m f1_macro: 0.19033008612193522
[2m[36m(func pid=39840)[0m f1_weighted: 0.2292287882126901
[2m[36m(func pid=39840)[0m f1_per_class: [0.23, 0.233, 0.195, 0.298, 0.044, 0.313, 0.163, 0.215, 0.021, 0.19]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=45540)[0m top1: 0.3423507462686567
[2m[36m(func pid=45540)[0m top5: 0.8292910447761194
[2m[36m(func pid=45540)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=45540)[0m f1_macro: 0.3009255418788288
[2m[36m(func pid=45540)[0m f1_weighted: 0.35761562442140055
[2m[36m(func pid=45540)[0m f1_per_class: [0.338, 0.44, 0.123, 0.376, 0.126, 0.44, 0.295, 0.338, 0.209, 0.324]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:11:59 (running for 00:14:04.96)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.602 |      0.19  |                   69 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.566 |      0.301 |                   47 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.623 |      0.289 |                   47 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.859 |      0.319 |                   42 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3087686567164179
[2m[36m(func pid=46161)[0m top5: 0.8418843283582089
[2m[36m(func pid=46161)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=46161)[0m f1_macro: 0.2887495731542275
[2m[36m(func pid=46161)[0m f1_weighted: 0.3225986044451257
[2m[36m(func pid=46161)[0m f1_per_class: [0.505, 0.319, 0.222, 0.448, 0.137, 0.379, 0.198, 0.329, 0.208, 0.142]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m top1: 0.427705223880597
[2m[36m(func pid=47180)[0m top5: 0.8894589552238806
[2m[36m(func pid=47180)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=47180)[0m f1_macro: 0.3736056899056719
[2m[36m(func pid=47180)[0m f1_weighted: 0.42064758906176153
[2m[36m(func pid=47180)[0m f1_per_class: [0.519, 0.489, 0.382, 0.405, 0.08, 0.248, 0.495, 0.338, 0.254, 0.525]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.6015 | Steps: 4 | Val loss: 2.1837 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.6434 | Steps: 4 | Val loss: 1.8474 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7741 | Steps: 4 | Val loss: 2.2786 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.8723 | Steps: 4 | Val loss: 7.2267 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=39840)[0m top1: 0.21361940298507462
[2m[36m(func pid=39840)[0m top5: 0.7215485074626866
[2m[36m(func pid=39840)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=39840)[0m f1_macro: 0.1893727061333687
[2m[36m(func pid=39840)[0m f1_weighted: 0.23771793973137437
[2m[36m(func pid=39840)[0m f1_per_class: [0.23, 0.211, 0.172, 0.311, 0.036, 0.287, 0.202, 0.214, 0.032, 0.199]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=45540)[0m top1: 0.333955223880597
[2m[36m(func pid=45540)[0m top5: 0.8306902985074627
[2m[36m(func pid=45540)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=45540)[0m f1_macro: 0.2931980432430807
[2m[36m(func pid=45540)[0m f1_weighted: 0.3514897615526906
[2m[36m(func pid=45540)[0m f1_per_class: [0.362, 0.413, 0.121, 0.379, 0.103, 0.433, 0.291, 0.333, 0.213, 0.284]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:12:04 (running for 00:14:10.11)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.601 |      0.189 |                   70 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.643 |      0.293 |                   48 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.774 |      0.297 |                   48 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.265 |      0.374 |                   43 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.32649253731343286
[2m[36m(func pid=46161)[0m top5: 0.8526119402985075
[2m[36m(func pid=46161)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=46161)[0m f1_macro: 0.2972693012705018
[2m[36m(func pid=46161)[0m f1_weighted: 0.3389908109979752
[2m[36m(func pid=46161)[0m f1_per_class: [0.515, 0.257, 0.215, 0.515, 0.19, 0.387, 0.222, 0.331, 0.208, 0.133]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m top1: 0.36800373134328357
[2m[36m(func pid=47180)[0m top5: 0.8097014925373134
[2m[36m(func pid=47180)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=47180)[0m f1_macro: 0.2995805759174707
[2m[36m(func pid=47180)[0m f1_weighted: 0.36956110692003535
[2m[36m(func pid=47180)[0m f1_per_class: [0.484, 0.178, 0.274, 0.57, 0.073, 0.138, 0.408, 0.339, 0.208, 0.324]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.5602 | Steps: 4 | Val loss: 2.1796 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.6479 | Steps: 4 | Val loss: 1.8651 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.1043 | Steps: 4 | Val loss: 2.2331 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.1439 | Steps: 4 | Val loss: 9.4553 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=39840)[0m top1: 0.2140858208955224
[2m[36m(func pid=39840)[0m top5: 0.7201492537313433
[2m[36m(func pid=39840)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=39840)[0m f1_macro: 0.18769938003397096
[2m[36m(func pid=39840)[0m f1_weighted: 0.2404322801159576
[2m[36m(func pid=39840)[0m f1_per_class: [0.202, 0.191, 0.168, 0.316, 0.034, 0.297, 0.214, 0.222, 0.034, 0.199]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=45540)[0m top1: 0.314365671641791
[2m[36m(func pid=45540)[0m top5: 0.8264925373134329
[2m[36m(func pid=45540)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=45540)[0m f1_macro: 0.28253051001318846
[2m[36m(func pid=45540)[0m f1_weighted: 0.3235733651516012
[2m[36m(func pid=45540)[0m f1_per_class: [0.389, 0.424, 0.153, 0.325, 0.098, 0.412, 0.253, 0.32, 0.183, 0.267]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.3381529850746269
[2m[36m(func pid=46161)[0m top5: 0.8418843283582089
[2m[36m(func pid=46161)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=46161)[0m f1_macro: 0.2970746393596782
[2m[36m(func pid=46161)[0m f1_weighted: 0.35515788024324957
[2m[36m(func pid=46161)[0m f1_per_class: [0.517, 0.267, 0.166, 0.529, 0.135, 0.375, 0.258, 0.344, 0.223, 0.156]
[2m[36m(func pid=46161)[0m 
== Status ==
Current time: 2024-01-07 12:12:09 (running for 00:14:15.56)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.56  |      0.188 |                   71 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.648 |      0.283 |                   49 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.104 |      0.297 |                   49 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.872 |      0.3   |                   44 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m top1: 0.2490671641791045
[2m[36m(func pid=47180)[0m top5: 0.7541977611940298
[2m[36m(func pid=47180)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=47180)[0m f1_macro: 0.20341592036035938
[2m[36m(func pid=47180)[0m f1_weighted: 0.28223546978044095
[2m[36m(func pid=47180)[0m f1_per_class: [0.136, 0.076, 0.203, 0.315, 0.079, 0.152, 0.463, 0.235, 0.159, 0.217]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.5958 | Steps: 4 | Val loss: 2.1768 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.6443 | Steps: 4 | Val loss: 1.8803 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.6661 | Steps: 4 | Val loss: 1.9781 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.1102 | Steps: 4 | Val loss: 8.6556 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=39840)[0m top1: 0.20755597014925373
[2m[36m(func pid=39840)[0m top5: 0.7290111940298507
[2m[36m(func pid=39840)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=39840)[0m f1_macro: 0.18405624607401808
[2m[36m(func pid=39840)[0m f1_weighted: 0.23196367837472184
[2m[36m(func pid=39840)[0m f1_per_class: [0.194, 0.186, 0.167, 0.293, 0.046, 0.316, 0.207, 0.195, 0.048, 0.19]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=45540)[0m top1: 0.3125
[2m[36m(func pid=45540)[0m top5: 0.8246268656716418
[2m[36m(func pid=45540)[0m f1_micro: 0.3125
[2m[36m(func pid=45540)[0m f1_macro: 0.27916471049891245
[2m[36m(func pid=45540)[0m f1_weighted: 0.32629225204184414
[2m[36m(func pid=45540)[0m f1_per_class: [0.368, 0.411, 0.134, 0.323, 0.094, 0.414, 0.27, 0.337, 0.191, 0.249]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:12:14 (running for 00:14:20.91)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.596 |      0.184 |                   72 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.644 |      0.279 |                   50 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.666 |      0.313 |                   50 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.144 |      0.203 |                   45 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.37173507462686567
[2m[36m(func pid=46161)[0m top5: 0.8801305970149254
[2m[36m(func pid=46161)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=46161)[0m f1_macro: 0.31281060481404155
[2m[36m(func pid=46161)[0m f1_weighted: 0.40386463163288966
[2m[36m(func pid=46161)[0m f1_per_class: [0.492, 0.336, 0.154, 0.498, 0.122, 0.359, 0.426, 0.296, 0.23, 0.215]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m top1: 0.25
[2m[36m(func pid=47180)[0m top5: 0.7667910447761194
[2m[36m(func pid=47180)[0m f1_micro: 0.25
[2m[36m(func pid=47180)[0m f1_macro: 0.2146444406534933
[2m[36m(func pid=47180)[0m f1_weighted: 0.28085768508008657
[2m[36m(func pid=47180)[0m f1_per_class: [0.136, 0.273, 0.111, 0.135, 0.134, 0.308, 0.449, 0.262, 0.17, 0.168]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.6117 | Steps: 4 | Val loss: 2.1667 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.5611 | Steps: 4 | Val loss: 1.8549 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.8116 | Steps: 4 | Val loss: 1.8364 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.7473 | Steps: 4 | Val loss: 7.4801 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=45540)[0m top1: 0.3269589552238806
[2m[36m(func pid=45540)[0m top5: 0.832089552238806
[2m[36m(func pid=45540)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=45540)[0m f1_macro: 0.29063049942238783
[2m[36m(func pid=45540)[0m f1_weighted: 0.3422010258459907
[2m[36m(func pid=45540)[0m f1_per_class: [0.391, 0.421, 0.133, 0.352, 0.097, 0.416, 0.283, 0.348, 0.207, 0.257]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=39840)[0m top1: 0.21875
[2m[36m(func pid=39840)[0m top5: 0.7439365671641791
[2m[36m(func pid=39840)[0m f1_micro: 0.21875
[2m[36m(func pid=39840)[0m f1_macro: 0.19017161724565695
[2m[36m(func pid=39840)[0m f1_weighted: 0.24710559164159765
[2m[36m(func pid=39840)[0m f1_per_class: [0.187, 0.215, 0.185, 0.305, 0.051, 0.306, 0.238, 0.167, 0.056, 0.193]
[2m[36m(func pid=39840)[0m 
== Status ==
Current time: 2024-01-07 12:12:20 (running for 00:14:26.23)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.612 |      0.19  |                   73 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.561 |      0.291 |                   51 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.812 |      0.333 |                   51 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.11  |      0.215 |                   46 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.40158582089552236
[2m[36m(func pid=46161)[0m top5: 0.9127798507462687
[2m[36m(func pid=46161)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=46161)[0m f1_macro: 0.3332693721689353
[2m[36m(func pid=46161)[0m f1_weighted: 0.4264150296821524
[2m[36m(func pid=46161)[0m f1_per_class: [0.489, 0.382, 0.215, 0.488, 0.146, 0.371, 0.48, 0.266, 0.257, 0.239]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m top1: 0.3260261194029851
[2m[36m(func pid=47180)[0m top5: 0.8106343283582089
[2m[36m(func pid=47180)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=47180)[0m f1_macro: 0.28210834317830924
[2m[36m(func pid=47180)[0m f1_weighted: 0.3419451421824931
[2m[36m(func pid=47180)[0m f1_per_class: [0.294, 0.473, 0.092, 0.353, 0.169, 0.367, 0.28, 0.325, 0.209, 0.26]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.5216 | Steps: 4 | Val loss: 1.8492 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.6126 | Steps: 4 | Val loss: 2.1769 | Batch size: 32 | lr: 0.0001 | Duration: 3.21s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.8687 | Steps: 4 | Val loss: 1.8903 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.9239 | Steps: 4 | Val loss: 7.3482 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=45540)[0m top1: 0.32742537313432835
[2m[36m(func pid=45540)[0m top5: 0.8316231343283582
[2m[36m(func pid=45540)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=45540)[0m f1_macro: 0.29814130741881106
[2m[36m(func pid=45540)[0m f1_weighted: 0.3389518671681711
[2m[36m(func pid=45540)[0m f1_per_class: [0.468, 0.438, 0.143, 0.356, 0.085, 0.395, 0.264, 0.316, 0.236, 0.28]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.4048507462686567
[2m[36m(func pid=46161)[0m top5: 0.9043843283582089
[2m[36m(func pid=46161)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=46161)[0m f1_macro: 0.34666671419439804
[2m[36m(func pid=46161)[0m f1_weighted: 0.42154813731342583
[2m[36m(func pid=46161)[0m f1_per_class: [0.432, 0.479, 0.28, 0.432, 0.142, 0.387, 0.446, 0.314, 0.24, 0.315]
== Status ==
Current time: 2024-01-07 12:12:25 (running for 00:14:31.38)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.612 |      0.19  |                   73 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.522 |      0.298 |                   52 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.869 |      0.347 |                   52 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.747 |      0.282 |                   47 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=39840)[0m top1: 0.20755597014925373
[2m[36m(func pid=39840)[0m top5: 0.7336753731343284
[2m[36m(func pid=39840)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=39840)[0m f1_macro: 0.18654158997939035
[2m[36m(func pid=39840)[0m f1_weighted: 0.232191137853822
[2m[36m(func pid=39840)[0m f1_per_class: [0.196, 0.2, 0.186, 0.266, 0.045, 0.312, 0.226, 0.192, 0.056, 0.187]
[2m[36m(func pid=39840)[0m 
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m top1: 0.40158582089552236
[2m[36m(func pid=47180)[0m top5: 0.8409514925373134
[2m[36m(func pid=47180)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=47180)[0m f1_macro: 0.34742298390538234
[2m[36m(func pid=47180)[0m f1_weighted: 0.35667107322308866
[2m[36m(func pid=47180)[0m f1_per_class: [0.571, 0.452, 0.236, 0.535, 0.145, 0.408, 0.118, 0.368, 0.22, 0.419]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.5334 | Steps: 4 | Val loss: 1.8181 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.7367 | Steps: 4 | Val loss: 1.9519 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=39840)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.5760 | Steps: 4 | Val loss: 2.1598 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.5447 | Steps: 4 | Val loss: 6.0472 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=45540)[0m top1: 0.3376865671641791
[2m[36m(func pid=45540)[0m top5: 0.840018656716418
[2m[36m(func pid=45540)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=45540)[0m f1_macro: 0.304422847151519
[2m[36m(func pid=45540)[0m f1_weighted: 0.3486986655945501
[2m[36m(func pid=45540)[0m f1_per_class: [0.467, 0.437, 0.157, 0.377, 0.085, 0.402, 0.274, 0.318, 0.242, 0.286]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:12:30 (running for 00:14:36.64)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.3305
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00004 | RUNNING    | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.613 |      0.187 |                   74 |
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.533 |      0.304 |                   53 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.737 |      0.347 |                   53 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.924 |      0.347 |                   48 |
| train_9b9e8_00008 | PENDING    |                    | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3903917910447761
[2m[36m(func pid=46161)[0m top5: 0.8899253731343284
[2m[36m(func pid=46161)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=46161)[0m f1_macro: 0.347437364159071
[2m[36m(func pid=46161)[0m f1_weighted: 0.40952894210107943
[2m[36m(func pid=46161)[0m f1_per_class: [0.509, 0.46, 0.333, 0.461, 0.12, 0.386, 0.387, 0.317, 0.223, 0.278]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=39840)[0m top1: 0.23134328358208955
[2m[36m(func pid=39840)[0m top5: 0.7509328358208955
[2m[36m(func pid=39840)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=39840)[0m f1_macro: 0.20147795907292937
[2m[36m(func pid=39840)[0m f1_weighted: 0.25765835143771665
[2m[36m(func pid=39840)[0m f1_per_class: [0.191, 0.226, 0.193, 0.299, 0.049, 0.35, 0.245, 0.215, 0.066, 0.18]
[2m[36m(func pid=47180)[0m top1: 0.42723880597014924
[2m[36m(func pid=47180)[0m top5: 0.8941231343283582
[2m[36m(func pid=47180)[0m f1_micro: 0.4272388059701493
[2m[36m(func pid=47180)[0m f1_macro: 0.37166806696605703
[2m[36m(func pid=47180)[0m f1_weighted: 0.42509892552538986
[2m[36m(func pid=47180)[0m f1_per_class: [0.611, 0.499, 0.228, 0.519, 0.13, 0.407, 0.335, 0.328, 0.299, 0.36]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.3733 | Steps: 4 | Val loss: 1.8181 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5806 | Steps: 4 | Val loss: 2.0382 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5272 | Steps: 4 | Val loss: 6.2046 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=45540)[0m top1: 0.345615671641791
[2m[36m(func pid=45540)[0m top5: 0.8386194029850746
[2m[36m(func pid=45540)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=45540)[0m f1_macro: 0.3087528732562934
[2m[36m(func pid=45540)[0m f1_weighted: 0.35600564535455215
[2m[36m(func pid=45540)[0m f1_per_class: [0.429, 0.431, 0.164, 0.402, 0.088, 0.406, 0.273, 0.344, 0.239, 0.311]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.36380597014925375
[2m[36m(func pid=46161)[0m top5: 0.8768656716417911
[2m[36m(func pid=46161)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=46161)[0m f1_macro: 0.32474469934546596
[2m[36m(func pid=46161)[0m f1_weighted: 0.38761432551368935
[2m[36m(func pid=46161)[0m f1_per_class: [0.42, 0.401, 0.324, 0.449, 0.143, 0.367, 0.373, 0.331, 0.211, 0.229]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m top1: 0.4006529850746269
[2m[36m(func pid=47180)[0m top5: 0.8931902985074627
[2m[36m(func pid=47180)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=47180)[0m f1_macro: 0.3223635558265968
[2m[36m(func pid=47180)[0m f1_weighted: 0.427529065142198
[2m[36m(func pid=47180)[0m f1_per_class: [0.557, 0.404, 0.149, 0.461, 0.104, 0.331, 0.517, 0.246, 0.226, 0.229]
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.4875 | Steps: 4 | Val loss: 1.7919 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.7140 | Steps: 4 | Val loss: 2.1451 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 12:12:35 (running for 00:14:41.81)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.373 |      0.309 |                   54 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.581 |      0.325 |                   54 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.545 |      0.372 |                   49 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=58891)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=58891)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=58891)[0m Configuration completed!
[2m[36m(func pid=58891)[0m New optimizer parameters:
[2m[36m(func pid=58891)[0m SGD (
[2m[36m(func pid=58891)[0m Parameter Group 0
[2m[36m(func pid=58891)[0m     dampening: 0
[2m[36m(func pid=58891)[0m     differentiable: False
[2m[36m(func pid=58891)[0m     foreach: None
[2m[36m(func pid=58891)[0m     lr: 0.0001
[2m[36m(func pid=58891)[0m     maximize: False
[2m[36m(func pid=58891)[0m     momentum: 0.99
[2m[36m(func pid=58891)[0m     nesterov: False
[2m[36m(func pid=58891)[0m     weight_decay: 0.0001
[2m[36m(func pid=58891)[0m )
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m top1: 0.36100746268656714
[2m[36m(func pid=45540)[0m top5: 0.8488805970149254
[2m[36m(func pid=45540)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=45540)[0m f1_macro: 0.3122415237558631
[2m[36m(func pid=45540)[0m f1_weighted: 0.37585366302315826
[2m[36m(func pid=45540)[0m f1_per_class: [0.446, 0.441, 0.168, 0.421, 0.103, 0.414, 0.32, 0.329, 0.19, 0.289]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:12:41 (running for 00:14:47.08)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.488 |      0.312 |                   55 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.714 |      0.31  |                   55 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.527 |      0.322 |                   50 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.34654850746268656
[2m[36m(func pid=46161)[0m top5: 0.8689365671641791
[2m[36m(func pid=46161)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=46161)[0m f1_macro: 0.30965218233694947
[2m[36m(func pid=46161)[0m f1_weighted: 0.37105080520957223
[2m[36m(func pid=46161)[0m f1_per_class: [0.359, 0.398, 0.301, 0.401, 0.164, 0.354, 0.369, 0.359, 0.209, 0.182]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.4702 | Steps: 4 | Val loss: 9.0965 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.4973 | Steps: 4 | Val loss: 1.8041 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.6222 | Steps: 4 | Val loss: 2.1861 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0050 | Steps: 4 | Val loss: 2.3177 | Batch size: 32 | lr: 0.0001 | Duration: 4.61s
[2m[36m(func pid=47180)[0m top1: 0.27845149253731344
[2m[36m(func pid=47180)[0m top5: 0.7994402985074627
[2m[36m(func pid=47180)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=47180)[0m f1_macro: 0.23134036275768755
[2m[36m(func pid=47180)[0m f1_weighted: 0.30716953008145537
[2m[36m(func pid=47180)[0m f1_per_class: [0.418, 0.31, 0.136, 0.231, 0.087, 0.138, 0.483, 0.211, 0.173, 0.126]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m top1: 0.35074626865671643
[2m[36m(func pid=45540)[0m top5: 0.8395522388059702
[2m[36m(func pid=45540)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=45540)[0m f1_macro: 0.299780113175002
[2m[36m(func pid=45540)[0m f1_weighted: 0.36611870402048907
[2m[36m(func pid=45540)[0m f1_per_class: [0.373, 0.392, 0.168, 0.431, 0.103, 0.417, 0.306, 0.35, 0.201, 0.256]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:12:46 (running for 00:14:52.31)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.497 |      0.3   |                   56 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.714 |      0.31  |                   55 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.47  |      0.231 |                   51 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  3.005 |      0.1   |                    1 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3474813432835821
[2m[36m(func pid=46161)[0m top5: 0.8614738805970149
[2m[36m(func pid=46161)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=46161)[0m f1_macro: 0.30701925813775244
[2m[36m(func pid=46161)[0m f1_weighted: 0.3651092129129679
[2m[36m(func pid=46161)[0m f1_per_class: [0.348, 0.42, 0.314, 0.322, 0.145, 0.363, 0.411, 0.339, 0.217, 0.191]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=58891)[0m top1: 0.17583955223880596
[2m[36m(func pid=58891)[0m top5: 0.534981343283582
[2m[36m(func pid=58891)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=58891)[0m f1_macro: 0.10022037586528325
[2m[36m(func pid=58891)[0m f1_weighted: 0.12389036451495639
[2m[36m(func pid=58891)[0m f1_per_class: [0.174, 0.31, 0.0, 0.102, 0.01, 0.289, 0.012, 0.011, 0.0, 0.094]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6760 | Steps: 4 | Val loss: 9.4215 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.3770 | Steps: 4 | Val loss: 1.8135 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.6098 | Steps: 4 | Val loss: 2.2165 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0017 | Steps: 4 | Val loss: 2.3459 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=47180)[0m top1: 0.24766791044776118
[2m[36m(func pid=47180)[0m top5: 0.7504664179104478
[2m[36m(func pid=47180)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=47180)[0m f1_macro: 0.20871121124199785
[2m[36m(func pid=47180)[0m f1_weighted: 0.280843150438354
[2m[36m(func pid=47180)[0m f1_per_class: [0.234, 0.271, 0.132, 0.257, 0.061, 0.091, 0.406, 0.302, 0.173, 0.16]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m top1: 0.3516791044776119
[2m[36m(func pid=45540)[0m top5: 0.8372201492537313
[2m[36m(func pid=45540)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=45540)[0m f1_macro: 0.3015657989840568
[2m[36m(func pid=45540)[0m f1_weighted: 0.3735409775481979
[2m[36m(func pid=45540)[0m f1_per_class: [0.387, 0.367, 0.153, 0.457, 0.094, 0.415, 0.319, 0.349, 0.238, 0.237]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:12:51 (running for 00:14:57.57)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.377 |      0.302 |                   57 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.61  |      0.3   |                   57 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.676 |      0.209 |                   52 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  3.005 |      0.1   |                    1 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3512126865671642
[2m[36m(func pid=46161)[0m top5: 0.8586753731343284
[2m[36m(func pid=46161)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=46161)[0m f1_macro: 0.3000682106466034
[2m[36m(func pid=46161)[0m f1_weighted: 0.3726912532261813
[2m[36m(func pid=46161)[0m f1_per_class: [0.378, 0.433, 0.184, 0.324, 0.143, 0.395, 0.421, 0.31, 0.208, 0.204]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=58891)[0m top1: 0.16930970149253732
[2m[36m(func pid=58891)[0m top5: 0.498134328358209
[2m[36m(func pid=58891)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=58891)[0m f1_macro: 0.09224649979763919
[2m[36m(func pid=58891)[0m f1_weighted: 0.12092128331200774
[2m[36m(func pid=58891)[0m f1_per_class: [0.128, 0.303, 0.0, 0.099, 0.009, 0.297, 0.009, 0.017, 0.0, 0.06]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.3511 | Steps: 4 | Val loss: 8.7656 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.3831 | Steps: 4 | Val loss: 1.8151 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7470 | Steps: 4 | Val loss: 2.2113 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9381 | Steps: 4 | Val loss: 2.3341 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=47180)[0m top1: 0.25093283582089554
[2m[36m(func pid=47180)[0m top5: 0.7649253731343284
[2m[36m(func pid=47180)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=47180)[0m f1_macro: 0.21494299485969007
[2m[36m(func pid=47180)[0m f1_weighted: 0.28676625527779176
[2m[36m(func pid=47180)[0m f1_per_class: [0.154, 0.207, 0.143, 0.395, 0.078, 0.167, 0.305, 0.307, 0.19, 0.204]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m top1: 0.3498134328358209
[2m[36m(func pid=45540)[0m top5: 0.8311567164179104
[2m[36m(func pid=45540)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=45540)[0m f1_macro: 0.29466039156015217
[2m[36m(func pid=45540)[0m f1_weighted: 0.3735455944990165
[2m[36m(func pid=45540)[0m f1_per_class: [0.394, 0.336, 0.157, 0.486, 0.093, 0.408, 0.321, 0.323, 0.21, 0.219]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:12:56 (running for 00:15:02.84)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.383 |      0.295 |                   58 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.61  |      0.3   |                   57 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.351 |      0.215 |                   53 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.938 |      0.085 |                    3 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.3474813432835821
[2m[36m(func pid=46161)[0m top5: 0.875
[2m[36m(func pid=46161)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=46161)[0m f1_macro: 0.29690092474624324
[2m[36m(func pid=46161)[0m f1_weighted: 0.3789685444227941
[2m[36m(func pid=46161)[0m f1_per_class: [0.437, 0.417, 0.081, 0.308, 0.113, 0.411, 0.463, 0.269, 0.221, 0.247]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=58891)[0m top1: 0.15858208955223882
[2m[36m(func pid=58891)[0m top5: 0.5125932835820896
[2m[36m(func pid=58891)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=58891)[0m f1_macro: 0.08526930492431065
[2m[36m(func pid=58891)[0m f1_weighted: 0.1248893466694402
[2m[36m(func pid=58891)[0m f1_per_class: [0.094, 0.267, 0.0, 0.126, 0.0, 0.289, 0.024, 0.023, 0.0, 0.029]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.4642 | Steps: 4 | Val loss: 1.8142 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.9737 | Steps: 4 | Val loss: 8.3630 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5239 | Steps: 4 | Val loss: 2.2785 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9029 | Steps: 4 | Val loss: 2.3242 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=45540)[0m top1: 0.34841417910447764
[2m[36m(func pid=45540)[0m top5: 0.8386194029850746
[2m[36m(func pid=45540)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=45540)[0m f1_macro: 0.29503699480155154
[2m[36m(func pid=45540)[0m f1_weighted: 0.37100183984184987
[2m[36m(func pid=45540)[0m f1_per_class: [0.36, 0.31, 0.189, 0.472, 0.095, 0.428, 0.333, 0.324, 0.224, 0.216]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=47180)[0m top1: 0.341884328358209
[2m[36m(func pid=47180)[0m top5: 0.8036380597014925
[2m[36m(func pid=47180)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=47180)[0m f1_macro: 0.2629910076973429
[2m[36m(func pid=47180)[0m f1_weighted: 0.33077185486722294
[2m[36m(func pid=47180)[0m f1_per_class: [0.359, 0.095, 0.123, 0.593, 0.1, 0.311, 0.255, 0.313, 0.222, 0.258]
[2m[36m(func pid=47180)[0m 
== Status ==
Current time: 2024-01-07 12:13:02 (running for 00:15:08.09)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.464 |      0.295 |                   59 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.747 |      0.297 |                   58 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.974 |      0.263 |                   54 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.903 |      0.081 |                    4 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.15578358208955223
[2m[36m(func pid=58891)[0m top5: 0.5219216417910447
[2m[36m(func pid=58891)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=58891)[0m f1_macro: 0.08133967618786428
[2m[36m(func pid=58891)[0m f1_weighted: 0.13384917881002392
[2m[36m(func pid=58891)[0m f1_per_class: [0.032, 0.236, 0.0, 0.139, 0.007, 0.298, 0.057, 0.044, 0.0, 0.0]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=46161)[0m top1: 0.34421641791044777
[2m[36m(func pid=46161)[0m top5: 0.8782649253731343
[2m[36m(func pid=46161)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=46161)[0m f1_macro: 0.29997488809817685
[2m[36m(func pid=46161)[0m f1_weighted: 0.36843473978959335
[2m[36m(func pid=46161)[0m f1_per_class: [0.476, 0.459, 0.071, 0.271, 0.108, 0.397, 0.45, 0.207, 0.224, 0.336]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.7539 | Steps: 4 | Val loss: 7.4797 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.5936 | Steps: 4 | Val loss: 1.8027 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5193 | Steps: 4 | Val loss: 2.2329 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9411 | Steps: 4 | Val loss: 2.3031 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=45540)[0m top1: 0.3521455223880597
[2m[36m(func pid=45540)[0m top5: 0.8395522388059702
[2m[36m(func pid=45540)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=45540)[0m f1_macro: 0.2978604902635337
[2m[36m(func pid=45540)[0m f1_weighted: 0.3770765272831565
[2m[36m(func pid=45540)[0m f1_per_class: [0.368, 0.34, 0.18, 0.477, 0.09, 0.415, 0.334, 0.324, 0.228, 0.22]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=47180)[0m top1: 0.34841417910447764
[2m[36m(func pid=47180)[0m top5: 0.8535447761194029
[2m[36m(func pid=47180)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=47180)[0m f1_macro: 0.28432493047698726
[2m[36m(func pid=47180)[0m f1_weighted: 0.37482785093165905
[2m[36m(func pid=47180)[0m f1_per_class: [0.427, 0.175, 0.076, 0.556, 0.094, 0.297, 0.404, 0.239, 0.234, 0.342]
[2m[36m(func pid=47180)[0m 
== Status ==
Current time: 2024-01-07 12:13:07 (running for 00:15:13.46)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.594 |      0.298 |                   60 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.519 |      0.298 |                   60 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.754 |      0.284 |                   55 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.903 |      0.081 |                    4 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.34701492537313433
[2m[36m(func pid=46161)[0m top5: 0.8675373134328358
[2m[36m(func pid=46161)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=46161)[0m f1_macro: 0.29813551875364375
[2m[36m(func pid=46161)[0m f1_weighted: 0.3843280532296068
[2m[36m(func pid=46161)[0m f1_per_class: [0.39, 0.436, 0.071, 0.35, 0.102, 0.38, 0.449, 0.244, 0.215, 0.344]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=58891)[0m top1: 0.15811567164179105
[2m[36m(func pid=58891)[0m top5: 0.5494402985074627
[2m[36m(func pid=58891)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=58891)[0m f1_macro: 0.09811099187563117
[2m[36m(func pid=58891)[0m f1_weighted: 0.14342746653289626
[2m[36m(func pid=58891)[0m f1_per_class: [0.029, 0.235, 0.121, 0.15, 0.013, 0.295, 0.075, 0.064, 0.0, 0.0]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.4874 | Steps: 4 | Val loss: 1.7770 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.2349 | Steps: 4 | Val loss: 7.3963 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.3365 | Steps: 4 | Val loss: 2.1327 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9430 | Steps: 4 | Val loss: 2.3056 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=45540)[0m top1: 0.36473880597014924
[2m[36m(func pid=45540)[0m top5: 0.8465485074626866
[2m[36m(func pid=45540)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=45540)[0m f1_macro: 0.3055937042187113
[2m[36m(func pid=45540)[0m f1_weighted: 0.38683734437018513
[2m[36m(func pid=45540)[0m f1_per_class: [0.404, 0.344, 0.168, 0.497, 0.103, 0.415, 0.344, 0.332, 0.213, 0.236]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=47180)[0m top1: 0.33908582089552236
[2m[36m(func pid=47180)[0m top5: 0.8414179104477612
[2m[36m(func pid=47180)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=47180)[0m f1_macro: 0.3020092776670743
[2m[36m(func pid=47180)[0m f1_weighted: 0.35544459554099883
[2m[36m(func pid=47180)[0m f1_per_class: [0.475, 0.49, 0.123, 0.279, 0.096, 0.273, 0.418, 0.249, 0.229, 0.388]
[2m[36m(func pid=47180)[0m 
== Status ==
Current time: 2024-01-07 12:13:12 (running for 00:15:18.79)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.487 |      0.306 |                   61 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.337 |      0.299 |                   61 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.235 |      0.302 |                   56 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.941 |      0.098 |                    5 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=46161)[0m top1: 0.36380597014925375
[2m[36m(func pid=46161)[0m top5: 0.8638059701492538
[2m[36m(func pid=46161)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=46161)[0m f1_macro: 0.29884557168227543
[2m[36m(func pid=46161)[0m f1_weighted: 0.3953687354175514
[2m[36m(func pid=46161)[0m f1_per_class: [0.352, 0.398, 0.123, 0.467, 0.114, 0.363, 0.408, 0.244, 0.207, 0.312]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=58891)[0m top1: 0.15578358208955223
[2m[36m(func pid=58891)[0m top5: 0.5475746268656716
[2m[36m(func pid=58891)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=58891)[0m f1_macro: 0.0989920497464474
[2m[36m(func pid=58891)[0m f1_weighted: 0.14210345224049448
[2m[36m(func pid=58891)[0m f1_per_class: [0.027, 0.238, 0.118, 0.134, 0.012, 0.302, 0.08, 0.065, 0.015, 0.0]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.5567 | Steps: 4 | Val loss: 1.7830 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.2255 | Steps: 4 | Val loss: 9.3990 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9257 | Steps: 4 | Val loss: 2.3024 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.5193 | Steps: 4 | Val loss: 2.0857 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=45540)[0m top1: 0.35774253731343286
[2m[36m(func pid=45540)[0m top5: 0.8470149253731343
[2m[36m(func pid=45540)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=45540)[0m f1_macro: 0.3064643821761293
[2m[36m(func pid=45540)[0m f1_weighted: 0.38079037220987444
[2m[36m(func pid=45540)[0m f1_per_class: [0.415, 0.339, 0.149, 0.488, 0.089, 0.421, 0.328, 0.337, 0.253, 0.249]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=47180)[0m top1: 0.2868470149253731
[2m[36m(func pid=47180)[0m top5: 0.7411380597014925
[2m[36m(func pid=47180)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=47180)[0m f1_macro: 0.2581875939491706
[2m[36m(func pid=47180)[0m f1_weighted: 0.2530784913301333
[2m[36m(func pid=47180)[0m f1_per_class: [0.3, 0.484, 0.215, 0.14, 0.095, 0.213, 0.237, 0.261, 0.245, 0.392]
[2m[36m(func pid=47180)[0m 
== Status ==
Current time: 2024-01-07 12:13:18 (running for 00:15:23.97)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.557 |      0.306 |                   62 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.337 |      0.299 |                   61 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.225 |      0.258 |                   57 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.926 |      0.107 |                    7 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.1609141791044776
[2m[36m(func pid=58891)[0m top5: 0.5517723880597015
[2m[36m(func pid=58891)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=58891)[0m f1_macro: 0.10727015809644513
[2m[36m(func pid=58891)[0m f1_weighted: 0.15287815757367218
[2m[36m(func pid=58891)[0m f1_per_class: [0.038, 0.243, 0.118, 0.149, 0.027, 0.303, 0.093, 0.088, 0.014, 0.0]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=46161)[0m top1: 0.375
[2m[36m(func pid=46161)[0m top5: 0.8689365671641791
[2m[36m(func pid=46161)[0m f1_micro: 0.375
[2m[36m(func pid=46161)[0m f1_macro: 0.2975253322090133
[2m[36m(func pid=46161)[0m f1_weighted: 0.3977801203557823
[2m[36m(func pid=46161)[0m f1_per_class: [0.366, 0.344, 0.186, 0.518, 0.108, 0.347, 0.405, 0.245, 0.214, 0.242]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.4562 | Steps: 4 | Val loss: 1.7553 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.0188 | Steps: 4 | Val loss: 8.7990 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9279 | Steps: 4 | Val loss: 2.2804 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.8339 | Steps: 4 | Val loss: 1.9677 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=47180)[0m top1: 0.2891791044776119
[2m[36m(func pid=47180)[0m top5: 0.753731343283582
[2m[36m(func pid=47180)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=47180)[0m f1_macro: 0.2504485455761665
[2m[36m(func pid=47180)[0m f1_weighted: 0.27616152219351364
[2m[36m(func pid=47180)[0m f1_per_class: [0.223, 0.482, 0.253, 0.264, 0.118, 0.157, 0.23, 0.27, 0.214, 0.294]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m top1: 0.3666044776119403
[2m[36m(func pid=45540)[0m top5: 0.8591417910447762
[2m[36m(func pid=45540)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=45540)[0m f1_macro: 0.31380159685591347
[2m[36m(func pid=45540)[0m f1_weighted: 0.39050058209623106
[2m[36m(func pid=45540)[0m f1_per_class: [0.44, 0.371, 0.143, 0.478, 0.094, 0.419, 0.35, 0.329, 0.262, 0.253]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:13:23 (running for 00:15:29.34)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.456 |      0.314 |                   63 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.519 |      0.298 |                   62 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.019 |      0.25  |                   58 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.928 |      0.102 |                    8 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.16277985074626866
[2m[36m(func pid=58891)[0m top5: 0.5951492537313433
[2m[36m(func pid=58891)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=58891)[0m f1_macro: 0.10187443983435744
[2m[36m(func pid=58891)[0m f1_weighted: 0.1548322071279976
[2m[36m(func pid=58891)[0m f1_per_class: [0.024, 0.247, 0.095, 0.156, 0.012, 0.31, 0.092, 0.082, 0.0, 0.0]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=46161)[0m top1: 0.3987873134328358
[2m[36m(func pid=46161)[0m top5: 0.8871268656716418
[2m[36m(func pid=46161)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=46161)[0m f1_macro: 0.3132787662137185
[2m[36m(func pid=46161)[0m f1_weighted: 0.4150055898927374
[2m[36m(func pid=46161)[0m f1_per_class: [0.407, 0.299, 0.28, 0.549, 0.096, 0.319, 0.463, 0.258, 0.229, 0.233]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.8380 | Steps: 4 | Val loss: 6.7799 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.5352 | Steps: 4 | Val loss: 1.7390 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9006 | Steps: 4 | Val loss: 2.2803 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5464 | Steps: 4 | Val loss: 1.9076 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=47180)[0m top1: 0.37966417910447764
[2m[36m(func pid=47180)[0m top5: 0.8381529850746269
[2m[36m(func pid=47180)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=47180)[0m f1_macro: 0.3041611420208952
[2m[36m(func pid=47180)[0m f1_weighted: 0.4080413314188775
[2m[36m(func pid=47180)[0m f1_per_class: [0.211, 0.471, 0.4, 0.482, 0.121, 0.201, 0.461, 0.245, 0.239, 0.211]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m top1: 0.36986940298507465
[2m[36m(func pid=45540)[0m top5: 0.867070895522388
[2m[36m(func pid=45540)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=45540)[0m f1_macro: 0.3176398882756967
[2m[36m(func pid=45540)[0m f1_weighted: 0.39704325100472365
[2m[36m(func pid=45540)[0m f1_per_class: [0.465, 0.361, 0.154, 0.479, 0.11, 0.411, 0.38, 0.329, 0.242, 0.246]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:13:28 (running for 00:15:34.46)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.535 |      0.318 |                   64 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.834 |      0.313 |                   63 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.838 |      0.304 |                   59 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.901 |      0.11  |                    9 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.15904850746268656
[2m[36m(func pid=58891)[0m top5: 0.5956156716417911
[2m[36m(func pid=58891)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=58891)[0m f1_macro: 0.10981919613831877
[2m[36m(func pid=58891)[0m f1_weighted: 0.1540001403418255
[2m[36m(func pid=58891)[0m f1_per_class: [0.057, 0.248, 0.136, 0.167, 0.029, 0.292, 0.082, 0.074, 0.013, 0.0]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=46161)[0m top1: 0.40951492537313433
[2m[36m(func pid=46161)[0m top5: 0.894589552238806
[2m[36m(func pid=46161)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=46161)[0m f1_macro: 0.33022165637748135
[2m[36m(func pid=46161)[0m f1_weighted: 0.4272498240846737
[2m[36m(func pid=46161)[0m f1_per_class: [0.455, 0.288, 0.333, 0.547, 0.121, 0.3, 0.508, 0.293, 0.229, 0.228]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4327 | Steps: 4 | Val loss: 6.2643 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8916 | Steps: 4 | Val loss: 2.2761 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.4790 | Steps: 4 | Val loss: 1.7071 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.7363 | Steps: 4 | Val loss: 1.9366 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=47180)[0m top1: 0.4221082089552239
[2m[36m(func pid=47180)[0m top5: 0.8694029850746269
[2m[36m(func pid=47180)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=47180)[0m f1_macro: 0.32588279698218836
[2m[36m(func pid=47180)[0m f1_weighted: 0.4265360011407863
[2m[36m(func pid=47180)[0m f1_per_class: [0.274, 0.335, 0.588, 0.524, 0.179, 0.194, 0.574, 0.139, 0.253, 0.197]
[2m[36m(func pid=47180)[0m 
== Status ==
Current time: 2024-01-07 12:13:33 (running for 00:15:39.71)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.535 |      0.318 |                   64 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.546 |      0.33  |                   64 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.433 |      0.326 |                   60 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.892 |      0.117 |                   10 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.16417910447761194
[2m[36m(func pid=58891)[0m top5: 0.613339552238806
[2m[36m(func pid=58891)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=58891)[0m f1_macro: 0.11650300690471341
[2m[36m(func pid=58891)[0m f1_weighted: 0.1672831667347129
[2m[36m(func pid=58891)[0m f1_per_class: [0.054, 0.241, 0.136, 0.199, 0.035, 0.283, 0.098, 0.106, 0.013, 0.0]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m top1: 0.3773320895522388
[2m[36m(func pid=45540)[0m top5: 0.8782649253731343
[2m[36m(func pid=45540)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=45540)[0m f1_macro: 0.31436468336722206
[2m[36m(func pid=45540)[0m f1_weighted: 0.40643993133955014
[2m[36m(func pid=45540)[0m f1_per_class: [0.42, 0.359, 0.154, 0.473, 0.138, 0.406, 0.427, 0.33, 0.199, 0.237]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.39552238805970147
[2m[36m(func pid=46161)[0m top5: 0.8917910447761194
[2m[36m(func pid=46161)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=46161)[0m f1_macro: 0.3342484113042033
[2m[36m(func pid=46161)[0m f1_weighted: 0.4246027361534631
[2m[36m(func pid=46161)[0m f1_per_class: [0.489, 0.354, 0.356, 0.513, 0.108, 0.324, 0.484, 0.282, 0.223, 0.209]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.9145 | Steps: 4 | Val loss: 6.4298 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8342 | Steps: 4 | Val loss: 2.2545 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.3792 | Steps: 4 | Val loss: 1.7160 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.5268 | Steps: 4 | Val loss: 2.0067 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=47180)[0m top1: 0.4048507462686567
[2m[36m(func pid=47180)[0m top5: 0.8722014925373134
[2m[36m(func pid=47180)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=47180)[0m f1_macro: 0.337941675440523
[2m[36m(func pid=47180)[0m f1_weighted: 0.4052451517786323
[2m[36m(func pid=47180)[0m f1_per_class: [0.378, 0.266, 0.714, 0.597, 0.111, 0.212, 0.442, 0.262, 0.187, 0.208]
[2m[36m(func pid=47180)[0m 
== Status ==
Current time: 2024-01-07 12:13:39 (running for 00:15:44.97)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.479 |      0.314 |                   65 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.736 |      0.334 |                   65 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.915 |      0.338 |                   61 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.834 |      0.125 |                   11 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.17350746268656717
[2m[36m(func pid=58891)[0m top5: 0.6403917910447762
[2m[36m(func pid=58891)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=58891)[0m f1_macro: 0.1251975544754928
[2m[36m(func pid=58891)[0m f1_weighted: 0.1809579301135407
[2m[36m(func pid=58891)[0m f1_per_class: [0.067, 0.24, 0.146, 0.219, 0.035, 0.29, 0.119, 0.122, 0.014, 0.0]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m top1: 0.37453358208955223
[2m[36m(func pid=45540)[0m top5: 0.8722014925373134
[2m[36m(func pid=45540)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=45540)[0m f1_macro: 0.3200175912005932
[2m[36m(func pid=45540)[0m f1_weighted: 0.40307945108045673
[2m[36m(func pid=45540)[0m f1_per_class: [0.466, 0.366, 0.183, 0.471, 0.116, 0.416, 0.402, 0.351, 0.208, 0.22]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.3689365671641791
[2m[36m(func pid=46161)[0m top5: 0.8810634328358209
[2m[36m(func pid=46161)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=46161)[0m f1_macro: 0.32980355412079326
[2m[36m(func pid=46161)[0m f1_weighted: 0.397961174071893
[2m[36m(func pid=46161)[0m f1_per_class: [0.508, 0.388, 0.351, 0.399, 0.119, 0.36, 0.467, 0.284, 0.218, 0.204]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.2630 | Steps: 4 | Val loss: 9.9656 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8174 | Steps: 4 | Val loss: 2.2478 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.4450 | Steps: 4 | Val loss: 1.7378 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4566 | Steps: 4 | Val loss: 2.1415 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=47180)[0m top1: 0.2891791044776119
[2m[36m(func pid=47180)[0m top5: 0.7252798507462687
[2m[36m(func pid=47180)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=47180)[0m f1_macro: 0.2791294276727669
[2m[36m(func pid=47180)[0m f1_weighted: 0.2730600041550944
[2m[36m(func pid=47180)[0m f1_per_class: [0.135, 0.307, 0.647, 0.535, 0.146, 0.186, 0.048, 0.307, 0.206, 0.275]
[2m[36m(func pid=47180)[0m 
== Status ==
Current time: 2024-01-07 12:13:44 (running for 00:15:50.15)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.379 |      0.32  |                   66 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.527 |      0.33  |                   66 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.263 |      0.279 |                   62 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.817 |      0.134 |                   12 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.18050373134328357
[2m[36m(func pid=58891)[0m top5: 0.648320895522388
[2m[36m(func pid=58891)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=58891)[0m f1_macro: 0.13359294698862573
[2m[36m(func pid=58891)[0m f1_weighted: 0.18633517700498312
[2m[36m(func pid=58891)[0m f1_per_class: [0.091, 0.234, 0.158, 0.229, 0.039, 0.306, 0.118, 0.148, 0.013, 0.0]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m top1: 0.37220149253731344
[2m[36m(func pid=45540)[0m top5: 0.8619402985074627
[2m[36m(func pid=45540)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=45540)[0m f1_macro: 0.3205722750864549
[2m[36m(func pid=45540)[0m f1_weighted: 0.4013546459523582
[2m[36m(func pid=45540)[0m f1_per_class: [0.511, 0.377, 0.174, 0.48, 0.103, 0.403, 0.389, 0.309, 0.243, 0.218]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.3572761194029851
[2m[36m(func pid=46161)[0m top5: 0.8619402985074627
[2m[36m(func pid=46161)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=46161)[0m f1_macro: 0.32463666798817437
[2m[36m(func pid=46161)[0m f1_weighted: 0.373451419557043
[2m[36m(func pid=46161)[0m f1_per_class: [0.496, 0.414, 0.377, 0.292, 0.137, 0.354, 0.47, 0.281, 0.259, 0.167]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.9037 | Steps: 4 | Val loss: 11.5183 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.7770 | Steps: 4 | Val loss: 2.2230 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.3750 | Steps: 4 | Val loss: 1.7432 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.8396 | Steps: 4 | Val loss: 2.1691 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:13:49 (running for 00:15:55.34)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.445 |      0.321 |                   67 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.457 |      0.325 |                   67 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.263 |      0.279 |                   62 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.777 |      0.138 |                   13 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m top1: 0.27238805970149255
[2m[36m(func pid=47180)[0m top5: 0.6464552238805971
[2m[36m(func pid=47180)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=47180)[0m f1_macro: 0.2492396249967455
[2m[36m(func pid=47180)[0m f1_weighted: 0.2551342944778189
[2m[36m(func pid=47180)[0m f1_per_class: [0.165, 0.421, 0.355, 0.45, 0.1, 0.166, 0.015, 0.29, 0.205, 0.325]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=58891)[0m top1: 0.19636194029850745
[2m[36m(func pid=58891)[0m top5: 0.6847014925373134
[2m[36m(func pid=58891)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=58891)[0m f1_macro: 0.1382745829624273
[2m[36m(func pid=58891)[0m f1_weighted: 0.20162613224107348
[2m[36m(func pid=58891)[0m f1_per_class: [0.102, 0.246, 0.136, 0.251, 0.038, 0.34, 0.132, 0.137, 0.0, 0.0]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m top1: 0.365205223880597
[2m[36m(func pid=45540)[0m top5: 0.8619402985074627
[2m[36m(func pid=45540)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=45540)[0m f1_macro: 0.3130883977022787
[2m[36m(func pid=45540)[0m f1_weighted: 0.40171016707764123
[2m[36m(func pid=45540)[0m f1_per_class: [0.488, 0.363, 0.15, 0.467, 0.095, 0.423, 0.408, 0.3, 0.224, 0.212]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.365205223880597
[2m[36m(func pid=46161)[0m top5: 0.8773320895522388
[2m[36m(func pid=46161)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=46161)[0m f1_macro: 0.32803313147584295
[2m[36m(func pid=46161)[0m f1_weighted: 0.3719905309703528
[2m[36m(func pid=46161)[0m f1_per_class: [0.514, 0.428, 0.393, 0.254, 0.16, 0.336, 0.498, 0.286, 0.236, 0.175]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.9526 | Steps: 4 | Val loss: 11.0979 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8130 | Steps: 4 | Val loss: 2.2057 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.4951 | Steps: 4 | Val loss: 1.7300 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.6960 | Steps: 4 | Val loss: 2.1352 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 12:13:54 (running for 00:16:00.81)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.375 |      0.313 |                   68 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.84  |      0.328 |                   68 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.904 |      0.249 |                   63 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.813 |      0.147 |                   14 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.2103544776119403
[2m[36m(func pid=58891)[0m top5: 0.7126865671641791
[2m[36m(func pid=58891)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=58891)[0m f1_macro: 0.147335350579691
[2m[36m(func pid=58891)[0m f1_weighted: 0.21888580138593175
[2m[36m(func pid=58891)[0m f1_per_class: [0.119, 0.252, 0.148, 0.283, 0.029, 0.361, 0.148, 0.133, 0.0, 0.0]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=47180)[0m top1: 0.26632462686567165
[2m[36m(func pid=47180)[0m top5: 0.6842350746268657
[2m[36m(func pid=47180)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=47180)[0m f1_macro: 0.269409260711471
[2m[36m(func pid=47180)[0m f1_weighted: 0.24089794901336406
[2m[36m(func pid=47180)[0m f1_per_class: [0.526, 0.475, 0.271, 0.334, 0.069, 0.17, 0.03, 0.227, 0.214, 0.377]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m top1: 0.3726679104477612
[2m[36m(func pid=45540)[0m top5: 0.8661380597014925
[2m[36m(func pid=45540)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=45540)[0m f1_macro: 0.3194667235791577
[2m[36m(func pid=45540)[0m f1_weighted: 0.4058276202847251
[2m[36m(func pid=45540)[0m f1_per_class: [0.496, 0.362, 0.157, 0.487, 0.107, 0.423, 0.398, 0.333, 0.208, 0.223]
[2m[36m(func pid=46161)[0m top1: 0.355410447761194
[2m[36m(func pid=46161)[0m top5: 0.8745335820895522
[2m[36m(func pid=46161)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=46161)[0m f1_macro: 0.3152068232106876
[2m[36m(func pid=46161)[0m f1_weighted: 0.3767072379540028
[2m[36m(func pid=46161)[0m f1_per_class: [0.403, 0.391, 0.313, 0.317, 0.233, 0.367, 0.475, 0.3, 0.194, 0.16]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7317 | Steps: 4 | Val loss: 2.1970 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.5474 | Steps: 4 | Val loss: 10.0161 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.3951 | Steps: 4 | Val loss: 1.7042 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.7307 | Steps: 4 | Val loss: 2.2145 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 12:14:00 (running for 00:16:06.04)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.495 |      0.319 |                   69 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.696 |      0.315 |                   69 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.953 |      0.269 |                   64 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.732 |      0.155 |                   15 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.2140858208955224
[2m[36m(func pid=58891)[0m top5: 0.7215485074626866
[2m[36m(func pid=58891)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=58891)[0m f1_macro: 0.15534076353129778
[2m[36m(func pid=58891)[0m f1_weighted: 0.22283062610091148
[2m[36m(func pid=58891)[0m f1_per_class: [0.118, 0.26, 0.16, 0.287, 0.03, 0.368, 0.146, 0.146, 0.0, 0.038]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=47180)[0m top1: 0.279384328358209
[2m[36m(func pid=47180)[0m top5: 0.7551305970149254
[2m[36m(func pid=47180)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=47180)[0m f1_macro: 0.2602435083800125
[2m[36m(func pid=47180)[0m f1_weighted: 0.26949586647496737
[2m[36m(func pid=47180)[0m f1_per_class: [0.316, 0.492, 0.22, 0.305, 0.063, 0.206, 0.142, 0.257, 0.177, 0.424]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m top1: 0.386660447761194
[2m[36m(func pid=45540)[0m top5: 0.8652052238805971
[2m[36m(func pid=45540)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=45540)[0m f1_macro: 0.32654350956100076
[2m[36m(func pid=45540)[0m f1_weighted: 0.41116754519418636
[2m[36m(func pid=45540)[0m f1_per_class: [0.46, 0.368, 0.214, 0.52, 0.116, 0.404, 0.389, 0.331, 0.216, 0.247]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.35261194029850745
[2m[36m(func pid=46161)[0m top5: 0.8568097014925373
[2m[36m(func pid=46161)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=46161)[0m f1_macro: 0.30653002880398306
[2m[36m(func pid=46161)[0m f1_weighted: 0.3821625634867511
[2m[36m(func pid=46161)[0m f1_per_class: [0.381, 0.363, 0.228, 0.432, 0.176, 0.372, 0.397, 0.333, 0.203, 0.181]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.6774 | Steps: 4 | Val loss: 2.1818 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.5929 | Steps: 4 | Val loss: 7.6849 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.3248 | Steps: 4 | Val loss: 1.7015 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.5918 | Steps: 4 | Val loss: 2.2884 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=58891)[0m top1: 0.22434701492537312
[2m[36m(func pid=58891)[0m top5: 0.7327425373134329
[2m[36m(func pid=58891)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=58891)[0m f1_macro: 0.16180643001795725
[2m[36m(func pid=58891)[0m f1_weighted: 0.23610546277800495
[2m[36m(func pid=58891)[0m f1_per_class: [0.135, 0.259, 0.157, 0.322, 0.03, 0.357, 0.159, 0.157, 0.0, 0.043]
[2m[36m(func pid=58891)[0m 
== Status ==
Current time: 2024-01-07 12:14:05 (running for 00:16:11.56)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.395 |      0.327 |                   70 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.731 |      0.307 |                   70 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.547 |      0.26  |                   65 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.677 |      0.162 |                   16 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=47180)[0m top1: 0.3591417910447761
[2m[36m(func pid=47180)[0m top5: 0.8353544776119403
[2m[36m(func pid=47180)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=47180)[0m f1_macro: 0.31416325405533096
[2m[36m(func pid=47180)[0m f1_weighted: 0.37650093897072573
[2m[36m(func pid=47180)[0m f1_per_class: [0.5, 0.505, 0.194, 0.377, 0.077, 0.274, 0.39, 0.23, 0.216, 0.378]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m top1: 0.38899253731343286
[2m[36m(func pid=45540)[0m top5: 0.8703358208955224
[2m[36m(func pid=45540)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=45540)[0m f1_macro: 0.32478214053787124
[2m[36m(func pid=45540)[0m f1_weighted: 0.41597801414308655
[2m[36m(func pid=45540)[0m f1_per_class: [0.464, 0.372, 0.203, 0.519, 0.108, 0.409, 0.407, 0.301, 0.221, 0.243]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.3423507462686567
[2m[36m(func pid=46161)[0m top5: 0.8456156716417911
[2m[36m(func pid=46161)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=46161)[0m f1_macro: 0.3010257045424625
[2m[36m(func pid=46161)[0m f1_weighted: 0.35287587909700374
[2m[36m(func pid=46161)[0m f1_per_class: [0.439, 0.29, 0.236, 0.512, 0.115, 0.367, 0.257, 0.359, 0.215, 0.219]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6741 | Steps: 4 | Val loss: 2.1805 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.2712 | Steps: 4 | Val loss: 6.6627 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.3767 | Steps: 4 | Val loss: 1.7212 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2411 | Steps: 4 | Val loss: 2.2448 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 12:14:10 (running for 00:16:16.93)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.325 |      0.325 |                   71 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.592 |      0.301 |                   71 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  2.593 |      0.314 |                   66 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.674 |      0.172 |                   17 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.23041044776119404
[2m[36m(func pid=58891)[0m top5: 0.7364738805970149
[2m[36m(func pid=58891)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=58891)[0m f1_macro: 0.17202604658675943
[2m[36m(func pid=58891)[0m f1_weighted: 0.24409561996763465
[2m[36m(func pid=58891)[0m f1_per_class: [0.137, 0.266, 0.164, 0.316, 0.047, 0.386, 0.171, 0.169, 0.019, 0.045]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=47180)[0m top1: 0.38619402985074625
[2m[36m(func pid=47180)[0m top5: 0.8642723880597015
[2m[36m(func pid=47180)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=47180)[0m f1_macro: 0.34030459893512466
[2m[36m(func pid=47180)[0m f1_weighted: 0.3933449241143114
[2m[36m(func pid=47180)[0m f1_per_class: [0.458, 0.498, 0.371, 0.336, 0.162, 0.299, 0.476, 0.221, 0.253, 0.328]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m top1: 0.37453358208955223
[2m[36m(func pid=45540)[0m top5: 0.8689365671641791
[2m[36m(func pid=45540)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=45540)[0m f1_macro: 0.31983071537528357
[2m[36m(func pid=45540)[0m f1_weighted: 0.40119342384996043
[2m[36m(func pid=45540)[0m f1_per_class: [0.471, 0.341, 0.242, 0.5, 0.099, 0.413, 0.393, 0.298, 0.206, 0.236]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.3521455223880597
[2m[36m(func pid=46161)[0m top5: 0.8568097014925373
[2m[36m(func pid=46161)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=46161)[0m f1_macro: 0.3188145907068031
[2m[36m(func pid=46161)[0m f1_weighted: 0.36773180634702657
[2m[36m(func pid=46161)[0m f1_per_class: [0.473, 0.329, 0.274, 0.508, 0.101, 0.347, 0.292, 0.342, 0.221, 0.301]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.6602 | Steps: 4 | Val loss: 2.1726 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 4.5057 | Steps: 4 | Val loss: 7.8857 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.3769 | Steps: 4 | Val loss: 1.7236 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.8255 | Steps: 4 | Val loss: 2.2342 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 12:14:16 (running for 00:16:22.45)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.377 |      0.32  |                   72 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.241 |      0.319 |                   72 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.271 |      0.34  |                   67 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.66  |      0.18  |                   18 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.23507462686567165
[2m[36m(func pid=58891)[0m top5: 0.7392723880597015
[2m[36m(func pid=58891)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=58891)[0m f1_macro: 0.1804841030010974
[2m[36m(func pid=58891)[0m f1_weighted: 0.24838248290860931
[2m[36m(func pid=58891)[0m f1_per_class: [0.134, 0.248, 0.235, 0.345, 0.045, 0.387, 0.165, 0.181, 0.022, 0.043]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=47180)[0m top1: 0.3400186567164179
[2m[36m(func pid=47180)[0m top5: 0.8190298507462687
[2m[36m(func pid=47180)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=47180)[0m f1_macro: 0.3231543812511735
[2m[36m(func pid=47180)[0m f1_weighted: 0.3370093125475759
[2m[36m(func pid=47180)[0m f1_per_class: [0.389, 0.474, 0.5, 0.451, 0.206, 0.308, 0.198, 0.229, 0.196, 0.282]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m top1: 0.37779850746268656
[2m[36m(func pid=45540)[0m top5: 0.8717350746268657
[2m[36m(func pid=45540)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=45540)[0m f1_macro: 0.32981147656437027
[2m[36m(func pid=45540)[0m f1_weighted: 0.4022290169381793
[2m[36m(func pid=45540)[0m f1_per_class: [0.479, 0.342, 0.289, 0.499, 0.096, 0.424, 0.383, 0.337, 0.213, 0.236]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.3474813432835821
[2m[36m(func pid=46161)[0m top5: 0.8591417910447762
[2m[36m(func pid=46161)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=46161)[0m f1_macro: 0.3221899672666151
[2m[36m(func pid=46161)[0m f1_weighted: 0.36531560743967706
[2m[36m(func pid=46161)[0m f1_per_class: [0.43, 0.395, 0.313, 0.456, 0.097, 0.341, 0.298, 0.349, 0.215, 0.328]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.6987 | Steps: 4 | Val loss: 2.1571 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.8460 | Steps: 4 | Val loss: 9.2429 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.3171 | Steps: 4 | Val loss: 1.7188 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.0086 | Steps: 4 | Val loss: 2.3703 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:14:21 (running for 00:16:27.76)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.377 |      0.33  |                   73 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.825 |      0.322 |                   73 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  4.506 |      0.323 |                   68 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.699 |      0.194 |                   19 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.24067164179104478
[2m[36m(func pid=58891)[0m top5: 0.7430037313432836
[2m[36m(func pid=58891)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=58891)[0m f1_macro: 0.19418652239867998
[2m[36m(func pid=58891)[0m f1_weighted: 0.2528585455538609
[2m[36m(func pid=58891)[0m f1_per_class: [0.137, 0.215, 0.235, 0.373, 0.027, 0.385, 0.162, 0.211, 0.023, 0.172]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=47180)[0m top1: 0.36380597014925375
[2m[36m(func pid=47180)[0m top5: 0.7672574626865671
[2m[36m(func pid=47180)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=47180)[0m f1_macro: 0.3323239057047175
[2m[36m(func pid=47180)[0m f1_weighted: 0.3210712172753861
[2m[36m(func pid=47180)[0m f1_per_class: [0.496, 0.484, 0.564, 0.546, 0.148, 0.181, 0.082, 0.25, 0.205, 0.367]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m top1: 0.37779850746268656
[2m[36m(func pid=45540)[0m top5: 0.8694029850746269
[2m[36m(func pid=45540)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=45540)[0m f1_macro: 0.32695299675064443
[2m[36m(func pid=45540)[0m f1_weighted: 0.39300514435803735
[2m[36m(func pid=45540)[0m f1_per_class: [0.402, 0.325, 0.32, 0.522, 0.097, 0.419, 0.338, 0.374, 0.224, 0.249]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=46161)[0m top1: 0.3185634328358209
[2m[36m(func pid=46161)[0m top5: 0.8456156716417911
[2m[36m(func pid=46161)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=46161)[0m f1_macro: 0.2901485136718084
[2m[36m(func pid=46161)[0m f1_weighted: 0.344828537248872
[2m[36m(func pid=46161)[0m f1_per_class: [0.224, 0.385, 0.206, 0.321, 0.097, 0.344, 0.372, 0.367, 0.201, 0.385]
[2m[36m(func pid=46161)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.6369 | Steps: 4 | Val loss: 2.1359 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.1989 | Steps: 4 | Val loss: 10.4410 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.3649 | Steps: 4 | Val loss: 1.7222 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=46161)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.3788 | Steps: 4 | Val loss: 2.6428 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 12:14:27 (running for 00:16:32.96)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=5
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (15 PENDING, 4 RUNNING, 5 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.317 |      0.327 |                   74 |
| train_9b9e8_00006 | RUNNING    | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  1.009 |      0.29  |                   74 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.846 |      0.332 |                   69 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.637 |      0.185 |                   20 |
| train_9b9e8_00009 | PENDING    |                    | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.2439365671641791
[2m[36m(func pid=58891)[0m top5: 0.7639925373134329
[2m[36m(func pid=58891)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=58891)[0m f1_macro: 0.18530999123245845
[2m[36m(func pid=58891)[0m f1_weighted: 0.25666703058257606
[2m[36m(func pid=58891)[0m f1_per_class: [0.134, 0.211, 0.219, 0.401, 0.034, 0.34, 0.175, 0.198, 0.0, 0.14]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=47180)[0m top1: 0.310634328358209
[2m[36m(func pid=47180)[0m top5: 0.7602611940298507
[2m[36m(func pid=47180)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=47180)[0m f1_macro: 0.3152941421962027
[2m[36m(func pid=47180)[0m f1_weighted: 0.2805562795280748
[2m[36m(func pid=47180)[0m f1_per_class: [0.446, 0.524, 0.6, 0.458, 0.136, 0.075, 0.054, 0.234, 0.151, 0.474]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=46161)[0m top1: 0.2789179104477612
[2m[36m(func pid=46161)[0m top5: 0.8022388059701493
[2m[36m(func pid=46161)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=46161)[0m f1_macro: 0.2632798802910585
[2m[36m(func pid=46161)[0m f1_weighted: 0.30435194763361945
[2m[36m(func pid=46161)[0m f1_per_class: [0.218, 0.397, 0.086, 0.177, 0.114, 0.35, 0.375, 0.317, 0.173, 0.427]
[2m[36m(func pid=45540)[0m top1: 0.38386194029850745
[2m[36m(func pid=45540)[0m top5: 0.8675373134328358
[2m[36m(func pid=45540)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=45540)[0m f1_macro: 0.33303303291767394
[2m[36m(func pid=45540)[0m f1_weighted: 0.3963004604989468
[2m[36m(func pid=45540)[0m f1_per_class: [0.414, 0.341, 0.333, 0.536, 0.101, 0.416, 0.324, 0.391, 0.215, 0.258]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.5640 | Steps: 4 | Val loss: 2.1191 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.8608 | Steps: 4 | Val loss: 7.7286 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.3718 | Steps: 4 | Val loss: 1.7338 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=58891)[0m top1: 0.2537313432835821
[2m[36m(func pid=58891)[0m top5: 0.7761194029850746
[2m[36m(func pid=58891)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=58891)[0m f1_macro: 0.19371135556775537
[2m[36m(func pid=58891)[0m f1_weighted: 0.2642497219205834
[2m[36m(func pid=58891)[0m f1_per_class: [0.137, 0.186, 0.231, 0.425, 0.043, 0.356, 0.182, 0.202, 0.024, 0.151]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=47180)[0m top1: 0.37080223880597013
[2m[36m(func pid=47180)[0m top5: 0.8428171641791045
[2m[36m(func pid=47180)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=47180)[0m f1_macro: 0.3576862187699933
[2m[36m(func pid=47180)[0m f1_weighted: 0.3693903804459573
[2m[36m(func pid=47180)[0m f1_per_class: [0.449, 0.504, 0.619, 0.357, 0.121, 0.199, 0.401, 0.257, 0.207, 0.463]
[2m[36m(func pid=45540)[0m top1: 0.3829291044776119
[2m[36m(func pid=45540)[0m top5: 0.8586753731343284
[2m[36m(func pid=45540)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=45540)[0m f1_macro: 0.3296829004650942
[2m[36m(func pid=45540)[0m f1_weighted: 0.3922114626807813
[2m[36m(func pid=45540)[0m f1_per_class: [0.398, 0.329, 0.308, 0.541, 0.103, 0.42, 0.312, 0.385, 0.224, 0.277]
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5145 | Steps: 4 | Val loss: 2.1022 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:14:32 (running for 00:16:38.35)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.365 |      0.333 |                   75 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.199 |      0.315 |                   70 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.564 |      0.194 |                   21 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=64137)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=64137)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=64137)[0m Configuration completed!
[2m[36m(func pid=64137)[0m New optimizer parameters:
[2m[36m(func pid=64137)[0m SGD (
[2m[36m(func pid=64137)[0m Parameter Group 0
[2m[36m(func pid=64137)[0m     dampening: 0
[2m[36m(func pid=64137)[0m     differentiable: False
[2m[36m(func pid=64137)[0m     foreach: None
[2m[36m(func pid=64137)[0m     lr: 0.001
[2m[36m(func pid=64137)[0m     maximize: False
[2m[36m(func pid=64137)[0m     momentum: 0.99
[2m[36m(func pid=64137)[0m     nesterov: False
[2m[36m(func pid=64137)[0m     weight_decay: 0.0001
[2m[36m(func pid=64137)[0m )
[2m[36m(func pid=64137)[0m 
== Status ==
Current time: 2024-01-07 12:14:37 (running for 00:16:43.82)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.372 |      0.33  |                   76 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.861 |      0.358 |                   71 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.514 |      0.202 |                   22 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.2653917910447761
[2m[36m(func pid=58891)[0m top5: 0.7756529850746269
[2m[36m(func pid=58891)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=58891)[0m f1_macro: 0.2019997470390479
[2m[36m(func pid=58891)[0m f1_weighted: 0.2686564833848608
[2m[36m(func pid=58891)[0m f1_per_class: [0.139, 0.17, 0.217, 0.444, 0.048, 0.37, 0.175, 0.227, 0.025, 0.203]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.3233 | Steps: 4 | Val loss: 1.7302 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.2530 | Steps: 4 | Val loss: 6.9509 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.5652 | Steps: 4 | Val loss: 2.0936 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9653 | Steps: 4 | Val loss: 2.3234 | Batch size: 32 | lr: 0.001 | Duration: 4.63s
[2m[36m(func pid=45540)[0m top1: 0.3773320895522388
[2m[36m(func pid=45540)[0m top5: 0.863339552238806
[2m[36m(func pid=45540)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=45540)[0m f1_macro: 0.31965374560353405
[2m[36m(func pid=45540)[0m f1_weighted: 0.38952544872242206
[2m[36m(func pid=45540)[0m f1_per_class: [0.385, 0.328, 0.258, 0.536, 0.114, 0.426, 0.314, 0.367, 0.206, 0.263]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=47180)[0m top1: 0.40718283582089554
[2m[36m(func pid=47180)[0m top5: 0.882929104477612
[2m[36m(func pid=47180)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=47180)[0m f1_macro: 0.34933683806820004
[2m[36m(func pid=47180)[0m f1_weighted: 0.4010039331137125
[2m[36m(func pid=47180)[0m f1_per_class: [0.433, 0.509, 0.51, 0.329, 0.147, 0.347, 0.494, 0.197, 0.191, 0.337]
[2m[36m(func pid=47180)[0m 
== Status ==
Current time: 2024-01-07 12:14:43 (running for 00:16:49.17)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.323 |      0.32  |                   77 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.253 |      0.349 |                   72 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.565 |      0.206 |                   23 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.2621268656716418
[2m[36m(func pid=58891)[0m top5: 0.7803171641791045
[2m[36m(func pid=58891)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=58891)[0m f1_macro: 0.2059274409631914
[2m[36m(func pid=58891)[0m f1_weighted: 0.2697018981096917
[2m[36m(func pid=58891)[0m f1_per_class: [0.133, 0.169, 0.216, 0.436, 0.075, 0.337, 0.199, 0.22, 0.025, 0.25]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m top1: 0.17723880597014927
[2m[36m(func pid=64137)[0m top5: 0.5377798507462687
[2m[36m(func pid=64137)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=64137)[0m f1_macro: 0.10972267431163578
[2m[36m(func pid=64137)[0m f1_weighted: 0.12643632247451886
[2m[36m(func pid=64137)[0m f1_per_class: [0.267, 0.328, 0.0, 0.096, 0.009, 0.273, 0.012, 0.034, 0.0, 0.078]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.3935 | Steps: 4 | Val loss: 1.7194 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.9727 | Steps: 4 | Val loss: 7.7288 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.5646 | Steps: 4 | Val loss: 2.0928 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9626 | Steps: 4 | Val loss: 2.3216 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=45540)[0m top1: 0.37779850746268656
[2m[36m(func pid=45540)[0m top5: 0.8666044776119403
[2m[36m(func pid=45540)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=45540)[0m f1_macro: 0.32637194674894915
[2m[36m(func pid=45540)[0m f1_weighted: 0.3894337750284349
[2m[36m(func pid=45540)[0m f1_per_class: [0.463, 0.345, 0.27, 0.538, 0.11, 0.417, 0.302, 0.356, 0.206, 0.257]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=47180)[0m top1: 0.3670708955223881
[2m[36m(func pid=47180)[0m top5: 0.8624067164179104
[2m[36m(func pid=47180)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=47180)[0m f1_macro: 0.3448052799920084
[2m[36m(func pid=47180)[0m f1_weighted: 0.36227361803391633
[2m[36m(func pid=47180)[0m f1_per_class: [0.486, 0.477, 0.419, 0.397, 0.152, 0.343, 0.298, 0.28, 0.217, 0.379]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=58891)[0m top1: 0.251865671641791
[2m[36m(func pid=58891)[0m top5: 0.7784514925373134
[2m[36m(func pid=58891)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=58891)[0m f1_macro: 0.19027516451886115
[2m[36m(func pid=58891)[0m f1_weighted: 0.25005829716987904
[2m[36m(func pid=58891)[0m f1_per_class: [0.134, 0.133, 0.167, 0.444, 0.069, 0.327, 0.149, 0.245, 0.025, 0.211]
== Status ==
Current time: 2024-01-07 12:14:48 (running for 00:16:54.55)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.394 |      0.326 |                   78 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.973 |      0.345 |                   73 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.565 |      0.19  |                   24 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  2.965 |      0.11  |                    1 |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m top1: 0.17024253731343283
[2m[36m(func pid=64137)[0m top5: 0.5345149253731343
[2m[36m(func pid=64137)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=64137)[0m f1_macro: 0.11004052497824605
[2m[36m(func pid=64137)[0m f1_weighted: 0.13075356467497917
[2m[36m(func pid=64137)[0m f1_per_class: [0.194, 0.318, 0.0, 0.116, 0.028, 0.308, 0.006, 0.011, 0.019, 0.1]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5586 | Steps: 4 | Val loss: 8.7030 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.4033 | Steps: 4 | Val loss: 1.7179 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.4970 | Steps: 4 | Val loss: 2.0727 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9178 | Steps: 4 | Val loss: 2.3090 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=47180)[0m top1: 0.33488805970149255
[2m[36m(func pid=47180)[0m top5: 0.8157649253731343
[2m[36m(func pid=47180)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=47180)[0m f1_macro: 0.3136648375482583
[2m[36m(func pid=47180)[0m f1_weighted: 0.329566798904618
[2m[36m(func pid=47180)[0m f1_per_class: [0.507, 0.458, 0.148, 0.411, 0.13, 0.328, 0.187, 0.299, 0.265, 0.404]
[2m[36m(func pid=47180)[0m 
[2m[36m(func pid=45540)[0m top1: 0.37779850746268656
[2m[36m(func pid=45540)[0m top5: 0.8694029850746269
[2m[36m(func pid=45540)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=45540)[0m f1_macro: 0.32544576342886006
[2m[36m(func pid=45540)[0m f1_weighted: 0.3879438143202997
[2m[36m(func pid=45540)[0m f1_per_class: [0.459, 0.326, 0.255, 0.54, 0.118, 0.413, 0.305, 0.375, 0.201, 0.263]
[2m[36m(func pid=45540)[0m 
== Status ==
Current time: 2024-01-07 12:14:53 (running for 00:16:59.85)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.331
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.403 |      0.325 |                   79 |
| train_9b9e8_00007 | RUNNING    | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  0.559 |      0.314 |                   74 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.565 |      0.19  |                   24 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  2.918 |      0.138 |                    3 |
| train_9b9e8_00010 | PENDING    |                    | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.15811567164179105
[2m[36m(func pid=64137)[0m top5: 0.5555037313432836
[2m[36m(func pid=64137)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=64137)[0m f1_macro: 0.13840082627500128
[2m[36m(func pid=64137)[0m f1_weighted: 0.14326641058004158
[2m[36m(func pid=64137)[0m f1_per_class: [0.324, 0.275, 0.19, 0.146, 0.03, 0.317, 0.024, 0.052, 0.025, 0.0]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m top1: 0.2560634328358209
[2m[36m(func pid=58891)[0m top5: 0.7863805970149254
[2m[36m(func pid=58891)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=58891)[0m f1_macro: 0.19304880124923943
[2m[36m(func pid=58891)[0m f1_weighted: 0.24887125966961024
[2m[36m(func pid=58891)[0m f1_per_class: [0.141, 0.113, 0.198, 0.457, 0.057, 0.334, 0.141, 0.238, 0.025, 0.226]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=47180)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.8036 | Steps: 4 | Val loss: 9.1795 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.4037 | Steps: 4 | Val loss: 1.7573 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8756 | Steps: 4 | Val loss: 2.2654 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.5044 | Steps: 4 | Val loss: 2.0667 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=47180)[0m top1: 0.30830223880597013
[2m[36m(func pid=47180)[0m top5: 0.7868470149253731
[2m[36m(func pid=47180)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=47180)[0m f1_macro: 0.29370513627203
[2m[36m(func pid=47180)[0m f1_weighted: 0.31278422864265376
[2m[36m(func pid=47180)[0m f1_per_class: [0.545, 0.424, 0.186, 0.397, 0.078, 0.292, 0.186, 0.289, 0.207, 0.333]
[2m[36m(func pid=45540)[0m top1: 0.36007462686567165
[2m[36m(func pid=45540)[0m top5: 0.8577425373134329
[2m[36m(func pid=45540)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=45540)[0m f1_macro: 0.3107260446851883
[2m[36m(func pid=45540)[0m f1_weighted: 0.3785501215238219
[2m[36m(func pid=45540)[0m f1_per_class: [0.44, 0.343, 0.19, 0.511, 0.112, 0.407, 0.299, 0.361, 0.206, 0.238]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=64137)[0m top1: 0.14972014925373134
[2m[36m(func pid=64137)[0m top5: 0.6413246268656716
[2m[36m(func pid=64137)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=64137)[0m f1_macro: 0.14417964053752672
[2m[36m(func pid=64137)[0m f1_weighted: 0.16241079837133768
[2m[36m(func pid=64137)[0m f1_per_class: [0.262, 0.234, 0.273, 0.207, 0.034, 0.313, 0.063, 0.026, 0.03, 0.0]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m top1: 0.26072761194029853
[2m[36m(func pid=58891)[0m top5: 0.7933768656716418
[2m[36m(func pid=58891)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=58891)[0m f1_macro: 0.20429526319153216
[2m[36m(func pid=58891)[0m f1_weighted: 0.26221826588865166
[2m[36m(func pid=58891)[0m f1_per_class: [0.131, 0.119, 0.162, 0.456, 0.074, 0.346, 0.172, 0.258, 0.026, 0.299]
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.2324 | Steps: 4 | Val loss: 1.7529 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7896 | Steps: 4 | Val loss: 2.2191 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=45540)[0m top1: 0.35774253731343286
[2m[36m(func pid=45540)[0m top5: 0.8572761194029851
[2m[36m(func pid=45540)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=45540)[0m f1_macro: 0.30967969769603365
[2m[36m(func pid=45540)[0m f1_weighted: 0.37449349912378127
[2m[36m(func pid=45540)[0m f1_per_class: [0.42, 0.377, 0.187, 0.499, 0.123, 0.408, 0.278, 0.357, 0.204, 0.243]
[2m[36m(func pid=64137)[0m top1: 0.1478544776119403
[2m[36m(func pid=64137)[0m top5: 0.7038246268656716
[2m[36m(func pid=64137)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=64137)[0m f1_macro: 0.15678431049008384
[2m[36m(func pid=64137)[0m f1_weighted: 0.17121710772937446
[2m[36m(func pid=64137)[0m f1_per_class: [0.321, 0.208, 0.4, 0.268, 0.031, 0.17, 0.098, 0.015, 0.057, 0.0]
== Status ==
Current time: 2024-01-07 12:14:59 (running for 00:17:05.09)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.404 |      0.311 |                   80 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.497 |      0.193 |                   25 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  2.876 |      0.144 |                    4 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 12:15:06 (running for 00:17:12.29)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.404 |      0.311 |                   80 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.504 |      0.204 |                   26 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  2.876 |      0.144 |                    4 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=65548)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=65548)[0m Configuration completed!
[2m[36m(func pid=65548)[0m New optimizer parameters:
[2m[36m(func pid=65548)[0m SGD (
[2m[36m(func pid=65548)[0m Parameter Group 0
[2m[36m(func pid=65548)[0m     dampening: 0
[2m[36m(func pid=65548)[0m     differentiable: False
[2m[36m(func pid=65548)[0m     foreach: None
[2m[36m(func pid=65548)[0m     lr: 0.01
[2m[36m(func pid=65548)[0m     maximize: False
[2m[36m(func pid=65548)[0m     momentum: 0.99
[2m[36m(func pid=65548)[0m     nesterov: False
[2m[36m(func pid=65548)[0m     weight_decay: 0.0001
[2m[36m(func pid=65548)[0m )
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.4340 | Steps: 4 | Val loss: 2.0415 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6608 | Steps: 4 | Val loss: 2.1790 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 1.3169 | Steps: 4 | Val loss: 1.7543 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9256 | Steps: 4 | Val loss: 2.3135 | Batch size: 32 | lr: 0.01 | Duration: 4.34s
== Status ==
Current time: 2024-01-07 12:15:11 (running for 00:17:17.33)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.232 |      0.31  |                   81 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.504 |      0.204 |                   26 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  2.79  |      0.157 |                    5 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.28171641791044777
[2m[36m(func pid=58891)[0m top5: 0.7994402985074627
[2m[36m(func pid=58891)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=58891)[0m f1_macro: 0.21252060181612972
[2m[36m(func pid=58891)[0m f1_weighted: 0.2725762267977504
[2m[36m(func pid=58891)[0m f1_per_class: [0.158, 0.115, 0.186, 0.484, 0.075, 0.351, 0.182, 0.248, 0.026, 0.301]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m top1: 0.15625
[2m[36m(func pid=64137)[0m top5: 0.7458022388059702
[2m[36m(func pid=64137)[0m f1_micro: 0.15625
[2m[36m(func pid=64137)[0m f1_macro: 0.16634276338813542
[2m[36m(func pid=64137)[0m f1_weighted: 0.18000671932404463
[2m[36m(func pid=64137)[0m f1_per_class: [0.338, 0.192, 0.424, 0.308, 0.063, 0.08, 0.131, 0.0, 0.064, 0.062]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=45540)[0m top1: 0.35774253731343286
[2m[36m(func pid=45540)[0m top5: 0.8614738805970149
[2m[36m(func pid=45540)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=45540)[0m f1_macro: 0.3149433185329603
[2m[36m(func pid=45540)[0m f1_weighted: 0.3783897858269943
[2m[36m(func pid=45540)[0m f1_per_class: [0.481, 0.387, 0.183, 0.484, 0.1, 0.405, 0.3, 0.329, 0.219, 0.26]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=65548)[0m top1: 0.17444029850746268
[2m[36m(func pid=65548)[0m top5: 0.5279850746268657
[2m[36m(func pid=65548)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=65548)[0m f1_macro: 0.109499646577025
[2m[36m(func pid=65548)[0m f1_weighted: 0.1265594978492464
[2m[36m(func pid=65548)[0m f1_per_class: [0.166, 0.351, 0.0, 0.119, 0.01, 0.106, 0.021, 0.159, 0.0, 0.163]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.5476 | Steps: 4 | Val loss: 2.0353 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5646 | Steps: 4 | Val loss: 2.1238 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 1.2079 | Steps: 4 | Val loss: 1.7325 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7930 | Steps: 4 | Val loss: 2.3900 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 12:15:17 (running for 00:17:23.02)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.317 |      0.315 |                   82 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.548 |      0.222 |                   28 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  2.661 |      0.166 |                    6 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  2.926 |      0.109 |                    1 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.29197761194029853
[2m[36m(func pid=58891)[0m top5: 0.7859141791044776
[2m[36m(func pid=58891)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=58891)[0m f1_macro: 0.22156982145908052
[2m[36m(func pid=58891)[0m f1_weighted: 0.2797725041219998
[2m[36m(func pid=58891)[0m f1_per_class: [0.151, 0.098, 0.182, 0.502, 0.053, 0.372, 0.185, 0.268, 0.025, 0.38]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m top1: 0.3736007462686567
[2m[36m(func pid=45540)[0m top5: 0.8689365671641791
[2m[36m(func pid=45540)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=45540)[0m f1_macro: 0.32810124338019603
[2m[36m(func pid=45540)[0m f1_weighted: 0.39926000596277017
[2m[36m(func pid=45540)[0m f1_per_class: [0.507, 0.405, 0.2, 0.48, 0.104, 0.415, 0.354, 0.355, 0.221, 0.241]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=64137)[0m top1: 0.16277985074626866
[2m[36m(func pid=64137)[0m top5: 0.7905783582089553
[2m[36m(func pid=64137)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=64137)[0m f1_macro: 0.19162062563218882
[2m[36m(func pid=64137)[0m f1_weighted: 0.1889620120616562
[2m[36m(func pid=64137)[0m f1_per_class: [0.414, 0.282, 0.317, 0.283, 0.084, 0.128, 0.104, 0.0, 0.07, 0.235]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m top1: 0.10027985074626866
[2m[36m(func pid=65548)[0m top5: 0.447294776119403
[2m[36m(func pid=65548)[0m f1_micro: 0.10027985074626866
[2m[36m(func pid=65548)[0m f1_macro: 0.14105661054912474
[2m[36m(func pid=65548)[0m f1_weighted: 0.07467241892678125
[2m[36m(func pid=65548)[0m f1_per_class: [0.131, 0.276, 0.649, 0.003, 0.032, 0.0, 0.03, 0.155, 0.0, 0.136]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.3194 | Steps: 4 | Val loss: 2.0227 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6129 | Steps: 4 | Val loss: 2.1008 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 1.2457 | Steps: 4 | Val loss: 1.7194 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.4298 | Steps: 4 | Val loss: 2.4922 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 12:15:22 (running for 00:17:28.28)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.208 |      0.328 |                   83 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.319 |      0.22  |                   29 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  2.565 |      0.192 |                    7 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  2.793 |      0.141 |                    2 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.2905783582089552
[2m[36m(func pid=58891)[0m top5: 0.7933768656716418
[2m[36m(func pid=58891)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=58891)[0m f1_macro: 0.21951362132121982
[2m[36m(func pid=58891)[0m f1_weighted: 0.28275290579790024
[2m[36m(func pid=58891)[0m f1_per_class: [0.148, 0.114, 0.165, 0.492, 0.06, 0.36, 0.201, 0.26, 0.024, 0.37]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m top1: 0.37220149253731344
[2m[36m(func pid=45540)[0m top5: 0.8745335820895522
[2m[36m(func pid=45540)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=45540)[0m f1_macro: 0.323889604377911
[2m[36m(func pid=45540)[0m f1_weighted: 0.39962577074484423
[2m[36m(func pid=45540)[0m f1_per_class: [0.489, 0.405, 0.195, 0.466, 0.104, 0.414, 0.376, 0.317, 0.221, 0.251]
[2m[36m(func pid=64137)[0m top1: 0.1501865671641791
[2m[36m(func pid=64137)[0m top5: 0.8022388059701493
[2m[36m(func pid=64137)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=64137)[0m f1_macro: 0.21546689272472505
[2m[36m(func pid=64137)[0m f1_weighted: 0.17125879171262298
[2m[36m(func pid=64137)[0m f1_per_class: [0.449, 0.276, 0.393, 0.268, 0.149, 0.095, 0.062, 0.0, 0.071, 0.391]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=65548)[0m top1: 0.08488805970149253
[2m[36m(func pid=65548)[0m top5: 0.45475746268656714
[2m[36m(func pid=65548)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=65548)[0m f1_macro: 0.11187516894750509
[2m[36m(func pid=65548)[0m f1_weighted: 0.06627176641489814
[2m[36m(func pid=65548)[0m f1_per_class: [0.206, 0.136, 0.136, 0.035, 0.029, 0.0, 0.009, 0.352, 0.102, 0.113]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3720 | Steps: 4 | Val loss: 2.0153 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.1738 | Steps: 4 | Val loss: 1.6858 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.4130 | Steps: 4 | Val loss: 2.0897 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.1811 | Steps: 4 | Val loss: 2.2301 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 12:15:27 (running for 00:17:33.46)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.246 |      0.324 |                   84 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.372 |      0.224 |                   30 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  2.613 |      0.215 |                    8 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  2.43  |      0.112 |                    3 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.2966417910447761
[2m[36m(func pid=58891)[0m top5: 0.7915111940298507
[2m[36m(func pid=58891)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=58891)[0m f1_macro: 0.22360094650665602
[2m[36m(func pid=58891)[0m f1_weighted: 0.28951844813763755
[2m[36m(func pid=58891)[0m f1_per_class: [0.155, 0.088, 0.14, 0.498, 0.076, 0.366, 0.226, 0.284, 0.024, 0.378]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m top1: 0.38992537313432835
[2m[36m(func pid=45540)[0m top5: 0.8815298507462687
[2m[36m(func pid=45540)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=45540)[0m f1_macro: 0.33676788363335114
[2m[36m(func pid=45540)[0m f1_weighted: 0.4139084263896754
[2m[36m(func pid=45540)[0m f1_per_class: [0.485, 0.447, 0.222, 0.453, 0.111, 0.419, 0.41, 0.301, 0.232, 0.288]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=64137)[0m top1: 0.16138059701492538
[2m[36m(func pid=64137)[0m top5: 0.7919776119402985
[2m[36m(func pid=64137)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=64137)[0m f1_macro: 0.2025650227089073
[2m[36m(func pid=64137)[0m f1_weighted: 0.18183606623081167
[2m[36m(func pid=64137)[0m f1_per_class: [0.455, 0.336, 0.216, 0.245, 0.118, 0.136, 0.074, 0.0, 0.074, 0.373]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m top1: 0.17397388059701493
[2m[36m(func pid=65548)[0m top5: 0.6996268656716418
[2m[36m(func pid=65548)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=65548)[0m f1_macro: 0.19572797823552757
[2m[36m(func pid=65548)[0m f1_weighted: 0.1659882062889708
[2m[36m(func pid=65548)[0m f1_per_class: [0.32, 0.257, 0.111, 0.179, 0.035, 0.271, 0.009, 0.401, 0.132, 0.242]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.3254 | Steps: 4 | Val loss: 1.9951 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.3262 | Steps: 4 | Val loss: 2.0941 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.2846 | Steps: 4 | Val loss: 1.7237 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:15:32 (running for 00:17:38.62)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.174 |      0.337 |                   85 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.325 |      0.218 |                   31 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  2.413 |      0.203 |                    9 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  2.181 |      0.196 |                    4 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.2980410447761194
[2m[36m(func pid=58891)[0m top5: 0.8064365671641791
[2m[36m(func pid=58891)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=58891)[0m f1_macro: 0.21817566169771233
[2m[36m(func pid=58891)[0m f1_weighted: 0.29924691104689
[2m[36m(func pid=58891)[0m f1_per_class: [0.151, 0.09, 0.136, 0.491, 0.067, 0.35, 0.277, 0.263, 0.023, 0.333]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.6581 | Steps: 4 | Val loss: 1.8452 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=64137)[0m top1: 0.18236940298507462
[2m[36m(func pid=64137)[0m top5: 0.777518656716418
[2m[36m(func pid=64137)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=64137)[0m f1_macro: 0.19814809651720883
[2m[36m(func pid=64137)[0m f1_weighted: 0.2089873803000534
[2m[36m(func pid=64137)[0m f1_per_class: [0.448, 0.355, 0.09, 0.264, 0.125, 0.209, 0.108, 0.031, 0.083, 0.27]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=45540)[0m top1: 0.3763992537313433
[2m[36m(func pid=45540)[0m top5: 0.8777985074626866
[2m[36m(func pid=45540)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=45540)[0m f1_macro: 0.32144037516139273
[2m[36m(func pid=45540)[0m f1_weighted: 0.40413730054261554
[2m[36m(func pid=45540)[0m f1_per_class: [0.449, 0.435, 0.186, 0.441, 0.101, 0.405, 0.407, 0.287, 0.247, 0.257]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=65548)[0m top1: 0.2994402985074627
[2m[36m(func pid=65548)[0m top5: 0.8386194029850746
[2m[36m(func pid=65548)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=65548)[0m f1_macro: 0.2704511492437239
[2m[36m(func pid=65548)[0m f1_weighted: 0.3055300503159012
[2m[36m(func pid=65548)[0m f1_per_class: [0.196, 0.355, 0.212, 0.358, 0.105, 0.429, 0.202, 0.348, 0.157, 0.341]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.2982 | Steps: 4 | Val loss: 1.9888 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.1564 | Steps: 4 | Val loss: 2.0538 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 1.2504 | Steps: 4 | Val loss: 1.8085 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 12:15:37 (running for 00:17:43.92)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.285 |      0.321 |                   86 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.298 |      0.221 |                   32 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  2.326 |      0.198 |                   10 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.658 |      0.27  |                    5 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.29990671641791045
[2m[36m(func pid=58891)[0m top5: 0.8176305970149254
[2m[36m(func pid=58891)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=58891)[0m f1_macro: 0.22084152186202227
[2m[36m(func pid=58891)[0m f1_weighted: 0.30729837407118327
[2m[36m(func pid=58891)[0m f1_per_class: [0.147, 0.077, 0.132, 0.477, 0.089, 0.329, 0.332, 0.262, 0.023, 0.34]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.1423 | Steps: 4 | Val loss: 1.6864 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=64137)[0m top1: 0.23134328358208955
[2m[36m(func pid=64137)[0m top5: 0.7803171641791045
[2m[36m(func pid=64137)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=64137)[0m f1_macro: 0.22547377274224806
[2m[36m(func pid=64137)[0m f1_weighted: 0.27475553494930216
[2m[36m(func pid=64137)[0m f1_per_class: [0.347, 0.321, 0.057, 0.342, 0.093, 0.287, 0.216, 0.206, 0.108, 0.278]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=45540)[0m top1: 0.333955223880597
[2m[36m(func pid=45540)[0m top5: 0.8666044776119403
[2m[36m(func pid=45540)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=45540)[0m f1_macro: 0.3049003682777859
[2m[36m(func pid=45540)[0m f1_weighted: 0.3679425838882771
[2m[36m(func pid=45540)[0m f1_per_class: [0.468, 0.374, 0.167, 0.365, 0.062, 0.406, 0.388, 0.303, 0.232, 0.284]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=65548)[0m top1: 0.37779850746268656
[2m[36m(func pid=65548)[0m top5: 0.8969216417910447
[2m[36m(func pid=65548)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=65548)[0m f1_macro: 0.32250933517911673
[2m[36m(func pid=65548)[0m f1_weighted: 0.40709872393202623
[2m[36m(func pid=65548)[0m f1_per_class: [0.268, 0.425, 0.231, 0.425, 0.157, 0.457, 0.43, 0.31, 0.141, 0.381]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.3698 | Steps: 4 | Val loss: 1.9691 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.1026 | Steps: 4 | Val loss: 2.0488 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 1.2122 | Steps: 4 | Val loss: 1.7757 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 12:15:43 (running for 00:17:49.31)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.25  |      0.305 |                   87 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.37  |      0.231 |                   33 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  2.156 |      0.225 |                   11 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.142 |      0.323 |                    6 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.3180970149253731
[2m[36m(func pid=58891)[0m top5: 0.8269589552238806
[2m[36m(func pid=58891)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=58891)[0m f1_macro: 0.2306557988664602
[2m[36m(func pid=58891)[0m f1_weighted: 0.32685492970470137
[2m[36m(func pid=58891)[0m f1_per_class: [0.172, 0.12, 0.112, 0.493, 0.089, 0.315, 0.361, 0.263, 0.023, 0.358]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m top1: 0.240205223880597
[2m[36m(func pid=64137)[0m top5: 0.7751865671641791
[2m[36m(func pid=64137)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=64137)[0m f1_macro: 0.22651767908698756
[2m[36m(func pid=64137)[0m f1_weighted: 0.27150796700916763
[2m[36m(func pid=64137)[0m f1_per_class: [0.223, 0.267, 0.061, 0.33, 0.119, 0.326, 0.213, 0.332, 0.145, 0.249]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.1653 | Steps: 4 | Val loss: 1.6933 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=45540)[0m top1: 0.3493470149253731
[2m[36m(func pid=45540)[0m top5: 0.8703358208955224
[2m[36m(func pid=45540)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=45540)[0m f1_macro: 0.31068994502247194
[2m[36m(func pid=45540)[0m f1_weighted: 0.38185118136402374
[2m[36m(func pid=45540)[0m f1_per_class: [0.46, 0.408, 0.189, 0.372, 0.067, 0.409, 0.411, 0.291, 0.231, 0.269]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=65548)[0m top1: 0.40298507462686567
[2m[36m(func pid=65548)[0m top5: 0.9216417910447762
[2m[36m(func pid=65548)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=65548)[0m f1_macro: 0.3415995797806672
[2m[36m(func pid=65548)[0m f1_weighted: 0.4292396066324418
[2m[36m(func pid=65548)[0m f1_per_class: [0.369, 0.371, 0.308, 0.525, 0.241, 0.443, 0.461, 0.182, 0.147, 0.369]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.2438 | Steps: 4 | Val loss: 1.9714 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.9467 | Steps: 4 | Val loss: 2.0048 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 1.2509 | Steps: 4 | Val loss: 1.7841 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:15:48 (running for 00:17:54.54)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.212 |      0.311 |                   88 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.37  |      0.231 |                   33 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.947 |      0.22  |                   13 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.165 |      0.342 |                    7 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.2453358208955224
[2m[36m(func pid=64137)[0m top5: 0.7761194029850746
[2m[36m(func pid=64137)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=64137)[0m f1_macro: 0.22015292361216643
[2m[36m(func pid=64137)[0m f1_weighted: 0.26373172146141427
[2m[36m(func pid=64137)[0m f1_per_class: [0.183, 0.26, 0.079, 0.376, 0.118, 0.287, 0.166, 0.308, 0.18, 0.246]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.5554 | Steps: 4 | Val loss: 1.7258 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=58891)[0m top1: 0.31203358208955223
[2m[36m(func pid=58891)[0m top5: 0.8288246268656716
[2m[36m(func pid=58891)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=58891)[0m f1_macro: 0.23050560841613552
[2m[36m(func pid=58891)[0m f1_weighted: 0.3254694306580639
[2m[36m(func pid=58891)[0m f1_per_class: [0.163, 0.101, 0.119, 0.479, 0.092, 0.355, 0.365, 0.267, 0.023, 0.34]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m top1: 0.345615671641791
[2m[36m(func pid=45540)[0m top5: 0.8675373134328358
[2m[36m(func pid=45540)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=45540)[0m f1_macro: 0.3061470509631036
[2m[36m(func pid=45540)[0m f1_weighted: 0.37894257362821426
[2m[36m(func pid=45540)[0m f1_per_class: [0.424, 0.39, 0.216, 0.369, 0.066, 0.411, 0.419, 0.278, 0.23, 0.26]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=65548)[0m top1: 0.4542910447761194
[2m[36m(func pid=65548)[0m top5: 0.9286380597014925
[2m[36m(func pid=65548)[0m f1_micro: 0.4542910447761194
[2m[36m(func pid=65548)[0m f1_macro: 0.3518826898684966
[2m[36m(func pid=65548)[0m f1_weighted: 0.45901222254998975
[2m[36m(func pid=65548)[0m f1_per_class: [0.471, 0.342, 0.364, 0.591, 0.164, 0.441, 0.526, 0.103, 0.152, 0.366]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.2860 | Steps: 4 | Val loss: 1.9436 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.8828 | Steps: 4 | Val loss: 1.9653 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 1.3761 | Steps: 4 | Val loss: 1.7545 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 12:15:54 (running for 00:18:00.14)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.251 |      0.306 |                   89 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.244 |      0.231 |                   34 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.883 |      0.23  |                   14 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.555 |      0.352 |                    8 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.33255597014925375
[2m[36m(func pid=58891)[0m top5: 0.8372201492537313
[2m[36m(func pid=58891)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=58891)[0m f1_macro: 0.24440816118542372
[2m[36m(func pid=58891)[0m f1_weighted: 0.3386161990223326
[2m[36m(func pid=58891)[0m f1_per_class: [0.189, 0.117, 0.133, 0.496, 0.112, 0.375, 0.367, 0.306, 0.023, 0.327]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m top1: 0.2644589552238806
[2m[36m(func pid=64137)[0m top5: 0.7915111940298507
[2m[36m(func pid=64137)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=64137)[0m f1_macro: 0.23008823380533955
[2m[36m(func pid=64137)[0m f1_weighted: 0.27654281053876073
[2m[36m(func pid=64137)[0m f1_per_class: [0.176, 0.316, 0.105, 0.397, 0.093, 0.348, 0.131, 0.329, 0.183, 0.223]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=45540)[0m top1: 0.3628731343283582
[2m[36m(func pid=45540)[0m top5: 0.8745335820895522
[2m[36m(func pid=45540)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=45540)[0m f1_macro: 0.3203011824188339
[2m[36m(func pid=45540)[0m f1_weighted: 0.3970483920595006
[2m[36m(func pid=45540)[0m f1_per_class: [0.449, 0.369, 0.25, 0.406, 0.081, 0.41, 0.447, 0.318, 0.234, 0.24]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.4734 | Steps: 4 | Val loss: 1.8568 | Batch size: 32 | lr: 0.01 | Duration: 3.19s
[2m[36m(func pid=65548)[0m top1: 0.4137126865671642
[2m[36m(func pid=65548)[0m top5: 0.9183768656716418
[2m[36m(func pid=65548)[0m f1_micro: 0.4137126865671642
[2m[36m(func pid=65548)[0m f1_macro: 0.3320865859674325
[2m[36m(func pid=65548)[0m f1_weighted: 0.43690544274313814
[2m[36m(func pid=65548)[0m f1_per_class: [0.444, 0.379, 0.258, 0.512, 0.104, 0.426, 0.494, 0.205, 0.178, 0.32]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.1121 | Steps: 4 | Val loss: 1.9087 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.6564 | Steps: 4 | Val loss: 1.9416 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 1.2236 | Steps: 4 | Val loss: 1.7311 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:15:59 (running for 00:18:05.43)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.376 |      0.32  |                   90 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.286 |      0.244 |                   35 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.656 |      0.236 |                   15 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.473 |      0.332 |                    9 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.27425373134328357
[2m[36m(func pid=64137)[0m top5: 0.7975746268656716
[2m[36m(func pid=64137)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=64137)[0m f1_macro: 0.2359438453246964
[2m[36m(func pid=64137)[0m f1_weighted: 0.27507202874441966
[2m[36m(func pid=64137)[0m f1_per_class: [0.187, 0.337, 0.144, 0.419, 0.089, 0.359, 0.085, 0.346, 0.184, 0.21]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m top1: 0.35401119402985076
[2m[36m(func pid=58891)[0m top5: 0.8498134328358209
[2m[36m(func pid=58891)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=58891)[0m f1_macro: 0.25587730901970446
[2m[36m(func pid=58891)[0m f1_weighted: 0.35597160951421064
[2m[36m(func pid=58891)[0m f1_per_class: [0.214, 0.137, 0.178, 0.51, 0.112, 0.391, 0.393, 0.308, 0.023, 0.294]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m top1: 0.36800373134328357
[2m[36m(func pid=45540)[0m top5: 0.8777985074626866
[2m[36m(func pid=45540)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=45540)[0m f1_macro: 0.3216533727034011
[2m[36m(func pid=45540)[0m f1_weighted: 0.4002442691038695
[2m[36m(func pid=45540)[0m f1_per_class: [0.443, 0.368, 0.258, 0.416, 0.09, 0.418, 0.446, 0.322, 0.225, 0.23]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 0.8481 | Steps: 4 | Val loss: 2.4878 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.2466 | Steps: 4 | Val loss: 1.9158 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.6679 | Steps: 4 | Val loss: 1.8868 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=65548)[0m top1: 0.333955223880597
[2m[36m(func pid=65548)[0m top5: 0.8558768656716418
[2m[36m(func pid=65548)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=65548)[0m f1_macro: 0.3148692647660591
[2m[36m(func pid=65548)[0m f1_weighted: 0.3559379099697334
[2m[36m(func pid=65548)[0m f1_per_class: [0.531, 0.43, 0.245, 0.332, 0.065, 0.403, 0.341, 0.318, 0.216, 0.267]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 1.2407 | Steps: 4 | Val loss: 1.7020 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 12:16:04 (running for 00:18:10.80)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.224 |      0.322 |                   91 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.112 |      0.256 |                   36 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.668 |      0.259 |                   16 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.848 |      0.315 |                   10 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.355410447761194
[2m[36m(func pid=58891)[0m top5: 0.847481343283582
[2m[36m(func pid=58891)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=58891)[0m f1_macro: 0.25408329438224336
[2m[36m(func pid=58891)[0m f1_weighted: 0.36307924187394025
[2m[36m(func pid=58891)[0m f1_per_class: [0.238, 0.145, 0.143, 0.511, 0.103, 0.367, 0.422, 0.303, 0.022, 0.288]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m top1: 0.30177238805970147
[2m[36m(func pid=64137)[0m top5: 0.8101679104477612
[2m[36m(func pid=64137)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=64137)[0m f1_macro: 0.25862701209772426
[2m[36m(func pid=64137)[0m f1_weighted: 0.28866295974088024
[2m[36m(func pid=64137)[0m f1_per_class: [0.282, 0.362, 0.176, 0.467, 0.098, 0.376, 0.054, 0.343, 0.216, 0.212]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=45540)[0m top1: 0.38386194029850745
[2m[36m(func pid=45540)[0m top5: 0.8810634328358209
[2m[36m(func pid=45540)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=45540)[0m f1_macro: 0.32473508705563614
[2m[36m(func pid=45540)[0m f1_weighted: 0.41464175439947654
[2m[36m(func pid=45540)[0m f1_per_class: [0.44, 0.401, 0.238, 0.461, 0.089, 0.408, 0.442, 0.296, 0.231, 0.241]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.2094 | Steps: 4 | Val loss: 3.1960 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.7903 | Steps: 4 | Val loss: 1.8765 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.1507 | Steps: 4 | Val loss: 1.9183 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=65548)[0m top1: 0.2756529850746269
[2m[36m(func pid=65548)[0m top5: 0.8101679104477612
[2m[36m(func pid=65548)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=65548)[0m f1_macro: 0.28893286378918126
[2m[36m(func pid=65548)[0m f1_weighted: 0.26843602705290104
[2m[36m(func pid=65548)[0m f1_per_class: [0.552, 0.4, 0.261, 0.206, 0.055, 0.374, 0.183, 0.342, 0.271, 0.245]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 1.1330 | Steps: 4 | Val loss: 1.7094 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 12:16:10 (running for 00:18:16.22)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.241 |      0.325 |                   92 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.151 |      0.252 |                   38 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.668 |      0.259 |                   16 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.209 |      0.289 |                   11 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.34375
[2m[36m(func pid=58891)[0m top5: 0.8381529850746269
[2m[36m(func pid=58891)[0m f1_micro: 0.34375
[2m[36m(func pid=58891)[0m f1_macro: 0.25197463821141286
[2m[36m(func pid=58891)[0m f1_weighted: 0.3543524710655901
[2m[36m(func pid=58891)[0m f1_per_class: [0.23, 0.149, 0.124, 0.506, 0.083, 0.327, 0.406, 0.291, 0.086, 0.319]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m top1: 0.302705223880597
[2m[36m(func pid=64137)[0m top5: 0.8208955223880597
[2m[36m(func pid=64137)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=64137)[0m f1_macro: 0.2670339849185469
[2m[36m(func pid=64137)[0m f1_weighted: 0.2930680128555757
[2m[36m(func pid=64137)[0m f1_per_class: [0.352, 0.365, 0.182, 0.442, 0.08, 0.4, 0.077, 0.352, 0.193, 0.227]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=45540)[0m top1: 0.37779850746268656
[2m[36m(func pid=45540)[0m top5: 0.8777985074626866
[2m[36m(func pid=45540)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=45540)[0m f1_macro: 0.32042187176336817
[2m[36m(func pid=45540)[0m f1_weighted: 0.4074566936196885
[2m[36m(func pid=45540)[0m f1_per_class: [0.414, 0.371, 0.231, 0.478, 0.086, 0.413, 0.414, 0.318, 0.231, 0.247]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.4116 | Steps: 4 | Val loss: 3.3786 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.1722 | Steps: 4 | Val loss: 1.8956 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.3110 | Steps: 4 | Val loss: 1.8332 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 1.1746 | Steps: 4 | Val loss: 1.7226 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=65548)[0m top1: 0.27238805970149255
[2m[36m(func pid=65548)[0m top5: 0.804570895522388
[2m[36m(func pid=65548)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=65548)[0m f1_macro: 0.2728916026078803
[2m[36m(func pid=65548)[0m f1_weighted: 0.26832188933539897
[2m[36m(func pid=65548)[0m f1_per_class: [0.487, 0.38, 0.258, 0.155, 0.073, 0.376, 0.253, 0.328, 0.239, 0.179]
[2m[36m(func pid=65548)[0m 
== Status ==
Current time: 2024-01-07 12:16:15 (running for 00:18:21.75)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.133 |      0.32  |                   93 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.151 |      0.252 |                   38 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.311 |      0.29  |                   18 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.412 |      0.273 |                   12 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m top1: 0.365205223880597
[2m[36m(func pid=58891)[0m top5: 0.8470149253731343
[2m[36m(func pid=58891)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=58891)[0m f1_macro: 0.2742109550405015
[2m[36m(func pid=58891)[0m f1_weighted: 0.3726341069120105
[2m[36m(func pid=58891)[0m f1_per_class: [0.306, 0.187, 0.159, 0.529, 0.083, 0.362, 0.4, 0.321, 0.063, 0.333]
[2m[36m(func pid=64137)[0m top1: 0.32975746268656714
[2m[36m(func pid=64137)[0m top5: 0.8334888059701493
[2m[36m(func pid=64137)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=64137)[0m f1_macro: 0.28955514585388065
[2m[36m(func pid=64137)[0m f1_weighted: 0.3187432745692906
[2m[36m(func pid=64137)[0m f1_per_class: [0.393, 0.363, 0.222, 0.485, 0.082, 0.434, 0.104, 0.359, 0.199, 0.254]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=45540)[0m top1: 0.3736007462686567
[2m[36m(func pid=45540)[0m top5: 0.878731343283582
[2m[36m(func pid=45540)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=45540)[0m f1_macro: 0.31744889613487104
[2m[36m(func pid=45540)[0m f1_weighted: 0.3996505701245789
[2m[36m(func pid=45540)[0m f1_per_class: [0.368, 0.375, 0.24, 0.482, 0.095, 0.409, 0.385, 0.323, 0.233, 0.264]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 0.8743 | Steps: 4 | Val loss: 3.2849 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.2676 | Steps: 4 | Val loss: 1.7827 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.1228 | Steps: 4 | Val loss: 1.8914 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 1.2308 | Steps: 4 | Val loss: 1.7023 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=65548)[0m top1: 0.30597014925373134
[2m[36m(func pid=65548)[0m top5: 0.8344216417910447
[2m[36m(func pid=65548)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=65548)[0m f1_macro: 0.2850048867524865
[2m[36m(func pid=65548)[0m f1_weighted: 0.30789051847281707
[2m[36m(func pid=65548)[0m f1_per_class: [0.482, 0.419, 0.231, 0.188, 0.098, 0.402, 0.333, 0.288, 0.224, 0.186]
[2m[36m(func pid=65548)[0m 
== Status ==
Current time: 2024-01-07 12:16:21 (running for 00:18:27.13)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.175 |      0.317 |                   94 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.172 |      0.274 |                   39 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.268 |      0.31  |                   19 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.874 |      0.285 |                   13 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.3498134328358209
[2m[36m(func pid=64137)[0m top5: 0.851679104477612
[2m[36m(func pid=64137)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=64137)[0m f1_macro: 0.3095245596120034
[2m[36m(func pid=64137)[0m f1_weighted: 0.33551300043739707
[2m[36m(func pid=64137)[0m f1_per_class: [0.452, 0.393, 0.258, 0.506, 0.095, 0.427, 0.117, 0.366, 0.214, 0.267]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=45540)[0m top1: 0.3829291044776119
[2m[36m(func pid=45540)[0m top5: 0.8805970149253731
[2m[36m(func pid=45540)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=45540)[0m f1_macro: 0.32562402402310003
[2m[36m(func pid=45540)[0m f1_weighted: 0.4071399577331736
[2m[36m(func pid=45540)[0m f1_per_class: [0.389, 0.396, 0.273, 0.492, 0.096, 0.414, 0.385, 0.322, 0.223, 0.264]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=58891)[0m top1: 0.3736007462686567
[2m[36m(func pid=58891)[0m top5: 0.8526119402985075
[2m[36m(func pid=58891)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=58891)[0m f1_macro: 0.2801890727206306
[2m[36m(func pid=58891)[0m f1_weighted: 0.3916329893409328
[2m[36m(func pid=58891)[0m f1_per_class: [0.321, 0.225, 0.144, 0.528, 0.082, 0.389, 0.435, 0.295, 0.082, 0.299]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.2576 | Steps: 4 | Val loss: 3.3176 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.5051 | Steps: 4 | Val loss: 1.7397 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 1.2104 | Steps: 4 | Val loss: 1.7046 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.1383 | Steps: 4 | Val loss: 1.8916 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=65548)[0m top1: 0.3204291044776119
[2m[36m(func pid=65548)[0m top5: 0.8638059701492538
[2m[36m(func pid=65548)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=65548)[0m f1_macro: 0.3004539427066508
[2m[36m(func pid=65548)[0m f1_weighted: 0.319083665115213
[2m[36m(func pid=65548)[0m f1_per_class: [0.509, 0.438, 0.245, 0.206, 0.142, 0.396, 0.341, 0.293, 0.199, 0.235]
[2m[36m(func pid=65548)[0m 
== Status ==
Current time: 2024-01-07 12:16:26 (running for 00:18:32.46)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.231 |      0.326 |                   95 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.123 |      0.28  |                   40 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.505 |      0.323 |                   20 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.258 |      0.3   |                   14 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.3568097014925373
[2m[36m(func pid=64137)[0m top5: 0.867070895522388
[2m[36m(func pid=64137)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=64137)[0m f1_macro: 0.32337982511557967
[2m[36m(func pid=64137)[0m f1_weighted: 0.34490391520665975
[2m[36m(func pid=64137)[0m f1_per_class: [0.474, 0.359, 0.338, 0.517, 0.104, 0.42, 0.154, 0.373, 0.235, 0.259]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=45540)[0m top1: 0.38013059701492535
[2m[36m(func pid=45540)[0m top5: 0.8819962686567164
[2m[36m(func pid=45540)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=45540)[0m f1_macro: 0.3225866211820355
[2m[36m(func pid=45540)[0m f1_weighted: 0.40313399104498376
[2m[36m(func pid=45540)[0m f1_per_class: [0.402, 0.362, 0.258, 0.5, 0.1, 0.412, 0.386, 0.314, 0.225, 0.266]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=58891)[0m top1: 0.36100746268656714
[2m[36m(func pid=58891)[0m top5: 0.8493470149253731
[2m[36m(func pid=58891)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=58891)[0m f1_macro: 0.28089217097634617
[2m[36m(func pid=58891)[0m f1_weighted: 0.3806638943182268
[2m[36m(func pid=58891)[0m f1_per_class: [0.338, 0.23, 0.111, 0.518, 0.077, 0.357, 0.407, 0.329, 0.104, 0.339]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8576 | Steps: 4 | Val loss: 3.0308 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.6015 | Steps: 4 | Val loss: 1.7002 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 1.3006 | Steps: 4 | Val loss: 1.6757 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.9508 | Steps: 4 | Val loss: 1.8754 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=65548)[0m top1: 0.37080223880597013
[2m[36m(func pid=65548)[0m top5: 0.8917910447761194
[2m[36m(func pid=65548)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=65548)[0m f1_macro: 0.3259805965109471
[2m[36m(func pid=65548)[0m f1_weighted: 0.3830441143737326
[2m[36m(func pid=65548)[0m f1_per_class: [0.487, 0.483, 0.283, 0.4, 0.143, 0.4, 0.355, 0.245, 0.205, 0.259]
[2m[36m(func pid=65548)[0m 
== Status ==
Current time: 2024-01-07 12:16:32 (running for 00:18:37.99)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.21  |      0.323 |                   96 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.138 |      0.281 |                   41 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.602 |      0.33  |                   21 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.858 |      0.326 |                   15 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.36380597014925375
[2m[36m(func pid=64137)[0m top5: 0.8857276119402985
[2m[36m(func pid=64137)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=64137)[0m f1_macro: 0.32984450792368714
[2m[36m(func pid=64137)[0m f1_weighted: 0.36143066785200706
[2m[36m(func pid=64137)[0m f1_per_class: [0.478, 0.331, 0.393, 0.527, 0.107, 0.43, 0.218, 0.355, 0.215, 0.245]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=45540)[0m top1: 0.3903917910447761
[2m[36m(func pid=45540)[0m top5: 0.8782649253731343
[2m[36m(func pid=45540)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=45540)[0m f1_macro: 0.32496031121008095
[2m[36m(func pid=45540)[0m f1_weighted: 0.4070424676427077
[2m[36m(func pid=45540)[0m f1_per_class: [0.387, 0.376, 0.216, 0.526, 0.122, 0.418, 0.362, 0.315, 0.249, 0.278]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=58891)[0m top1: 0.3763992537313433
[2m[36m(func pid=58891)[0m top5: 0.8535447761194029
[2m[36m(func pid=58891)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=58891)[0m f1_macro: 0.29641941425915125
[2m[36m(func pid=58891)[0m f1_weighted: 0.3944006652496535
[2m[36m(func pid=58891)[0m f1_per_class: [0.4, 0.275, 0.136, 0.529, 0.076, 0.356, 0.41, 0.33, 0.122, 0.329]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.6726 | Steps: 4 | Val loss: 3.0088 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.1527 | Steps: 4 | Val loss: 1.6696 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 1.3406 | Steps: 4 | Val loss: 1.6412 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.9553 | Steps: 4 | Val loss: 1.8735 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=65548)[0m top1: 0.39925373134328357
[2m[36m(func pid=65548)[0m top5: 0.8889925373134329
[2m[36m(func pid=65548)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=65548)[0m f1_macro: 0.33897520859183355
[2m[36m(func pid=65548)[0m f1_weighted: 0.4188651738537417
[2m[36m(func pid=65548)[0m f1_per_class: [0.453, 0.457, 0.28, 0.542, 0.207, 0.413, 0.352, 0.268, 0.189, 0.229]
[2m[36m(func pid=65548)[0m 
== Status ==
Current time: 2024-01-07 12:16:37 (running for 00:18:43.37)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.301 |      0.325 |                   97 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.951 |      0.296 |                   42 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.153 |      0.348 |                   22 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.673 |      0.339 |                   16 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.3894589552238806
[2m[36m(func pid=64137)[0m top5: 0.8969216417910447
[2m[36m(func pid=64137)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=64137)[0m f1_macro: 0.3479561501373851
[2m[36m(func pid=64137)[0m f1_weighted: 0.39434242620777044
[2m[36m(func pid=64137)[0m f1_per_class: [0.482, 0.355, 0.353, 0.538, 0.118, 0.433, 0.293, 0.387, 0.235, 0.286]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=45540)[0m top1: 0.4048507462686567
[2m[36m(func pid=45540)[0m top5: 0.8894589552238806
[2m[36m(func pid=45540)[0m f1_micro: 0.40485074626865664
[2m[36m(func pid=45540)[0m f1_macro: 0.34033692969549795
[2m[36m(func pid=45540)[0m f1_weighted: 0.41475264694995845
[2m[36m(func pid=45540)[0m f1_per_class: [0.425, 0.36, 0.277, 0.545, 0.144, 0.439, 0.362, 0.348, 0.247, 0.257]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=58891)[0m top1: 0.36473880597014924
[2m[36m(func pid=58891)[0m top5: 0.8544776119402985
[2m[36m(func pid=58891)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=58891)[0m f1_macro: 0.294316382305421
[2m[36m(func pid=58891)[0m f1_weighted: 0.3853028473869179
[2m[36m(func pid=58891)[0m f1_per_class: [0.446, 0.241, 0.122, 0.522, 0.068, 0.37, 0.398, 0.333, 0.124, 0.32]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.5835 | Steps: 4 | Val loss: 3.1092 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.5905 | Steps: 4 | Val loss: 1.6175 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 1.1882 | Steps: 4 | Val loss: 1.6542 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.0177 | Steps: 4 | Val loss: 1.8832 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=65548)[0m top1: 0.4295708955223881
[2m[36m(func pid=65548)[0m top5: 0.9043843283582089
[2m[36m(func pid=65548)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=65548)[0m f1_macro: 0.3520358702215828
[2m[36m(func pid=65548)[0m f1_weighted: 0.42894698851105845
[2m[36m(func pid=65548)[0m f1_per_class: [0.446, 0.389, 0.313, 0.597, 0.231, 0.4, 0.368, 0.297, 0.205, 0.273]
[2m[36m(func pid=65548)[0m 
== Status ==
Current time: 2024-01-07 12:16:42 (running for 00:18:48.74)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00005 | RUNNING    | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.341 |      0.34  |                   98 |
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.955 |      0.294 |                   43 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.59  |      0.372 |                   23 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.583 |      0.352 |                   17 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.40951492537313433
[2m[36m(func pid=64137)[0m top5: 0.9160447761194029
[2m[36m(func pid=64137)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=64137)[0m f1_macro: 0.37206171497477747
[2m[36m(func pid=64137)[0m f1_weighted: 0.42011031959504624
[2m[36m(func pid=64137)[0m f1_per_class: [0.574, 0.407, 0.375, 0.522, 0.126, 0.441, 0.357, 0.355, 0.255, 0.308]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=45540)[0m top1: 0.40531716417910446
[2m[36m(func pid=45540)[0m top5: 0.882929104477612
[2m[36m(func pid=45540)[0m f1_micro: 0.40531716417910446
[2m[36m(func pid=45540)[0m f1_macro: 0.34266824260979883
[2m[36m(func pid=45540)[0m f1_weighted: 0.41306920254449603
[2m[36m(func pid=45540)[0m f1_per_class: [0.422, 0.353, 0.292, 0.546, 0.154, 0.439, 0.353, 0.38, 0.249, 0.238]
[2m[36m(func pid=45540)[0m 
[2m[36m(func pid=58891)[0m top1: 0.3596082089552239
[2m[36m(func pid=58891)[0m top5: 0.8563432835820896
[2m[36m(func pid=58891)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=58891)[0m f1_macro: 0.29446017974329214
[2m[36m(func pid=58891)[0m f1_weighted: 0.38830154520805543
[2m[36m(func pid=58891)[0m f1_per_class: [0.441, 0.292, 0.107, 0.512, 0.066, 0.364, 0.391, 0.323, 0.143, 0.306]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.5681 | Steps: 4 | Val loss: 3.3567 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.4273 | Steps: 4 | Val loss: 1.6185 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=45540)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 1.1136 | Steps: 4 | Val loss: 1.6867 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.9480 | Steps: 4 | Val loss: 1.8973 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=65548)[0m top1: 0.4123134328358209
[2m[36m(func pid=65548)[0m top5: 0.9053171641791045
[2m[36m(func pid=65548)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=65548)[0m f1_macro: 0.31141418448216873
[2m[36m(func pid=65548)[0m f1_weighted: 0.398298529153646
[2m[36m(func pid=65548)[0m f1_per_class: [0.411, 0.252, 0.234, 0.578, 0.128, 0.365, 0.385, 0.293, 0.192, 0.276]
[2m[36m(func pid=65548)[0m 
== Status ==
Current time: 2024-01-07 12:16:48 (running for 00:18:53.94)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (13 PENDING, 3 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  2.018 |      0.294 |                   44 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.59  |      0.372 |                   23 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.568 |      0.311 |                   18 |
| train_9b9e8_00011 | PENDING    |                    | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=45540)[0m top1: 0.39132462686567165
[2m[36m(func pid=45540)[0m top5: 0.878731343283582
[2m[36m(func pid=45540)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=45540)[0m f1_macro: 0.3285679270386267
[2m[36m(func pid=45540)[0m f1_weighted: 0.4067355000350963
[2m[36m(func pid=45540)[0m f1_per_class: [0.402, 0.354, 0.236, 0.529, 0.128, 0.434, 0.355, 0.375, 0.238, 0.235]
[2m[36m(func pid=64137)[0m top1: 0.41091417910447764
[2m[36m(func pid=64137)[0m top5: 0.9207089552238806
[2m[36m(func pid=64137)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=64137)[0m f1_macro: 0.35878514625202684
[2m[36m(func pid=64137)[0m f1_weighted: 0.42709855907668354
[2m[36m(func pid=64137)[0m f1_per_class: [0.496, 0.455, 0.333, 0.512, 0.112, 0.428, 0.387, 0.3, 0.235, 0.331]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m top1: 0.3358208955223881
[2m[36m(func pid=58891)[0m top5: 0.8493470149253731
[2m[36m(func pid=58891)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=58891)[0m f1_macro: 0.2876035048719514
[2m[36m(func pid=58891)[0m f1_weighted: 0.3764554919249659
[2m[36m(func pid=58891)[0m f1_per_class: [0.463, 0.277, 0.086, 0.448, 0.061, 0.383, 0.414, 0.327, 0.117, 0.301]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.5562 | Steps: 4 | Val loss: 3.5369 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.4136 | Steps: 4 | Val loss: 1.5965 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.7660 | Steps: 4 | Val loss: 1.8869 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=65548)[0m top1: 0.39225746268656714
[2m[36m(func pid=65548)[0m top5: 0.8894589552238806
[2m[36m(func pid=65548)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=65548)[0m f1_macro: 0.29439092659158206
[2m[36m(func pid=65548)[0m f1_weighted: 0.4031477209126155
[2m[36m(func pid=65548)[0m f1_per_class: [0.362, 0.245, 0.141, 0.579, 0.163, 0.317, 0.428, 0.304, 0.188, 0.217]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=64137)[0m top1: 0.4291044776119403
[2m[36m(func pid=64137)[0m top5: 0.9291044776119403
[2m[36m(func pid=64137)[0m f1_micro: 0.4291044776119403
[2m[36m(func pid=64137)[0m f1_macro: 0.3660252959698328
[2m[36m(func pid=64137)[0m f1_weighted: 0.447421823600237
[2m[36m(func pid=64137)[0m f1_per_class: [0.547, 0.457, 0.324, 0.5, 0.123, 0.444, 0.471, 0.224, 0.218, 0.353]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m top1: 0.34888059701492535
[2m[36m(func pid=58891)[0m top5: 0.8470149253731343
[2m[36m(func pid=58891)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=58891)[0m f1_macro: 0.29525165827294014
[2m[36m(func pid=58891)[0m f1_weighted: 0.386875440716795
[2m[36m(func pid=58891)[0m f1_per_class: [0.474, 0.285, 0.113, 0.478, 0.066, 0.381, 0.412, 0.342, 0.127, 0.274]
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.0352 | Steps: 4 | Val loss: 4.0556 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.3109 | Steps: 4 | Val loss: 1.6300 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 12:16:53 (running for 00:18:59.25)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.948 |      0.288 |                   45 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.414 |      0.366 |                   25 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.556 |      0.294 |                   19 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=70356)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=70356)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=70356)[0m Configuration completed!
[2m[36m(func pid=70356)[0m New optimizer parameters:
[2m[36m(func pid=70356)[0m SGD (
[2m[36m(func pid=70356)[0m Parameter Group 0
[2m[36m(func pid=70356)[0m     dampening: 0
[2m[36m(func pid=70356)[0m     differentiable: False
[2m[36m(func pid=70356)[0m     foreach: None
[2m[36m(func pid=70356)[0m     lr: 0.1
[2m[36m(func pid=70356)[0m     maximize: False
[2m[36m(func pid=70356)[0m     momentum: 0.99
[2m[36m(func pid=70356)[0m     nesterov: False
[2m[36m(func pid=70356)[0m     weight_decay: 0.0001
[2m[36m(func pid=70356)[0m )
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m top1: 0.32882462686567165
[2m[36m(func pid=65548)[0m top5: 0.8493470149253731
[2m[36m(func pid=65548)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=65548)[0m f1_macro: 0.26831229653417854
[2m[36m(func pid=65548)[0m f1_weighted: 0.36831705113659224
[2m[36m(func pid=65548)[0m f1_per_class: [0.308, 0.325, 0.104, 0.417, 0.172, 0.259, 0.445, 0.298, 0.19, 0.164]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=64137)[0m top1: 0.4076492537313433
[2m[36m(func pid=64137)[0m top5: 0.9356343283582089
[2m[36m(func pid=64137)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=64137)[0m f1_macro: 0.3488163692793445
[2m[36m(func pid=64137)[0m f1_weighted: 0.4287188752665278
[2m[36m(func pid=64137)[0m f1_per_class: [0.537, 0.432, 0.324, 0.443, 0.105, 0.407, 0.496, 0.204, 0.215, 0.325]
== Status ==
Current time: 2024-01-07 12:16:58 (running for 00:19:04.50)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.766 |      0.295 |                   46 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.311 |      0.349 |                   26 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.035 |      0.268 |                   20 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.9072 | Steps: 4 | Val loss: 1.8851 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.4971 | Steps: 4 | Val loss: 5.0531 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9155 | Steps: 4 | Val loss: 2.3337 | Batch size: 32 | lr: 0.1 | Duration: 4.35s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.3355 | Steps: 4 | Val loss: 1.6940 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=58891)[0m top1: 0.3414179104477612
[2m[36m(func pid=58891)[0m top5: 0.8456156716417911
[2m[36m(func pid=58891)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=58891)[0m f1_macro: 0.28820886776014565
[2m[36m(func pid=58891)[0m f1_weighted: 0.37771087365923856
[2m[36m(func pid=58891)[0m f1_per_class: [0.407, 0.278, 0.102, 0.456, 0.075, 0.393, 0.402, 0.351, 0.158, 0.261]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m top1: 0.27472014925373134
[2m[36m(func pid=65548)[0m top5: 0.7826492537313433
[2m[36m(func pid=65548)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=65548)[0m f1_macro: 0.23178627663865198
[2m[36m(func pid=65548)[0m f1_weighted: 0.291972387744318
[2m[36m(func pid=65548)[0m f1_per_class: [0.351, 0.338, 0.092, 0.164, 0.132, 0.18, 0.452, 0.267, 0.215, 0.128]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m top1: 0.16930970149253732
[2m[36m(func pid=70356)[0m top5: 0.6856343283582089
[2m[36m(func pid=70356)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=70356)[0m f1_macro: 0.16633647313721564
[2m[36m(func pid=70356)[0m f1_weighted: 0.15438514917510543
[2m[36m(func pid=70356)[0m f1_per_class: [0.083, 0.0, 0.741, 0.276, 0.062, 0.0, 0.226, 0.0, 0.0, 0.276]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m top1: 0.40111940298507465
[2m[36m(func pid=64137)[0m top5: 0.9291044776119403
[2m[36m(func pid=64137)[0m f1_micro: 0.40111940298507465
[2m[36m(func pid=64137)[0m f1_macro: 0.33591155405905565
[2m[36m(func pid=64137)[0m f1_weighted: 0.42460885585778685
[2m[36m(func pid=64137)[0m f1_per_class: [0.496, 0.434, 0.273, 0.425, 0.095, 0.421, 0.503, 0.17, 0.214, 0.328]
== Status ==
Current time: 2024-01-07 12:17:03 (running for 00:19:09.68)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.907 |      0.288 |                   47 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.336 |      0.336 |                   27 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.497 |      0.232 |                   21 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  2.916 |      0.166 |                    1 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.7545 | Steps: 4 | Val loss: 1.8810 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.5577 | Steps: 4 | Val loss: 2.9070 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.6659 | Steps: 4 | Val loss: 5.7966 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.1920 | Steps: 4 | Val loss: 1.7778 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=58891)[0m top1: 0.3414179104477612
[2m[36m(func pid=58891)[0m top5: 0.847481343283582
[2m[36m(func pid=58891)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=58891)[0m f1_macro: 0.2960969662927496
[2m[36m(func pid=58891)[0m f1_weighted: 0.3768205982088156
[2m[36m(func pid=58891)[0m f1_per_class: [0.451, 0.275, 0.117, 0.458, 0.064, 0.389, 0.392, 0.354, 0.194, 0.268]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=70356)[0m top1: 0.1455223880597015
[2m[36m(func pid=70356)[0m top5: 0.695429104477612
[2m[36m(func pid=70356)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=70356)[0m f1_macro: 0.2001673972220289
[2m[36m(func pid=70356)[0m f1_weighted: 0.13298715743840867
[2m[36m(func pid=70356)[0m f1_per_class: [0.238, 0.07, 0.218, 0.142, 0.059, 0.355, 0.003, 0.449, 0.084, 0.382]
[2m[36m(func pid=70356)[0m 
== Status ==
Current time: 2024-01-07 12:17:08 (running for 00:19:14.81)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.754 |      0.296 |                   48 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.336 |      0.336 |                   27 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.666 |      0.226 |                   22 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  2.558 |      0.2   |                    2 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=65548)[0m top1: 0.2691231343283582
[2m[36m(func pid=65548)[0m top5: 0.7341417910447762
[2m[36m(func pid=65548)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=65548)[0m f1_macro: 0.2258231304215701
[2m[36m(func pid=65548)[0m f1_weighted: 0.2633539275538176
[2m[36m(func pid=65548)[0m f1_per_class: [0.352, 0.32, 0.112, 0.029, 0.153, 0.203, 0.482, 0.277, 0.2, 0.131]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=64137)[0m top1: 0.37919776119402987
[2m[36m(func pid=64137)[0m top5: 0.9216417910447762
[2m[36m(func pid=64137)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=64137)[0m f1_macro: 0.31645332472327636
[2m[36m(func pid=64137)[0m f1_weighted: 0.40756956657423216
[2m[36m(func pid=64137)[0m f1_per_class: [0.5, 0.423, 0.203, 0.41, 0.096, 0.388, 0.484, 0.17, 0.193, 0.298]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.8360 | Steps: 4 | Val loss: 1.8933 | Batch size: 32 | lr: 0.0001 | Duration: 3.19s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.4961 | Steps: 4 | Val loss: 4.1441 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5118 | Steps: 4 | Val loss: 6.0729 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.3356 | Steps: 4 | Val loss: 1.7804 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=58891)[0m top1: 0.32276119402985076
[2m[36m(func pid=58891)[0m top5: 0.8404850746268657
[2m[36m(func pid=58891)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=58891)[0m f1_macro: 0.2823076072818102
[2m[36m(func pid=58891)[0m f1_weighted: 0.35905178447053326
[2m[36m(func pid=58891)[0m f1_per_class: [0.466, 0.26, 0.099, 0.438, 0.069, 0.377, 0.372, 0.339, 0.147, 0.256]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=70356)[0m top1: 0.25513059701492535
[2m[36m(func pid=70356)[0m top5: 0.6958955223880597
[2m[36m(func pid=70356)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=70356)[0m f1_macro: 0.2847533425358086
[2m[36m(func pid=70356)[0m f1_weighted: 0.25147773752154334
[2m[36m(func pid=70356)[0m f1_per_class: [0.608, 0.42, 0.06, 0.316, 0.145, 0.394, 0.0, 0.401, 0.14, 0.364]
[2m[36m(func pid=70356)[0m 
== Status ==
Current time: 2024-01-07 12:17:14 (running for 00:19:20.37)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.836 |      0.282 |                   49 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.192 |      0.316 |                   28 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.512 |      0.235 |                   23 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  2.496 |      0.285 |                    3 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=65548)[0m top1: 0.27705223880597013
[2m[36m(func pid=65548)[0m top5: 0.715018656716418
[2m[36m(func pid=65548)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=65548)[0m f1_macro: 0.23459968872808817
[2m[36m(func pid=65548)[0m f1_weighted: 0.27061413597970574
[2m[36m(func pid=65548)[0m f1_per_class: [0.385, 0.338, 0.123, 0.01, 0.134, 0.264, 0.49, 0.276, 0.196, 0.13]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=64137)[0m top1: 0.38526119402985076
[2m[36m(func pid=64137)[0m top5: 0.9169776119402985
[2m[36m(func pid=64137)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=64137)[0m f1_macro: 0.31953082581673414
[2m[36m(func pid=64137)[0m f1_weighted: 0.41224314264306816
[2m[36m(func pid=64137)[0m f1_per_class: [0.504, 0.425, 0.211, 0.396, 0.112, 0.393, 0.507, 0.188, 0.19, 0.269]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.8964 | Steps: 4 | Val loss: 1.8869 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.4483 | Steps: 4 | Val loss: 3.3612 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7998 | Steps: 4 | Val loss: 6.1624 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.9245 | Steps: 4 | Val loss: 1.8562 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=58891)[0m top1: 0.3269589552238806
[2m[36m(func pid=58891)[0m top5: 0.8367537313432836
[2m[36m(func pid=58891)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=58891)[0m f1_macro: 0.2885051834246805
[2m[36m(func pid=58891)[0m f1_weighted: 0.36407212504267195
[2m[36m(func pid=58891)[0m f1_per_class: [0.484, 0.3, 0.106, 0.417, 0.072, 0.379, 0.381, 0.351, 0.154, 0.241]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=70356)[0m top1: 0.365205223880597
[2m[36m(func pid=70356)[0m top5: 0.8992537313432836
[2m[36m(func pid=70356)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=70356)[0m f1_macro: 0.287902769320007
[2m[36m(func pid=70356)[0m f1_weighted: 0.38562637070378336
[2m[36m(func pid=70356)[0m f1_per_class: [0.34, 0.186, 0.242, 0.487, 0.086, 0.405, 0.473, 0.241, 0.124, 0.295]
[2m[36m(func pid=70356)[0m 
== Status ==
Current time: 2024-01-07 12:17:19 (running for 00:19:25.88)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.896 |      0.289 |                   50 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.336 |      0.32  |                   29 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.8   |      0.247 |                   24 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  2.448 |      0.288 |                    4 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=65548)[0m top1: 0.28544776119402987
[2m[36m(func pid=65548)[0m top5: 0.7168843283582089
[2m[36m(func pid=65548)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=65548)[0m f1_macro: 0.24743583911070974
[2m[36m(func pid=65548)[0m f1_weighted: 0.27492813659827675
[2m[36m(func pid=65548)[0m f1_per_class: [0.449, 0.335, 0.141, 0.003, 0.124, 0.299, 0.49, 0.287, 0.211, 0.135]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=64137)[0m top1: 0.3694029850746269
[2m[36m(func pid=64137)[0m top5: 0.9132462686567164
[2m[36m(func pid=64137)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=64137)[0m f1_macro: 0.31681166350635737
[2m[36m(func pid=64137)[0m f1_weighted: 0.4024690395994794
[2m[36m(func pid=64137)[0m f1_per_class: [0.561, 0.389, 0.183, 0.387, 0.099, 0.386, 0.494, 0.233, 0.198, 0.237]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.8499 | Steps: 4 | Val loss: 1.8890 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.8184 | Steps: 4 | Val loss: 6.8402 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.1841 | Steps: 4 | Val loss: 5.7603 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=58891)[0m top1: 0.33302238805970147
[2m[36m(func pid=58891)[0m top5: 0.8330223880597015
[2m[36m(func pid=58891)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=58891)[0m f1_macro: 0.28820505860858764
[2m[36m(func pid=58891)[0m f1_weighted: 0.372687768549951
[2m[36m(func pid=58891)[0m f1_per_class: [0.459, 0.309, 0.117, 0.425, 0.075, 0.403, 0.395, 0.319, 0.162, 0.217]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.0510 | Steps: 4 | Val loss: 1.9402 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=70356)[0m top1: 0.30830223880597013
[2m[36m(func pid=70356)[0m top5: 0.7737873134328358
[2m[36m(func pid=70356)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=70356)[0m f1_macro: 0.19806313777395773
[2m[36m(func pid=70356)[0m f1_weighted: 0.3016434871727112
[2m[36m(func pid=70356)[0m f1_per_class: [0.127, 0.158, 0.095, 0.16, 0.103, 0.375, 0.574, 0.147, 0.026, 0.216]
[2m[36m(func pid=70356)[0m 
== Status ==
Current time: 2024-01-07 12:17:25 (running for 00:19:31.32)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.85  |      0.288 |                   51 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.924 |      0.317 |                   30 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.184 |      0.254 |                   25 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  1.818 |      0.198 |                    5 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=65548)[0m top1: 0.292910447761194
[2m[36m(func pid=65548)[0m top5: 0.7541977611940298
[2m[36m(func pid=65548)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=65548)[0m f1_macro: 0.254081020479346
[2m[36m(func pid=65548)[0m f1_weighted: 0.29263079288709776
[2m[36m(func pid=65548)[0m f1_per_class: [0.424, 0.34, 0.131, 0.051, 0.115, 0.361, 0.482, 0.276, 0.208, 0.152]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=64137)[0m top1: 0.3498134328358209
[2m[36m(func pid=64137)[0m top5: 0.9011194029850746
[2m[36m(func pid=64137)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=64137)[0m f1_macro: 0.3013317075582005
[2m[36m(func pid=64137)[0m f1_weighted: 0.38917759689389175
[2m[36m(func pid=64137)[0m f1_per_class: [0.492, 0.369, 0.128, 0.391, 0.102, 0.395, 0.457, 0.257, 0.176, 0.246]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.8831 | Steps: 4 | Val loss: 1.9000 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.4525 | Steps: 4 | Val loss: 8.7273 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.4122 | Steps: 4 | Val loss: 5.0909 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.7100 | Steps: 4 | Val loss: 1.9683 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=58891)[0m top1: 0.322294776119403
[2m[36m(func pid=58891)[0m top5: 0.8278917910447762
[2m[36m(func pid=58891)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=58891)[0m f1_macro: 0.2861915021749938
[2m[36m(func pid=58891)[0m f1_weighted: 0.362973265437931
[2m[36m(func pid=58891)[0m f1_per_class: [0.471, 0.318, 0.121, 0.4, 0.062, 0.394, 0.383, 0.314, 0.174, 0.225]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=70356)[0m top1: 0.21175373134328357
[2m[36m(func pid=70356)[0m top5: 0.7075559701492538
[2m[36m(func pid=70356)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=70356)[0m f1_macro: 0.20223305975290712
[2m[36m(func pid=70356)[0m f1_weighted: 0.22565533848429467
[2m[36m(func pid=70356)[0m f1_per_class: [0.205, 0.302, 0.05, 0.184, 0.1, 0.372, 0.167, 0.309, 0.145, 0.189]
[2m[36m(func pid=70356)[0m 
== Status ==
Current time: 2024-01-07 12:17:30 (running for 00:19:36.90)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.883 |      0.286 |                   52 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.71  |      0.297 |                   32 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.184 |      0.254 |                   25 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  3.452 |      0.202 |                    6 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.34281716417910446
[2m[36m(func pid=64137)[0m top5: 0.8917910447761194
[2m[36m(func pid=64137)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=64137)[0m f1_macro: 0.29680827022126877
[2m[36m(func pid=64137)[0m f1_weighted: 0.38190817892566303
[2m[36m(func pid=64137)[0m f1_per_class: [0.476, 0.348, 0.124, 0.406, 0.102, 0.404, 0.426, 0.273, 0.185, 0.224]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m top1: 0.31902985074626866
[2m[36m(func pid=65548)[0m top5: 0.8199626865671642
[2m[36m(func pid=65548)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=65548)[0m f1_macro: 0.28365304724777024
[2m[36m(func pid=65548)[0m f1_weighted: 0.3330798438529796
[2m[36m(func pid=65548)[0m f1_per_class: [0.491, 0.339, 0.154, 0.204, 0.117, 0.378, 0.46, 0.273, 0.228, 0.193]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.9247 | Steps: 4 | Val loss: 1.8828 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.3067 | Steps: 4 | Val loss: 8.5187 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=58891)[0m top1: 0.32882462686567165
[2m[36m(func pid=58891)[0m top5: 0.8348880597014925
[2m[36m(func pid=58891)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=58891)[0m f1_macro: 0.29522380285117666
[2m[36m(func pid=58891)[0m f1_weighted: 0.36496561857872617
[2m[36m(func pid=58891)[0m f1_per_class: [0.479, 0.352, 0.137, 0.395, 0.066, 0.401, 0.364, 0.349, 0.172, 0.236]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.0636 | Steps: 4 | Val loss: 2.0531 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7882 | Steps: 4 | Val loss: 4.7307 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=70356)[0m top1: 0.28125
[2m[36m(func pid=70356)[0m top5: 0.7779850746268657
[2m[36m(func pid=70356)[0m f1_micro: 0.28125
[2m[36m(func pid=70356)[0m f1_macro: 0.2666304636734824
[2m[36m(func pid=70356)[0m f1_weighted: 0.2735924117332141
[2m[36m(func pid=70356)[0m f1_per_class: [0.496, 0.499, 0.16, 0.327, 0.074, 0.285, 0.096, 0.266, 0.132, 0.331]
[2m[36m(func pid=70356)[0m 
== Status ==
Current time: 2024-01-07 12:17:36 (running for 00:19:42.57)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.925 |      0.295 |                   53 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.064 |      0.29  |                   33 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.412 |      0.284 |                   26 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  2.307 |      0.267 |                    7 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.3246268656716418
[2m[36m(func pid=64137)[0m top5: 0.8740671641791045
[2m[36m(func pid=64137)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=64137)[0m f1_macro: 0.28977418486991857
[2m[36m(func pid=64137)[0m f1_weighted: 0.3644606920334149
[2m[36m(func pid=64137)[0m f1_per_class: [0.471, 0.307, 0.114, 0.375, 0.097, 0.407, 0.408, 0.333, 0.186, 0.199]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m top1: 0.33722014925373134
[2m[36m(func pid=65548)[0m top5: 0.8726679104477612
[2m[36m(func pid=65548)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=65548)[0m f1_macro: 0.30005283577229597
[2m[36m(func pid=65548)[0m f1_weighted: 0.36063107805432465
[2m[36m(func pid=65548)[0m f1_per_class: [0.521, 0.312, 0.182, 0.326, 0.113, 0.359, 0.459, 0.254, 0.238, 0.235]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.7687 | Steps: 4 | Val loss: 1.9149 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.7055 | Steps: 4 | Val loss: 7.0181 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=58891)[0m top1: 0.28777985074626866
[2m[36m(func pid=58891)[0m top5: 0.8274253731343284
[2m[36m(func pid=58891)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=58891)[0m f1_macro: 0.2801008739818082
[2m[36m(func pid=58891)[0m f1_weighted: 0.32086211744522913
[2m[36m(func pid=58891)[0m f1_per_class: [0.504, 0.315, 0.159, 0.333, 0.049, 0.408, 0.296, 0.334, 0.148, 0.254]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.7692 | Steps: 4 | Val loss: 2.1241 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6206 | Steps: 4 | Val loss: 4.5826 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=70356)[0m top1: 0.365205223880597
[2m[36m(func pid=70356)[0m top5: 0.8628731343283582
[2m[36m(func pid=70356)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=70356)[0m f1_macro: 0.33355952605914585
[2m[36m(func pid=70356)[0m f1_weighted: 0.3641323963356859
[2m[36m(func pid=70356)[0m f1_per_class: [0.516, 0.491, 0.273, 0.501, 0.145, 0.355, 0.197, 0.276, 0.227, 0.357]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m top1: 0.30923507462686567
[2m[36m(func pid=64137)[0m top5: 0.8661380597014925
[2m[36m(func pid=64137)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=64137)[0m f1_macro: 0.2806849099391601
[2m[36m(func pid=64137)[0m f1_weighted: 0.3468410104358667
[2m[36m(func pid=64137)[0m f1_per_class: [0.449, 0.307, 0.109, 0.349, 0.108, 0.403, 0.377, 0.336, 0.176, 0.192]
[2m[36m(func pid=64137)[0m == Status ==
Current time: 2024-01-07 12:17:41 (running for 00:19:47.90)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.769 |      0.28  |                   54 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.769 |      0.281 |                   34 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.788 |      0.3   |                   27 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  3.706 |      0.334 |                    8 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=65548)[0m top1: 0.34888059701492535
[2m[36m(func pid=65548)[0m top5: 0.8927238805970149
[2m[36m(func pid=65548)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=65548)[0m f1_macro: 0.3135904885721921
[2m[36m(func pid=65548)[0m f1_weighted: 0.36806778523181255
[2m[36m(func pid=65548)[0m f1_per_class: [0.539, 0.259, 0.222, 0.439, 0.128, 0.339, 0.408, 0.268, 0.252, 0.28]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.8494 | Steps: 4 | Val loss: 1.9136 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 4.0061 | Steps: 4 | Val loss: 6.4416 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=58891)[0m top1: 0.283115671641791
[2m[36m(func pid=58891)[0m top5: 0.8264925373134329
[2m[36m(func pid=58891)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=58891)[0m f1_macro: 0.2780149999327315
[2m[36m(func pid=58891)[0m f1_weighted: 0.3113539998121006
[2m[36m(func pid=58891)[0m f1_per_class: [0.496, 0.318, 0.16, 0.333, 0.05, 0.418, 0.258, 0.343, 0.136, 0.267]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8134 | Steps: 4 | Val loss: 2.1588 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=70356)[0m top1: 0.4468283582089552
[2m[36m(func pid=70356)[0m top5: 0.9365671641791045
[2m[36m(func pid=70356)[0m f1_micro: 0.4468283582089552
[2m[36m(func pid=70356)[0m f1_macro: 0.35806342194806934
[2m[36m(func pid=70356)[0m f1_weighted: 0.4487599116338808
[2m[36m(func pid=70356)[0m f1_per_class: [0.604, 0.396, 0.342, 0.534, 0.174, 0.349, 0.539, 0.115, 0.115, 0.412]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.1167 | Steps: 4 | Val loss: 4.4638 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 12:17:47 (running for 00:19:53.09)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.849 |      0.278 |                   55 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.813 |      0.278 |                   35 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.621 |      0.314 |                   28 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  4.006 |      0.358 |                    9 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.3003731343283582
[2m[36m(func pid=64137)[0m top5: 0.8619402985074627
[2m[36m(func pid=64137)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=64137)[0m f1_macro: 0.27845394062005985
[2m[36m(func pid=64137)[0m f1_weighted: 0.3332611390556502
[2m[36m(func pid=64137)[0m f1_per_class: [0.435, 0.301, 0.126, 0.316, 0.11, 0.398, 0.362, 0.375, 0.167, 0.195]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.8059 | Steps: 4 | Val loss: 1.9094 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=65548)[0m top1: 0.38013059701492535
[2m[36m(func pid=65548)[0m top5: 0.9113805970149254
[2m[36m(func pid=65548)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=65548)[0m f1_macro: 0.33157847909157945
[2m[36m(func pid=65548)[0m f1_weighted: 0.390616925840842
[2m[36m(func pid=65548)[0m f1_per_class: [0.516, 0.277, 0.286, 0.521, 0.134, 0.339, 0.393, 0.28, 0.261, 0.309]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 4.4228 | Steps: 4 | Val loss: 6.6998 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.1514 | Steps: 4 | Val loss: 2.1712 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=58891)[0m top1: 0.2891791044776119
[2m[36m(func pid=58891)[0m top5: 0.8278917910447762
[2m[36m(func pid=58891)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=58891)[0m f1_macro: 0.2820354350957013
[2m[36m(func pid=58891)[0m f1_weighted: 0.31555327446947845
[2m[36m(func pid=58891)[0m f1_per_class: [0.488, 0.34, 0.167, 0.332, 0.052, 0.411, 0.259, 0.356, 0.16, 0.256]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=70356)[0m top1: 0.45988805970149255
[2m[36m(func pid=70356)[0m top5: 0.9388992537313433
[2m[36m(func pid=70356)[0m f1_micro: 0.45988805970149255
[2m[36m(func pid=70356)[0m f1_macro: 0.3717092037316327
[2m[36m(func pid=70356)[0m f1_weighted: 0.46291272985607373
[2m[36m(func pid=70356)[0m f1_per_class: [0.638, 0.494, 0.306, 0.513, 0.168, 0.378, 0.543, 0.043, 0.18, 0.455]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.4991 | Steps: 4 | Val loss: 4.4755 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 12:17:52 (running for 00:19:58.35)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.806 |      0.282 |                   56 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.151 |      0.287 |                   36 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.117 |      0.332 |                   29 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  4.423 |      0.372 |                   10 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.31156716417910446
[2m[36m(func pid=64137)[0m top5: 0.8572761194029851
[2m[36m(func pid=64137)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=64137)[0m f1_macro: 0.2869503276774193
[2m[36m(func pid=64137)[0m f1_weighted: 0.34073320010258557
[2m[36m(func pid=64137)[0m f1_per_class: [0.441, 0.333, 0.139, 0.377, 0.119, 0.405, 0.305, 0.388, 0.18, 0.183]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.6942 | Steps: 4 | Val loss: 1.9376 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=65548)[0m top1: 0.37406716417910446
[2m[36m(func pid=65548)[0m top5: 0.9109141791044776
[2m[36m(func pid=65548)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=65548)[0m f1_macro: 0.32831389703366487
[2m[36m(func pid=65548)[0m f1_weighted: 0.38211815461691556
[2m[36m(func pid=65548)[0m f1_per_class: [0.44, 0.333, 0.32, 0.512, 0.138, 0.342, 0.351, 0.251, 0.224, 0.373]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9463 | Steps: 4 | Val loss: 9.9295 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.5761 | Steps: 4 | Val loss: 2.1774 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=58891)[0m top1: 0.27472014925373134
[2m[36m(func pid=58891)[0m top5: 0.8152985074626866
[2m[36m(func pid=58891)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=58891)[0m f1_macro: 0.27338943762338686
[2m[36m(func pid=58891)[0m f1_weighted: 0.29831077162204933
[2m[36m(func pid=58891)[0m f1_per_class: [0.446, 0.315, 0.162, 0.305, 0.049, 0.413, 0.24, 0.365, 0.179, 0.261]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=70356)[0m top1: 0.3605410447761194
[2m[36m(func pid=70356)[0m top5: 0.8666044776119403
[2m[36m(func pid=70356)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=70356)[0m f1_macro: 0.34470688939366556
[2m[36m(func pid=70356)[0m f1_weighted: 0.36126601712160294
[2m[36m(func pid=70356)[0m f1_per_class: [0.608, 0.482, 0.277, 0.29, 0.172, 0.334, 0.398, 0.238, 0.192, 0.458]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.9623 | Steps: 4 | Val loss: 4.2810 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:17:57 (running for 00:20:03.85)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.694 |      0.273 |                   57 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.576 |      0.282 |                   37 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.499 |      0.328 |                   30 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  2.946 |      0.345 |                   11 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.30597014925373134
[2m[36m(func pid=64137)[0m top5: 0.8610074626865671
[2m[36m(func pid=64137)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=64137)[0m f1_macro: 0.28213881063438784
[2m[36m(func pid=64137)[0m f1_weighted: 0.32854356121312783
[2m[36m(func pid=64137)[0m f1_per_class: [0.421, 0.347, 0.132, 0.37, 0.13, 0.395, 0.266, 0.384, 0.192, 0.184]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.7261 | Steps: 4 | Val loss: 1.9308 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=65548)[0m top1: 0.40158582089552236
[2m[36m(func pid=65548)[0m top5: 0.9244402985074627
[2m[36m(func pid=65548)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=65548)[0m f1_macro: 0.3710894391758906
[2m[36m(func pid=65548)[0m f1_weighted: 0.40789539881289616
[2m[36m(func pid=65548)[0m f1_per_class: [0.471, 0.436, 0.511, 0.507, 0.169, 0.384, 0.361, 0.234, 0.223, 0.415]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.2937 | Steps: 4 | Val loss: 14.6949 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=58891)[0m top1: 0.2737873134328358
[2m[36m(func pid=58891)[0m top5: 0.820429104477612
[2m[36m(func pid=58891)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=58891)[0m f1_macro: 0.2691400631327743
[2m[36m(func pid=58891)[0m f1_weighted: 0.2949644052912156
[2m[36m(func pid=58891)[0m f1_per_class: [0.418, 0.318, 0.173, 0.299, 0.051, 0.411, 0.236, 0.358, 0.171, 0.255]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.7240 | Steps: 4 | Val loss: 2.1554 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=70356)[0m top1: 0.29151119402985076
[2m[36m(func pid=70356)[0m top5: 0.8152985074626866
[2m[36m(func pid=70356)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=70356)[0m f1_macro: 0.306138893356296
[2m[36m(func pid=70356)[0m f1_weighted: 0.24736572945299592
[2m[36m(func pid=70356)[0m f1_per_class: [0.636, 0.425, 0.338, 0.122, 0.175, 0.209, 0.242, 0.274, 0.205, 0.434]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.3789 | Steps: 4 | Val loss: 4.3227 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 12:18:03 (running for 00:20:09.25)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.726 |      0.269 |                   58 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.724 |      0.284 |                   38 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.962 |      0.371 |                   31 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  2.294 |      0.306 |                   12 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.3111007462686567
[2m[36m(func pid=64137)[0m top5: 0.8619402985074627
[2m[36m(func pid=64137)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=64137)[0m f1_macro: 0.2836583525922163
[2m[36m(func pid=64137)[0m f1_weighted: 0.32835132248642823
[2m[36m(func pid=64137)[0m f1_per_class: [0.4, 0.366, 0.143, 0.365, 0.149, 0.411, 0.259, 0.366, 0.178, 0.201]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.6867 | Steps: 4 | Val loss: 1.9284 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 4.7832 | Steps: 4 | Val loss: 12.0310 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=65548)[0m top1: 0.4197761194029851
[2m[36m(func pid=65548)[0m top5: 0.9193097014925373
[2m[36m(func pid=65548)[0m f1_micro: 0.4197761194029851
[2m[36m(func pid=65548)[0m f1_macro: 0.39307093926705367
[2m[36m(func pid=65548)[0m f1_weighted: 0.4223947116367788
[2m[36m(func pid=65548)[0m f1_per_class: [0.5, 0.491, 0.571, 0.494, 0.191, 0.402, 0.378, 0.235, 0.23, 0.438]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m top1: 0.27658582089552236
[2m[36m(func pid=58891)[0m top5: 0.8208955223880597
[2m[36m(func pid=58891)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=58891)[0m f1_macro: 0.2684461653681094
[2m[36m(func pid=58891)[0m f1_weighted: 0.2932064301075625
[2m[36m(func pid=58891)[0m f1_per_class: [0.387, 0.334, 0.16, 0.279, 0.058, 0.416, 0.237, 0.375, 0.17, 0.269]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8078 | Steps: 4 | Val loss: 2.2297 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=70356)[0m top1: 0.332089552238806
[2m[36m(func pid=70356)[0m top5: 0.84375
[2m[36m(func pid=70356)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=70356)[0m f1_macro: 0.3084509170406185
[2m[36m(func pid=70356)[0m f1_weighted: 0.3450243925777701
[2m[36m(func pid=70356)[0m f1_per_class: [0.62, 0.396, 0.211, 0.354, 0.152, 0.245, 0.365, 0.27, 0.236, 0.235]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7562 | Steps: 4 | Val loss: 4.4393 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 12:18:08 (running for 00:20:14.59)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.687 |      0.268 |                   59 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.808 |      0.275 |                   39 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.379 |      0.393 |                   32 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  4.783 |      0.308 |                   13 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.3003731343283582
[2m[36m(func pid=64137)[0m top5: 0.8563432835820896
[2m[36m(func pid=64137)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=64137)[0m f1_macro: 0.2749659083051317
[2m[36m(func pid=64137)[0m f1_weighted: 0.31566393169459284
[2m[36m(func pid=64137)[0m f1_per_class: [0.373, 0.36, 0.111, 0.371, 0.152, 0.405, 0.216, 0.367, 0.195, 0.199]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 3.5194 | Steps: 4 | Val loss: 12.4486 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.7364 | Steps: 4 | Val loss: 1.9105 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=65548)[0m top1: 0.4183768656716418
[2m[36m(func pid=65548)[0m top5: 0.9132462686567164
[2m[36m(func pid=65548)[0m f1_micro: 0.4183768656716418
[2m[36m(func pid=65548)[0m f1_macro: 0.38823569336239794
[2m[36m(func pid=65548)[0m f1_weighted: 0.4205069854986224
[2m[36m(func pid=65548)[0m f1_per_class: [0.453, 0.524, 0.6, 0.462, 0.175, 0.386, 0.392, 0.244, 0.221, 0.426]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m top1: 0.39505597014925375
[2m[36m(func pid=70356)[0m top5: 0.882929104477612
[2m[36m(func pid=70356)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=70356)[0m f1_macro: 0.29062400412008776
[2m[36m(func pid=70356)[0m f1_weighted: 0.3910499209172097
[2m[36m(func pid=70356)[0m f1_per_class: [0.594, 0.149, 0.22, 0.483, 0.131, 0.27, 0.571, 0.083, 0.266, 0.139]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6451 | Steps: 4 | Val loss: 2.1908 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=58891)[0m top1: 0.2798507462686567
[2m[36m(func pid=58891)[0m top5: 0.8269589552238806
[2m[36m(func pid=58891)[0m f1_micro: 0.2798507462686567
[2m[36m(func pid=58891)[0m f1_macro: 0.2693814554688909
[2m[36m(func pid=58891)[0m f1_weighted: 0.2912746969977512
[2m[36m(func pid=58891)[0m f1_per_class: [0.365, 0.359, 0.142, 0.243, 0.063, 0.415, 0.25, 0.357, 0.22, 0.281]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.9258 | Steps: 4 | Val loss: 4.7440 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 12:18:13 (running for 00:20:19.84)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.736 |      0.269 |                   60 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.645 |      0.28  |                   40 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.756 |      0.388 |                   33 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  3.519 |      0.291 |                   14 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.31156716417910446
[2m[36m(func pid=64137)[0m top5: 0.8666044776119403
[2m[36m(func pid=64137)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=64137)[0m f1_macro: 0.2799961420796543
[2m[36m(func pid=64137)[0m f1_weighted: 0.32453566146931684
[2m[36m(func pid=64137)[0m f1_per_class: [0.363, 0.402, 0.119, 0.384, 0.136, 0.396, 0.213, 0.377, 0.178, 0.232]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 3.4804 | Steps: 4 | Val loss: 16.9554 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.6429 | Steps: 4 | Val loss: 1.8852 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=65548)[0m top1: 0.4137126865671642
[2m[36m(func pid=65548)[0m top5: 0.8959888059701493
[2m[36m(func pid=65548)[0m f1_micro: 0.4137126865671642
[2m[36m(func pid=65548)[0m f1_macro: 0.3916272984642542
[2m[36m(func pid=65548)[0m f1_weighted: 0.4163311600879654
[2m[36m(func pid=65548)[0m f1_per_class: [0.397, 0.54, 0.611, 0.405, 0.228, 0.338, 0.433, 0.293, 0.213, 0.459]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m top1: 0.3530783582089552
[2m[36m(func pid=70356)[0m top5: 0.8446828358208955
[2m[36m(func pid=70356)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=70356)[0m f1_macro: 0.25387133979633886
[2m[36m(func pid=70356)[0m f1_weighted: 0.33009310229900446
[2m[36m(func pid=70356)[0m f1_per_class: [0.581, 0.031, 0.21, 0.33, 0.114, 0.315, 0.576, 0.031, 0.235, 0.115]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.9197 | Steps: 4 | Val loss: 2.2038 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=58891)[0m top1: 0.29197761194029853
[2m[36m(func pid=58891)[0m top5: 0.8325559701492538
[2m[36m(func pid=58891)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=58891)[0m f1_macro: 0.2734519752115309
[2m[36m(func pid=58891)[0m f1_weighted: 0.30365706048619473
[2m[36m(func pid=58891)[0m f1_per_class: [0.364, 0.386, 0.156, 0.255, 0.065, 0.42, 0.267, 0.341, 0.207, 0.274]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.3722 | Steps: 4 | Val loss: 5.4919 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 12:18:19 (running for 00:20:25.17)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.643 |      0.273 |                   61 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.92  |      0.286 |                   41 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.926 |      0.392 |                   34 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  3.48  |      0.254 |                   15 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.32276119402985076
[2m[36m(func pid=64137)[0m top5: 0.8656716417910447
[2m[36m(func pid=64137)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=64137)[0m f1_macro: 0.2860889301563426
[2m[36m(func pid=64137)[0m f1_weighted: 0.3347750806932943
[2m[36m(func pid=64137)[0m f1_per_class: [0.349, 0.43, 0.121, 0.387, 0.152, 0.403, 0.225, 0.374, 0.193, 0.227]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 5.6065 | Steps: 4 | Val loss: 17.3385 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.6492 | Steps: 4 | Val loss: 1.8917 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=65548)[0m top1: 0.3908582089552239
[2m[36m(func pid=65548)[0m top5: 0.8796641791044776
[2m[36m(func pid=65548)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=65548)[0m f1_macro: 0.35932658103306186
[2m[36m(func pid=65548)[0m f1_weighted: 0.3812217978535408
[2m[36m(func pid=65548)[0m f1_per_class: [0.335, 0.521, 0.537, 0.292, 0.193, 0.256, 0.465, 0.322, 0.198, 0.475]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m top1: 0.3087686567164179
[2m[36m(func pid=70356)[0m top5: 0.7994402985074627
[2m[36m(func pid=70356)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=70356)[0m f1_macro: 0.2648276372761644
[2m[36m(func pid=70356)[0m f1_weighted: 0.3249174159500361
[2m[36m(func pid=70356)[0m f1_per_class: [0.575, 0.089, 0.153, 0.294, 0.101, 0.334, 0.516, 0.22, 0.239, 0.127]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=58891)[0m top1: 0.28544776119402987
[2m[36m(func pid=58891)[0m top5: 0.832089552238806
[2m[36m(func pid=58891)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=58891)[0m f1_macro: 0.27775915858550204
[2m[36m(func pid=58891)[0m f1_weighted: 0.29470498274552037
[2m[36m(func pid=58891)[0m f1_per_class: [0.385, 0.373, 0.187, 0.248, 0.059, 0.418, 0.243, 0.359, 0.226, 0.279]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6088 | Steps: 4 | Val loss: 2.1884 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.1164 | Steps: 4 | Val loss: 6.3267 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 12:18:24 (running for 00:20:30.56)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.649 |      0.278 |                   62 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.609 |      0.291 |                   42 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.372 |      0.359 |                   35 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  5.606 |      0.265 |                   16 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.3278917910447761
[2m[36m(func pid=64137)[0m top5: 0.8731343283582089
[2m[36m(func pid=64137)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=64137)[0m f1_macro: 0.29147866227087615
[2m[36m(func pid=64137)[0m f1_weighted: 0.3374892967799259
[2m[36m(func pid=64137)[0m f1_per_class: [0.388, 0.445, 0.122, 0.406, 0.148, 0.395, 0.209, 0.37, 0.19, 0.242]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 11.6711 | Steps: 4 | Val loss: 21.3109 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.6239 | Steps: 4 | Val loss: 1.8677 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=65548)[0m top1: 0.35867537313432835
[2m[36m(func pid=65548)[0m top5: 0.851679104477612
[2m[36m(func pid=65548)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=65548)[0m f1_macro: 0.3294029291242248
[2m[36m(func pid=65548)[0m f1_weighted: 0.3320256779657994
[2m[36m(func pid=65548)[0m f1_per_class: [0.31, 0.485, 0.537, 0.19, 0.177, 0.192, 0.446, 0.308, 0.208, 0.441]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m top1: 0.23134328358208955
[2m[36m(func pid=70356)[0m top5: 0.7336753731343284
[2m[36m(func pid=70356)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=70356)[0m f1_macro: 0.24476340638290975
[2m[36m(func pid=70356)[0m f1_weighted: 0.22432927615379436
[2m[36m(func pid=70356)[0m f1_per_class: [0.558, 0.258, 0.181, 0.149, 0.126, 0.31, 0.219, 0.245, 0.245, 0.156]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=58891)[0m top1: 0.2994402985074627
[2m[36m(func pid=58891)[0m top5: 0.8339552238805971
[2m[36m(func pid=58891)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=58891)[0m f1_macro: 0.286103746014165
[2m[36m(func pid=58891)[0m f1_weighted: 0.30704695233891877
[2m[36m(func pid=58891)[0m f1_per_class: [0.408, 0.404, 0.197, 0.257, 0.063, 0.415, 0.259, 0.347, 0.248, 0.264]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.9715 | Steps: 4 | Val loss: 2.1730 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.3702 | Steps: 4 | Val loss: 6.4685 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 12:18:30 (running for 00:20:36.02)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.624 |      0.286 |                   63 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.972 |      0.308 |                   43 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.116 |      0.329 |                   36 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 11.671 |      0.245 |                   17 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.34328358208955223
[2m[36m(func pid=64137)[0m top5: 0.8791977611940298
[2m[36m(func pid=64137)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=64137)[0m f1_macro: 0.3080466829615817
[2m[36m(func pid=64137)[0m f1_weighted: 0.34960822265069563
[2m[36m(func pid=64137)[0m f1_per_class: [0.44, 0.453, 0.138, 0.405, 0.18, 0.417, 0.232, 0.364, 0.208, 0.243]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8125 | Steps: 4 | Val loss: 25.5809 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.7038 | Steps: 4 | Val loss: 1.8855 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=65548)[0m top1: 0.34841417910447764
[2m[36m(func pid=65548)[0m top5: 0.8386194029850746
[2m[36m(func pid=65548)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=65548)[0m f1_macro: 0.3057919142306189
[2m[36m(func pid=65548)[0m f1_weighted: 0.32513972152836607
[2m[36m(func pid=65548)[0m f1_per_class: [0.284, 0.479, 0.524, 0.171, 0.176, 0.167, 0.464, 0.295, 0.204, 0.293]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m top1: 0.261660447761194
[2m[36m(func pid=70356)[0m top5: 0.695429104477612
[2m[36m(func pid=70356)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=70356)[0m f1_macro: 0.27035073309384067
[2m[36m(func pid=70356)[0m f1_weighted: 0.20659993256627712
[2m[36m(func pid=70356)[0m f1_per_class: [0.515, 0.386, 0.31, 0.163, 0.154, 0.344, 0.046, 0.295, 0.245, 0.245]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=58891)[0m top1: 0.29244402985074625
[2m[36m(func pid=58891)[0m top5: 0.8325559701492538
[2m[36m(func pid=58891)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=58891)[0m f1_macro: 0.2803231925734848
[2m[36m(func pid=58891)[0m f1_weighted: 0.30034758342789597
[2m[36m(func pid=58891)[0m f1_per_class: [0.37, 0.394, 0.212, 0.253, 0.063, 0.417, 0.248, 0.361, 0.22, 0.266]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8768 | Steps: 4 | Val loss: 2.2348 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.7413 | Steps: 4 | Val loss: 6.2950 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 12:18:35 (running for 00:20:41.37)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.704 |      0.28  |                   64 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.877 |      0.293 |                   44 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  2.37  |      0.306 |                   37 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  2.812 |      0.27  |                   18 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.3306902985074627
[2m[36m(func pid=64137)[0m top5: 0.8731343283582089
[2m[36m(func pid=64137)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=64137)[0m f1_macro: 0.29285315120761746
[2m[36m(func pid=64137)[0m f1_weighted: 0.33485081402047606
[2m[36m(func pid=64137)[0m f1_per_class: [0.39, 0.464, 0.123, 0.382, 0.145, 0.386, 0.213, 0.369, 0.196, 0.258]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 10.9159 | Steps: 4 | Val loss: 27.9952 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.6998 | Steps: 4 | Val loss: 1.9061 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=65548)[0m top1: 0.3344216417910448
[2m[36m(func pid=65548)[0m top5: 0.8353544776119403
[2m[36m(func pid=65548)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=65548)[0m f1_macro: 0.28239486840650996
[2m[36m(func pid=65548)[0m f1_weighted: 0.3397057676418242
[2m[36m(func pid=65548)[0m f1_per_class: [0.268, 0.444, 0.407, 0.253, 0.151, 0.16, 0.473, 0.276, 0.185, 0.207]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m top1: 0.2957089552238806
[2m[36m(func pid=70356)[0m top5: 0.7215485074626866
[2m[36m(func pid=70356)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=70356)[0m f1_macro: 0.29921214365113336
[2m[36m(func pid=70356)[0m f1_weighted: 0.23434743333170469
[2m[36m(func pid=70356)[0m f1_per_class: [0.496, 0.411, 0.358, 0.28, 0.154, 0.31, 0.012, 0.341, 0.264, 0.366]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=58891)[0m top1: 0.2873134328358209
[2m[36m(func pid=58891)[0m top5: 0.8236940298507462
[2m[36m(func pid=58891)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=58891)[0m f1_macro: 0.2730845270299083
[2m[36m(func pid=58891)[0m f1_weighted: 0.2945793949597562
[2m[36m(func pid=58891)[0m f1_per_class: [0.342, 0.396, 0.197, 0.263, 0.066, 0.402, 0.227, 0.359, 0.217, 0.263]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6435 | Steps: 4 | Val loss: 2.1763 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.4884 | Steps: 4 | Val loss: 5.9411 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.9933 | Steps: 4 | Val loss: 25.0742 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 12:18:40 (running for 00:20:46.78)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.7   |      0.273 |                   65 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.644 |      0.307 |                   45 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.741 |      0.282 |                   38 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 10.916 |      0.299 |                   19 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.35027985074626866
[2m[36m(func pid=64137)[0m top5: 0.8871268656716418
[2m[36m(func pid=64137)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=64137)[0m f1_macro: 0.3065005941914133
[2m[36m(func pid=64137)[0m f1_weighted: 0.3542545818007036
[2m[36m(func pid=64137)[0m f1_per_class: [0.393, 0.477, 0.145, 0.4, 0.162, 0.383, 0.252, 0.371, 0.203, 0.279]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.5197 | Steps: 4 | Val loss: 1.9022 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=65548)[0m top1: 0.3362873134328358
[2m[36m(func pid=65548)[0m top5: 0.8596082089552238
[2m[36m(func pid=65548)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=65548)[0m f1_macro: 0.28662074841037055
[2m[36m(func pid=65548)[0m f1_weighted: 0.36315347370417567
[2m[36m(func pid=65548)[0m f1_per_class: [0.282, 0.313, 0.386, 0.359, 0.168, 0.234, 0.499, 0.286, 0.177, 0.162]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m top1: 0.3199626865671642
[2m[36m(func pid=70356)[0m top5: 0.7518656716417911
[2m[36m(func pid=70356)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=70356)[0m f1_macro: 0.28437032694199
[2m[36m(func pid=70356)[0m f1_weighted: 0.27459409381431027
[2m[36m(func pid=70356)[0m f1_per_class: [0.359, 0.405, 0.289, 0.443, 0.179, 0.258, 0.031, 0.389, 0.193, 0.298]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=58891)[0m top1: 0.291044776119403
[2m[36m(func pid=58891)[0m top5: 0.8227611940298507
[2m[36m(func pid=58891)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=58891)[0m f1_macro: 0.2809697062758461
[2m[36m(func pid=58891)[0m f1_weighted: 0.29631450901618106
[2m[36m(func pid=58891)[0m f1_per_class: [0.395, 0.403, 0.209, 0.256, 0.064, 0.409, 0.227, 0.357, 0.238, 0.253]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.6540 | Steps: 4 | Val loss: 2.1528 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.6918 | Steps: 4 | Val loss: 6.3494 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 12:18:46 (running for 00:20:52.01)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.52  |      0.281 |                   66 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.654 |      0.312 |                   46 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.488 |      0.287 |                   39 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  2.993 |      0.284 |                   20 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.36007462686567165
[2m[36m(func pid=64137)[0m top5: 0.8955223880597015
[2m[36m(func pid=64137)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=64137)[0m f1_macro: 0.3117258419174224
[2m[36m(func pid=64137)[0m f1_weighted: 0.36906074689196366
[2m[36m(func pid=64137)[0m f1_per_class: [0.391, 0.482, 0.135, 0.441, 0.15, 0.384, 0.258, 0.38, 0.215, 0.282]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 13.1010 | Steps: 4 | Val loss: 25.2057 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.6881 | Steps: 4 | Val loss: 1.8692 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=70356)[0m top1: 0.2775186567164179
[2m[36m(func pid=70356)[0m top5: 0.7910447761194029
[2m[36m(func pid=70356)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=70356)[0m f1_macro: 0.25126616669261553
[2m[36m(func pid=70356)[0m f1_weighted: 0.2632546098379825
[2m[36m(func pid=70356)[0m f1_per_class: [0.233, 0.251, 0.278, 0.413, 0.182, 0.31, 0.119, 0.304, 0.165, 0.257]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=65548)[0m top1: 0.31529850746268656
[2m[36m(func pid=65548)[0m top5: 0.8423507462686567
[2m[36m(func pid=65548)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=65548)[0m f1_macro: 0.2671107818333486
[2m[36m(func pid=65548)[0m f1_weighted: 0.34496899321200436
[2m[36m(func pid=65548)[0m f1_per_class: [0.278, 0.188, 0.293, 0.383, 0.165, 0.296, 0.466, 0.293, 0.181, 0.128]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5590 | Steps: 4 | Val loss: 2.0865 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=58891)[0m top1: 0.3148320895522388
[2m[36m(func pid=58891)[0m top5: 0.8246268656716418
[2m[36m(func pid=58891)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=58891)[0m f1_macro: 0.3019129643449495
[2m[36m(func pid=58891)[0m f1_weighted: 0.3200550608514822
[2m[36m(func pid=58891)[0m f1_per_class: [0.418, 0.42, 0.267, 0.321, 0.057, 0.415, 0.226, 0.368, 0.253, 0.274]
[2m[36m(func pid=58891)[0m 
== Status ==
Current time: 2024-01-07 12:18:51 (running for 00:20:57.24)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.688 |      0.302 |                   67 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.559 |      0.333 |                   47 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.692 |      0.267 |                   40 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 13.101 |      0.251 |                   21 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.3908582089552239
[2m[36m(func pid=64137)[0m top5: 0.9053171641791045
[2m[36m(func pid=64137)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=64137)[0m f1_macro: 0.3331344458650002
[2m[36m(func pid=64137)[0m f1_weighted: 0.39817795833865305
[2m[36m(func pid=64137)[0m f1_per_class: [0.42, 0.511, 0.156, 0.473, 0.148, 0.408, 0.3, 0.344, 0.233, 0.338]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 3.4868 | Steps: 4 | Val loss: 22.0556 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.6120 | Steps: 4 | Val loss: 6.4990 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.5482 | Steps: 4 | Val loss: 1.8481 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=70356)[0m top1: 0.30923507462686567
[2m[36m(func pid=70356)[0m top5: 0.8376865671641791
[2m[36m(func pid=70356)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=70356)[0m f1_macro: 0.2516334578675825
[2m[36m(func pid=70356)[0m f1_weighted: 0.3314505678550238
[2m[36m(func pid=70356)[0m f1_per_class: [0.206, 0.174, 0.244, 0.362, 0.16, 0.307, 0.464, 0.211, 0.169, 0.22]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=65548)[0m top1: 0.34328358208955223
[2m[36m(func pid=65548)[0m top5: 0.8470149253731343
[2m[36m(func pid=65548)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=65548)[0m f1_macro: 0.27433181818731206
[2m[36m(func pid=65548)[0m f1_weighted: 0.3579731952894895
[2m[36m(func pid=65548)[0m f1_per_class: [0.362, 0.091, 0.265, 0.474, 0.163, 0.304, 0.471, 0.282, 0.201, 0.129]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5919 | Steps: 4 | Val loss: 2.0635 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=58891)[0m top1: 0.3204291044776119
[2m[36m(func pid=58891)[0m top5: 0.8386194029850746
[2m[36m(func pid=58891)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=58891)[0m f1_macro: 0.30418312673684533
[2m[36m(func pid=58891)[0m f1_weighted: 0.32675904873202327
[2m[36m(func pid=58891)[0m f1_per_class: [0.449, 0.427, 0.255, 0.331, 0.06, 0.411, 0.238, 0.354, 0.249, 0.267]
[2m[36m(func pid=58891)[0m 
== Status ==
Current time: 2024-01-07 12:18:56 (running for 00:21:02.51)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.548 |      0.304 |                   68 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.592 |      0.344 |                   48 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.612 |      0.274 |                   41 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  3.487 |      0.252 |                   22 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=64137)[0m top1: 0.394589552238806

[2m[36m(func pid=64137)[0m top5: 0.9071828358208955
[2m[36m(func pid=64137)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=64137)[0m f1_macro: 0.3436920716500471
[2m[36m(func pid=64137)[0m f1_weighted: 0.40272450863191206
[2m[36m(func pid=64137)[0m f1_per_class: [0.488, 0.499, 0.174, 0.481, 0.142, 0.406, 0.305, 0.37, 0.238, 0.333]
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 4.5810 | Steps: 4 | Val loss: 24.2041 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6434 | Steps: 4 | Val loss: 6.9472 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.6745 | Steps: 4 | Val loss: 1.8359 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=70356)[0m top1: 0.341884328358209
[2m[36m(func pid=70356)[0m top5: 0.8157649253731343
[2m[36m(func pid=70356)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=70356)[0m f1_macro: 0.23862225209811463
[2m[36m(func pid=70356)[0m f1_weighted: 0.32740360677627506
[2m[36m(func pid=70356)[0m f1_per_class: [0.311, 0.169, 0.213, 0.266, 0.179, 0.25, 0.591, 0.031, 0.193, 0.183]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.6264 | Steps: 4 | Val loss: 2.0190 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=65548)[0m top1: 0.35074626865671643
[2m[36m(func pid=65548)[0m top5: 0.8428171641791045
[2m[36m(func pid=65548)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=65548)[0m f1_macro: 0.283518982651196
[2m[36m(func pid=65548)[0m f1_weighted: 0.35622269224664255
[2m[36m(func pid=65548)[0m f1_per_class: [0.456, 0.046, 0.263, 0.501, 0.161, 0.338, 0.443, 0.306, 0.197, 0.124]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m top1: 0.324160447761194
[2m[36m(func pid=58891)[0m top5: 0.8381529850746269
[2m[36m(func pid=58891)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=58891)[0m f1_macro: 0.3012697247841577
[2m[36m(func pid=58891)[0m f1_weighted: 0.3265724056404574
[2m[36m(func pid=58891)[0m f1_per_class: [0.415, 0.435, 0.276, 0.337, 0.061, 0.41, 0.23, 0.364, 0.231, 0.254]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 12.0753 | Steps: 4 | Val loss: 27.9939 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:19:01 (running for 00:21:07.81)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.674 |      0.301 |                   69 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.626 |      0.347 |                   49 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.643 |      0.284 |                   42 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  4.581 |      0.239 |                   23 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.39972014925373134
[2m[36m(func pid=64137)[0m top5: 0.9207089552238806
[2m[36m(func pid=64137)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=64137)[0m f1_macro: 0.34666500690726815
[2m[36m(func pid=64137)[0m f1_weighted: 0.407427211980837
[2m[36m(func pid=64137)[0m f1_per_class: [0.518, 0.485, 0.22, 0.496, 0.15, 0.393, 0.326, 0.33, 0.226, 0.322]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.0848 | Steps: 4 | Val loss: 7.3165 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.4967 | Steps: 4 | Val loss: 1.8374 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=70356)[0m top1: 0.33861940298507465
[2m[36m(func pid=70356)[0m top5: 0.7985074626865671
[2m[36m(func pid=70356)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=70356)[0m f1_macro: 0.2333858326181864
[2m[36m(func pid=70356)[0m f1_weighted: 0.3138426466897254
[2m[36m(func pid=70356)[0m f1_per_class: [0.486, 0.191, 0.127, 0.204, 0.161, 0.212, 0.598, 0.03, 0.196, 0.129]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.5077 | Steps: 4 | Val loss: 1.9866 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=65548)[0m top1: 0.34421641791044777
[2m[36m(func pid=65548)[0m top5: 0.8330223880597015
[2m[36m(func pid=65548)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=65548)[0m f1_macro: 0.2782032154092896
[2m[36m(func pid=65548)[0m f1_weighted: 0.34948279327964105
[2m[36m(func pid=65548)[0m f1_per_class: [0.481, 0.037, 0.217, 0.503, 0.143, 0.352, 0.419, 0.306, 0.193, 0.133]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=58891)[0m top1: 0.32136194029850745
[2m[36m(func pid=58891)[0m top5: 0.8386194029850746
[2m[36m(func pid=58891)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=58891)[0m f1_macro: 0.3006645859053656
[2m[36m(func pid=58891)[0m f1_weighted: 0.3263431626579805
[2m[36m(func pid=58891)[0m f1_per_class: [0.402, 0.427, 0.245, 0.334, 0.065, 0.406, 0.237, 0.361, 0.246, 0.282]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 4.3177 | Steps: 4 | Val loss: 29.0269 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 12:19:07 (running for 00:21:13.17)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.497 |      0.301 |                   70 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.508 |      0.357 |                   50 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.085 |      0.278 |                   43 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 12.075 |      0.233 |                   24 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.41651119402985076
[2m[36m(func pid=64137)[0m top5: 0.9188432835820896
[2m[36m(func pid=64137)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=64137)[0m f1_macro: 0.35672540660327606
[2m[36m(func pid=64137)[0m f1_weighted: 0.4258310090650803
[2m[36m(func pid=64137)[0m f1_per_class: [0.496, 0.503, 0.243, 0.505, 0.153, 0.404, 0.366, 0.32, 0.234, 0.343]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.6160 | Steps: 4 | Val loss: 7.7236 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.5290 | Steps: 4 | Val loss: 1.8354 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=70356)[0m top1: 0.3101679104477612
[2m[36m(func pid=70356)[0m top5: 0.7915111940298507
[2m[36m(func pid=70356)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=70356)[0m f1_macro: 0.26206853157296617
[2m[36m(func pid=70356)[0m f1_weighted: 0.3368900145485872
[2m[36m(func pid=70356)[0m f1_per_class: [0.615, 0.263, 0.075, 0.218, 0.101, 0.271, 0.559, 0.19, 0.218, 0.111]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.0432 | Steps: 4 | Val loss: 2.0450 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=58891)[0m top1: 0.3218283582089552
[2m[36m(func pid=58891)[0m top5: 0.8418843283582089
[2m[36m(func pid=58891)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=58891)[0m f1_macro: 0.3010212997419292
[2m[36m(func pid=58891)[0m f1_weighted: 0.32712831129131065
[2m[36m(func pid=58891)[0m f1_per_class: [0.395, 0.43, 0.226, 0.331, 0.058, 0.414, 0.236, 0.363, 0.269, 0.288]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m top1: 0.3246268656716418
[2m[36m(func pid=65548)[0m top5: 0.8097014925373134
[2m[36m(func pid=65548)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=65548)[0m f1_macro: 0.2684239244225537
[2m[36m(func pid=65548)[0m f1_weighted: 0.3347182043930793
[2m[36m(func pid=65548)[0m f1_per_class: [0.517, 0.042, 0.156, 0.491, 0.115, 0.343, 0.383, 0.282, 0.206, 0.15]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 9.4557 | Steps: 4 | Val loss: 31.4062 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=64137)[0m top1: 0.41044776119402987
[2m[36m(func pid=64137)[0m top5: 0.9197761194029851
[2m[36m(func pid=64137)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=64137)[0m f1_macro: 0.3491602088149957
[2m[36m(func pid=64137)[0m f1_weighted: 0.4191318395991947
[2m[36m(func pid=64137)[0m f1_per_class: [0.513, 0.497, 0.25, 0.509, 0.118, 0.398, 0.355, 0.27, 0.241, 0.341]
== Status ==
Current time: 2024-01-07 12:19:12 (running for 00:21:18.50)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.529 |      0.301 |                   71 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.043 |      0.349 |                   51 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.616 |      0.268 |                   44 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  4.318 |      0.262 |                   25 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.4760 | Steps: 4 | Val loss: 1.8351 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6944 | Steps: 4 | Val loss: 8.2740 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=70356)[0m top1: 0.27052238805970147
[2m[36m(func pid=70356)[0m top5: 0.7938432835820896
[2m[36m(func pid=70356)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=70356)[0m f1_macro: 0.253380626748296
[2m[36m(func pid=70356)[0m f1_weighted: 0.3067121209194191
[2m[36m(func pid=70356)[0m f1_per_class: [0.622, 0.273, 0.072, 0.236, 0.109, 0.196, 0.456, 0.236, 0.198, 0.137]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.8884 | Steps: 4 | Val loss: 1.9926 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=58891)[0m top1: 0.31669776119402987
[2m[36m(func pid=58891)[0m top5: 0.847481343283582
[2m[36m(func pid=58891)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=58891)[0m f1_macro: 0.3022086687807258
[2m[36m(func pid=58891)[0m f1_weighted: 0.32354936173819726
[2m[36m(func pid=58891)[0m f1_per_class: [0.402, 0.404, 0.282, 0.327, 0.07, 0.417, 0.243, 0.361, 0.25, 0.266]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m top1: 0.28824626865671643
[2m[36m(func pid=65548)[0m top5: 0.7821828358208955
[2m[36m(func pid=65548)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=65548)[0m f1_macro: 0.25076886588213376
[2m[36m(func pid=65548)[0m f1_weighted: 0.30321827199216567
[2m[36m(func pid=65548)[0m f1_per_class: [0.454, 0.09, 0.11, 0.451, 0.093, 0.337, 0.292, 0.296, 0.193, 0.192]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 6.7582 | Steps: 4 | Val loss: 37.1274 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=64137)[0m top1: 0.4183768656716418
[2m[36m(func pid=64137)[0m top5: 0.925839552238806
[2m[36m(func pid=64137)[0m f1_micro: 0.4183768656716418
[2m[36m(func pid=64137)[0m f1_macro: 0.3601349813193043
[2m[36m(func pid=64137)[0m f1_weighted: 0.42537441554390737
[2m[36m(func pid=64137)[0m f1_per_class: [0.519, 0.476, 0.342, 0.542, 0.141, 0.396, 0.355, 0.278, 0.228, 0.324]
== Status ==
Current time: 2024-01-07 12:19:17 (running for 00:21:23.88)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.476 |      0.302 |                   72 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.888 |      0.36  |                   52 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  2.694 |      0.251 |                   45 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  9.456 |      0.253 |                   26 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.4146 | Steps: 4 | Val loss: 1.8271 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.1176 | Steps: 4 | Val loss: 8.4150 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=70356)[0m top1: 0.21548507462686567
[2m[36m(func pid=70356)[0m top5: 0.777518656716418
[2m[36m(func pid=70356)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=70356)[0m f1_macro: 0.22048385173105028
[2m[36m(func pid=70356)[0m f1_weighted: 0.2298617235992196
[2m[36m(func pid=70356)[0m f1_per_class: [0.529, 0.333, 0.076, 0.228, 0.095, 0.162, 0.193, 0.217, 0.187, 0.184]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.7074 | Steps: 4 | Val loss: 2.0753 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=58891)[0m top1: 0.31902985074626866
[2m[36m(func pid=58891)[0m top5: 0.8526119402985075
[2m[36m(func pid=58891)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=58891)[0m f1_macro: 0.3078152800520497
[2m[36m(func pid=58891)[0m f1_weighted: 0.3311233453050774
[2m[36m(func pid=58891)[0m f1_per_class: [0.44, 0.391, 0.273, 0.351, 0.064, 0.413, 0.254, 0.354, 0.248, 0.291]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m top1: 0.27658582089552236
[2m[36m(func pid=65548)[0m top5: 0.7742537313432836
[2m[36m(func pid=65548)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=65548)[0m f1_macro: 0.24779844604565815
[2m[36m(func pid=65548)[0m f1_weighted: 0.2793259292551134
[2m[36m(func pid=65548)[0m f1_per_class: [0.484, 0.088, 0.136, 0.464, 0.089, 0.328, 0.204, 0.285, 0.184, 0.216]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 14.7202 | Steps: 4 | Val loss: 36.7710 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=64137)[0m top1: 0.4146455223880597
[2m[36m(func pid=64137)[0m top5: 0.925839552238806
[2m[36m(func pid=64137)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=64137)[0m f1_macro: 0.3590163149855994
[2m[36m(func pid=64137)[0m f1_weighted: 0.42382528900097666
[2m[36m(func pid=64137)[0m f1_per_class: [0.563, 0.494, 0.295, 0.515, 0.122, 0.398, 0.362, 0.274, 0.236, 0.331]
[2m[36m(func pid=64137)[0m 
== Status ==
Current time: 2024-01-07 12:19:23 (running for 00:21:29.16)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.415 |      0.308 |                   73 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.707 |      0.359 |                   53 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  2.118 |      0.248 |                   46 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  6.758 |      0.22  |                   27 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.6020 | Steps: 4 | Val loss: 1.8419 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7220 | Steps: 4 | Val loss: 8.1430 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=70356)[0m top1: 0.2681902985074627
[2m[36m(func pid=70356)[0m top5: 0.7784514925373134
[2m[36m(func pid=70356)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=70356)[0m f1_macro: 0.2649553671849279
[2m[36m(func pid=70356)[0m f1_weighted: 0.2780396806215873
[2m[36m(func pid=70356)[0m f1_per_class: [0.613, 0.387, 0.086, 0.331, 0.067, 0.288, 0.16, 0.275, 0.172, 0.269]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.9263 | Steps: 4 | Val loss: 2.0086 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=58891)[0m top1: 0.31576492537313433
[2m[36m(func pid=58891)[0m top5: 0.8432835820895522
[2m[36m(func pid=58891)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=58891)[0m f1_macro: 0.30000090267056095
[2m[36m(func pid=58891)[0m f1_weighted: 0.3277791999608539
[2m[36m(func pid=58891)[0m f1_per_class: [0.387, 0.392, 0.27, 0.38, 0.063, 0.405, 0.222, 0.349, 0.252, 0.28]
[2m[36m(func pid=58891)[0m 
[2m[36m(func pid=65548)[0m top1: 0.27005597014925375
[2m[36m(func pid=65548)[0m top5: 0.7747201492537313
[2m[36m(func pid=65548)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=65548)[0m f1_macro: 0.24931739389797225
[2m[36m(func pid=65548)[0m f1_weighted: 0.28731061400653013
[2m[36m(func pid=65548)[0m f1_per_class: [0.427, 0.134, 0.113, 0.397, 0.091, 0.346, 0.262, 0.292, 0.178, 0.253]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 4.7626 | Steps: 4 | Val loss: 34.1463 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 12:19:28 (running for 00:21:34.44)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.33
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00008 | RUNNING    | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.602 |      0.3   |                   74 |
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.926 |      0.358 |                   54 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  2.722 |      0.249 |                   47 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 14.72  |      0.265 |                   28 |
| train_9b9e8_00012 | PENDING    |                    | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=64137)[0m top1: 0.41744402985074625
[2m[36m(func pid=64137)[0m top5: 0.9347014925373134
[2m[36m(func pid=64137)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=64137)[0m f1_macro: 0.3579892518044514
[2m[36m(func pid=64137)[0m f1_weighted: 0.43226827387916816
[2m[36m(func pid=64137)[0m f1_per_class: [0.566, 0.47, 0.299, 0.5, 0.11, 0.408, 0.42, 0.25, 0.232, 0.326]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=58891)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.4318 | Steps: 4 | Val loss: 1.8271 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.5024 | Steps: 4 | Val loss: 8.3382 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=70356)[0m top1: 0.31529850746268656
[2m[36m(func pid=70356)[0m top5: 0.7915111940298507
[2m[36m(func pid=70356)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=70356)[0m f1_macro: 0.29752245637505353
[2m[36m(func pid=70356)[0m f1_weighted: 0.328741916401394
[2m[36m(func pid=70356)[0m f1_per_class: [0.538, 0.422, 0.117, 0.375, 0.075, 0.363, 0.225, 0.339, 0.229, 0.292]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.7960 | Steps: 4 | Val loss: 2.0141 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=58891)[0m top1: 0.3260261194029851
[2m[36m(func pid=58891)[0m top5: 0.8498134328358209
[2m[36m(func pid=58891)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=58891)[0m f1_macro: 0.3073089977677393
[2m[36m(func pid=58891)[0m f1_weighted: 0.34075246597497005
[2m[36m(func pid=58891)[0m f1_per_class: [0.437, 0.389, 0.273, 0.413, 0.064, 0.413, 0.232, 0.349, 0.232, 0.271]
[2m[36m(func pid=65548)[0m top1: 0.25513059701492535
[2m[36m(func pid=65548)[0m top5: 0.7719216417910447
[2m[36m(func pid=65548)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=65548)[0m f1_macro: 0.24034846288261064
[2m[36m(func pid=65548)[0m f1_weighted: 0.2754472999400137
[2m[36m(func pid=65548)[0m f1_per_class: [0.387, 0.2, 0.107, 0.33, 0.095, 0.362, 0.253, 0.248, 0.169, 0.251]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 10.8441 | Steps: 4 | Val loss: 34.2135 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=64137)[0m top1: 0.427705223880597
[2m[36m(func pid=64137)[0m top5: 0.9291044776119403
[2m[36m(func pid=64137)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=64137)[0m f1_macro: 0.36692344912179453
[2m[36m(func pid=64137)[0m f1_weighted: 0.4425695282338378
[2m[36m(func pid=64137)[0m f1_per_class: [0.566, 0.474, 0.306, 0.515, 0.121, 0.413, 0.426, 0.302, 0.23, 0.316]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.3703 | Steps: 4 | Val loss: 8.1367 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=70356)[0m top1: 0.3521455223880597
[2m[36m(func pid=70356)[0m top5: 0.8050373134328358
[2m[36m(func pid=70356)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=70356)[0m f1_macro: 0.3275085032044228
[2m[36m(func pid=70356)[0m f1_weighted: 0.35144723320999677
[2m[36m(func pid=70356)[0m f1_per_class: [0.523, 0.478, 0.222, 0.393, 0.054, 0.371, 0.24, 0.349, 0.249, 0.396]
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.3025 | Steps: 4 | Val loss: 2.0123 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=65548)[0m top1: 0.25093283582089554
[2m[36m(func pid=65548)[0m top5: 0.7728544776119403
[2m[36m(func pid=65548)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=65548)[0m f1_macro: 0.23492016020211706
[2m[36m(func pid=65548)[0m f1_weighted: 0.2700216659225832
[2m[36m(func pid=65548)[0m f1_per_class: [0.323, 0.243, 0.119, 0.259, 0.119, 0.35, 0.281, 0.279, 0.166, 0.21]
[2m[36m(func pid=64137)[0m top1: 0.4221082089552239
[2m[36m(func pid=64137)[0m top5: 0.9300373134328358
[2m[36m(func pid=64137)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=64137)[0m f1_macro: 0.36472707870011917
[2m[36m(func pid=64137)[0m f1_weighted: 0.4401072677799873
[2m[36m(func pid=64137)[0m f1_per_class: [0.566, 0.454, 0.347, 0.504, 0.122, 0.409, 0.447, 0.271, 0.239, 0.289]
== Status ==
Current time: 2024-01-07 12:19:33 (running for 00:21:39.90)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.796 |      0.367 |                   55 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.502 |      0.24  |                   48 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  4.763 |      0.298 |                   29 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=77753)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=77753)[0m Configuration completed!
[2m[36m(func pid=77753)[0m New optimizer parameters:
[2m[36m(func pid=77753)[0m SGD (
[2m[36m(func pid=77753)[0m Parameter Group 0
[2m[36m(func pid=77753)[0m     dampening: 0
[2m[36m(func pid=77753)[0m     differentiable: False
[2m[36m(func pid=77753)[0m     foreach: None
[2m[36m(func pid=77753)[0m     lr: 0.0001
[2m[36m(func pid=77753)[0m     maximize: False
[2m[36m(func pid=77753)[0m     momentum: 0.9
[2m[36m(func pid=77753)[0m     nesterov: False
[2m[36m(func pid=77753)[0m     weight_decay: 0.0001
[2m[36m(func pid=77753)[0m )
[2m[36m(func pid=77753)[0m 
== Status ==
Current time: 2024-01-07 12:19:41 (running for 00:21:46.93)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.796 |      0.367 |                   55 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.37  |      0.235 |                   49 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  4.763 |      0.298 |                   29 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 11.0001 | Steps: 4 | Val loss: 29.2589 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.3755 | Steps: 4 | Val loss: 8.0056 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.2381 | Steps: 4 | Val loss: 2.0512 | Batch size: 32 | lr: 0.001 | Duration: 3.22s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9717 | Steps: 4 | Val loss: 2.3231 | Batch size: 32 | lr: 0.0001 | Duration: 4.77s
== Status ==
Current time: 2024-01-07 12:19:46 (running for 00:21:51.95)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  1.303 |      0.365 |                   56 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.37  |      0.235 |                   49 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 10.844 |      0.328 |                   30 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.37220149253731344
[2m[36m(func pid=70356)[0m top5: 0.8614738805970149
[2m[36m(func pid=70356)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=70356)[0m f1_macro: 0.32707257207349627
[2m[36m(func pid=70356)[0m f1_weighted: 0.3839352957638841
[2m[36m(func pid=70356)[0m f1_per_class: [0.352, 0.437, 0.379, 0.398, 0.093, 0.386, 0.393, 0.257, 0.215, 0.36]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m top1: 0.41138059701492535
[2m[36m(func pid=64137)[0m top5: 0.9253731343283582
[2m[36m(func pid=64137)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=64137)[0m f1_macro: 0.3520774358465104
[2m[36m(func pid=64137)[0m f1_weighted: 0.43407754289106676
[2m[36m(func pid=64137)[0m f1_per_class: [0.559, 0.445, 0.306, 0.475, 0.124, 0.404, 0.465, 0.277, 0.213, 0.253]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m top1: 0.27005597014925375
[2m[36m(func pid=65548)[0m top5: 0.7672574626865671
[2m[36m(func pid=65548)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=65548)[0m f1_macro: 0.24247583357541505
[2m[36m(func pid=65548)[0m f1_weighted: 0.29209394011972384
[2m[36m(func pid=65548)[0m f1_per_class: [0.251, 0.295, 0.142, 0.239, 0.131, 0.347, 0.348, 0.275, 0.179, 0.219]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m top1: 0.177705223880597
[2m[36m(func pid=77753)[0m top5: 0.5293843283582089
[2m[36m(func pid=77753)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=77753)[0m f1_macro: 0.10250267148105507
[2m[36m(func pid=77753)[0m f1_weighted: 0.12321816038827116
[2m[36m(func pid=77753)[0m f1_per_class: [0.176, 0.335, 0.0, 0.083, 0.01, 0.273, 0.018, 0.011, 0.0, 0.118]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7167 | Steps: 4 | Val loss: 2.1378 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 5.1321 | Steps: 4 | Val loss: 30.3996 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6105 | Steps: 4 | Val loss: 8.1269 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9822 | Steps: 4 | Val loss: 2.3261 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 12:19:51 (running for 00:21:57.77)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.717 |      0.336 |                   58 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.376 |      0.242 |                   50 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 11     |      0.327 |                   31 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.972 |      0.103 |                    1 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=64137)[0m top1: 0.39132462686567165
[2m[36m(func pid=64137)[0m top5: 0.9188432835820896
[2m[36m(func pid=64137)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=64137)[0m f1_macro: 0.33603196762828647
[2m[36m(func pid=64137)[0m f1_weighted: 0.4155794121095355
[2m[36m(func pid=64137)[0m f1_per_class: [0.531, 0.431, 0.265, 0.432, 0.137, 0.381, 0.464, 0.277, 0.206, 0.237]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=70356)[0m top1: 0.38759328358208955
[2m[36m(func pid=70356)[0m top5: 0.8600746268656716
[2m[36m(func pid=70356)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=70356)[0m f1_macro: 0.34632035434901104
[2m[36m(func pid=70356)[0m f1_weighted: 0.39925454464627624
[2m[36m(func pid=70356)[0m f1_per_class: [0.31, 0.472, 0.611, 0.375, 0.084, 0.377, 0.467, 0.15, 0.218, 0.4]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=65548)[0m top1: 0.2868470149253731
[2m[36m(func pid=65548)[0m top5: 0.7784514925373134
[2m[36m(func pid=65548)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=65548)[0m f1_macro: 0.2552551112590947
[2m[36m(func pid=65548)[0m f1_weighted: 0.3062619256408576
[2m[36m(func pid=65548)[0m f1_per_class: [0.198, 0.362, 0.166, 0.223, 0.153, 0.348, 0.367, 0.289, 0.211, 0.237]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m top1: 0.16277985074626866
[2m[36m(func pid=77753)[0m top5: 0.5125932835820896
[2m[36m(func pid=77753)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=77753)[0m f1_macro: 0.09013719539671466
[2m[36m(func pid=77753)[0m f1_weighted: 0.12067415778790781
[2m[36m(func pid=77753)[0m f1_per_class: [0.111, 0.289, 0.0, 0.102, 0.008, 0.274, 0.024, 0.018, 0.0, 0.075]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.5381 | Steps: 4 | Val loss: 2.2989 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 3.4381 | Steps: 4 | Val loss: 31.7554 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.6287 | Steps: 4 | Val loss: 7.7624 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9817 | Steps: 4 | Val loss: 2.3372 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 12:19:57 (running for 00:22:03.12)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.538 |      0.319 |                   59 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.611 |      0.255 |                   51 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  5.132 |      0.346 |                   32 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.982 |      0.09  |                    2 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=64137)[0m top1: 0.3712686567164179
[2m[36m(func pid=64137)[0m top5: 0.9071828358208955
[2m[36m(func pid=64137)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=64137)[0m f1_macro: 0.3190909170286179
[2m[36m(func pid=64137)[0m f1_weighted: 0.3962721694803112
[2m[36m(func pid=64137)[0m f1_per_class: [0.479, 0.432, 0.206, 0.371, 0.118, 0.387, 0.455, 0.287, 0.229, 0.228]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=70356)[0m top1: 0.3712686567164179
[2m[36m(func pid=70356)[0m top5: 0.8610074626865671
[2m[36m(func pid=70356)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=70356)[0m f1_macro: 0.3356561046489806
[2m[36m(func pid=70356)[0m f1_weighted: 0.3890646995681265
[2m[36m(func pid=70356)[0m f1_per_class: [0.306, 0.399, 0.769, 0.339, 0.102, 0.334, 0.536, 0.113, 0.196, 0.262]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=65548)[0m top1: 0.29617537313432835
[2m[36m(func pid=65548)[0m top5: 0.8125
[2m[36m(func pid=65548)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=65548)[0m f1_macro: 0.2628839238004856
[2m[36m(func pid=65548)[0m f1_weighted: 0.3189594691813233
[2m[36m(func pid=65548)[0m f1_per_class: [0.164, 0.344, 0.19, 0.235, 0.171, 0.308, 0.42, 0.304, 0.214, 0.278]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m top1: 0.14972014925373134
[2m[36m(func pid=77753)[0m top5: 0.5051305970149254
[2m[36m(func pid=77753)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=77753)[0m f1_macro: 0.08594395768075823
[2m[36m(func pid=77753)[0m f1_weighted: 0.12185524851962189
[2m[36m(func pid=77753)[0m f1_per_class: [0.105, 0.251, 0.0, 0.106, 0.007, 0.279, 0.041, 0.046, 0.0, 0.025]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 15.1145 | Steps: 4 | Val loss: 37.3602 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6579 | Steps: 4 | Val loss: 2.4175 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.0943 | Steps: 4 | Val loss: 8.0154 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9717 | Steps: 4 | Val loss: 2.3447 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 12:20:02 (running for 00:22:08.37)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.538 |      0.319 |                   59 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.629 |      0.263 |                   52 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 15.115 |      0.304 |                   34 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.982 |      0.086 |                    3 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.31716417910447764
[2m[36m(func pid=70356)[0m top5: 0.8367537313432836
[2m[36m(func pid=70356)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=70356)[0m f1_macro: 0.3042176264382746
[2m[36m(func pid=70356)[0m f1_weighted: 0.3387674942520239
[2m[36m(func pid=70356)[0m f1_per_class: [0.298, 0.315, 0.769, 0.247, 0.085, 0.267, 0.528, 0.142, 0.164, 0.227]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m top1: 0.34375
[2m[36m(func pid=64137)[0m top5: 0.8922574626865671
[2m[36m(func pid=64137)[0m f1_micro: 0.34375
[2m[36m(func pid=64137)[0m f1_macro: 0.29842470853798664
[2m[36m(func pid=64137)[0m f1_weighted: 0.3695621671738401
[2m[36m(func pid=64137)[0m f1_per_class: [0.435, 0.414, 0.176, 0.329, 0.107, 0.382, 0.424, 0.279, 0.217, 0.223]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m top1: 0.29757462686567165
[2m[36m(func pid=65548)[0m top5: 0.8372201492537313
[2m[36m(func pid=65548)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=65548)[0m f1_macro: 0.2772379473249279
[2m[36m(func pid=65548)[0m f1_weighted: 0.314708354308385
[2m[36m(func pid=65548)[0m f1_per_class: [0.168, 0.37, 0.25, 0.256, 0.152, 0.3, 0.365, 0.311, 0.25, 0.35]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m top1: 0.14319029850746268
[2m[36m(func pid=77753)[0m top5: 0.4920708955223881
[2m[36m(func pid=77753)[0m f1_micro: 0.14319029850746268
[2m[36m(func pid=77753)[0m f1_macro: 0.08449646498738353
[2m[36m(func pid=77753)[0m f1_weighted: 0.12209803525606218
[2m[36m(func pid=77753)[0m f1_per_class: [0.062, 0.222, 0.061, 0.117, 0.006, 0.287, 0.048, 0.042, 0.0, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 6.9870 | Steps: 4 | Val loss: 41.9404 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2750 | Steps: 4 | Val loss: 2.4631 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.8412 | Steps: 4 | Val loss: 7.7754 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9945 | Steps: 4 | Val loss: 2.3289 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 12:20:07 (running for 00:22:13.63)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.658 |      0.298 |                   60 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.094 |      0.277 |                   53 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  6.987 |      0.28  |                   35 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.972 |      0.084 |                    4 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=64137)[0m top1: 0.332089552238806
[2m[36m(func pid=64137)[0m top5: 0.8843283582089553
[2m[36m(func pid=64137)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=64137)[0m f1_macro: 0.29041587671894825
[2m[36m(func pid=64137)[0m f1_weighted: 0.35855768727590503
[2m[36m(func pid=64137)[0m f1_per_class: [0.413, 0.407, 0.164, 0.316, 0.111, 0.368, 0.406, 0.303, 0.212, 0.205]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=70356)[0m top1: 0.2789179104477612
[2m[36m(func pid=70356)[0m top5: 0.7975746268656716
[2m[36m(func pid=70356)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=70356)[0m f1_macro: 0.2802324629405246
[2m[36m(func pid=70356)[0m f1_weighted: 0.3141180925577815
[2m[36m(func pid=70356)[0m f1_per_class: [0.279, 0.249, 0.69, 0.257, 0.068, 0.223, 0.485, 0.209, 0.15, 0.194]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=65548)[0m top1: 0.3204291044776119
[2m[36m(func pid=65548)[0m top5: 0.8386194029850746
[2m[36m(func pid=65548)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=65548)[0m f1_macro: 0.29217098974970057
[2m[36m(func pid=65548)[0m f1_weighted: 0.32834824728960493
[2m[36m(func pid=65548)[0m f1_per_class: [0.222, 0.403, 0.349, 0.334, 0.123, 0.275, 0.325, 0.311, 0.24, 0.339]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m top1: 0.13666044776119404
[2m[36m(func pid=77753)[0m top5: 0.5149253731343284
[2m[36m(func pid=77753)[0m f1_micro: 0.13666044776119404
[2m[36m(func pid=77753)[0m f1_macro: 0.08888464251432653
[2m[36m(func pid=77753)[0m f1_weighted: 0.12825242141535648
[2m[36m(func pid=77753)[0m f1_per_class: [0.04, 0.188, 0.111, 0.142, 0.006, 0.271, 0.069, 0.052, 0.011, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4852 | Steps: 4 | Val loss: 2.5850 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 11.2142 | Steps: 4 | Val loss: 45.9561 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 3.0849 | Steps: 4 | Val loss: 7.7424 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9838 | Steps: 4 | Val loss: 2.3282 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:20:13 (running for 00:22:19.03)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.275 |      0.29  |                   61 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.841 |      0.292 |                   54 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 11.214 |      0.253 |                   36 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.994 |      0.089 |                    5 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=64137)[0m top1: 0.31296641791044777
[2m[36m(func pid=64137)[0m top5: 0.8656716417910447
[2m[36m(func pid=64137)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=64137)[0m f1_macro: 0.2789681700522554
[2m[36m(func pid=64137)[0m f1_weighted: 0.3388892605199636
[2m[36m(func pid=64137)[0m f1_per_class: [0.391, 0.386, 0.15, 0.264, 0.119, 0.353, 0.408, 0.323, 0.177, 0.219]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=70356)[0m top1: 0.24766791044776118
[2m[36m(func pid=70356)[0m top5: 0.7513992537313433
[2m[36m(func pid=70356)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=70356)[0m f1_macro: 0.25330672432100965
[2m[36m(func pid=70356)[0m f1_weighted: 0.2820607214251226
[2m[36m(func pid=70356)[0m f1_per_class: [0.273, 0.218, 0.541, 0.275, 0.072, 0.198, 0.384, 0.247, 0.145, 0.18]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=65548)[0m top1: 0.353544776119403
[2m[36m(func pid=65548)[0m top5: 0.8563432835820896
[2m[36m(func pid=65548)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=65548)[0m f1_macro: 0.312858966497291
[2m[36m(func pid=65548)[0m f1_weighted: 0.3475977860303074
[2m[36m(func pid=65548)[0m f1_per_class: [0.333, 0.421, 0.4, 0.419, 0.082, 0.188, 0.323, 0.289, 0.281, 0.392]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m top1: 0.14272388059701493
[2m[36m(func pid=77753)[0m top5: 0.5247201492537313
[2m[36m(func pid=77753)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=77753)[0m f1_macro: 0.09326506437315125
[2m[36m(func pid=77753)[0m f1_weighted: 0.13622095649310123
[2m[36m(func pid=77753)[0m f1_per_class: [0.034, 0.182, 0.095, 0.162, 0.021, 0.284, 0.071, 0.07, 0.012, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.3475 | Steps: 4 | Val loss: 2.6312 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 3.5550 | Steps: 4 | Val loss: 48.6502 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.2693 | Steps: 4 | Val loss: 7.6917 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9382 | Steps: 4 | Val loss: 2.3172 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:20:18 (running for 00:22:24.05)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.485 |      0.279 |                   62 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  3.085 |      0.313 |                   55 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  3.555 |      0.236 |                   37 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.984 |      0.093 |                    6 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.23694029850746268
[2m[36m(func pid=70356)[0m top5: 0.7117537313432836
[2m[36m(func pid=70356)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=70356)[0m f1_macro: 0.23606365594440906
[2m[36m(func pid=70356)[0m f1_weighted: 0.26478897275783886
[2m[36m(func pid=70356)[0m f1_per_class: [0.272, 0.145, 0.392, 0.26, 0.086, 0.266, 0.349, 0.296, 0.172, 0.123]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m top1: 0.3185634328358209
[2m[36m(func pid=64137)[0m top5: 0.8568097014925373
[2m[36m(func pid=64137)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=64137)[0m f1_macro: 0.2798405759716405
[2m[36m(func pid=64137)[0m f1_weighted: 0.3426754318621108
[2m[36m(func pid=64137)[0m f1_per_class: [0.351, 0.399, 0.161, 0.252, 0.12, 0.365, 0.42, 0.33, 0.184, 0.216]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m top1: 0.3628731343283582
[2m[36m(func pid=65548)[0m top5: 0.8619402985074627
[2m[36m(func pid=65548)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=65548)[0m f1_macro: 0.3435284749469343
[2m[36m(func pid=65548)[0m f1_weighted: 0.34480422790682397
[2m[36m(func pid=65548)[0m f1_per_class: [0.433, 0.437, 0.558, 0.459, 0.08, 0.208, 0.241, 0.333, 0.268, 0.418]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m top1: 0.14085820895522388
[2m[36m(func pid=77753)[0m top5: 0.5419776119402985
[2m[36m(func pid=77753)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=77753)[0m f1_macro: 0.09242607079990746
[2m[36m(func pid=77753)[0m f1_weighted: 0.13816415740574595
[2m[36m(func pid=77753)[0m f1_per_class: [0.03, 0.163, 0.1, 0.167, 0.02, 0.277, 0.087, 0.067, 0.012, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.3994 | Steps: 4 | Val loss: 51.3476 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.4601 | Steps: 4 | Val loss: 2.7071 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.7360 | Steps: 4 | Val loss: 8.2926 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9268 | Steps: 4 | Val loss: 2.3163 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 12:20:23 (running for 00:22:29.27)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.348 |      0.28  |                   63 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.269 |      0.344 |                   56 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  3.399 |      0.226 |                   38 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.938 |      0.092 |                    7 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.22434701492537312
[2m[36m(func pid=70356)[0m top5: 0.6823694029850746
[2m[36m(func pid=70356)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=70356)[0m f1_macro: 0.2260139740734651
[2m[36m(func pid=70356)[0m f1_weighted: 0.24631649810754191
[2m[36m(func pid=70356)[0m f1_per_class: [0.326, 0.122, 0.22, 0.238, 0.101, 0.291, 0.293, 0.368, 0.22, 0.08]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m top1: 0.30736940298507465
[2m[36m(func pid=64137)[0m top5: 0.8493470149253731
[2m[36m(func pid=64137)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=64137)[0m f1_macro: 0.27277889341586725
[2m[36m(func pid=64137)[0m f1_weighted: 0.33285873810095884
[2m[36m(func pid=64137)[0m f1_per_class: [0.344, 0.396, 0.129, 0.265, 0.111, 0.368, 0.376, 0.327, 0.195, 0.216]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m top1: 0.34654850746268656
[2m[36m(func pid=65548)[0m top5: 0.8446828358208955
[2m[36m(func pid=65548)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=65548)[0m f1_macro: 0.3359961846602654
[2m[36m(func pid=65548)[0m f1_weighted: 0.3187270846267172
[2m[36m(func pid=65548)[0m f1_per_class: [0.465, 0.422, 0.579, 0.466, 0.077, 0.203, 0.163, 0.295, 0.257, 0.434]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m top1: 0.146455223880597
[2m[36m(func pid=77753)[0m top5: 0.5513059701492538
[2m[36m(func pid=77753)[0m f1_micro: 0.146455223880597
[2m[36m(func pid=77753)[0m f1_macro: 0.0974155961060753
[2m[36m(func pid=77753)[0m f1_weighted: 0.14441090129868597
[2m[36m(func pid=77753)[0m f1_per_class: [0.044, 0.176, 0.095, 0.171, 0.019, 0.296, 0.089, 0.073, 0.011, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 10.8902 | Steps: 4 | Val loss: 54.3231 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4182 | Steps: 4 | Val loss: 2.7734 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 3.1542 | Steps: 4 | Val loss: 8.6584 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9542 | Steps: 4 | Val loss: 2.3145 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 12:20:28 (running for 00:22:34.50)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.46  |      0.273 |                   64 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.736 |      0.336 |                   57 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 10.89  |      0.221 |                   39 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.927 |      0.097 |                    8 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.21082089552238806
[2m[36m(func pid=70356)[0m top5: 0.683768656716418
[2m[36m(func pid=70356)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=70356)[0m f1_macro: 0.22071810761402447
[2m[36m(func pid=70356)[0m f1_weighted: 0.23714253685530984
[2m[36m(func pid=70356)[0m f1_per_class: [0.403, 0.133, 0.106, 0.242, 0.116, 0.295, 0.246, 0.379, 0.217, 0.071]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m top1: 0.2957089552238806
[2m[36m(func pid=64137)[0m top5: 0.8418843283582089
[2m[36m(func pid=64137)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=64137)[0m f1_macro: 0.2667955035186502
[2m[36m(func pid=64137)[0m f1_weighted: 0.32534882769606066
[2m[36m(func pid=64137)[0m f1_per_class: [0.364, 0.386, 0.116, 0.283, 0.108, 0.358, 0.346, 0.321, 0.179, 0.207]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m top1: 0.33955223880597013
[2m[36m(func pid=65548)[0m top5: 0.84375
[2m[36m(func pid=65548)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=65548)[0m f1_macro: 0.33493862438391603
[2m[36m(func pid=65548)[0m f1_weighted: 0.30765927785758046
[2m[36m(func pid=65548)[0m f1_per_class: [0.506, 0.411, 0.564, 0.47, 0.066, 0.247, 0.11, 0.295, 0.25, 0.43]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m top1: 0.14412313432835822
[2m[36m(func pid=77753)[0m top5: 0.5499067164179104
[2m[36m(func pid=77753)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=77753)[0m f1_macro: 0.09469087312880416
[2m[36m(func pid=77753)[0m f1_weighted: 0.14663899474501418
[2m[36m(func pid=77753)[0m f1_per_class: [0.025, 0.175, 0.083, 0.181, 0.022, 0.293, 0.091, 0.064, 0.012, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 14.9622 | Steps: 4 | Val loss: 55.0782 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4048 | Steps: 4 | Val loss: 2.7395 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.1114 | Steps: 4 | Val loss: 8.0891 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 12:20:33 (running for 00:22:39.74)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.418 |      0.267 |                   65 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  3.154 |      0.335 |                   58 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 14.962 |      0.218 |                   40 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.954 |      0.095 |                    9 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.2103544776119403
[2m[36m(func pid=70356)[0m top5: 0.6833022388059702
[2m[36m(func pid=70356)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=70356)[0m f1_macro: 0.21837472750173975
[2m[36m(func pid=70356)[0m f1_weighted: 0.23888126465819814
[2m[36m(func pid=70356)[0m f1_per_class: [0.386, 0.175, 0.056, 0.251, 0.055, 0.285, 0.221, 0.38, 0.247, 0.128]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9377 | Steps: 4 | Val loss: 2.3014 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=64137)[0m top1: 0.30130597014925375
[2m[36m(func pid=64137)[0m top5: 0.840018656716418
[2m[36m(func pid=64137)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=64137)[0m f1_macro: 0.2696608369355605
[2m[36m(func pid=64137)[0m f1_weighted: 0.32528902241131225
[2m[36m(func pid=64137)[0m f1_per_class: [0.325, 0.383, 0.156, 0.317, 0.124, 0.368, 0.311, 0.333, 0.184, 0.195]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m top1: 0.34468283582089554
[2m[36m(func pid=65548)[0m top5: 0.8521455223880597
[2m[36m(func pid=65548)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=65548)[0m f1_macro: 0.34231755918609796
[2m[36m(func pid=65548)[0m f1_weighted: 0.32352084237730966
[2m[36m(func pid=65548)[0m f1_per_class: [0.479, 0.405, 0.558, 0.475, 0.081, 0.271, 0.146, 0.344, 0.252, 0.412]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m top1: 0.14132462686567165
[2m[36m(func pid=77753)[0m top5: 0.5629664179104478
[2m[36m(func pid=77753)[0m f1_micro: 0.14132462686567165
[2m[36m(func pid=77753)[0m f1_macro: 0.09239739636557887
[2m[36m(func pid=77753)[0m f1_weighted: 0.14198794660988945
[2m[36m(func pid=77753)[0m f1_per_class: [0.025, 0.151, 0.091, 0.184, 0.019, 0.293, 0.087, 0.061, 0.012, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.7635 | Steps: 4 | Val loss: 52.3278 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.5191 | Steps: 4 | Val loss: 2.8078 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.5930 | Steps: 4 | Val loss: 8.0162 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=70356)[0m top1: 0.23180970149253732
[2m[36m(func pid=70356)[0m top5: 0.7472014925373134
[2m[36m(func pid=70356)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=70356)[0m f1_macro: 0.24596062150824555
[2m[36m(func pid=70356)[0m f1_weighted: 0.2760823473218562
[2m[36m(func pid=70356)[0m f1_per_class: [0.43, 0.268, 0.042, 0.23, 0.051, 0.27, 0.308, 0.39, 0.263, 0.208]
== Status ==
Current time: 2024-01-07 12:20:39 (running for 00:22:44.94)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.405 |      0.27  |                   66 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.111 |      0.342 |                   59 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  2.763 |      0.246 |                   41 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.938 |      0.092 |                   10 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9113 | Steps: 4 | Val loss: 2.3036 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=64137)[0m top1: 0.2905783582089552
[2m[36m(func pid=64137)[0m top5: 0.8311567164179104
[2m[36m(func pid=64137)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=64137)[0m f1_macro: 0.2595615065750196
[2m[36m(func pid=64137)[0m f1_weighted: 0.3164279232748972
[2m[36m(func pid=64137)[0m f1_per_class: [0.315, 0.384, 0.142, 0.326, 0.122, 0.335, 0.289, 0.322, 0.178, 0.183]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m top1: 0.3376865671641791
[2m[36m(func pid=65548)[0m top5: 0.8451492537313433
[2m[36m(func pid=65548)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=65548)[0m f1_macro: 0.3366775110674275
[2m[36m(func pid=65548)[0m f1_weighted: 0.33156141845043235
[2m[36m(func pid=65548)[0m f1_per_class: [0.451, 0.388, 0.542, 0.477, 0.084, 0.334, 0.165, 0.338, 0.225, 0.364]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m top1: 0.1501865671641791
[2m[36m(func pid=77753)[0m top5: 0.558768656716418
[2m[36m(func pid=77753)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=77753)[0m f1_macro: 0.10158179043132845
[2m[36m(func pid=77753)[0m f1_weighted: 0.1546579930309646
[2m[36m(func pid=77753)[0m f1_per_class: [0.025, 0.177, 0.108, 0.199, 0.018, 0.293, 0.095, 0.089, 0.011, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 10.2820 | Steps: 4 | Val loss: 49.1430 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.5865 | Steps: 4 | Val loss: 2.7049 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.6256 | Steps: 4 | Val loss: 8.3118 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 12:20:44 (running for 00:22:50.17)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.519 |      0.26  |                   67 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.593 |      0.337 |                   60 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 10.282 |      0.277 |                   42 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.911 |      0.102 |                   11 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.27611940298507465
[2m[36m(func pid=70356)[0m top5: 0.7817164179104478
[2m[36m(func pid=70356)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=70356)[0m f1_macro: 0.27684562863618284
[2m[36m(func pid=70356)[0m f1_weighted: 0.3199670822116591
[2m[36m(func pid=70356)[0m f1_per_class: [0.505, 0.36, 0.052, 0.263, 0.046, 0.217, 0.386, 0.377, 0.258, 0.305]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9301 | Steps: 4 | Val loss: 2.2997 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=64137)[0m top1: 0.3069029850746269
[2m[36m(func pid=64137)[0m top5: 0.8456156716417911
[2m[36m(func pid=64137)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=64137)[0m f1_macro: 0.2709626676494848
[2m[36m(func pid=64137)[0m f1_weighted: 0.3390996052842966
[2m[36m(func pid=64137)[0m f1_per_class: [0.318, 0.394, 0.134, 0.346, 0.149, 0.344, 0.337, 0.322, 0.175, 0.19]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m top1: 0.30597014925373134
[2m[36m(func pid=65548)[0m top5: 0.8250932835820896
[2m[36m(func pid=65548)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=65548)[0m f1_macro: 0.3064555252773964
[2m[36m(func pid=65548)[0m f1_weighted: 0.31007834980142324
[2m[36m(func pid=65548)[0m f1_per_class: [0.417, 0.364, 0.491, 0.439, 0.082, 0.336, 0.157, 0.309, 0.209, 0.262]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m top1: 0.15205223880597016
[2m[36m(func pid=77753)[0m top5: 0.5746268656716418
[2m[36m(func pid=77753)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=77753)[0m f1_macro: 0.10781814985215302
[2m[36m(func pid=77753)[0m f1_weighted: 0.1560373903331331
[2m[36m(func pid=77753)[0m f1_per_class: [0.061, 0.158, 0.121, 0.211, 0.02, 0.29, 0.094, 0.114, 0.01, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 8.6908 | Steps: 4 | Val loss: 45.8262 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.5224 | Steps: 4 | Val loss: 2.6966 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7989 | Steps: 4 | Val loss: 9.0681 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 12:20:49 (running for 00:22:55.44)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.587 |      0.271 |                   68 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.626 |      0.306 |                   61 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  8.691 |      0.308 |                   43 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.93  |      0.108 |                   12 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.34654850746268656
[2m[36m(func pid=70356)[0m top5: 0.8003731343283582
[2m[36m(func pid=70356)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=70356)[0m f1_macro: 0.30834937390695916
[2m[36m(func pid=70356)[0m f1_weighted: 0.3593122152627147
[2m[36m(func pid=70356)[0m f1_per_class: [0.562, 0.452, 0.107, 0.259, 0.053, 0.224, 0.466, 0.338, 0.262, 0.362]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.9221 | Steps: 4 | Val loss: 2.2932 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=64137)[0m top1: 0.3050373134328358
[2m[36m(func pid=64137)[0m top5: 0.84375
[2m[36m(func pid=64137)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=64137)[0m f1_macro: 0.2707659307854695
[2m[36m(func pid=64137)[0m f1_weighted: 0.3373413846103325
[2m[36m(func pid=64137)[0m f1_per_class: [0.321, 0.398, 0.123, 0.333, 0.171, 0.333, 0.345, 0.328, 0.165, 0.191]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m top1: 0.2691231343283582
[2m[36m(func pid=65548)[0m top5: 0.7994402985074627
[2m[36m(func pid=65548)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=65548)[0m f1_macro: 0.2779097681681394
[2m[36m(func pid=65548)[0m f1_weighted: 0.27109653800999944
[2m[36m(func pid=65548)[0m f1_per_class: [0.308, 0.386, 0.464, 0.297, 0.109, 0.388, 0.142, 0.298, 0.171, 0.216]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m top1: 0.14692164179104478
[2m[36m(func pid=77753)[0m top5: 0.5886194029850746
[2m[36m(func pid=77753)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=77753)[0m f1_macro: 0.10510917772910669
[2m[36m(func pid=77753)[0m f1_weighted: 0.15552123187263794
[2m[36m(func pid=77753)[0m f1_per_class: [0.054, 0.152, 0.111, 0.196, 0.015, 0.289, 0.111, 0.104, 0.019, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 8.9914 | Steps: 4 | Val loss: 45.8976 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.8385 | Steps: 4 | Val loss: 2.7150 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=70356)[0m top1: 0.37593283582089554
[2m[36m(func pid=70356)[0m top5: 0.8055037313432836
[2m[36m(func pid=70356)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=70356)[0m f1_macro: 0.3300406073491703
[2m[36m(func pid=70356)[0m f1_weighted: 0.3685389839190973
[2m[36m(func pid=70356)[0m f1_per_class: [0.543, 0.475, 0.316, 0.263, 0.064, 0.25, 0.469, 0.326, 0.26, 0.333]
[2m[36m(func pid=70356)[0m 
== Status ==
Current time: 2024-01-07 12:20:54 (running for 00:23:00.61)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.522 |      0.271 |                   69 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.799 |      0.278 |                   62 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  8.991 |      0.33  |                   44 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.922 |      0.105 |                   13 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.4365 | Steps: 4 | Val loss: 10.7459 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.9232 | Steps: 4 | Val loss: 2.2851 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=64137)[0m top1: 0.30830223880597013
[2m[36m(func pid=64137)[0m top5: 0.8428171641791045
[2m[36m(func pid=64137)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=64137)[0m f1_macro: 0.27132287529254584
[2m[36m(func pid=64137)[0m f1_weighted: 0.33861739619806713
[2m[36m(func pid=64137)[0m f1_per_class: [0.328, 0.402, 0.116, 0.332, 0.146, 0.336, 0.344, 0.337, 0.17, 0.203]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=65548)[0m top1: 0.23041044776119404
[2m[36m(func pid=65548)[0m top5: 0.75
[2m[36m(func pid=65548)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=65548)[0m f1_macro: 0.2597151486826489
[2m[36m(func pid=65548)[0m f1_weighted: 0.21722942143526855
[2m[36m(func pid=65548)[0m f1_per_class: [0.286, 0.371, 0.545, 0.151, 0.136, 0.377, 0.115, 0.293, 0.144, 0.179]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=77753)[0m top1: 0.14458955223880596
[2m[36m(func pid=77753)[0m top5: 0.605410447761194
[2m[36m(func pid=77753)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=77753)[0m f1_macro: 0.10354436626434878
[2m[36m(func pid=77753)[0m f1_weighted: 0.15419212744890465
[2m[36m(func pid=77753)[0m f1_per_class: [0.046, 0.154, 0.105, 0.184, 0.019, 0.283, 0.119, 0.106, 0.019, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 14.6028 | Steps: 4 | Val loss: 42.9307 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.7501 | Steps: 4 | Val loss: 2.7651 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 12:20:59 (running for 00:23:05.89)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.839 |      0.271 |                   70 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  2.437 |      0.26  |                   63 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 14.603 |      0.359 |                   45 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.923 |      0.104 |                   14 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.3880597014925373
[2m[36m(func pid=70356)[0m top5: 0.8344216417910447
[2m[36m(func pid=70356)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=70356)[0m f1_macro: 0.35857798445475303
[2m[36m(func pid=70356)[0m f1_weighted: 0.3838557423776479
[2m[36m(func pid=70356)[0m f1_per_class: [0.533, 0.486, 0.512, 0.332, 0.081, 0.249, 0.445, 0.318, 0.283, 0.347]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m top1: 0.29757462686567165
[2m[36m(func pid=64137)[0m top5: 0.8311567164179104
[2m[36m(func pid=64137)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=64137)[0m f1_macro: 0.2630478410425552
[2m[36m(func pid=64137)[0m f1_weighted: 0.3263915251296873
[2m[36m(func pid=64137)[0m f1_per_class: [0.285, 0.375, 0.123, 0.334, 0.165, 0.328, 0.324, 0.321, 0.179, 0.197]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8601 | Steps: 4 | Val loss: 2.2840 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6874 | Steps: 4 | Val loss: 11.6606 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=77753)[0m top1: 0.15205223880597016
[2m[36m(func pid=77753)[0m top5: 0.6100746268656716
[2m[36m(func pid=77753)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=77753)[0m f1_macro: 0.11064496131484751
[2m[36m(func pid=77753)[0m f1_weighted: 0.16378920768976496
[2m[36m(func pid=77753)[0m f1_per_class: [0.056, 0.157, 0.108, 0.201, 0.02, 0.304, 0.122, 0.12, 0.018, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 12.8392 | Steps: 4 | Val loss: 39.1435 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=65548)[0m top1: 0.22108208955223882
[2m[36m(func pid=65548)[0m top5: 0.7028917910447762
[2m[36m(func pid=65548)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=65548)[0m f1_macro: 0.2513234896497869
[2m[36m(func pid=65548)[0m f1_weighted: 0.19662332778367486
[2m[36m(func pid=65548)[0m f1_per_class: [0.429, 0.361, 0.421, 0.079, 0.147, 0.373, 0.117, 0.27, 0.148, 0.169]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3568 | Steps: 4 | Val loss: 2.6990 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:21:05 (running for 00:23:11.14)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.75  |      0.263 |                   71 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.687 |      0.251 |                   64 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 12.839 |      0.384 |                   46 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.86  |      0.111 |                   15 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.3833955223880597
[2m[36m(func pid=70356)[0m top5: 0.8708022388059702
[2m[36m(func pid=70356)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=70356)[0m f1_macro: 0.38449397358184056
[2m[36m(func pid=70356)[0m f1_weighted: 0.3907923036922022
[2m[36m(func pid=70356)[0m f1_per_class: [0.568, 0.451, 0.71, 0.436, 0.09, 0.27, 0.378, 0.314, 0.267, 0.361]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m top1: 0.31669776119402987
[2m[36m(func pid=64137)[0m top5: 0.8414179104477612
[2m[36m(func pid=64137)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=64137)[0m f1_macro: 0.2767101845473331
[2m[36m(func pid=64137)[0m f1_weighted: 0.34068584837432225
[2m[36m(func pid=64137)[0m f1_per_class: [0.295, 0.389, 0.135, 0.363, 0.142, 0.35, 0.319, 0.347, 0.196, 0.23]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8897 | Steps: 4 | Val loss: 2.2832 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.7523 | Steps: 4 | Val loss: 12.3112 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=77753)[0m top1: 0.1501865671641791
[2m[36m(func pid=77753)[0m top5: 0.6058768656716418
[2m[36m(func pid=77753)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=77753)[0m f1_macro: 0.10955198029928462
[2m[36m(func pid=77753)[0m f1_weighted: 0.1652145992015114
[2m[36m(func pid=77753)[0m f1_per_class: [0.048, 0.147, 0.093, 0.196, 0.026, 0.302, 0.138, 0.124, 0.021, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 9.0271 | Steps: 4 | Val loss: 40.9477 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4198 | Steps: 4 | Val loss: 2.5799 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=65548)[0m top1: 0.2196828358208955
[2m[36m(func pid=65548)[0m top5: 0.6823694029850746
[2m[36m(func pid=65548)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=65548)[0m f1_macro: 0.249865069503812
[2m[36m(func pid=65548)[0m f1_weighted: 0.18241452484128762
[2m[36m(func pid=65548)[0m f1_per_class: [0.494, 0.357, 0.393, 0.049, 0.152, 0.35, 0.102, 0.278, 0.164, 0.159]
[2m[36m(func pid=65548)[0m 
== Status ==
Current time: 2024-01-07 12:21:10 (running for 00:23:16.36)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.357 |      0.277 |                   72 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.752 |      0.25  |                   65 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  9.027 |      0.379 |                   47 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.89  |      0.11  |                   16 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.35867537313432835
[2m[36m(func pid=70356)[0m top5: 0.8759328358208955
[2m[36m(func pid=70356)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=70356)[0m f1_macro: 0.37853196004753376
[2m[36m(func pid=70356)[0m f1_weighted: 0.3598728075391174
[2m[36m(func pid=70356)[0m f1_per_class: [0.56, 0.354, 0.769, 0.461, 0.151, 0.297, 0.294, 0.343, 0.239, 0.317]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m top1: 0.3381529850746269
[2m[36m(func pid=64137)[0m top5: 0.8544776119402985
[2m[36m(func pid=64137)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=64137)[0m f1_macro: 0.29316850887970625
[2m[36m(func pid=64137)[0m f1_weighted: 0.35627314426155327
[2m[36m(func pid=64137)[0m f1_per_class: [0.309, 0.424, 0.16, 0.404, 0.146, 0.362, 0.3, 0.367, 0.215, 0.243]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.9370 | Steps: 4 | Val loss: 2.2816 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.8183 | Steps: 4 | Val loss: 12.3488 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 7.2578 | Steps: 4 | Val loss: 48.9283 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=77753)[0m top1: 0.15111940298507462
[2m[36m(func pid=77753)[0m top5: 0.6198694029850746
[2m[36m(func pid=77753)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=77753)[0m f1_macro: 0.11012520341891419
[2m[36m(func pid=77753)[0m f1_weighted: 0.16540362323178714
[2m[36m(func pid=77753)[0m f1_per_class: [0.052, 0.123, 0.103, 0.218, 0.028, 0.298, 0.133, 0.127, 0.021, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.3011 | Steps: 4 | Val loss: 2.4899 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=65548)[0m top1: 0.22434701492537312
[2m[36m(func pid=65548)[0m top5: 0.6767723880597015
[2m[36m(func pid=65548)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=65548)[0m f1_macro: 0.24640963721372944
[2m[36m(func pid=65548)[0m f1_weighted: 0.18629124081433426
[2m[36m(func pid=65548)[0m f1_per_class: [0.485, 0.322, 0.353, 0.023, 0.173, 0.338, 0.164, 0.271, 0.189, 0.146]
[2m[36m(func pid=65548)[0m 
== Status ==
Current time: 2024-01-07 12:21:15 (running for 00:23:21.74)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=9
Bracket: Iter 75.000: 0.329
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 4 RUNNING, 9 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00009 | RUNNING    | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.42  |      0.293 |                   73 |
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  2.818 |      0.246 |                   66 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  7.258 |      0.35  |                   48 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.937 |      0.11  |                   17 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (3 PENDING, 1 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.324160447761194
[2m[36m(func pid=70356)[0m top5: 0.8250932835820896
[2m[36m(func pid=70356)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=70356)[0m f1_macro: 0.3497036959484007
[2m[36m(func pid=70356)[0m f1_weighted: 0.30709651328917265
[2m[36m(func pid=70356)[0m f1_per_class: [0.553, 0.2, 0.769, 0.489, 0.104, 0.285, 0.192, 0.3, 0.249, 0.356]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=64137)[0m top1: 0.3516791044776119
[2m[36m(func pid=64137)[0m top5: 0.8647388059701493
[2m[36m(func pid=64137)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=64137)[0m f1_macro: 0.3027363553899368
[2m[36m(func pid=64137)[0m f1_weighted: 0.36880322534795806
[2m[36m(func pid=64137)[0m f1_per_class: [0.351, 0.455, 0.155, 0.412, 0.155, 0.362, 0.317, 0.36, 0.199, 0.262]
[2m[36m(func pid=64137)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8693 | Steps: 4 | Val loss: 2.2738 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.8253 | Steps: 4 | Val loss: 11.6739 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 11.1697 | Steps: 4 | Val loss: 56.0750 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=64137)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.5447 | Steps: 4 | Val loss: 2.4027 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=77753)[0m top1: 0.15578358208955223
[2m[36m(func pid=77753)[0m top5: 0.6296641791044776
[2m[36m(func pid=77753)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=77753)[0m f1_macro: 0.11180228514904977
[2m[36m(func pid=77753)[0m f1_weighted: 0.1727129561836998
[2m[36m(func pid=77753)[0m f1_per_class: [0.043, 0.134, 0.103, 0.231, 0.028, 0.295, 0.141, 0.121, 0.022, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=65548)[0m top1: 0.24580223880597016
[2m[36m(func pid=65548)[0m top5: 0.6861007462686567
[2m[36m(func pid=65548)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=65548)[0m f1_macro: 0.2522042622169437
[2m[36m(func pid=65548)[0m f1_weighted: 0.21067289764503605
[2m[36m(func pid=65548)[0m f1_per_class: [0.424, 0.326, 0.304, 0.026, 0.2, 0.349, 0.236, 0.27, 0.233, 0.154]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=64137)[0m top1: 0.36473880597014924
[2m[36m(func pid=64137)[0m top5: 0.871268656716418
[2m[36m(func pid=64137)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=64137)[0m f1_macro: 0.31281308090866417
[2m[36m(func pid=64137)[0m f1_weighted: 0.38377838354672555
[2m[36m(func pid=64137)[0m f1_per_class: [0.384, 0.458, 0.16, 0.436, 0.172, 0.364, 0.342, 0.346, 0.203, 0.264]
== Status ==
Current time: 2024-01-07 12:21:21 (running for 00:23:27.19)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.32675
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (11 PENDING, 3 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.825 |      0.252 |                   67 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  7.258 |      0.35  |                   48 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.869 |      0.112 |                   18 |
| train_9b9e8_00013 | PENDING    |                    | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 1 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.31576492537313433
[2m[36m(func pid=70356)[0m top5: 0.784981343283582
[2m[36m(func pid=70356)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=70356)[0m f1_macro: 0.3371328034139915
[2m[36m(func pid=70356)[0m f1_weighted: 0.28154294533814955
[2m[36m(func pid=70356)[0m f1_per_class: [0.5, 0.111, 0.769, 0.502, 0.156, 0.312, 0.14, 0.285, 0.246, 0.35]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.8467 | Steps: 4 | Val loss: 2.2634 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.8339 | Steps: 4 | Val loss: 10.7143 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=77753)[0m top1: 0.1623134328358209
[2m[36m(func pid=77753)[0m top5: 0.6408582089552238
[2m[36m(func pid=77753)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=77753)[0m f1_macro: 0.11525552692814404
[2m[36m(func pid=77753)[0m f1_weighted: 0.18062386717935033
[2m[36m(func pid=77753)[0m f1_per_class: [0.044, 0.149, 0.1, 0.235, 0.028, 0.308, 0.152, 0.114, 0.023, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 11.5289 | Steps: 4 | Val loss: 56.6892 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=65548)[0m top1: 0.27052238805970147
[2m[36m(func pid=65548)[0m top5: 0.7080223880597015
[2m[36m(func pid=65548)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=65548)[0m f1_macro: 0.26631722463458735
[2m[36m(func pid=65548)[0m f1_weighted: 0.2519925675102614
[2m[36m(func pid=65548)[0m f1_per_class: [0.411, 0.324, 0.3, 0.079, 0.212, 0.374, 0.318, 0.26, 0.247, 0.138]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=70356)[0m top1: 0.292910447761194
[2m[36m(func pid=70356)[0m top5: 0.7649253731343284
[2m[36m(func pid=70356)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=70356)[0m f1_macro: 0.31525360334655267
[2m[36m(func pid=70356)[0m f1_weighted: 0.27276977543843534
[2m[36m(func pid=70356)[0m f1_per_class: [0.387, 0.126, 0.786, 0.466, 0.143, 0.31, 0.152, 0.278, 0.211, 0.294]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.8149 | Steps: 4 | Val loss: 2.2654 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.3555 | Steps: 4 | Val loss: 10.0391 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=77753)[0m top1: 0.15951492537313433
[2m[36m(func pid=77753)[0m top5: 0.6385261194029851
[2m[36m(func pid=77753)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=77753)[0m f1_macro: 0.11473492016021052
[2m[36m(func pid=77753)[0m f1_weighted: 0.17915759763417283
[2m[36m(func pid=77753)[0m f1_per_class: [0.054, 0.148, 0.089, 0.232, 0.026, 0.31, 0.148, 0.117, 0.023, 0.0]
== Status ==
Current time: 2024-01-07 12:21:26 (running for 00:23:32.83)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.32675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.834 |      0.266 |                   68 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 11.529 |      0.315 |                   50 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.847 |      0.115 |                   19 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=82573)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=82573)[0m Configuration completed!
[2m[36m(func pid=82573)[0m New optimizer parameters:
[2m[36m(func pid=82573)[0m SGD (
[2m[36m(func pid=82573)[0m Parameter Group 0
[2m[36m(func pid=82573)[0m     dampening: 0
[2m[36m(func pid=82573)[0m     differentiable: False
[2m[36m(func pid=82573)[0m     foreach: None
[2m[36m(func pid=82573)[0m     lr: 0.001
[2m[36m(func pid=82573)[0m     maximize: False
[2m[36m(func pid=82573)[0m     momentum: 0.9
[2m[36m(func pid=82573)[0m     nesterov: False
[2m[36m(func pid=82573)[0m     weight_decay: 0.0001
[2m[36m(func pid=82573)[0m )
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 9.7239 | Steps: 4 | Val loss: 61.4974 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=65548)[0m top1: 0.2980410447761194
[2m[36m(func pid=65548)[0m top5: 0.7173507462686567
[2m[36m(func pid=65548)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=65548)[0m f1_macro: 0.26710958096340026
[2m[36m(func pid=65548)[0m f1_weighted: 0.2938975199282208
[2m[36m(func pid=65548)[0m f1_per_class: [0.341, 0.363, 0.226, 0.146, 0.148, 0.401, 0.369, 0.266, 0.246, 0.165]
[2m[36m(func pid=65548)[0m 
== Status ==
Current time: 2024-01-07 12:21:32 (running for 00:23:38.36)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.32675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.356 |      0.267 |                   69 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  9.724 |      0.269 |                   51 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.815 |      0.115 |                   20 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.24486940298507462
[2m[36m(func pid=70356)[0m top5: 0.7252798507462687
[2m[36m(func pid=70356)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=70356)[0m f1_macro: 0.2691461726264767
[2m[36m(func pid=70356)[0m f1_weighted: 0.24955498143205918
[2m[36m(func pid=70356)[0m f1_per_class: [0.178, 0.119, 0.714, 0.334, 0.129, 0.345, 0.213, 0.272, 0.172, 0.214]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.9050 | Steps: 4 | Val loss: 2.2681 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.9698 | Steps: 4 | Val loss: 9.6248 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9381 | Steps: 4 | Val loss: 2.3104 | Batch size: 32 | lr: 0.001 | Duration: 4.62s
[2m[36m(func pid=77753)[0m top1: 0.15671641791044777
[2m[36m(func pid=77753)[0m top5: 0.628731343283582
[2m[36m(func pid=77753)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=77753)[0m f1_macro: 0.1150871704644987
[2m[36m(func pid=77753)[0m f1_weighted: 0.1780014260476067
[2m[36m(func pid=77753)[0m f1_per_class: [0.059, 0.138, 0.105, 0.246, 0.025, 0.287, 0.144, 0.126, 0.02, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 12.7284 | Steps: 4 | Val loss: 74.7882 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=65548)[0m top1: 0.3148320895522388
[2m[36m(func pid=65548)[0m top5: 0.7364738805970149
[2m[36m(func pid=65548)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=65548)[0m f1_macro: 0.27145766565176616
[2m[36m(func pid=65548)[0m f1_weighted: 0.3136700398507169
[2m[36m(func pid=65548)[0m f1_per_class: [0.274, 0.377, 0.171, 0.161, 0.143, 0.417, 0.411, 0.26, 0.245, 0.256]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=82573)[0m top1: 0.18983208955223882
[2m[36m(func pid=82573)[0m top5: 0.5443097014925373
[2m[36m(func pid=82573)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=82573)[0m f1_macro: 0.11486131278110812
[2m[36m(func pid=82573)[0m f1_weighted: 0.13300155756443366
[2m[36m(func pid=82573)[0m f1_per_class: [0.274, 0.327, 0.0, 0.121, 0.012, 0.29, 0.009, 0.0, 0.0, 0.115]
[2m[36m(func pid=82573)[0m 
== Status ==
Current time: 2024-01-07 12:21:37 (running for 00:23:43.66)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.32675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.97  |      0.271 |                   70 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 12.728 |      0.251 |                   52 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.905 |      0.115 |                   21 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.938 |      0.115 |                    1 |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.1982276119402985
[2m[36m(func pid=70356)[0m top5: 0.679570895522388
[2m[36m(func pid=70356)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=70356)[0m f1_macro: 0.2507803738957901
[2m[36m(func pid=70356)[0m f1_weighted: 0.20854230800832887
[2m[36m(func pid=70356)[0m f1_per_class: [0.104, 0.165, 0.714, 0.106, 0.168, 0.334, 0.268, 0.293, 0.168, 0.188]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.8856 | Steps: 4 | Val loss: 2.2710 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.6178 | Steps: 4 | Val loss: 8.7404 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9825 | Steps: 4 | Val loss: 2.3173 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=77753)[0m top1: 0.1609141791044776
[2m[36m(func pid=77753)[0m top5: 0.6231343283582089
[2m[36m(func pid=77753)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=77753)[0m f1_macro: 0.126890646819365
[2m[36m(func pid=77753)[0m f1_weighted: 0.18019063794408097
[2m[36m(func pid=77753)[0m f1_per_class: [0.104, 0.155, 0.162, 0.234, 0.029, 0.289, 0.147, 0.13, 0.019, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 14.5445 | Steps: 4 | Val loss: 74.3795 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=65548)[0m top1: 0.3451492537313433
[2m[36m(func pid=65548)[0m top5: 0.7658582089552238
[2m[36m(func pid=65548)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=65548)[0m f1_macro: 0.29061228995089666
[2m[36m(func pid=65548)[0m f1_weighted: 0.35394120303303117
[2m[36m(func pid=65548)[0m f1_per_class: [0.263, 0.431, 0.182, 0.238, 0.121, 0.379, 0.456, 0.256, 0.247, 0.333]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=82573)[0m top1: 0.1791044776119403
[2m[36m(func pid=82573)[0m top5: 0.5247201492537313
[2m[36m(func pid=82573)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=82573)[0m f1_macro: 0.11065568273755355
[2m[36m(func pid=82573)[0m f1_weighted: 0.13355627543859472
[2m[36m(func pid=82573)[0m f1_per_class: [0.269, 0.291, 0.0, 0.139, 0.01, 0.306, 0.009, 0.011, 0.0, 0.071]
[2m[36m(func pid=82573)[0m 
== Status ==
Current time: 2024-01-07 12:21:42 (running for 00:23:48.81)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.32675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.618 |      0.291 |                   71 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 14.545 |      0.244 |                   53 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.886 |      0.127 |                   22 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.982 |      0.111 |                    2 |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.19869402985074627
[2m[36m(func pid=70356)[0m top5: 0.6879664179104478
[2m[36m(func pid=70356)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=70356)[0m f1_macro: 0.2439215220955032
[2m[36m(func pid=70356)[0m f1_weighted: 0.2146143261248387
[2m[36m(func pid=70356)[0m f1_per_class: [0.106, 0.229, 0.69, 0.085, 0.146, 0.249, 0.303, 0.308, 0.171, 0.153]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.8779 | Steps: 4 | Val loss: 2.2637 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.2806 | Steps: 4 | Val loss: 8.8438 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9511 | Steps: 4 | Val loss: 2.3050 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=77753)[0m top1: 0.16651119402985073
[2m[36m(func pid=77753)[0m top5: 0.6361940298507462
[2m[36m(func pid=77753)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=77753)[0m f1_macro: 0.13132146854662127
[2m[36m(func pid=77753)[0m f1_weighted: 0.18522526099320524
[2m[36m(func pid=77753)[0m f1_per_class: [0.095, 0.175, 0.176, 0.224, 0.034, 0.307, 0.156, 0.126, 0.019, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 15.7458 | Steps: 4 | Val loss: 73.7481 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=65548)[0m top1: 0.3474813432835821
[2m[36m(func pid=65548)[0m top5: 0.7709888059701493
[2m[36m(func pid=65548)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=65548)[0m f1_macro: 0.29373222223852125
[2m[36m(func pid=65548)[0m f1_weighted: 0.3628897914146623
[2m[36m(func pid=65548)[0m f1_per_class: [0.211, 0.416, 0.18, 0.265, 0.109, 0.332, 0.483, 0.272, 0.254, 0.413]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=82573)[0m top1: 0.17490671641791045
[2m[36m(func pid=82573)[0m top5: 0.5401119402985075
[2m[36m(func pid=82573)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=82573)[0m f1_macro: 0.12497777069576484
[2m[36m(func pid=82573)[0m f1_weighted: 0.1471300035944731
[2m[36m(func pid=82573)[0m f1_per_class: [0.238, 0.275, 0.085, 0.137, 0.0, 0.309, 0.057, 0.036, 0.024, 0.09]
[2m[36m(func pid=82573)[0m 
== Status ==
Current time: 2024-01-07 12:21:48 (running for 00:23:54.03)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.32675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  0.281 |      0.294 |                   72 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 15.746 |      0.233 |                   54 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.878 |      0.131 |                   23 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.951 |      0.125 |                    3 |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.21548507462686567
[2m[36m(func pid=70356)[0m top5: 0.6884328358208955
[2m[36m(func pid=70356)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=70356)[0m f1_macro: 0.23258462136082617
[2m[36m(func pid=70356)[0m f1_weighted: 0.23234133781622462
[2m[36m(func pid=70356)[0m f1_per_class: [0.121, 0.29, 0.588, 0.079, 0.112, 0.167, 0.369, 0.299, 0.167, 0.132]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.9154 | Steps: 4 | Val loss: 2.2548 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.6282 | Steps: 4 | Val loss: 8.4218 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8644 | Steps: 4 | Val loss: 2.2626 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 7.4220 | Steps: 4 | Val loss: 66.6178 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=77753)[0m top1: 0.16930970149253732
[2m[36m(func pid=77753)[0m top5: 0.6506529850746269
[2m[36m(func pid=77753)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=77753)[0m f1_macro: 0.13058278071613696
[2m[36m(func pid=77753)[0m f1_weighted: 0.19128371901430768
[2m[36m(func pid=77753)[0m f1_per_class: [0.101, 0.151, 0.154, 0.237, 0.027, 0.302, 0.177, 0.139, 0.017, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=65548)[0m top1: 0.37593283582089554
[2m[36m(func pid=65548)[0m top5: 0.7961753731343284
[2m[36m(func pid=65548)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=65548)[0m f1_macro: 0.31083193303053125
[2m[36m(func pid=65548)[0m f1_weighted: 0.39099959991307337
[2m[36m(func pid=65548)[0m f1_per_class: [0.232, 0.463, 0.203, 0.312, 0.098, 0.302, 0.513, 0.3, 0.224, 0.462]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=82573)[0m top1: 0.19263059701492538
[2m[36m(func pid=82573)[0m top5: 0.6184701492537313
[2m[36m(func pid=82573)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=82573)[0m f1_macro: 0.14077065348321166
[2m[36m(func pid=82573)[0m f1_weighted: 0.19150389479050658
[2m[36m(func pid=82573)[0m f1_per_class: [0.234, 0.228, 0.086, 0.209, 0.017, 0.332, 0.149, 0.09, 0.01, 0.053]
[2m[36m(func pid=82573)[0m 
== Status ==
Current time: 2024-01-07 12:21:53 (running for 00:23:59.44)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.32675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.628 |      0.311 |                   73 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  7.422 |      0.238 |                   55 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.915 |      0.131 |                   24 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.864 |      0.141 |                    4 |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.26492537313432835
[2m[36m(func pid=70356)[0m top5: 0.710820895522388
[2m[36m(func pid=70356)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=70356)[0m f1_macro: 0.23842737149800106
[2m[36m(func pid=70356)[0m f1_weighted: 0.27035915816675127
[2m[36m(func pid=70356)[0m f1_per_class: [0.148, 0.386, 0.449, 0.087, 0.09, 0.135, 0.442, 0.312, 0.188, 0.147]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.8373 | Steps: 4 | Val loss: 2.2524 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.1591 | Steps: 4 | Val loss: 8.2891 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8115 | Steps: 4 | Val loss: 2.2298 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=77753)[0m top1: 0.16884328358208955
[2m[36m(func pid=77753)[0m top5: 0.6534514925373134
[2m[36m(func pid=77753)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=77753)[0m f1_macro: 0.13148952137279685
[2m[36m(func pid=77753)[0m f1_weighted: 0.1917829782834078
[2m[36m(func pid=77753)[0m f1_per_class: [0.102, 0.141, 0.167, 0.23, 0.028, 0.297, 0.194, 0.133, 0.025, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 16.7871 | Steps: 4 | Val loss: 64.7951 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=65548)[0m top1: 0.365205223880597
[2m[36m(func pid=65548)[0m top5: 0.8115671641791045
[2m[36m(func pid=65548)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=65548)[0m f1_macro: 0.29868195836421324
[2m[36m(func pid=65548)[0m f1_weighted: 0.37985645795826584
[2m[36m(func pid=65548)[0m f1_per_class: [0.24, 0.445, 0.197, 0.346, 0.09, 0.186, 0.498, 0.306, 0.208, 0.471]
[2m[36m(func pid=65548)[0m 
[2m[36m(func pid=82573)[0m top1: 0.21548507462686567
[2m[36m(func pid=82573)[0m top5: 0.6669776119402985
[2m[36m(func pid=82573)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=82573)[0m f1_macro: 0.1529232579080272
[2m[36m(func pid=82573)[0m f1_weighted: 0.22929458155630097
[2m[36m(func pid=82573)[0m f1_per_class: [0.206, 0.151, 0.083, 0.289, 0.019, 0.356, 0.228, 0.139, 0.019, 0.039]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.8119 | Steps: 4 | Val loss: 2.2541 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 12:21:59 (running for 00:24:05.02)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.32675
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00010 | RUNNING    | 192.168.7.53:65548 | 0.01   |       0.99 |         0.0001 |  1.159 |      0.299 |                   74 |
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 16.787 |      0.254 |                   56 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.837 |      0.131 |                   25 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.812 |      0.153 |                    5 |
| train_9b9e8_00014 | PENDING    |                    | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.314365671641791
[2m[36m(func pid=70356)[0m top5: 0.7234141791044776
[2m[36m(func pid=70356)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=70356)[0m f1_macro: 0.2544707238219446
[2m[36m(func pid=70356)[0m f1_weighted: 0.30405499936172187
[2m[36m(func pid=70356)[0m f1_per_class: [0.243, 0.431, 0.377, 0.136, 0.09, 0.119, 0.486, 0.287, 0.211, 0.163]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=65548)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 3.5016 | Steps: 4 | Val loss: 8.0860 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7361 | Steps: 4 | Val loss: 2.2000 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=77753)[0m top1: 0.16930970149253732
[2m[36m(func pid=77753)[0m top5: 0.6455223880597015
[2m[36m(func pid=77753)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=77753)[0m f1_macro: 0.13291176029354165
[2m[36m(func pid=77753)[0m f1_weighted: 0.19104817837357288
[2m[36m(func pid=77753)[0m f1_per_class: [0.115, 0.161, 0.14, 0.222, 0.028, 0.328, 0.174, 0.135, 0.026, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 11.2968 | Steps: 4 | Val loss: 63.6539 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=65548)[0m top1: 0.3689365671641791
[2m[36m(func pid=65548)[0m top5: 0.8278917910447762
[2m[36m(func pid=65548)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=65548)[0m f1_macro: 0.2985901332552052
[2m[36m(func pid=65548)[0m f1_weighted: 0.3829951406906863
[2m[36m(func pid=65548)[0m f1_per_class: [0.301, 0.464, 0.169, 0.43, 0.078, 0.09, 0.457, 0.283, 0.207, 0.508]
[2m[36m(func pid=82573)[0m top1: 0.22994402985074627
[2m[36m(func pid=82573)[0m top5: 0.695429104477612
[2m[36m(func pid=82573)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=82573)[0m f1_macro: 0.15930794237031698
[2m[36m(func pid=82573)[0m f1_weighted: 0.23922429505560538
[2m[36m(func pid=82573)[0m f1_per_class: [0.177, 0.078, 0.14, 0.364, 0.035, 0.374, 0.227, 0.137, 0.019, 0.042]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.8124 | Steps: 4 | Val loss: 2.2503 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=70356)[0m top1: 0.3516791044776119
[2m[36m(func pid=70356)[0m top5: 0.7416044776119403
[2m[36m(func pid=70356)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=70356)[0m f1_macro: 0.27438246332892907
[2m[36m(func pid=70356)[0m f1_weighted: 0.325916700847812
[2m[36m(func pid=70356)[0m f1_per_class: [0.4, 0.455, 0.316, 0.158, 0.095, 0.113, 0.517, 0.278, 0.232, 0.181]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7388 | Steps: 4 | Val loss: 2.1460 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=77753)[0m top1: 0.17537313432835822
[2m[36m(func pid=77753)[0m top5: 0.6515858208955224
[2m[36m(func pid=77753)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=77753)[0m f1_macro: 0.13626642906160016
[2m[36m(func pid=77753)[0m f1_weighted: 0.19958831307694
[2m[36m(func pid=77753)[0m f1_per_class: [0.112, 0.145, 0.162, 0.236, 0.033, 0.333, 0.199, 0.126, 0.017, 0.0]
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 8.8601 | Steps: 4 | Val loss: 60.3151 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=82573)[0m top1: 0.2677238805970149
[2m[36m(func pid=82573)[0m top5: 0.757929104477612
[2m[36m(func pid=82573)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=82573)[0m f1_macro: 0.18196738234234222
[2m[36m(func pid=82573)[0m f1_weighted: 0.2754480911370503
[2m[36m(func pid=82573)[0m f1_per_class: [0.229, 0.13, 0.147, 0.415, 0.023, 0.363, 0.264, 0.179, 0.013, 0.056]
[2m[36m(func pid=70356)[0m top1: 0.3763992537313433
[2m[36m(func pid=70356)[0m top5: 0.7709888059701493
[2m[36m(func pid=70356)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=70356)[0m f1_macro: 0.2982527499992484
[2m[36m(func pid=70356)[0m f1_weighted: 0.3568130776778017
[2m[36m(func pid=70356)[0m f1_per_class: [0.51, 0.46, 0.27, 0.249, 0.106, 0.133, 0.514, 0.29, 0.231, 0.22]
== Status ==
Current time: 2024-01-07 12:22:04 (running for 00:24:10.27)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 11.297 |      0.274 |                   57 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.812 |      0.133 |                   26 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.736 |      0.159 |                    6 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=84655)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=84655)[0m Configuration completed!
[2m[36m(func pid=84655)[0m New optimizer parameters:
[2m[36m(func pid=84655)[0m SGD (
[2m[36m(func pid=84655)[0m Parameter Group 0
[2m[36m(func pid=84655)[0m     dampening: 0
[2m[36m(func pid=84655)[0m     differentiable: False
[2m[36m(func pid=84655)[0m     foreach: None
[2m[36m(func pid=84655)[0m     lr: 0.01
[2m[36m(func pid=84655)[0m     maximize: False
[2m[36m(func pid=84655)[0m     momentum: 0.9
[2m[36m(func pid=84655)[0m     nesterov: False
[2m[36m(func pid=84655)[0m     weight_decay: 0.0001
[2m[36m(func pid=84655)[0m )
[2m[36m(func pid=84655)[0m 
== Status ==
Current time: 2024-01-07 12:22:11 (running for 00:24:17.67)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 11.297 |      0.274 |                   57 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.812 |      0.133 |                   26 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.739 |      0.182 |                    7 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7045 | Steps: 4 | Val loss: 2.1264 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 5.3383 | Steps: 4 | Val loss: 52.6995 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.7894 | Steps: 4 | Val loss: 2.2448 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9582 | Steps: 4 | Val loss: 2.2558 | Batch size: 32 | lr: 0.01 | Duration: 4.55s
== Status ==
Current time: 2024-01-07 12:22:16 (running for 00:24:22.69)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  8.86  |      0.298 |                   58 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.812 |      0.136 |                   27 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.739 |      0.182 |                    7 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.3927238805970149
[2m[36m(func pid=70356)[0m top5: 0.808768656716418
[2m[36m(func pid=70356)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=70356)[0m f1_macro: 0.3143474822286677
[2m[36m(func pid=70356)[0m f1_weighted: 0.3910776775429583
[2m[36m(func pid=70356)[0m f1_per_class: [0.559, 0.455, 0.208, 0.363, 0.092, 0.131, 0.51, 0.343, 0.261, 0.221]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m top1: 0.17723880597014927
[2m[36m(func pid=77753)[0m top5: 0.6576492537313433
[2m[36m(func pid=77753)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=77753)[0m f1_macro: 0.1355573693832149
[2m[36m(func pid=77753)[0m f1_weighted: 0.2013155696781326
[2m[36m(func pid=77753)[0m f1_per_class: [0.116, 0.139, 0.143, 0.251, 0.029, 0.328, 0.195, 0.128, 0.027, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.28404850746268656
[2m[36m(func pid=82573)[0m top5: 0.7686567164179104
[2m[36m(func pid=82573)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=82573)[0m f1_macro: 0.20617910458506383
[2m[36m(func pid=82573)[0m f1_weighted: 0.2825054508627438
[2m[36m(func pid=82573)[0m f1_per_class: [0.267, 0.09, 0.151, 0.455, 0.046, 0.341, 0.266, 0.196, 0.042, 0.207]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m top1: 0.22574626865671643
[2m[36m(func pid=84655)[0m top5: 0.5951492537313433
[2m[36m(func pid=84655)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=84655)[0m f1_macro: 0.14162331572397982
[2m[36m(func pid=84655)[0m f1_weighted: 0.16830332531279246
[2m[36m(func pid=84655)[0m f1_per_class: [0.37, 0.361, 0.0, 0.221, 0.0, 0.265, 0.009, 0.037, 0.0, 0.154]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 5.2463 | Steps: 4 | Val loss: 48.2531 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.8176 | Steps: 4 | Val loss: 2.2440 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6448 | Steps: 4 | Val loss: 2.0992 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7648 | Steps: 4 | Val loss: 2.1179 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:22:22 (running for 00:24:28.44)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  5.338 |      0.314 |                   59 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.789 |      0.136 |                   28 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.645 |      0.224 |                    9 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  2.958 |      0.142 |                    1 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3045708955223881
[2m[36m(func pid=82573)[0m top5: 0.7896455223880597
[2m[36m(func pid=82573)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=82573)[0m f1_macro: 0.22425007547090092
[2m[36m(func pid=82573)[0m f1_weighted: 0.3097987061622174
[2m[36m(func pid=82573)[0m f1_per_class: [0.287, 0.121, 0.203, 0.458, 0.044, 0.334, 0.336, 0.2, 0.045, 0.215]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=70356)[0m top1: 0.4155783582089552
[2m[36m(func pid=70356)[0m top5: 0.8362873134328358
[2m[36m(func pid=70356)[0m f1_micro: 0.41557835820895517
[2m[36m(func pid=70356)[0m f1_macro: 0.3329446145636942
[2m[36m(func pid=70356)[0m f1_weighted: 0.428347070424127
[2m[36m(func pid=70356)[0m f1_per_class: [0.524, 0.449, 0.187, 0.481, 0.073, 0.235, 0.481, 0.395, 0.255, 0.25]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m top1: 0.17863805970149255
[2m[36m(func pid=77753)[0m top5: 0.6581156716417911
[2m[36m(func pid=77753)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=77753)[0m f1_macro: 0.13834547802505134
[2m[36m(func pid=77753)[0m f1_weighted: 0.20462701553695503
[2m[36m(func pid=77753)[0m f1_per_class: [0.121, 0.149, 0.143, 0.256, 0.029, 0.311, 0.198, 0.149, 0.028, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m top1: 0.27425373134328357
[2m[36m(func pid=84655)[0m top5: 0.7033582089552238
[2m[36m(func pid=84655)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=84655)[0m f1_macro: 0.18989811285926322
[2m[36m(func pid=84655)[0m f1_weighted: 0.1989896175696295
[2m[36m(func pid=84655)[0m f1_per_class: [0.571, 0.2, 0.259, 0.385, 0.0, 0.324, 0.015, 0.016, 0.0, 0.129]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6100 | Steps: 4 | Val loss: 2.0751 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 7.8137 | Steps: 4 | Val loss: 49.2658 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.8465 | Steps: 4 | Val loss: 2.2490 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.4643 | Steps: 4 | Val loss: 2.1650 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 12:22:27 (running for 00:24:33.69)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  5.246 |      0.333 |                   60 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.818 |      0.138 |                   29 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.61  |      0.251 |                   10 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  2.765 |      0.19  |                    2 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.324160447761194
[2m[36m(func pid=82573)[0m top5: 0.8036380597014925
[2m[36m(func pid=82573)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=82573)[0m f1_macro: 0.25135436118558296
[2m[36m(func pid=82573)[0m f1_weighted: 0.3192904276365879
[2m[36m(func pid=82573)[0m f1_per_class: [0.349, 0.177, 0.338, 0.502, 0.037, 0.366, 0.273, 0.203, 0.061, 0.207]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=70356)[0m top1: 0.4146455223880597
[2m[36m(func pid=70356)[0m top5: 0.8484141791044776
[2m[36m(func pid=70356)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=70356)[0m f1_macro: 0.34375573602684806
[2m[36m(func pid=70356)[0m f1_weighted: 0.4198105112715909
[2m[36m(func pid=70356)[0m f1_per_class: [0.593, 0.425, 0.187, 0.511, 0.093, 0.315, 0.404, 0.387, 0.238, 0.284]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m top1: 0.1767723880597015
[2m[36m(func pid=77753)[0m top5: 0.6501865671641791
[2m[36m(func pid=77753)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=77753)[0m f1_macro: 0.13606366843414888
[2m[36m(func pid=77753)[0m f1_weighted: 0.20243015820158852
[2m[36m(func pid=77753)[0m f1_per_class: [0.136, 0.154, 0.13, 0.241, 0.036, 0.312, 0.207, 0.117, 0.028, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m top1: 0.2513992537313433
[2m[36m(func pid=84655)[0m top5: 0.6739738805970149
[2m[36m(func pid=84655)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=84655)[0m f1_macro: 0.19567213982767379
[2m[36m(func pid=84655)[0m f1_weighted: 0.19414088641921426
[2m[36m(func pid=84655)[0m f1_per_class: [0.581, 0.0, 0.044, 0.427, 0.07, 0.336, 0.024, 0.24, 0.0, 0.235]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.5537 | Steps: 4 | Val loss: 2.0523 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 9.7639 | Steps: 4 | Val loss: 54.4051 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.8232 | Steps: 4 | Val loss: 2.2407 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.3622 | Steps: 4 | Val loss: 2.1236 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 12:22:33 (running for 00:24:38.95)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  7.814 |      0.344 |                   61 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.847 |      0.136 |                   30 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.554 |      0.275 |                   11 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  2.464 |      0.196 |                    3 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3246268656716418
[2m[36m(func pid=82573)[0m top5: 0.8125
[2m[36m(func pid=82573)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=82573)[0m f1_macro: 0.274619534588786
[2m[36m(func pid=82573)[0m f1_weighted: 0.3257801710440541
[2m[36m(func pid=82573)[0m f1_per_class: [0.347, 0.266, 0.431, 0.477, 0.063, 0.384, 0.258, 0.188, 0.051, 0.281]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=70356)[0m top1: 0.38992537313432835
[2m[36m(func pid=70356)[0m top5: 0.8269589552238806
[2m[36m(func pid=70356)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=70356)[0m f1_macro: 0.3336682890079666
[2m[36m(func pid=70356)[0m f1_weighted: 0.38359116490684214
[2m[36m(func pid=70356)[0m f1_per_class: [0.579, 0.334, 0.144, 0.523, 0.129, 0.369, 0.3, 0.41, 0.229, 0.32]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m top1: 0.18516791044776118
[2m[36m(func pid=77753)[0m top5: 0.6688432835820896
[2m[36m(func pid=77753)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=77753)[0m f1_macro: 0.14255247480064928
[2m[36m(func pid=77753)[0m f1_weighted: 0.21029319897038526
[2m[36m(func pid=77753)[0m f1_per_class: [0.132, 0.181, 0.143, 0.247, 0.04, 0.322, 0.205, 0.123, 0.03, 0.0]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m top1: 0.2644589552238806
[2m[36m(func pid=84655)[0m top5: 0.6898320895522388
[2m[36m(func pid=84655)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=84655)[0m f1_macro: 0.1970994434323079
[2m[36m(func pid=84655)[0m f1_weighted: 0.22790433563785575
[2m[36m(func pid=84655)[0m f1_per_class: [0.23, 0.0, 0.171, 0.503, 0.055, 0.303, 0.077, 0.357, 0.0, 0.275]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5207 | Steps: 4 | Val loss: 2.0394 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 10.5376 | Steps: 4 | Val loss: 59.3005 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.8076 | Steps: 4 | Val loss: 2.2341 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.9647 | Steps: 4 | Val loss: 1.9190 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 12:22:38 (running for 00:24:44.18)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  9.764 |      0.334 |                   62 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.823 |      0.143 |                   31 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.521 |      0.298 |                   12 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  2.362 |      0.197 |                    4 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.32975746268656714
[2m[36m(func pid=82573)[0m top5: 0.8069029850746269
[2m[36m(func pid=82573)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=82573)[0m f1_macro: 0.2977255442498385
[2m[36m(func pid=82573)[0m f1_weighted: 0.3352572908001009
[2m[36m(func pid=82573)[0m f1_per_class: [0.34, 0.329, 0.55, 0.475, 0.048, 0.407, 0.24, 0.206, 0.048, 0.333]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=70356)[0m top1: 0.3805970149253731
[2m[36m(func pid=70356)[0m top5: 0.8078358208955224
[2m[36m(func pid=70356)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=70356)[0m f1_macro: 0.33367151270834594
[2m[36m(func pid=70356)[0m f1_weighted: 0.36040412299865143
[2m[36m(func pid=70356)[0m f1_per_class: [0.605, 0.284, 0.135, 0.537, 0.139, 0.428, 0.212, 0.408, 0.232, 0.357]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m top1: 0.19309701492537312
[2m[36m(func pid=77753)[0m top5: 0.679570895522388
[2m[36m(func pid=77753)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=77753)[0m f1_macro: 0.14803796558612176
[2m[36m(func pid=77753)[0m f1_weighted: 0.2173549666498063
[2m[36m(func pid=77753)[0m f1_per_class: [0.129, 0.193, 0.163, 0.268, 0.042, 0.327, 0.202, 0.126, 0.011, 0.019]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m top1: 0.2891791044776119
[2m[36m(func pid=84655)[0m top5: 0.8278917910447762
[2m[36m(func pid=84655)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=84655)[0m f1_macro: 0.23918629253588053
[2m[36m(func pid=84655)[0m f1_weighted: 0.3025899297031988
[2m[36m(func pid=84655)[0m f1_per_class: [0.142, 0.062, 0.358, 0.473, 0.092, 0.298, 0.324, 0.309, 0.091, 0.243]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.4203 | Steps: 4 | Val loss: 2.0373 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 8.2237 | Steps: 4 | Val loss: 63.1391 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.7640 | Steps: 4 | Val loss: 2.2319 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.0098 | Steps: 4 | Val loss: 1.7340 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 12:22:43 (running for 00:24:49.51)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 10.538 |      0.334 |                   63 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.808 |      0.148 |                   32 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.42  |      0.293 |                   13 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.965 |      0.239 |                    5 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3148320895522388
[2m[36m(func pid=82573)[0m top5: 0.804570895522388
[2m[36m(func pid=82573)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=82573)[0m f1_macro: 0.29258940507418807
[2m[36m(func pid=82573)[0m f1_weighted: 0.32394254733675454
[2m[36m(func pid=82573)[0m f1_per_class: [0.342, 0.349, 0.537, 0.441, 0.049, 0.402, 0.224, 0.211, 0.054, 0.318]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=70356)[0m top1: 0.3670708955223881
[2m[36m(func pid=70356)[0m top5: 0.7779850746268657
[2m[36m(func pid=70356)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=70356)[0m f1_macro: 0.31370683544737
[2m[36m(func pid=70356)[0m f1_weighted: 0.34857220577752646
[2m[36m(func pid=70356)[0m f1_per_class: [0.571, 0.237, 0.129, 0.558, 0.117, 0.442, 0.183, 0.39, 0.238, 0.271]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m top1: 0.19449626865671643
[2m[36m(func pid=77753)[0m top5: 0.679570895522388
[2m[36m(func pid=77753)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=77753)[0m f1_macro: 0.1493469896988753
[2m[36m(func pid=77753)[0m f1_weighted: 0.21821117827426187
[2m[36m(func pid=77753)[0m f1_per_class: [0.131, 0.175, 0.14, 0.27, 0.041, 0.346, 0.203, 0.133, 0.02, 0.033]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3451492537313433
[2m[36m(func pid=84655)[0m top5: 0.8819962686567164
[2m[36m(func pid=84655)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=84655)[0m f1_macro: 0.2859571421658948
[2m[36m(func pid=84655)[0m f1_weighted: 0.3518208612023516
[2m[36m(func pid=84655)[0m f1_per_class: [0.323, 0.2, 0.5, 0.545, 0.129, 0.236, 0.356, 0.263, 0.13, 0.176]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.3895 | Steps: 4 | Val loss: 2.0400 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.7780 | Steps: 4 | Val loss: 2.2421 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 4.7163 | Steps: 4 | Val loss: 68.7768 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 12:22:48 (running for 00:24:54.63)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  8.224 |      0.314 |                   64 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.764 |      0.149 |                   33 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.389 |      0.292 |                   14 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  2.01  |      0.286 |                    6 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.30130597014925375
[2m[36m(func pid=82573)[0m top5: 0.7994402985074627
[2m[36m(func pid=82573)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=82573)[0m f1_macro: 0.2918023318024958
[2m[36m(func pid=82573)[0m f1_weighted: 0.3151648873871218
[2m[36m(func pid=82573)[0m f1_per_class: [0.365, 0.323, 0.5, 0.408, 0.046, 0.408, 0.228, 0.245, 0.065, 0.33]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.6712 | Steps: 4 | Val loss: 1.7468 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=70356)[0m top1: 0.3269589552238806
[2m[36m(func pid=70356)[0m top5: 0.7518656716417911
[2m[36m(func pid=70356)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=70356)[0m f1_macro: 0.29407799568059884
[2m[36m(func pid=70356)[0m f1_weighted: 0.31504997097035603
[2m[36m(func pid=70356)[0m f1_per_class: [0.575, 0.184, 0.127, 0.525, 0.084, 0.411, 0.141, 0.394, 0.277, 0.222]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m top1: 0.18796641791044777
[2m[36m(func pid=77753)[0m top5: 0.6660447761194029
[2m[36m(func pid=77753)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=77753)[0m f1_macro: 0.14676261360299883
[2m[36m(func pid=77753)[0m f1_weighted: 0.21485437905021176
[2m[36m(func pid=77753)[0m f1_per_class: [0.147, 0.176, 0.123, 0.258, 0.038, 0.331, 0.207, 0.136, 0.019, 0.032]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3582089552238806
[2m[36m(func pid=84655)[0m top5: 0.8558768656716418
[2m[36m(func pid=84655)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=84655)[0m f1_macro: 0.30058736744441983
[2m[36m(func pid=84655)[0m f1_weighted: 0.3843433038377658
[2m[36m(func pid=84655)[0m f1_per_class: [0.389, 0.399, 0.214, 0.436, 0.11, 0.43, 0.364, 0.324, 0.175, 0.166]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.3670 | Steps: 4 | Val loss: 2.0390 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 3.9439 | Steps: 4 | Val loss: 75.1584 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.7544 | Steps: 4 | Val loss: 2.2499 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 12:22:53 (running for 00:24:59.92)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  4.716 |      0.294 |                   65 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.778 |      0.147 |                   34 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.367 |      0.272 |                   15 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.671 |      0.301 |                    7 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.28544776119402987
[2m[36m(func pid=82573)[0m top5: 0.8036380597014925
[2m[36m(func pid=82573)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=82573)[0m f1_macro: 0.2715987667666414
[2m[36m(func pid=82573)[0m f1_weighted: 0.29860314412040756
[2m[36m(func pid=82573)[0m f1_per_class: [0.311, 0.295, 0.44, 0.368, 0.055, 0.419, 0.23, 0.228, 0.08, 0.289]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.6722 | Steps: 4 | Val loss: 1.9005 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=70356)[0m top1: 0.28638059701492535
[2m[36m(func pid=70356)[0m top5: 0.7168843283582089
[2m[36m(func pid=70356)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=70356)[0m f1_macro: 0.2739031417992782
[2m[36m(func pid=70356)[0m f1_weighted: 0.28716214107392857
[2m[36m(func pid=70356)[0m f1_per_class: [0.581, 0.186, 0.144, 0.459, 0.069, 0.387, 0.125, 0.397, 0.231, 0.161]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m top1: 0.18097014925373134
[2m[36m(func pid=77753)[0m top5: 0.652518656716418
[2m[36m(func pid=77753)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=77753)[0m f1_macro: 0.1406338076785604
[2m[36m(func pid=77753)[0m f1_weighted: 0.20798118925735593
[2m[36m(func pid=77753)[0m f1_per_class: [0.143, 0.183, 0.099, 0.235, 0.041, 0.319, 0.206, 0.145, 0.02, 0.016]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m top1: 0.31156716417910446
[2m[36m(func pid=84655)[0m top5: 0.8027052238805971
[2m[36m(func pid=84655)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=84655)[0m f1_macro: 0.28403954716953994
[2m[36m(func pid=84655)[0m f1_weighted: 0.33315082846072264
[2m[36m(func pid=84655)[0m f1_per_class: [0.425, 0.367, 0.179, 0.373, 0.113, 0.412, 0.266, 0.351, 0.223, 0.132]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.3114 | Steps: 4 | Val loss: 2.0126 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 15.8143 | Steps: 4 | Val loss: 81.4612 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.7721 | Steps: 4 | Val loss: 2.2463 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 12:22:59 (running for 00:25:05.23)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  3.944 |      0.274 |                   66 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.754 |      0.141 |                   35 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.311 |      0.277 |                   16 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.672 |      0.284 |                    8 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.29197761194029853
[2m[36m(func pid=82573)[0m top5: 0.8134328358208955
[2m[36m(func pid=82573)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=82573)[0m f1_macro: 0.27726482200212527
[2m[36m(func pid=82573)[0m f1_weighted: 0.29532333249872894
[2m[36m(func pid=82573)[0m f1_per_class: [0.326, 0.301, 0.489, 0.361, 0.063, 0.422, 0.219, 0.212, 0.119, 0.261]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.4005 | Steps: 4 | Val loss: 1.8757 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=70356)[0m top1: 0.2579291044776119
[2m[36m(func pid=70356)[0m top5: 0.6935634328358209
[2m[36m(func pid=70356)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=70356)[0m f1_macro: 0.26016522377406015
[2m[36m(func pid=70356)[0m f1_weighted: 0.2552562465267207
[2m[36m(func pid=70356)[0m f1_per_class: [0.545, 0.164, 0.215, 0.37, 0.072, 0.386, 0.12, 0.382, 0.217, 0.129]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m top1: 0.17863805970149255
[2m[36m(func pid=77753)[0m top5: 0.6571828358208955
[2m[36m(func pid=77753)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=77753)[0m f1_macro: 0.13845888595276026
[2m[36m(func pid=77753)[0m f1_weighted: 0.20750918971417234
[2m[36m(func pid=77753)[0m f1_per_class: [0.131, 0.182, 0.101, 0.233, 0.036, 0.299, 0.216, 0.14, 0.03, 0.017]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m top1: 0.34095149253731344
[2m[36m(func pid=84655)[0m top5: 0.8255597014925373
[2m[36m(func pid=84655)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=84655)[0m f1_macro: 0.3098362559497631
[2m[36m(func pid=84655)[0m f1_weighted: 0.3526943636646432
[2m[36m(func pid=84655)[0m f1_per_class: [0.556, 0.419, 0.11, 0.484, 0.093, 0.393, 0.19, 0.341, 0.268, 0.246]
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.2681 | Steps: 4 | Val loss: 1.9933 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 9.7540 | Steps: 4 | Val loss: 83.8659 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.7655 | Steps: 4 | Val loss: 2.2341 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:23:04 (running for 00:25:10.66)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 15.814 |      0.26  |                   67 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.772 |      0.138 |                   36 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.268 |      0.28  |                   17 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.401 |      0.31  |                    9 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3031716417910448
[2m[36m(func pid=82573)[0m top5: 0.8152985074626866
[2m[36m(func pid=82573)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=82573)[0m f1_macro: 0.27995042857135305
[2m[36m(func pid=82573)[0m f1_weighted: 0.30071173820243896
[2m[36m(func pid=82573)[0m f1_per_class: [0.361, 0.321, 0.423, 0.382, 0.071, 0.411, 0.203, 0.235, 0.126, 0.265]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.2730 | Steps: 4 | Val loss: 1.7780 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=70356)[0m top1: 0.2392723880597015
[2m[36m(func pid=70356)[0m top5: 0.6800373134328358
[2m[36m(func pid=70356)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=70356)[0m f1_macro: 0.2563094633516437
[2m[36m(func pid=70356)[0m f1_weighted: 0.23868112013417284
[2m[36m(func pid=70356)[0m f1_per_class: [0.539, 0.174, 0.277, 0.304, 0.057, 0.374, 0.128, 0.373, 0.211, 0.127]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m top1: 0.19309701492537312
[2m[36m(func pid=77753)[0m top5: 0.6749067164179104
[2m[36m(func pid=77753)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=77753)[0m f1_macro: 0.1482501953920726
[2m[36m(func pid=77753)[0m f1_weighted: 0.22331656599714467
[2m[36m(func pid=77753)[0m f1_per_class: [0.131, 0.188, 0.107, 0.24, 0.034, 0.339, 0.24, 0.149, 0.037, 0.018]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.2492 | Steps: 4 | Val loss: 1.9675 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=84655)[0m top1: 0.38619402985074625
[2m[36m(func pid=84655)[0m top5: 0.8689365671641791
[2m[36m(func pid=84655)[0m f1_micro: 0.3861940298507463
[2m[36m(func pid=84655)[0m f1_macro: 0.3227752254393099
[2m[36m(func pid=84655)[0m f1_weighted: 0.400008443553282
[2m[36m(func pid=84655)[0m f1_per_class: [0.523, 0.365, 0.097, 0.553, 0.115, 0.413, 0.321, 0.287, 0.23, 0.325]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.7101 | Steps: 4 | Val loss: 2.2376 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 29.6149 | Steps: 4 | Val loss: 83.9681 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 12:23:10 (running for 00:25:16.06)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  9.754 |      0.256 |                   68 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.765 |      0.148 |                   37 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.249 |      0.287 |                   18 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.273 |      0.323 |                   10 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.333955223880597
[2m[36m(func pid=82573)[0m top5: 0.8227611940298507
[2m[36m(func pid=82573)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=82573)[0m f1_macro: 0.28716909400613133
[2m[36m(func pid=82573)[0m f1_weighted: 0.3256520585387342
[2m[36m(func pid=82573)[0m f1_per_class: [0.397, 0.331, 0.308, 0.472, 0.083, 0.423, 0.184, 0.276, 0.118, 0.28]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=77753)[0m top1: 0.18516791044776118
[2m[36m(func pid=77753)[0m top5: 0.6646455223880597
[2m[36m(func pid=77753)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=77753)[0m f1_macro: 0.1446570689478827
[2m[36m(func pid=77753)[0m f1_weighted: 0.2106335642968629
[2m[36m(func pid=77753)[0m f1_per_class: [0.123, 0.173, 0.105, 0.23, 0.038, 0.351, 0.21, 0.162, 0.036, 0.019]
[2m[36m(func pid=70356)[0m top1: 0.23460820895522388
[2m[36m(func pid=70356)[0m top5: 0.6739738805970149
[2m[36m(func pid=70356)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=70356)[0m f1_macro: 0.2494951891671373
[2m[36m(func pid=70356)[0m f1_weighted: 0.23656693632856293
[2m[36m(func pid=70356)[0m f1_per_class: [0.487, 0.209, 0.241, 0.222, 0.049, 0.388, 0.18, 0.347, 0.209, 0.163]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.1137 | Steps: 4 | Val loss: 1.7128 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.1193 | Steps: 4 | Val loss: 1.9587 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=84655)[0m top1: 0.41744402985074625
[2m[36m(func pid=84655)[0m top5: 0.8731343283582089
[2m[36m(func pid=84655)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=84655)[0m f1_macro: 0.32843383380951546
[2m[36m(func pid=84655)[0m f1_weighted: 0.44032434408440835
[2m[36m(func pid=84655)[0m f1_per_class: [0.405, 0.425, 0.115, 0.55, 0.134, 0.417, 0.439, 0.247, 0.211, 0.341]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 8.3630 | Steps: 4 | Val loss: 79.4444 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.7177 | Steps: 4 | Val loss: 2.2367 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 12:23:15 (running for 00:25:21.27)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 29.615 |      0.249 |                   69 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.71  |      0.145 |                   38 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.119 |      0.288 |                   19 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.114 |      0.328 |                   11 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.34888059701492535
[2m[36m(func pid=82573)[0m top5: 0.8232276119402985
[2m[36m(func pid=82573)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=82573)[0m f1_macro: 0.28765753071387595
[2m[36m(func pid=82573)[0m f1_weighted: 0.3287835448572732
[2m[36m(func pid=82573)[0m f1_per_class: [0.44, 0.295, 0.261, 0.528, 0.081, 0.427, 0.155, 0.306, 0.123, 0.263]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=70356)[0m top1: 0.24860074626865672
[2m[36m(func pid=70356)[0m top5: 0.6851679104477612
[2m[36m(func pid=70356)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=70356)[0m f1_macro: 0.25051296974528137
[2m[36m(func pid=70356)[0m f1_weighted: 0.2552691288136766
[2m[36m(func pid=70356)[0m f1_per_class: [0.427, 0.267, 0.202, 0.171, 0.066, 0.392, 0.254, 0.373, 0.229, 0.125]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.1789 | Steps: 4 | Val loss: 1.6781 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=77753)[0m top1: 0.18516791044776118
[2m[36m(func pid=77753)[0m top5: 0.6651119402985075
[2m[36m(func pid=77753)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=77753)[0m f1_macro: 0.14191210778938962
[2m[36m(func pid=77753)[0m f1_weighted: 0.2130512027423914
[2m[36m(func pid=77753)[0m f1_per_class: [0.125, 0.183, 0.088, 0.228, 0.03, 0.346, 0.219, 0.146, 0.034, 0.019]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.3319 | Steps: 4 | Val loss: 1.9570 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=84655)[0m top1: 0.42630597014925375
[2m[36m(func pid=84655)[0m top5: 0.8815298507462687
[2m[36m(func pid=84655)[0m f1_micro: 0.4263059701492538
[2m[36m(func pid=84655)[0m f1_macro: 0.33452278453642653
[2m[36m(func pid=84655)[0m f1_weighted: 0.4534724961596547
[2m[36m(func pid=84655)[0m f1_per_class: [0.436, 0.437, 0.179, 0.537, 0.164, 0.409, 0.497, 0.222, 0.187, 0.275]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 26.9746 | Steps: 4 | Val loss: 78.9164 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.7656 | Steps: 4 | Val loss: 2.2321 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 12:23:20 (running for 00:25:26.54)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  8.363 |      0.251 |                   70 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.718 |      0.142 |                   39 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.332 |      0.276 |                   20 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.179 |      0.335 |                   12 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3493470149253731
[2m[36m(func pid=82573)[0m top5: 0.8111007462686567
[2m[36m(func pid=82573)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=82573)[0m f1_macro: 0.27551068628627523
[2m[36m(func pid=82573)[0m f1_weighted: 0.3149119141297328
[2m[36m(func pid=82573)[0m f1_per_class: [0.444, 0.231, 0.242, 0.555, 0.087, 0.43, 0.12, 0.314, 0.092, 0.238]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=70356)[0m top1: 0.271455223880597
[2m[36m(func pid=70356)[0m top5: 0.6870335820895522
[2m[36m(func pid=70356)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=70356)[0m f1_macro: 0.2526254899426865
[2m[36m(func pid=70356)[0m f1_weighted: 0.2764322665362472
[2m[36m(func pid=70356)[0m f1_per_class: [0.36, 0.347, 0.184, 0.12, 0.07, 0.404, 0.327, 0.371, 0.232, 0.11]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m top1: 0.18889925373134328
[2m[36m(func pid=77753)[0m top5: 0.6683768656716418
[2m[36m(func pid=77753)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=77753)[0m f1_macro: 0.14366841183596427
[2m[36m(func pid=77753)[0m f1_weighted: 0.21867598122559678
[2m[36m(func pid=77753)[0m f1_per_class: [0.137, 0.167, 0.091, 0.235, 0.034, 0.337, 0.245, 0.137, 0.036, 0.017]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.4569 | Steps: 4 | Val loss: 1.6995 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.1697 | Steps: 4 | Val loss: 1.9287 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 3.4538 | Steps: 4 | Val loss: 71.8962 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=84655)[0m top1: 0.40904850746268656
[2m[36m(func pid=84655)[0m top5: 0.8936567164179104
[2m[36m(func pid=84655)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=84655)[0m f1_macro: 0.34031812387529636
[2m[36m(func pid=84655)[0m f1_weighted: 0.4366630648188511
[2m[36m(func pid=84655)[0m f1_per_class: [0.492, 0.409, 0.264, 0.547, 0.145, 0.356, 0.452, 0.287, 0.161, 0.29]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.7771 | Steps: 4 | Val loss: 2.2368 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 12:23:25 (running for 00:25:31.78)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 | 26.975 |      0.253 |                   71 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.766 |      0.144 |                   40 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.17  |      0.279 |                   21 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.457 |      0.34  |                   13 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.35867537313432835
[2m[36m(func pid=82573)[0m top5: 0.8208955223880597
[2m[36m(func pid=82573)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=82573)[0m f1_macro: 0.2790758206780321
[2m[36m(func pid=82573)[0m f1_weighted: 0.3152339126449468
[2m[36m(func pid=82573)[0m f1_per_class: [0.483, 0.221, 0.231, 0.568, 0.12, 0.427, 0.113, 0.318, 0.092, 0.217]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=70356)[0m top1: 0.33302238805970147
[2m[36m(func pid=70356)[0m top5: 0.71875
[2m[36m(func pid=70356)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=70356)[0m f1_macro: 0.2854335372277595
[2m[36m(func pid=70356)[0m f1_weighted: 0.33816305311445544
[2m[36m(func pid=70356)[0m f1_per_class: [0.326, 0.382, 0.268, 0.146, 0.088, 0.419, 0.481, 0.386, 0.225, 0.133]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m top1: 0.18516791044776118
[2m[36m(func pid=77753)[0m top5: 0.6739738805970149
[2m[36m(func pid=77753)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=77753)[0m f1_macro: 0.14499927130817516
[2m[36m(func pid=77753)[0m f1_weighted: 0.21524420160404936
[2m[36m(func pid=77753)[0m f1_per_class: [0.144, 0.191, 0.083, 0.232, 0.045, 0.33, 0.222, 0.148, 0.037, 0.018]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.2239 | Steps: 4 | Val loss: 1.7638 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.0575 | Steps: 4 | Val loss: 1.9276 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 5.5120 | Steps: 4 | Val loss: 69.8250 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=84655)[0m top1: 0.3689365671641791
[2m[36m(func pid=84655)[0m top5: 0.8782649253731343
[2m[36m(func pid=84655)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=84655)[0m f1_macro: 0.328292757221973
[2m[36m(func pid=84655)[0m f1_weighted: 0.39024677241427036
[2m[36m(func pid=84655)[0m f1_per_class: [0.508, 0.424, 0.245, 0.501, 0.13, 0.416, 0.302, 0.31, 0.181, 0.264]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.7612 | Steps: 4 | Val loss: 2.2143 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 12:23:31 (running for 00:25:37.11)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  3.454 |      0.285 |                   72 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.777 |      0.145 |                   41 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.058 |      0.273 |                   22 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.224 |      0.328 |                   14 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3628731343283582
[2m[36m(func pid=82573)[0m top5: 0.8236940298507462
[2m[36m(func pid=82573)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=82573)[0m f1_macro: 0.2728739791830499
[2m[36m(func pid=82573)[0m f1_weighted: 0.3128500562144322
[2m[36m(func pid=82573)[0m f1_per_class: [0.483, 0.176, 0.178, 0.586, 0.129, 0.435, 0.111, 0.332, 0.077, 0.221]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=70356)[0m top1: 0.3628731343283582
[2m[36m(func pid=70356)[0m top5: 0.7411380597014925
[2m[36m(func pid=70356)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=70356)[0m f1_macro: 0.294174072193344
[2m[36m(func pid=70356)[0m f1_weighted: 0.3581546157767947
[2m[36m(func pid=70356)[0m f1_per_class: [0.27, 0.41, 0.325, 0.158, 0.132, 0.356, 0.547, 0.402, 0.195, 0.148]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m top1: 0.19682835820895522
[2m[36m(func pid=77753)[0m top5: 0.7028917910447762
[2m[36m(func pid=77753)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=77753)[0m f1_macro: 0.1488436148178513
[2m[36m(func pid=77753)[0m f1_weighted: 0.22884256659597813
[2m[36m(func pid=77753)[0m f1_per_class: [0.151, 0.189, 0.083, 0.268, 0.04, 0.321, 0.241, 0.135, 0.036, 0.025]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.3756 | Steps: 4 | Val loss: 1.8920 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.0881 | Steps: 4 | Val loss: 1.9151 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=84655)[0m top1: 0.32742537313432835
[2m[36m(func pid=84655)[0m top5: 0.8404850746268657
[2m[36m(func pid=84655)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=84655)[0m f1_macro: 0.31958381306979505
[2m[36m(func pid=84655)[0m f1_weighted: 0.338303603593667
[2m[36m(func pid=84655)[0m f1_per_class: [0.547, 0.42, 0.286, 0.32, 0.094, 0.405, 0.295, 0.319, 0.197, 0.311]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.9752 | Steps: 4 | Val loss: 66.4621 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.7272 | Steps: 4 | Val loss: 2.2184 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=82573)[0m top1: 0.3512126865671642
[2m[36m(func pid=82573)[0m top5: 0.8283582089552238
[2m[36m(func pid=82573)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=82573)[0m f1_macro: 0.2658819809021008
[2m[36m(func pid=82573)[0m f1_weighted: 0.3096141221649089
[2m[36m(func pid=82573)[0m f1_per_class: [0.505, 0.138, 0.138, 0.574, 0.11, 0.424, 0.137, 0.332, 0.078, 0.222]
[2m[36m(func pid=82573)[0m 
== Status ==
Current time: 2024-01-07 12:23:36 (running for 00:25:42.58)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  5.512 |      0.294 |                   73 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.761 |      0.149 |                   42 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.088 |      0.266 |                   23 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.376 |      0.32  |                   15 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=70356)[0m top1: 0.39365671641791045
[2m[36m(func pid=70356)[0m top5: 0.7686567164179104
[2m[36m(func pid=70356)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=70356)[0m f1_macro: 0.31376197754492646
[2m[36m(func pid=70356)[0m f1_weighted: 0.38358446190677353
[2m[36m(func pid=70356)[0m f1_per_class: [0.253, 0.429, 0.433, 0.235, 0.147, 0.332, 0.569, 0.338, 0.162, 0.239]
[2m[36m(func pid=70356)[0m 
[2m[36m(func pid=77753)[0m top1: 0.19029850746268656
[2m[36m(func pid=77753)[0m top5: 0.6940298507462687
[2m[36m(func pid=77753)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=77753)[0m f1_macro: 0.14715864898718062
[2m[36m(func pid=77753)[0m f1_weighted: 0.2189430401487522
[2m[36m(func pid=77753)[0m f1_per_class: [0.158, 0.143, 0.1, 0.292, 0.04, 0.302, 0.213, 0.155, 0.045, 0.024]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.1238 | Steps: 4 | Val loss: 2.0025 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.0586 | Steps: 4 | Val loss: 1.8778 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=70356)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 6.5927 | Steps: 4 | Val loss: 64.9028 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=84655)[0m top1: 0.30736940298507465
[2m[36m(func pid=84655)[0m top5: 0.8330223880597015
[2m[36m(func pid=84655)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=84655)[0m f1_macro: 0.31345509498802615
[2m[36m(func pid=84655)[0m f1_weighted: 0.30180554116506303
[2m[36m(func pid=84655)[0m f1_per_class: [0.58, 0.425, 0.312, 0.122, 0.066, 0.39, 0.357, 0.312, 0.216, 0.354]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.7440 | Steps: 4 | Val loss: 2.2240 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 12:23:42 (running for 00:25:47.98)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.3245
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00011 | RUNNING    | 192.168.7.53:70356 | 0.1    |       0.99 |         0.0001 |  0.975 |      0.314 |                   74 |
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.727 |      0.147 |                   43 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.059 |      0.285 |                   24 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.124 |      0.313 |                   16 |
| train_9b9e8_00015 | PENDING    |                    | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3763992537313433
[2m[36m(func pid=82573)[0m top5: 0.8493470149253731
[2m[36m(func pid=82573)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=82573)[0m f1_macro: 0.2848921920211714
[2m[36m(func pid=82573)[0m f1_weighted: 0.33282808921763396
[2m[36m(func pid=82573)[0m f1_per_class: [0.568, 0.152, 0.166, 0.592, 0.12, 0.451, 0.177, 0.322, 0.064, 0.236]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=70356)[0m top1: 0.41277985074626866
[2m[36m(func pid=70356)[0m top5: 0.7919776119402985
[2m[36m(func pid=70356)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=70356)[0m f1_macro: 0.32120613428981587
[2m[36m(func pid=70356)[0m f1_weighted: 0.3896141161839397
[2m[36m(func pid=70356)[0m f1_per_class: [0.285, 0.432, 0.436, 0.28, 0.154, 0.245, 0.579, 0.296, 0.174, 0.33]
[2m[36m(func pid=77753)[0m top1: 0.1828358208955224
[2m[36m(func pid=77753)[0m top5: 0.6884328358208955
[2m[36m(func pid=77753)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=77753)[0m f1_macro: 0.14533381144198593
[2m[36m(func pid=77753)[0m f1_weighted: 0.21079087199431287
[2m[36m(func pid=77753)[0m f1_per_class: [0.171, 0.162, 0.104, 0.27, 0.042, 0.296, 0.198, 0.154, 0.036, 0.021]
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.7976 | Steps: 4 | Val loss: 1.8118 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.9906 | Steps: 4 | Val loss: 1.8830 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=84655)[0m top1: 0.36427238805970147
[2m[36m(func pid=84655)[0m top5: 0.8763992537313433
[2m[36m(func pid=84655)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=84655)[0m f1_macro: 0.3223684308358322
[2m[36m(func pid=84655)[0m f1_weighted: 0.38629617984512343
[2m[36m(func pid=84655)[0m f1_per_class: [0.592, 0.428, 0.169, 0.282, 0.075, 0.385, 0.498, 0.308, 0.196, 0.29]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.7276 | Steps: 4 | Val loss: 2.2135 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=82573)[0m top1: 0.363339552238806
[2m[36m(func pid=82573)[0m top5: 0.84375
[2m[36m(func pid=82573)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=82573)[0m f1_macro: 0.27709892150416177
[2m[36m(func pid=82573)[0m f1_weighted: 0.32302077029954274
[2m[36m(func pid=82573)[0m f1_per_class: [0.568, 0.148, 0.131, 0.59, 0.103, 0.449, 0.149, 0.332, 0.065, 0.235]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=77753)[0m top1: 0.2042910447761194
[2m[36m(func pid=77753)[0m top5: 0.6902985074626866
[2m[36m(func pid=77753)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=77753)[0m f1_macro: 0.1564740918721928
[2m[36m(func pid=77753)[0m f1_weighted: 0.22810850589648834
[2m[36m(func pid=77753)[0m f1_per_class: [0.161, 0.143, 0.154, 0.327, 0.032, 0.329, 0.198, 0.176, 0.022, 0.022]
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.2749 | Steps: 4 | Val loss: 1.6944 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.1143 | Steps: 4 | Val loss: 1.8680 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 12:23:47 (running for 00:25:53.33)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.744 |      0.145 |                   44 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.991 |      0.277 |                   25 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.798 |      0.322 |                   17 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=84655)[0m top1: 0.40625
[2m[36m(func pid=84655)[0m top5: 0.8899253731343284
[2m[36m(func pid=84655)[0m f1_micro: 0.40625
[2m[36m(func pid=84655)[0m f1_macro: 0.3288383899034586
[2m[36m(func pid=84655)[0m f1_weighted: 0.4374102992636163
[2m[36m(func pid=84655)[0m f1_per_class: [0.561, 0.349, 0.211, 0.477, 0.109, 0.397, 0.542, 0.287, 0.148, 0.208]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=89025)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=89025)[0m Configuration completed!
[2m[36m(func pid=89025)[0m New optimizer parameters:
[2m[36m(func pid=89025)[0m SGD (
[2m[36m(func pid=89025)[0m Parameter Group 0
[2m[36m(func pid=89025)[0m     dampening: 0
[2m[36m(func pid=89025)[0m     differentiable: False
[2m[36m(func pid=89025)[0m     foreach: None
[2m[36m(func pid=89025)[0m     lr: 0.1
[2m[36m(func pid=89025)[0m     maximize: False
[2m[36m(func pid=89025)[0m     momentum: 0.9
[2m[36m(func pid=89025)[0m     nesterov: False
[2m[36m(func pid=89025)[0m     weight_decay: 0.0001
[2m[36m(func pid=89025)[0m )
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=77753)[0m 
== Status ==
Current time: 2024-01-07 12:23:52 (running for 00:25:58.61)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.728 |      0.156 |                   45 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  2.114 |      0.286 |                   26 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.275 |      0.329 |                   18 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3694029850746269
[2m[36m(func pid=82573)[0m top5: 0.8498134328358209
[2m[36m(func pid=82573)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=82573)[0m f1_macro: 0.28603167980800015
[2m[36m(func pid=82573)[0m f1_weighted: 0.3416748456952694
[2m[36m(func pid=82573)[0m f1_per_class: [0.547, 0.142, 0.115, 0.587, 0.115, 0.462, 0.21, 0.325, 0.114, 0.243]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.8903 | Steps: 4 | Val loss: 1.6714 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.7158 | Steps: 4 | Val loss: 2.2184 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.8918 | Steps: 4 | Val loss: 1.8598 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9266 | Steps: 4 | Val loss: 2.2568 | Batch size: 32 | lr: 0.1 | Duration: 4.53s
[2m[36m(func pid=84655)[0m top1: 0.4230410447761194
[2m[36m(func pid=84655)[0m top5: 0.8955223880597015
[2m[36m(func pid=84655)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=84655)[0m f1_macro: 0.35503495366978666
[2m[36m(func pid=84655)[0m f1_weighted: 0.45226548381715603
[2m[36m(func pid=84655)[0m f1_per_class: [0.52, 0.47, 0.31, 0.491, 0.136, 0.368, 0.51, 0.314, 0.171, 0.26]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m top1: 0.208955223880597
[2m[36m(func pid=77753)[0m top5: 0.6819029850746269
[2m[36m(func pid=77753)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=77753)[0m f1_macro: 0.15782334570847734
[2m[36m(func pid=77753)[0m f1_weighted: 0.23607036049500046
[2m[36m(func pid=77753)[0m f1_per_class: [0.149, 0.139, 0.125, 0.346, 0.032, 0.341, 0.203, 0.182, 0.042, 0.02]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.37173507462686567
[2m[36m(func pid=82573)[0m top5: 0.8586753731343284
[2m[36m(func pid=82573)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=82573)[0m f1_macro: 0.29100109412617103
[2m[36m(func pid=82573)[0m f1_weighted: 0.35787836228098546
[2m[36m(func pid=82573)[0m f1_per_class: [0.533, 0.186, 0.126, 0.57, 0.12, 0.458, 0.258, 0.327, 0.094, 0.236]
[2m[36m(func pid=82573)[0m 
== Status ==
Current time: 2024-01-07 12:23:57 (running for 00:26:03.85)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.716 |      0.158 |                   46 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.892 |      0.291 |                   27 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.89  |      0.355 |                   19 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.06669776119402986
[2m[36m(func pid=89025)[0m top5: 0.7229477611940298
[2m[36m(func pid=89025)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=89025)[0m f1_macro: 0.07312831767855724
[2m[36m(func pid=89025)[0m f1_weighted: 0.0688704245718191
[2m[36m(func pid=89025)[0m f1_per_class: [0.295, 0.063, 0.019, 0.146, 0.0, 0.0, 0.015, 0.062, 0.066, 0.065]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.8442 | Steps: 4 | Val loss: 1.8436 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7677 | Steps: 4 | Val loss: 2.2163 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.8964 | Steps: 4 | Val loss: 1.8480 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.3928 | Steps: 4 | Val loss: 2.0850 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=84655)[0m top1: 0.39132462686567165
[2m[36m(func pid=84655)[0m top5: 0.867070895522388
[2m[36m(func pid=84655)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=84655)[0m f1_macro: 0.3460557891192104
[2m[36m(func pid=84655)[0m f1_weighted: 0.42056936318966
[2m[36m(func pid=84655)[0m f1_per_class: [0.515, 0.498, 0.342, 0.495, 0.167, 0.325, 0.402, 0.304, 0.167, 0.246]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m top1: 0.20708955223880596
[2m[36m(func pid=77753)[0m top5: 0.6814365671641791
[2m[36m(func pid=77753)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=77753)[0m f1_macro: 0.15682542336458077
[2m[36m(func pid=77753)[0m f1_weighted: 0.2358099703466824
[2m[36m(func pid=77753)[0m f1_per_class: [0.157, 0.142, 0.11, 0.331, 0.035, 0.341, 0.214, 0.189, 0.03, 0.019]
[2m[36m(func pid=77753)[0m 
== Status ==
Current time: 2024-01-07 12:24:03 (running for 00:26:08.99)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.768 |      0.157 |                   47 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.896 |      0.302 |                   28 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.844 |      0.346 |                   20 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  2.927 |      0.073 |                    1 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3787313432835821
[2m[36m(func pid=82573)[0m top5: 0.8628731343283582
[2m[36m(func pid=82573)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=82573)[0m f1_macro: 0.3019719568926166
[2m[36m(func pid=82573)[0m f1_weighted: 0.3760962307129097
[2m[36m(func pid=82573)[0m f1_per_class: [0.534, 0.261, 0.132, 0.568, 0.125, 0.461, 0.277, 0.321, 0.11, 0.231]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.332089552238806
[2m[36m(func pid=89025)[0m top5: 0.7924440298507462
[2m[36m(func pid=89025)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=89025)[0m f1_macro: 0.27158344224864256
[2m[36m(func pid=89025)[0m f1_weighted: 0.28539167160100665
[2m[36m(func pid=89025)[0m f1_per_class: [0.171, 0.106, 0.733, 0.192, 0.106, 0.18, 0.535, 0.348, 0.028, 0.316]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.7370 | Steps: 4 | Val loss: 2.2074 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.4920 | Steps: 4 | Val loss: 1.9633 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.9086 | Steps: 4 | Val loss: 1.8499 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9313 | Steps: 4 | Val loss: 2.8760 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=77753)[0m top1: 0.20708955223880596
[2m[36m(func pid=77753)[0m top5: 0.7019589552238806
[2m[36m(func pid=77753)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=77753)[0m f1_macro: 0.1566783746555475
[2m[36m(func pid=77753)[0m f1_weighted: 0.2385971644107751
[2m[36m(func pid=77753)[0m f1_per_class: [0.168, 0.144, 0.1, 0.305, 0.036, 0.351, 0.247, 0.163, 0.034, 0.019]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m top1: 0.365205223880597
[2m[36m(func pid=84655)[0m top5: 0.8470149253731343
[2m[36m(func pid=84655)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=84655)[0m f1_macro: 0.33775036395576635
[2m[36m(func pid=84655)[0m f1_weighted: 0.39075096211912824
[2m[36m(func pid=84655)[0m f1_per_class: [0.487, 0.458, 0.394, 0.457, 0.162, 0.34, 0.342, 0.381, 0.17, 0.185]
[2m[36m(func pid=84655)[0m 
== Status ==
Current time: 2024-01-07 12:24:08 (running for 00:26:14.10)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.737 |      0.157 |                   48 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.909 |      0.297 |                   29 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.492 |      0.338 |                   21 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  2.393 |      0.272 |                    2 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3656716417910448
[2m[36m(func pid=82573)[0m top5: 0.8572761194029851
[2m[36m(func pid=82573)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=82573)[0m f1_macro: 0.29672739821101474
[2m[36m(func pid=82573)[0m f1_weighted: 0.36772467009408105
[2m[36m(func pid=82573)[0m f1_per_class: [0.521, 0.279, 0.151, 0.543, 0.122, 0.457, 0.267, 0.311, 0.102, 0.214]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.21455223880597016
[2m[36m(func pid=89025)[0m top5: 0.738339552238806
[2m[36m(func pid=89025)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=89025)[0m f1_macro: 0.23628412841222496
[2m[36m(func pid=89025)[0m f1_weighted: 0.21889265489647483
[2m[36m(func pid=89025)[0m f1_per_class: [0.288, 0.153, 0.522, 0.295, 0.094, 0.381, 0.113, 0.318, 0.126, 0.074]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.7209 | Steps: 4 | Val loss: 2.2030 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.9032 | Steps: 4 | Val loss: 2.1033 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.8557 | Steps: 4 | Val loss: 1.8540 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9058 | Steps: 4 | Val loss: 3.5469 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=77753)[0m top1: 0.20522388059701493
[2m[36m(func pid=77753)[0m top5: 0.710820895522388
[2m[36m(func pid=77753)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=77753)[0m f1_macro: 0.15420431502884852
[2m[36m(func pid=77753)[0m f1_weighted: 0.23667727149636006
[2m[36m(func pid=77753)[0m f1_per_class: [0.183, 0.151, 0.095, 0.32, 0.03, 0.32, 0.234, 0.16, 0.03, 0.019]
[2m[36m(func pid=77753)[0m 
== Status ==
Current time: 2024-01-07 12:24:13 (running for 00:26:19.23)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.721 |      0.154 |                   49 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.909 |      0.297 |                   29 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.903 |      0.288 |                   22 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  2.931 |      0.236 |                    3 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=84655)[0m top1: 0.31156716417910446
[2m[36m(func pid=84655)[0m top5: 0.8157649253731343
[2m[36m(func pid=84655)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=84655)[0m f1_macro: 0.28847614837584
[2m[36m(func pid=84655)[0m f1_weighted: 0.34412667563688243
[2m[36m(func pid=84655)[0m f1_per_class: [0.193, 0.368, 0.342, 0.347, 0.197, 0.369, 0.351, 0.386, 0.157, 0.173]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m top1: 0.36473880597014924
[2m[36m(func pid=82573)[0m top5: 0.8502798507462687
[2m[36m(func pid=82573)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=82573)[0m f1_macro: 0.2875713524212316
[2m[36m(func pid=82573)[0m f1_weighted: 0.36569697804895396
[2m[36m(func pid=82573)[0m f1_per_class: [0.437, 0.271, 0.169, 0.537, 0.103, 0.456, 0.274, 0.327, 0.1, 0.2]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.37546641791044777
[2m[36m(func pid=89025)[0m top5: 0.7747201492537313
[2m[36m(func pid=89025)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=89025)[0m f1_macro: 0.28830696747428236
[2m[36m(func pid=89025)[0m f1_weighted: 0.33113891533667594
[2m[36m(func pid=89025)[0m f1_per_class: [0.393, 0.307, 0.276, 0.53, 0.126, 0.317, 0.203, 0.218, 0.199, 0.313]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.7087 | Steps: 4 | Val loss: 2.2018 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.0397 | Steps: 4 | Val loss: 2.1268 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.9759 | Steps: 4 | Val loss: 1.8387 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.2199 | Steps: 4 | Val loss: 4.3786 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 12:24:18 (running for 00:26:24.33)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.709 |      0.156 |                   50 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.856 |      0.288 |                   30 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.903 |      0.288 |                   22 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  2.906 |      0.288 |                    4 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.2042910447761194
[2m[36m(func pid=77753)[0m top5: 0.7112873134328358
[2m[36m(func pid=77753)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=77753)[0m f1_macro: 0.15584323022611607
[2m[36m(func pid=77753)[0m f1_weighted: 0.23617373664544955
[2m[36m(func pid=77753)[0m f1_per_class: [0.169, 0.17, 0.103, 0.307, 0.031, 0.319, 0.234, 0.155, 0.032, 0.038]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.3614738805970149
[2m[36m(func pid=82573)[0m top5: 0.8568097014925373
[2m[36m(func pid=82573)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=82573)[0m f1_macro: 0.2960375907927012
[2m[36m(func pid=82573)[0m f1_weighted: 0.37374168920971906
[2m[36m(func pid=82573)[0m f1_per_class: [0.431, 0.295, 0.184, 0.509, 0.112, 0.453, 0.311, 0.318, 0.155, 0.192]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m top1: 0.2971082089552239
[2m[36m(func pid=84655)[0m top5: 0.8222947761194029
[2m[36m(func pid=84655)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=84655)[0m f1_macro: 0.27345552888343133
[2m[36m(func pid=84655)[0m f1_weighted: 0.32375971495153255
[2m[36m(func pid=84655)[0m f1_per_class: [0.164, 0.34, 0.299, 0.282, 0.183, 0.41, 0.35, 0.359, 0.18, 0.167]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m top1: 0.21735074626865672
[2m[36m(func pid=89025)[0m top5: 0.7378731343283582
[2m[36m(func pid=89025)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=89025)[0m f1_macro: 0.22061162786020136
[2m[36m(func pid=89025)[0m f1_weighted: 0.23051665370085495
[2m[36m(func pid=89025)[0m f1_per_class: [0.349, 0.402, 0.049, 0.258, 0.06, 0.201, 0.121, 0.255, 0.09, 0.423]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.6739 | Steps: 4 | Val loss: 2.2084 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.0578 | Steps: 4 | Val loss: 1.9971 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.8672 | Steps: 4 | Val loss: 1.8190 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7778 | Steps: 4 | Val loss: 3.6693 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 12:24:23 (running for 00:26:29.55)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.674 |      0.162 |                   51 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.976 |      0.296 |                   31 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.04  |      0.273 |                   23 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  2.22  |      0.221 |                    5 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.20055970149253732
[2m[36m(func pid=77753)[0m top5: 0.6977611940298507
[2m[36m(func pid=77753)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=77753)[0m f1_macro: 0.16224438193011842
[2m[36m(func pid=77753)[0m f1_weighted: 0.23000226500514478
[2m[36m(func pid=77753)[0m f1_per_class: [0.151, 0.152, 0.162, 0.315, 0.038, 0.313, 0.211, 0.194, 0.033, 0.053]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3358208955223881
[2m[36m(func pid=84655)[0m top5: 0.8395522388059702
[2m[36m(func pid=84655)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=84655)[0m f1_macro: 0.2822335087963276
[2m[36m(func pid=84655)[0m f1_weighted: 0.36500729589301645
[2m[36m(func pid=84655)[0m f1_per_class: [0.274, 0.296, 0.17, 0.434, 0.119, 0.404, 0.364, 0.383, 0.187, 0.191]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m top1: 0.36613805970149255
[2m[36m(func pid=82573)[0m top5: 0.8652052238805971
[2m[36m(func pid=82573)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=82573)[0m f1_macro: 0.2968513050379368
[2m[36m(func pid=82573)[0m f1_weighted: 0.38160787219647174
[2m[36m(func pid=82573)[0m f1_per_class: [0.441, 0.333, 0.169, 0.499, 0.092, 0.455, 0.326, 0.319, 0.125, 0.209]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.2994402985074627
[2m[36m(func pid=89025)[0m top5: 0.7868470149253731
[2m[36m(func pid=89025)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=89025)[0m f1_macro: 0.2967563477446458
[2m[36m(func pid=89025)[0m f1_weighted: 0.28620007865653996
[2m[36m(func pid=89025)[0m f1_per_class: [0.549, 0.412, 0.202, 0.13, 0.159, 0.311, 0.341, 0.303, 0.188, 0.371]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.7010 | Steps: 4 | Val loss: 2.2100 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.8410 | Steps: 4 | Val loss: 1.8228 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.7708 | Steps: 4 | Val loss: 1.9814 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.5214 | Steps: 4 | Val loss: 4.8900 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=77753)[0m top1: 0.19542910447761194
[2m[36m(func pid=77753)[0m top5: 0.7033582089552238
[2m[36m(func pid=77753)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=77753)[0m f1_macro: 0.1606575935127918
[2m[36m(func pid=77753)[0m f1_weighted: 0.22588192600383708
[2m[36m(func pid=77753)[0m f1_per_class: [0.169, 0.18, 0.176, 0.298, 0.034, 0.272, 0.216, 0.168, 0.041, 0.053]
[2m[36m(func pid=77753)[0m 
== Status ==
Current time: 2024-01-07 12:24:29 (running for 00:26:35.04)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.701 |      0.161 |                   52 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.867 |      0.297 |                   32 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.058 |      0.282 |                   24 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  2.778 |      0.297 |                    6 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3521455223880597
[2m[36m(func pid=82573)[0m top5: 0.8619402985074627
[2m[36m(func pid=82573)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=82573)[0m f1_macro: 0.3009038431941069
[2m[36m(func pid=82573)[0m f1_weighted: 0.35932953520178595
[2m[36m(func pid=82573)[0m f1_per_class: [0.481, 0.359, 0.194, 0.466, 0.105, 0.446, 0.263, 0.33, 0.134, 0.231]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m top1: 0.34468283582089554
[2m[36m(func pid=84655)[0m top5: 0.8493470149253731
[2m[36m(func pid=84655)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=84655)[0m f1_macro: 0.27311075331955376
[2m[36m(func pid=84655)[0m f1_weighted: 0.37802590072425585
[2m[36m(func pid=84655)[0m f1_per_class: [0.24, 0.211, 0.149, 0.43, 0.111, 0.397, 0.477, 0.333, 0.178, 0.205]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m top1: 0.2733208955223881
[2m[36m(func pid=89025)[0m top5: 0.7560634328358209
[2m[36m(func pid=89025)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=89025)[0m f1_macro: 0.2109682894516053
[2m[36m(func pid=89025)[0m f1_weighted: 0.319217818008497
[2m[36m(func pid=89025)[0m f1_per_class: [0.172, 0.261, 0.059, 0.294, 0.101, 0.354, 0.458, 0.046, 0.168, 0.196]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.6780 | Steps: 4 | Val loss: 2.2157 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.8096 | Steps: 4 | Val loss: 1.8263 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 0.5917 | Steps: 4 | Val loss: 1.8912 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.3609 | Steps: 4 | Val loss: 6.6854 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 12:24:34 (running for 00:26:40.11)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.678 |      0.165 |                   53 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.841 |      0.301 |                   33 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.771 |      0.273 |                   25 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.521 |      0.211 |                    7 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.1982276119402985
[2m[36m(func pid=77753)[0m top5: 0.6805037313432836
[2m[36m(func pid=77753)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=77753)[0m f1_macro: 0.16478090860266717
[2m[36m(func pid=77753)[0m f1_weighted: 0.2239292813097184
[2m[36m(func pid=77753)[0m f1_per_class: [0.168, 0.175, 0.195, 0.331, 0.031, 0.258, 0.182, 0.181, 0.041, 0.085]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.35401119402985076
[2m[36m(func pid=82573)[0m top5: 0.8605410447761194
[2m[36m(func pid=82573)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=82573)[0m f1_macro: 0.30015891531597216
[2m[36m(func pid=82573)[0m f1_weighted: 0.35831546892248684
[2m[36m(func pid=82573)[0m f1_per_class: [0.481, 0.355, 0.176, 0.496, 0.091, 0.432, 0.238, 0.344, 0.129, 0.261]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3666044776119403
[2m[36m(func pid=84655)[0m top5: 0.8694029850746269
[2m[36m(func pid=84655)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=84655)[0m f1_macro: 0.2974114282098671
[2m[36m(func pid=84655)[0m f1_weighted: 0.4017629193037853
[2m[36m(func pid=84655)[0m f1_per_class: [0.363, 0.317, 0.124, 0.419, 0.1, 0.395, 0.499, 0.316, 0.19, 0.251]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m top1: 0.21548507462686567
[2m[36m(func pid=89025)[0m top5: 0.6595149253731343
[2m[36m(func pid=89025)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=89025)[0m f1_macro: 0.19092841445220182
[2m[36m(func pid=89025)[0m f1_weighted: 0.20738594896523113
[2m[36m(func pid=89025)[0m f1_per_class: [0.116, 0.379, 0.104, 0.232, 0.067, 0.329, 0.051, 0.205, 0.196, 0.23]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.6822 | Steps: 4 | Val loss: 2.2118 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.9365 | Steps: 4 | Val loss: 1.8078 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.6701 | Steps: 4 | Val loss: 1.8140 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.5274 | Steps: 4 | Val loss: 4.7685 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 12:24:39 (running for 00:26:45.49)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.682 |      0.156 |                   54 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.81  |      0.3   |                   34 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.592 |      0.297 |                   26 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.361 |      0.191 |                    8 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.19263059701492538
[2m[36m(func pid=77753)[0m top5: 0.6930970149253731
[2m[36m(func pid=77753)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=77753)[0m f1_macro: 0.15644843247586612
[2m[36m(func pid=77753)[0m f1_weighted: 0.2178790276307909
[2m[36m(func pid=77753)[0m f1_per_class: [0.163, 0.174, 0.165, 0.302, 0.035, 0.273, 0.185, 0.192, 0.034, 0.04]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.3628731343283582
[2m[36m(func pid=82573)[0m top5: 0.8614738805970149
[2m[36m(func pid=82573)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=82573)[0m f1_macro: 0.30891405034815833
[2m[36m(func pid=82573)[0m f1_weighted: 0.3595652161920899
[2m[36m(func pid=82573)[0m f1_per_class: [0.512, 0.351, 0.203, 0.523, 0.094, 0.463, 0.204, 0.328, 0.157, 0.253]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m top1: 0.386660447761194
[2m[36m(func pid=84655)[0m top5: 0.8843283582089553
[2m[36m(func pid=84655)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=84655)[0m f1_macro: 0.31914249901675357
[2m[36m(func pid=84655)[0m f1_weighted: 0.4189369123423268
[2m[36m(func pid=84655)[0m f1_per_class: [0.441, 0.397, 0.131, 0.427, 0.124, 0.404, 0.495, 0.296, 0.211, 0.266]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m top1: 0.33488805970149255
[2m[36m(func pid=89025)[0m top5: 0.8250932835820896
[2m[36m(func pid=89025)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=89025)[0m f1_macro: 0.31386507113085466
[2m[36m(func pid=89025)[0m f1_weighted: 0.3374246103025965
[2m[36m(func pid=89025)[0m f1_per_class: [0.316, 0.296, 0.632, 0.49, 0.153, 0.375, 0.243, 0.262, 0.171, 0.202]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.7095 | Steps: 4 | Val loss: 2.2115 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.8084 | Steps: 4 | Val loss: 1.8045 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.8598 | Steps: 4 | Val loss: 1.8647 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.2673 | Steps: 4 | Val loss: 4.3143 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 12:24:44 (running for 00:26:50.75)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.709 |      0.165 |                   55 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.937 |      0.309 |                   35 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.67  |      0.319 |                   27 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.527 |      0.314 |                    9 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.20009328358208955
[2m[36m(func pid=77753)[0m top5: 0.7010261194029851
[2m[36m(func pid=77753)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=77753)[0m f1_macro: 0.1652358204687158
[2m[36m(func pid=77753)[0m f1_weighted: 0.22531950437777204
[2m[36m(func pid=77753)[0m f1_per_class: [0.152, 0.173, 0.163, 0.298, 0.054, 0.288, 0.207, 0.189, 0.034, 0.093]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.3591417910447761
[2m[36m(func pid=82573)[0m top5: 0.8568097014925373
[2m[36m(func pid=82573)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=82573)[0m f1_macro: 0.30086065367997583
[2m[36m(func pid=82573)[0m f1_weighted: 0.3439128252762825
[2m[36m(func pid=82573)[0m f1_per_class: [0.516, 0.316, 0.22, 0.546, 0.088, 0.448, 0.158, 0.323, 0.138, 0.255]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3833955223880597
[2m[36m(func pid=84655)[0m top5: 0.8875932835820896
[2m[36m(func pid=84655)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=84655)[0m f1_macro: 0.32016651246563144
[2m[36m(func pid=84655)[0m f1_weighted: 0.41456635851568
[2m[36m(func pid=84655)[0m f1_per_class: [0.483, 0.414, 0.158, 0.396, 0.114, 0.387, 0.505, 0.291, 0.194, 0.259]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m top1: 0.36240671641791045
[2m[36m(func pid=89025)[0m top5: 0.8605410447761194
[2m[36m(func pid=89025)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=89025)[0m f1_macro: 0.3394299428815166
[2m[36m(func pid=89025)[0m f1_weighted: 0.36175238981946156
[2m[36m(func pid=89025)[0m f1_per_class: [0.389, 0.213, 0.667, 0.537, 0.179, 0.403, 0.3, 0.307, 0.19, 0.21]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.6517 | Steps: 4 | Val loss: 2.2030 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.7764 | Steps: 4 | Val loss: 1.8079 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.7574 | Steps: 4 | Val loss: 1.9848 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 0.7449 | Steps: 4 | Val loss: 4.7166 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=77753)[0m top1: 0.21361940298507462
[2m[36m(func pid=77753)[0m top5: 0.7061567164179104
[2m[36m(func pid=77753)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=77753)[0m f1_macro: 0.17275697352123282
[2m[36m(func pid=77753)[0m f1_weighted: 0.23637895633308478
[2m[36m(func pid=77753)[0m f1_per_class: [0.169, 0.197, 0.168, 0.325, 0.046, 0.318, 0.19, 0.205, 0.035, 0.074]
[2m[36m(func pid=77753)[0m 
== Status ==
Current time: 2024-01-07 12:24:50 (running for 00:26:56.35)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.652 |      0.173 |                   56 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.808 |      0.301 |                   36 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.86  |      0.32  |                   28 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.267 |      0.339 |                   10 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.365205223880597
[2m[36m(func pid=82573)[0m top5: 0.8549440298507462
[2m[36m(func pid=82573)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=82573)[0m f1_macro: 0.30060107655620605
[2m[36m(func pid=82573)[0m f1_weighted: 0.354203139140513
[2m[36m(func pid=82573)[0m f1_per_class: [0.517, 0.31, 0.174, 0.56, 0.09, 0.433, 0.188, 0.328, 0.148, 0.259]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m top1: 0.34841417910447764
[2m[36m(func pid=84655)[0m top5: 0.8698694029850746
[2m[36m(func pid=84655)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=84655)[0m f1_macro: 0.3031440311084445
[2m[36m(func pid=84655)[0m f1_weighted: 0.3843392363594572
[2m[36m(func pid=84655)[0m f1_per_class: [0.515, 0.43, 0.107, 0.341, 0.098, 0.34, 0.461, 0.309, 0.188, 0.241]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m top1: 0.30970149253731344
[2m[36m(func pid=89025)[0m top5: 0.8236940298507462
[2m[36m(func pid=89025)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=89025)[0m f1_macro: 0.2989794166515537
[2m[36m(func pid=89025)[0m f1_weighted: 0.30922897261317556
[2m[36m(func pid=89025)[0m f1_per_class: [0.271, 0.296, 0.429, 0.375, 0.174, 0.384, 0.233, 0.366, 0.199, 0.263]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.6757 | Steps: 4 | Val loss: 2.1957 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.7379 | Steps: 4 | Val loss: 1.8208 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.7821 | Steps: 4 | Val loss: 1.9231 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.2657 | Steps: 4 | Val loss: 4.2783 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:24:55 (running for 00:27:01.88)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.652 |      0.173 |                   56 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.738 |      0.303 |                   38 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.757 |      0.303 |                   29 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  0.745 |      0.299 |                   11 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.208955223880597
[2m[36m(func pid=77753)[0m top5: 0.710820895522388
[2m[36m(func pid=77753)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=77753)[0m f1_macro: 0.16895719908507495
[2m[36m(func pid=77753)[0m f1_weighted: 0.23440767729657666
[2m[36m(func pid=77753)[0m f1_per_class: [0.178, 0.187, 0.15, 0.321, 0.035, 0.312, 0.198, 0.192, 0.031, 0.086]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.3591417910447761
[2m[36m(func pid=82573)[0m top5: 0.8530783582089553
[2m[36m(func pid=82573)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=82573)[0m f1_macro: 0.3026736661638846
[2m[36m(func pid=82573)[0m f1_weighted: 0.35163270922705464
[2m[36m(func pid=82573)[0m f1_per_class: [0.508, 0.277, 0.16, 0.545, 0.112, 0.464, 0.197, 0.327, 0.178, 0.257]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m top1: 0.36800373134328357
[2m[36m(func pid=84655)[0m top5: 0.8838619402985075
[2m[36m(func pid=84655)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=84655)[0m f1_macro: 0.3120995498706328
[2m[36m(func pid=84655)[0m f1_weighted: 0.40233258833575847
[2m[36m(func pid=84655)[0m f1_per_class: [0.549, 0.453, 0.11, 0.339, 0.099, 0.351, 0.507, 0.301, 0.167, 0.244]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m top1: 0.3292910447761194
[2m[36m(func pid=89025)[0m top5: 0.8470149253731343
[2m[36m(func pid=89025)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=89025)[0m f1_macro: 0.29637225544366225
[2m[36m(func pid=89025)[0m f1_weighted: 0.3435209197147721
[2m[36m(func pid=89025)[0m f1_per_class: [0.39, 0.28, 0.21, 0.384, 0.175, 0.353, 0.361, 0.338, 0.213, 0.26]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.6883 | Steps: 4 | Val loss: 2.1940 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.7599 | Steps: 4 | Val loss: 1.8185 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.1556 | Steps: 4 | Val loss: 1.9050 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.9909 | Steps: 4 | Val loss: 4.6846 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 12:25:01 (running for 00:27:07.29)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.676 |      0.169 |                   57 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.76  |      0.3   |                   39 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.782 |      0.312 |                   30 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.266 |      0.296 |                   12 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.21455223880597016
[2m[36m(func pid=77753)[0m top5: 0.7056902985074627
[2m[36m(func pid=77753)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=77753)[0m f1_macro: 0.1749369249699399
[2m[36m(func pid=77753)[0m f1_weighted: 0.23741899233058117
[2m[36m(func pid=77753)[0m f1_per_class: [0.236, 0.195, 0.126, 0.345, 0.034, 0.274, 0.191, 0.19, 0.032, 0.127]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.3521455223880597
[2m[36m(func pid=82573)[0m top5: 0.851679104477612
[2m[36m(func pid=82573)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=82573)[0m f1_macro: 0.30005815052562157
[2m[36m(func pid=82573)[0m f1_weighted: 0.3577731120270591
[2m[36m(func pid=82573)[0m f1_per_class: [0.508, 0.285, 0.134, 0.526, 0.112, 0.464, 0.231, 0.339, 0.167, 0.234]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3787313432835821
[2m[36m(func pid=84655)[0m top5: 0.8740671641791045
[2m[36m(func pid=84655)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=84655)[0m f1_macro: 0.3124910870760506
[2m[36m(func pid=84655)[0m f1_weighted: 0.415288471354054
[2m[36m(func pid=84655)[0m f1_per_class: [0.491, 0.454, 0.133, 0.452, 0.073, 0.333, 0.452, 0.315, 0.184, 0.239]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m top1: 0.3306902985074627
[2m[36m(func pid=89025)[0m top5: 0.8614738805970149
[2m[36m(func pid=89025)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=89025)[0m f1_macro: 0.27953846801645554
[2m[36m(func pid=89025)[0m f1_weighted: 0.32952408938431516
[2m[36m(func pid=89025)[0m f1_per_class: [0.4, 0.089, 0.202, 0.394, 0.262, 0.287, 0.469, 0.164, 0.206, 0.321]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.6322 | Steps: 4 | Val loss: 1.7967 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.6859 | Steps: 4 | Val loss: 2.1818 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.6078 | Steps: 4 | Val loss: 1.9273 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9832 | Steps: 4 | Val loss: 4.9582 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:25:06 (running for 00:27:12.53)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.688 |      0.175 |                   58 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.632 |      0.309 |                   40 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.156 |      0.312 |                   31 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.991 |      0.28  |                   13 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3628731343283582
[2m[36m(func pid=82573)[0m top5: 0.8600746268656716
[2m[36m(func pid=82573)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=82573)[0m f1_macro: 0.3091853098031335
[2m[36m(func pid=82573)[0m f1_weighted: 0.37813459822189394
[2m[36m(func pid=82573)[0m f1_per_class: [0.465, 0.307, 0.158, 0.506, 0.123, 0.456, 0.309, 0.339, 0.193, 0.237]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=77753)[0m top1: 0.22434701492537312
[2m[36m(func pid=77753)[0m top5: 0.7332089552238806
[2m[36m(func pid=77753)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=77753)[0m f1_macro: 0.17936218956436054
[2m[36m(func pid=77753)[0m f1_weighted: 0.25484428035594175
[2m[36m(func pid=77753)[0m f1_per_class: [0.194, 0.236, 0.14, 0.328, 0.044, 0.28, 0.245, 0.168, 0.049, 0.109]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3763992537313433
[2m[36m(func pid=84655)[0m top5: 0.8652052238805971
[2m[36m(func pid=84655)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=84655)[0m f1_macro: 0.3214330573371631
[2m[36m(func pid=84655)[0m f1_weighted: 0.4043762281373439
[2m[36m(func pid=84655)[0m f1_per_class: [0.508, 0.378, 0.177, 0.506, 0.143, 0.4, 0.37, 0.369, 0.188, 0.176]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m top1: 0.2947761194029851
[2m[36m(func pid=89025)[0m top5: 0.8465485074626866
[2m[36m(func pid=89025)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=89025)[0m f1_macro: 0.29734311480767417
[2m[36m(func pid=89025)[0m f1_weighted: 0.30390315485415814
[2m[36m(func pid=89025)[0m f1_per_class: [0.429, 0.418, 0.369, 0.38, 0.115, 0.193, 0.214, 0.311, 0.183, 0.362]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.7471 | Steps: 4 | Val loss: 1.7729 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.6072 | Steps: 4 | Val loss: 2.1845 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7372 | Steps: 4 | Val loss: 2.0292 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.8798 | Steps: 4 | Val loss: 7.0117 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 12:25:11 (running for 00:27:17.81)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.686 |      0.179 |                   59 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.747 |      0.316 |                   41 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.608 |      0.321 |                   32 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.983 |      0.297 |                   14 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3712686567164179
[2m[36m(func pid=82573)[0m top5: 0.8666044776119403
[2m[36m(func pid=82573)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=82573)[0m f1_macro: 0.3159567915929554
[2m[36m(func pid=82573)[0m f1_weighted: 0.389837690648025
[2m[36m(func pid=82573)[0m f1_per_class: [0.517, 0.328, 0.141, 0.514, 0.102, 0.453, 0.328, 0.33, 0.194, 0.253]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=77753)[0m top1: 0.2140858208955224
[2m[36m(func pid=77753)[0m top5: 0.7252798507462687
[2m[36m(func pid=77753)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=77753)[0m f1_macro: 0.17439024075955772
[2m[36m(func pid=77753)[0m f1_weighted: 0.2418654718999733
[2m[36m(func pid=77753)[0m f1_per_class: [0.189, 0.243, 0.134, 0.322, 0.029, 0.25, 0.212, 0.176, 0.045, 0.141]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=89025)[0m top1: 0.27611940298507465
[2m[36m(func pid=89025)[0m top5: 0.6949626865671642
[2m[36m(func pid=89025)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=89025)[0m f1_macro: 0.2663095959082338
[2m[36m(func pid=89025)[0m f1_weighted: 0.24673581036982214
[2m[36m(func pid=89025)[0m f1_per_class: [0.436, 0.461, 0.241, 0.305, 0.079, 0.206, 0.062, 0.326, 0.181, 0.365]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3572761194029851
[2m[36m(func pid=84655)[0m top5: 0.8484141791044776
[2m[36m(func pid=84655)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=84655)[0m f1_macro: 0.3008721320993795
[2m[36m(func pid=84655)[0m f1_weighted: 0.37997873394750314
[2m[36m(func pid=84655)[0m f1_per_class: [0.362, 0.322, 0.23, 0.517, 0.139, 0.388, 0.323, 0.356, 0.227, 0.144]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.6810 | Steps: 4 | Val loss: 2.1894 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.7300 | Steps: 4 | Val loss: 1.7757 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.7271 | Steps: 4 | Val loss: 2.0292 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7003 | Steps: 4 | Val loss: 8.5607 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:25:17 (running for 00:27:23.31)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.607 |      0.174 |                   60 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.73  |      0.316 |                   42 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.737 |      0.301 |                   33 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  0.88  |      0.266 |                   15 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.20942164179104478
[2m[36m(func pid=77753)[0m top5: 0.7103544776119403
[2m[36m(func pid=77753)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=77753)[0m f1_macro: 0.17627333880015328
[2m[36m(func pid=77753)[0m f1_weighted: 0.23438167414749006
[2m[36m(func pid=77753)[0m f1_per_class: [0.199, 0.241, 0.126, 0.306, 0.034, 0.269, 0.194, 0.181, 0.042, 0.17]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.3726679104477612
[2m[36m(func pid=82573)[0m top5: 0.8652052238805971
[2m[36m(func pid=82573)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=82573)[0m f1_macro: 0.3158104565116898
[2m[36m(func pid=82573)[0m f1_weighted: 0.3895728737848842
[2m[36m(func pid=82573)[0m f1_per_class: [0.48, 0.327, 0.161, 0.512, 0.117, 0.453, 0.329, 0.338, 0.193, 0.247]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m top1: 0.33675373134328357
[2m[36m(func pid=84655)[0m top5: 0.851679104477612
[2m[36m(func pid=84655)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=84655)[0m f1_macro: 0.2917896534021987
[2m[36m(func pid=84655)[0m f1_weighted: 0.36145919260830434
[2m[36m(func pid=84655)[0m f1_per_class: [0.294, 0.314, 0.257, 0.418, 0.149, 0.378, 0.365, 0.362, 0.21, 0.169]
[2m[36m(func pid=89025)[0m top1: 0.22108208955223882
[2m[36m(func pid=89025)[0m top5: 0.6338619402985075
[2m[36m(func pid=89025)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=89025)[0m f1_macro: 0.18881668270153507
[2m[36m(func pid=89025)[0m f1_weighted: 0.20576866708793298
[2m[36m(func pid=89025)[0m f1_per_class: [0.329, 0.149, 0.137, 0.428, 0.103, 0.278, 0.015, 0.122, 0.236, 0.09]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.6612 | Steps: 4 | Val loss: 2.1797 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.7337 | Steps: 4 | Val loss: 1.7616 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.7654 | Steps: 4 | Val loss: 7.4244 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.6678 | Steps: 4 | Val loss: 2.0703 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 12:25:22 (running for 00:27:28.57)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.661 |      0.189 |                   62 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.73  |      0.316 |                   42 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.727 |      0.292 |                   34 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  0.7   |      0.189 |                   16 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.2234141791044776
[2m[36m(func pid=77753)[0m top5: 0.722481343283582
[2m[36m(func pid=77753)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=77753)[0m f1_macro: 0.18876443622297
[2m[36m(func pid=77753)[0m f1_weighted: 0.24691844657065268
[2m[36m(func pid=77753)[0m f1_per_class: [0.224, 0.224, 0.162, 0.329, 0.034, 0.31, 0.203, 0.199, 0.042, 0.161]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.3829291044776119
[2m[36m(func pid=82573)[0m top5: 0.8614738805970149
[2m[36m(func pid=82573)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=82573)[0m f1_macro: 0.3203030180887326
[2m[36m(func pid=82573)[0m f1_weighted: 0.3950790219372388
[2m[36m(func pid=82573)[0m f1_per_class: [0.444, 0.322, 0.173, 0.521, 0.118, 0.452, 0.338, 0.36, 0.213, 0.261]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.28404850746268656
[2m[36m(func pid=89025)[0m top5: 0.6823694029850746
[2m[36m(func pid=89025)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=89025)[0m f1_macro: 0.24190762291733686
[2m[36m(func pid=89025)[0m f1_weighted: 0.27013132615446084
[2m[36m(func pid=89025)[0m f1_per_class: [0.269, 0.364, 0.113, 0.456, 0.122, 0.293, 0.034, 0.346, 0.218, 0.204]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3381529850746269
[2m[36m(func pid=84655)[0m top5: 0.8409514925373134
[2m[36m(func pid=84655)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=84655)[0m f1_macro: 0.2958056123430404
[2m[36m(func pid=84655)[0m f1_weighted: 0.35546662196265527
[2m[36m(func pid=84655)[0m f1_per_class: [0.345, 0.422, 0.205, 0.321, 0.116, 0.372, 0.375, 0.336, 0.223, 0.242]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.6822 | Steps: 4 | Val loss: 2.1826 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.7211 | Steps: 4 | Val loss: 1.7852 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.6342 | Steps: 4 | Val loss: 5.9618 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.6582 | Steps: 4 | Val loss: 2.0028 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 12:25:27 (running for 00:27:33.84)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.682 |      0.19  |                   63 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.734 |      0.32  |                   43 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.668 |      0.296 |                   35 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  2.765 |      0.242 |                   17 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.22761194029850745
[2m[36m(func pid=77753)[0m top5: 0.7196828358208955
[2m[36m(func pid=77753)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=77753)[0m f1_macro: 0.1900076579195725
[2m[36m(func pid=77753)[0m f1_weighted: 0.24865048747858556
[2m[36m(func pid=77753)[0m f1_per_class: [0.24, 0.221, 0.164, 0.352, 0.033, 0.324, 0.184, 0.2, 0.032, 0.15]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.36986940298507465
[2m[36m(func pid=82573)[0m top5: 0.8558768656716418
[2m[36m(func pid=82573)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=82573)[0m f1_macro: 0.30665025321963524
[2m[36m(func pid=82573)[0m f1_weighted: 0.3859770064555831
[2m[36m(func pid=82573)[0m f1_per_class: [0.339, 0.313, 0.171, 0.501, 0.125, 0.444, 0.342, 0.365, 0.205, 0.261]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.35074626865671643
[2m[36m(func pid=89025)[0m top5: 0.7854477611940298
[2m[36m(func pid=89025)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=89025)[0m f1_macro: 0.2780048016954704
[2m[36m(func pid=89025)[0m f1_weighted: 0.324895935641521
[2m[36m(func pid=89025)[0m f1_per_class: [0.32, 0.462, 0.168, 0.484, 0.053, 0.128, 0.188, 0.347, 0.207, 0.424]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3572761194029851
[2m[36m(func pid=84655)[0m top5: 0.8563432835820896
[2m[36m(func pid=84655)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=84655)[0m f1_macro: 0.31994758177273064
[2m[36m(func pid=84655)[0m f1_weighted: 0.37740437738699584
[2m[36m(func pid=84655)[0m f1_per_class: [0.475, 0.45, 0.198, 0.38, 0.118, 0.381, 0.367, 0.322, 0.212, 0.295]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6185 | Steps: 4 | Val loss: 2.1825 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.5643 | Steps: 4 | Val loss: 1.7801 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.7469 | Steps: 4 | Val loss: 6.2336 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.8245 | Steps: 4 | Val loss: 2.0350 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 12:25:33 (running for 00:27:39.22)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.618 |      0.192 |                   64 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.721 |      0.307 |                   44 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.658 |      0.32  |                   36 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.634 |      0.278 |                   18 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.2234141791044776
[2m[36m(func pid=77753)[0m top5: 0.7243470149253731
[2m[36m(func pid=77753)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=77753)[0m f1_macro: 0.19222709610659133
[2m[36m(func pid=77753)[0m f1_weighted: 0.2449537475349563
[2m[36m(func pid=77753)[0m f1_per_class: [0.2, 0.211, 0.188, 0.327, 0.034, 0.34, 0.193, 0.21, 0.032, 0.186]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.3596082089552239
[2m[36m(func pid=82573)[0m top5: 0.8624067164179104
[2m[36m(func pid=82573)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=82573)[0m f1_macro: 0.3037481132986103
[2m[36m(func pid=82573)[0m f1_weighted: 0.3754432869878325
[2m[36m(func pid=82573)[0m f1_per_class: [0.336, 0.302, 0.22, 0.473, 0.138, 0.463, 0.342, 0.32, 0.171, 0.272]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.373134328358209
[2m[36m(func pid=89025)[0m top5: 0.7789179104477612
[2m[36m(func pid=89025)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=89025)[0m f1_macro: 0.25990083496939154
[2m[36m(func pid=89025)[0m f1_weighted: 0.3490544174895598
[2m[36m(func pid=89025)[0m f1_per_class: [0.251, 0.087, 0.178, 0.578, 0.085, 0.137, 0.403, 0.311, 0.268, 0.301]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.34701492537313433
[2m[36m(func pid=84655)[0m top5: 0.8605410447761194
[2m[36m(func pid=84655)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=84655)[0m f1_macro: 0.31232702761444664
[2m[36m(func pid=84655)[0m f1_weighted: 0.37274019120106994
[2m[36m(func pid=84655)[0m f1_per_class: [0.509, 0.389, 0.172, 0.465, 0.112, 0.373, 0.314, 0.307, 0.198, 0.284]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6405 | Steps: 4 | Val loss: 2.1945 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.6981 | Steps: 4 | Val loss: 1.7636 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.6747 | Steps: 4 | Val loss: 6.1352 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.6951 | Steps: 4 | Val loss: 2.0218 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 12:25:38 (running for 00:27:44.57)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.64  |      0.18  |                   65 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.564 |      0.304 |                   45 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.824 |      0.312 |                   37 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.747 |      0.26  |                   19 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.20988805970149255
[2m[36m(func pid=77753)[0m top5: 0.7103544776119403
[2m[36m(func pid=77753)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=77753)[0m f1_macro: 0.18032806820899416
[2m[36m(func pid=77753)[0m f1_weighted: 0.23159743456333098
[2m[36m(func pid=77753)[0m f1_per_class: [0.197, 0.209, 0.18, 0.306, 0.044, 0.303, 0.188, 0.194, 0.033, 0.149]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.37546641791044777
[2m[36m(func pid=82573)[0m top5: 0.8689365671641791
[2m[36m(func pid=82573)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=82573)[0m f1_macro: 0.31907080703925894
[2m[36m(func pid=82573)[0m f1_weighted: 0.3902574778674465
[2m[36m(func pid=82573)[0m f1_per_class: [0.402, 0.341, 0.235, 0.5, 0.121, 0.458, 0.339, 0.33, 0.183, 0.282]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.30223880597014924
[2m[36m(func pid=89025)[0m top5: 0.8120335820895522
[2m[36m(func pid=89025)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=89025)[0m f1_macro: 0.2549557720134358
[2m[36m(func pid=89025)[0m f1_weighted: 0.3032176807072918
[2m[36m(func pid=89025)[0m f1_per_class: [0.49, 0.162, 0.112, 0.247, 0.154, 0.201, 0.499, 0.237, 0.182, 0.267]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3414179104477612
[2m[36m(func pid=84655)[0m top5: 0.8684701492537313
[2m[36m(func pid=84655)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=84655)[0m f1_macro: 0.3087519240117584
[2m[36m(func pid=84655)[0m f1_weighted: 0.3766287163244303
[2m[36m(func pid=84655)[0m f1_per_class: [0.542, 0.318, 0.197, 0.449, 0.108, 0.368, 0.381, 0.333, 0.191, 0.201]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.6316 | Steps: 4 | Val loss: 2.1895 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.7273 | Steps: 4 | Val loss: 1.7872 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.3266 | Steps: 4 | Val loss: 8.2430 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.1032 | Steps: 4 | Val loss: 2.0831 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 12:25:43 (running for 00:27:49.85)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.632 |      0.182 |                   66 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.698 |      0.319 |                   46 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.695 |      0.309 |                   38 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.675 |      0.255 |                   20 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.2126865671641791
[2m[36m(func pid=77753)[0m top5: 0.7056902985074627
[2m[36m(func pid=77753)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=77753)[0m f1_macro: 0.18237216125831893
[2m[36m(func pid=77753)[0m f1_weighted: 0.2355997894725639
[2m[36m(func pid=77753)[0m f1_per_class: [0.216, 0.211, 0.137, 0.314, 0.033, 0.299, 0.193, 0.192, 0.034, 0.196]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.3516791044776119
[2m[36m(func pid=82573)[0m top5: 0.8558768656716418
[2m[36m(func pid=82573)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=82573)[0m f1_macro: 0.3036945808490935
[2m[36m(func pid=82573)[0m f1_weighted: 0.3629546635885238
[2m[36m(func pid=82573)[0m f1_per_class: [0.357, 0.315, 0.224, 0.484, 0.104, 0.423, 0.287, 0.347, 0.215, 0.282]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.26725746268656714
[2m[36m(func pid=89025)[0m top5: 0.6958955223880597
[2m[36m(func pid=89025)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=89025)[0m f1_macro: 0.2429719609998841
[2m[36m(func pid=89025)[0m f1_weighted: 0.24895885335342813
[2m[36m(func pid=89025)[0m f1_per_class: [0.316, 0.373, 0.177, 0.003, 0.136, 0.279, 0.4, 0.251, 0.175, 0.319]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3344216417910448
[2m[36m(func pid=84655)[0m top5: 0.8414179104477612
[2m[36m(func pid=84655)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=84655)[0m f1_macro: 0.28930145389092116
[2m[36m(func pid=84655)[0m f1_weighted: 0.36628490053643026
[2m[36m(func pid=84655)[0m f1_per_class: [0.354, 0.289, 0.226, 0.43, 0.121, 0.375, 0.39, 0.336, 0.199, 0.172]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.6076 | Steps: 4 | Val loss: 2.1744 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.5766 | Steps: 4 | Val loss: 1.7907 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.9408 | Steps: 4 | Val loss: 8.1566 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.5108 | Steps: 4 | Val loss: 2.1760 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 12:25:49 (running for 00:27:55.16)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.608 |      0.2   |                   67 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.727 |      0.304 |                   47 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.103 |      0.289 |                   39 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.327 |      0.243 |                   21 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.23507462686567165
[2m[36m(func pid=77753)[0m top5: 0.7252798507462687
[2m[36m(func pid=77753)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=77753)[0m f1_macro: 0.19997552823457343
[2m[36m(func pid=77753)[0m f1_weighted: 0.2578003147050939
[2m[36m(func pid=77753)[0m f1_per_class: [0.193, 0.198, 0.212, 0.348, 0.042, 0.329, 0.226, 0.214, 0.044, 0.194]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.3572761194029851
[2m[36m(func pid=82573)[0m top5: 0.8568097014925373
[2m[36m(func pid=82573)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=82573)[0m f1_macro: 0.3141920147321611
[2m[36m(func pid=82573)[0m f1_weighted: 0.3641301793104907
[2m[36m(func pid=82573)[0m f1_per_class: [0.387, 0.331, 0.267, 0.491, 0.12, 0.442, 0.265, 0.348, 0.209, 0.284]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.22807835820895522
[2m[36m(func pid=89025)[0m top5: 0.6842350746268657
[2m[36m(func pid=89025)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=89025)[0m f1_macro: 0.24469883635615056
[2m[36m(func pid=89025)[0m f1_weighted: 0.23366765500639716
[2m[36m(func pid=89025)[0m f1_per_class: [0.513, 0.391, 0.264, 0.029, 0.04, 0.261, 0.312, 0.259, 0.159, 0.219]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3162313432835821
[2m[36m(func pid=84655)[0m top5: 0.8283582089552238
[2m[36m(func pid=84655)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=84655)[0m f1_macro: 0.28361448916740334
[2m[36m(func pid=84655)[0m f1_weighted: 0.3505011360744208
[2m[36m(func pid=84655)[0m f1_per_class: [0.339, 0.332, 0.286, 0.329, 0.162, 0.305, 0.436, 0.337, 0.183, 0.128]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6092 | Steps: 4 | Val loss: 2.1816 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.6260 | Steps: 4 | Val loss: 1.8212 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.8711 | Steps: 4 | Val loss: 5.9458 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.7396 | Steps: 4 | Val loss: 2.2672 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 12:25:54 (running for 00:28:00.67)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.609 |      0.189 |                   68 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.577 |      0.314 |                   48 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.511 |      0.284 |                   40 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.941 |      0.245 |                   22 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.22108208955223882
[2m[36m(func pid=77753)[0m top5: 0.7154850746268657
[2m[36m(func pid=77753)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=77753)[0m f1_macro: 0.18859094610265786
[2m[36m(func pid=77753)[0m f1_weighted: 0.24300678974717155
[2m[36m(func pid=77753)[0m f1_per_class: [0.214, 0.166, 0.187, 0.329, 0.032, 0.341, 0.21, 0.209, 0.035, 0.161]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.3358208955223881
[2m[36m(func pid=82573)[0m top5: 0.8442164179104478
[2m[36m(func pid=82573)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=82573)[0m f1_macro: 0.2985535283012931
[2m[36m(func pid=82573)[0m f1_weighted: 0.3437989064783653
[2m[36m(func pid=82573)[0m f1_per_class: [0.387, 0.326, 0.233, 0.477, 0.098, 0.418, 0.225, 0.343, 0.204, 0.275]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.3451492537313433
[2m[36m(func pid=89025)[0m top5: 0.784981343283582
[2m[36m(func pid=89025)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=89025)[0m f1_macro: 0.2968563025691942
[2m[36m(func pid=89025)[0m f1_weighted: 0.36505884485657286
[2m[36m(func pid=89025)[0m f1_per_class: [0.426, 0.273, 0.304, 0.505, 0.071, 0.297, 0.351, 0.299, 0.23, 0.212]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.30363805970149255
[2m[36m(func pid=84655)[0m top5: 0.8176305970149254
[2m[36m(func pid=84655)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=84655)[0m f1_macro: 0.2810548160355724
[2m[36m(func pid=84655)[0m f1_weighted: 0.3250901478223445
[2m[36m(func pid=84655)[0m f1_per_class: [0.388, 0.381, 0.302, 0.241, 0.147, 0.295, 0.41, 0.309, 0.18, 0.16]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.5860 | Steps: 4 | Val loss: 2.1624 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.5319 | Steps: 4 | Val loss: 1.8314 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.7183 | Steps: 4 | Val loss: 6.1838 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.1865 | Steps: 4 | Val loss: 2.2069 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 12:26:00 (running for 00:28:05.94)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.609 |      0.189 |                   68 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.532 |      0.294 |                   50 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.74  |      0.281 |                   41 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  0.871 |      0.297 |                   23 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3283582089552239
[2m[36m(func pid=82573)[0m top5: 0.8418843283582089
[2m[36m(func pid=82573)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=82573)[0m f1_macro: 0.2943454418548356
[2m[36m(func pid=82573)[0m f1_weighted: 0.3455839085848167
[2m[36m(func pid=82573)[0m f1_per_class: [0.398, 0.335, 0.229, 0.454, 0.081, 0.41, 0.254, 0.33, 0.204, 0.25]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=77753)[0m top1: 0.23227611940298507
[2m[36m(func pid=77753)[0m top5: 0.7402052238805971
[2m[36m(func pid=77753)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=77753)[0m f1_macro: 0.20282655745744482
[2m[36m(func pid=77753)[0m f1_weighted: 0.26015356469901824
[2m[36m(func pid=77753)[0m f1_per_class: [0.273, 0.2, 0.186, 0.32, 0.038, 0.337, 0.253, 0.213, 0.029, 0.179]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=89025)[0m top1: 0.36986940298507465
[2m[36m(func pid=89025)[0m top5: 0.8050373134328358
[2m[36m(func pid=89025)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=89025)[0m f1_macro: 0.3060210094818431
[2m[36m(func pid=89025)[0m f1_weighted: 0.37088881486336756
[2m[36m(func pid=89025)[0m f1_per_class: [0.263, 0.402, 0.426, 0.517, 0.083, 0.41, 0.252, 0.274, 0.278, 0.154]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.31529850746268656
[2m[36m(func pid=84655)[0m top5: 0.8456156716417911
[2m[36m(func pid=84655)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=84655)[0m f1_macro: 0.3089073074490404
[2m[36m(func pid=84655)[0m f1_weighted: 0.3482870955529691
[2m[36m(func pid=84655)[0m f1_per_class: [0.46, 0.429, 0.333, 0.292, 0.187, 0.301, 0.402, 0.325, 0.141, 0.22]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.6627 | Steps: 4 | Val loss: 2.1536 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.5404 | Steps: 4 | Val loss: 1.8108 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.3138 | Steps: 4 | Val loss: 9.6599 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.7226 | Steps: 4 | Val loss: 1.9804 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 12:26:05 (running for 00:28:11.37)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.663 |      0.204 |                   70 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.532 |      0.294 |                   50 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  1.186 |      0.309 |                   42 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.718 |      0.306 |                   24 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.24067164179104478
[2m[36m(func pid=77753)[0m top5: 0.7476679104477612
[2m[36m(func pid=77753)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=77753)[0m f1_macro: 0.20432816968689824
[2m[36m(func pid=77753)[0m f1_weighted: 0.26583064972895915
[2m[36m(func pid=77753)[0m f1_per_class: [0.235, 0.176, 0.196, 0.357, 0.045, 0.331, 0.256, 0.201, 0.054, 0.194]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.345615671641791
[2m[36m(func pid=82573)[0m top5: 0.8535447761194029
[2m[36m(func pid=82573)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=82573)[0m f1_macro: 0.29903847536608813
[2m[36m(func pid=82573)[0m f1_weighted: 0.3716289334366538
[2m[36m(func pid=82573)[0m f1_per_class: [0.388, 0.348, 0.197, 0.453, 0.088, 0.405, 0.342, 0.293, 0.223, 0.254]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.2103544776119403
[2m[36m(func pid=89025)[0m top5: 0.6786380597014925
[2m[36m(func pid=89025)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=89025)[0m f1_macro: 0.19860253735277839
[2m[36m(func pid=89025)[0m f1_weighted: 0.20932735388055745
[2m[36m(func pid=89025)[0m f1_per_class: [0.145, 0.385, 0.29, 0.026, 0.0, 0.383, 0.218, 0.245, 0.22, 0.074]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3773320895522388
[2m[36m(func pid=84655)[0m top5: 0.871268656716418
[2m[36m(func pid=84655)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=84655)[0m f1_macro: 0.3367867445720523
[2m[36m(func pid=84655)[0m f1_weighted: 0.40835537482353157
[2m[36m(func pid=84655)[0m f1_per_class: [0.408, 0.47, 0.306, 0.45, 0.167, 0.346, 0.409, 0.347, 0.167, 0.298]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.5975 | Steps: 4 | Val loss: 2.1534 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.4016 | Steps: 4 | Val loss: 1.7893 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.3113 | Steps: 4 | Val loss: 8.7991 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5476 | Steps: 4 | Val loss: 1.8728 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 12:26:10 (running for 00:28:16.73)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.598 |      0.2   |                   71 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.54  |      0.299 |                   51 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.723 |      0.337 |                   43 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  3.314 |      0.199 |                   25 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.23880597014925373
[2m[36m(func pid=77753)[0m top5: 0.7513992537313433
[2m[36m(func pid=77753)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=77753)[0m f1_macro: 0.19964737825219922
[2m[36m(func pid=77753)[0m f1_weighted: 0.26348543015088854
[2m[36m(func pid=77753)[0m f1_per_class: [0.235, 0.139, 0.17, 0.368, 0.046, 0.332, 0.26, 0.195, 0.048, 0.204]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=82573)[0m top1: 0.3572761194029851
[2m[36m(func pid=82573)[0m top5: 0.8563432835820896
[2m[36m(func pid=82573)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=82573)[0m f1_macro: 0.3110632669763662
[2m[36m(func pid=82573)[0m f1_weighted: 0.3848410607089551
[2m[36m(func pid=82573)[0m f1_per_class: [0.44, 0.397, 0.2, 0.446, 0.087, 0.41, 0.36, 0.283, 0.238, 0.25]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.2621268656716418
[2m[36m(func pid=89025)[0m top5: 0.7000932835820896
[2m[36m(func pid=89025)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=89025)[0m f1_macro: 0.26183106098814185
[2m[36m(func pid=89025)[0m f1_weighted: 0.2317035236864835
[2m[36m(func pid=89025)[0m f1_per_class: [0.507, 0.464, 0.295, 0.099, 0.136, 0.329, 0.172, 0.248, 0.165, 0.203]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.40298507462686567
[2m[36m(func pid=84655)[0m top5: 0.8843283582089553
[2m[36m(func pid=84655)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=84655)[0m f1_macro: 0.3334983388900459
[2m[36m(func pid=84655)[0m f1_weighted: 0.4145392249431691
[2m[36m(func pid=84655)[0m f1_per_class: [0.422, 0.422, 0.226, 0.56, 0.15, 0.402, 0.338, 0.31, 0.186, 0.317]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.5016 | Steps: 4 | Val loss: 1.8021 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.6431 | Steps: 4 | Val loss: 2.1495 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.6884 | Steps: 4 | Val loss: 7.1841 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5625 | Steps: 4 | Val loss: 1.8973 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=82573)[0m top1: 0.35027985074626866
[2m[36m(func pid=82573)[0m top5: 0.855410447761194
[2m[36m(func pid=82573)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=82573)[0m f1_macro: 0.30475851732306947
[2m[36m(func pid=82573)[0m f1_weighted: 0.3774927262439151
[2m[36m(func pid=82573)[0m f1_per_class: [0.372, 0.41, 0.189, 0.414, 0.083, 0.402, 0.358, 0.311, 0.248, 0.26]
[2m[36m(func pid=82573)[0m 
== Status ==
Current time: 2024-01-07 12:26:16 (running for 00:28:22.10)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.598 |      0.2   |                   71 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.502 |      0.305 |                   53 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.548 |      0.333 |                   44 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  2.311 |      0.262 |                   26 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.2490671641791045
[2m[36m(func pid=77753)[0m top5: 0.7448694029850746
[2m[36m(func pid=77753)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=77753)[0m f1_macro: 0.20357620781076355
[2m[36m(func pid=77753)[0m f1_weighted: 0.27254271216199394
[2m[36m(func pid=77753)[0m f1_per_class: [0.211, 0.125, 0.205, 0.395, 0.05, 0.338, 0.274, 0.186, 0.047, 0.205]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=89025)[0m top1: 0.2826492537313433
[2m[36m(func pid=89025)[0m top5: 0.7985074626865671
[2m[36m(func pid=89025)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=89025)[0m f1_macro: 0.2855563300345196
[2m[36m(func pid=89025)[0m f1_weighted: 0.33113410607412175
[2m[36m(func pid=89025)[0m f1_per_class: [0.537, 0.329, 0.238, 0.39, 0.071, 0.197, 0.358, 0.255, 0.147, 0.333]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.4039179104477612
[2m[36m(func pid=84655)[0m top5: 0.882929104477612
[2m[36m(func pid=84655)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=84655)[0m f1_macro: 0.3310300393960218
[2m[36m(func pid=84655)[0m f1_weighted: 0.41424135704958104
[2m[36m(func pid=84655)[0m f1_per_class: [0.394, 0.435, 0.174, 0.562, 0.14, 0.398, 0.33, 0.305, 0.199, 0.374]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.4064 | Steps: 4 | Val loss: 1.7651 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.5931 | Steps: 4 | Val loss: 2.1450 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.6304 | Steps: 4 | Val loss: 7.6003 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.6268 | Steps: 4 | Val loss: 1.8115 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 12:26:21 (running for 00:28:27.35)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.643 |      0.204 |                   72 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.406 |      0.318 |                   54 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.562 |      0.331 |                   45 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.688 |      0.286 |                   27 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3596082089552239
[2m[36m(func pid=82573)[0m top5: 0.8698694029850746
[2m[36m(func pid=82573)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=82573)[0m f1_macro: 0.317710937541041
[2m[36m(func pid=82573)[0m f1_weighted: 0.3835396040104649
[2m[36m(func pid=82573)[0m f1_per_class: [0.44, 0.44, 0.186, 0.395, 0.081, 0.404, 0.373, 0.302, 0.26, 0.297]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=77753)[0m top1: 0.25652985074626866
[2m[36m(func pid=77753)[0m top5: 0.746268656716418
[2m[36m(func pid=77753)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=77753)[0m f1_macro: 0.20927789008291997
[2m[36m(func pid=77753)[0m f1_weighted: 0.28299753586103277
[2m[36m(func pid=77753)[0m f1_per_class: [0.247, 0.15, 0.22, 0.405, 0.032, 0.327, 0.284, 0.206, 0.048, 0.174]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=89025)[0m top1: 0.3111007462686567
[2m[36m(func pid=89025)[0m top5: 0.8041044776119403
[2m[36m(func pid=89025)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=89025)[0m f1_macro: 0.28789035233518806
[2m[36m(func pid=89025)[0m f1_weighted: 0.3370931297023492
[2m[36m(func pid=89025)[0m f1_per_class: [0.568, 0.227, 0.216, 0.553, 0.042, 0.023, 0.33, 0.304, 0.225, 0.392]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.43097014925373134
[2m[36m(func pid=84655)[0m top5: 0.8899253731343284
[2m[36m(func pid=84655)[0m f1_micro: 0.43097014925373134
[2m[36m(func pid=84655)[0m f1_macro: 0.35138332445893006
[2m[36m(func pid=84655)[0m f1_weighted: 0.4488725831692185
[2m[36m(func pid=84655)[0m f1_per_class: [0.417, 0.484, 0.206, 0.56, 0.134, 0.402, 0.411, 0.311, 0.242, 0.345]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.4676 | Steps: 4 | Val loss: 1.7585 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.5814 | Steps: 4 | Val loss: 2.1575 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.1825 | Steps: 4 | Val loss: 7.4786 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.6182 | Steps: 4 | Val loss: 1.7890 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 12:26:26 (running for 00:28:32.80)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.323
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00012 | RUNNING    | 192.168.7.53:77753 | 0.0001 |       0.9  |         0.0001 |  2.593 |      0.209 |                   73 |
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.468 |      0.322 |                   55 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.627 |      0.351 |                   46 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.63  |      0.288 |                   28 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.36100746268656714
[2m[36m(func pid=82573)[0m top5: 0.8703358208955224
[2m[36m(func pid=82573)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=82573)[0m f1_macro: 0.32195722369315927
[2m[36m(func pid=82573)[0m f1_weighted: 0.38289429910603995
[2m[36m(func pid=82573)[0m f1_per_class: [0.407, 0.434, 0.2, 0.382, 0.078, 0.418, 0.373, 0.342, 0.269, 0.315]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=77753)[0m top1: 0.24720149253731344
[2m[36m(func pid=77753)[0m top5: 0.7364738805970149
[2m[36m(func pid=77753)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=77753)[0m f1_macro: 0.2067239157048651
[2m[36m(func pid=77753)[0m f1_weighted: 0.27069753331547314
[2m[36m(func pid=77753)[0m f1_per_class: [0.222, 0.152, 0.225, 0.403, 0.032, 0.336, 0.242, 0.203, 0.036, 0.218]
[2m[36m(func pid=77753)[0m 
[2m[36m(func pid=89025)[0m top1: 0.3180970149253731
[2m[36m(func pid=89025)[0m top5: 0.784981343283582
[2m[36m(func pid=89025)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=89025)[0m f1_macro: 0.295902950344219
[2m[36m(func pid=89025)[0m f1_weighted: 0.33963399363470653
[2m[36m(func pid=89025)[0m f1_per_class: [0.542, 0.344, 0.061, 0.503, 0.098, 0.338, 0.199, 0.315, 0.233, 0.326]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.42490671641791045
[2m[36m(func pid=84655)[0m top5: 0.902518656716418
[2m[36m(func pid=84655)[0m f1_micro: 0.42490671641791045
[2m[36m(func pid=84655)[0m f1_macro: 0.34865973567959985
[2m[36m(func pid=84655)[0m f1_weighted: 0.44620680383745
[2m[36m(func pid=84655)[0m f1_per_class: [0.395, 0.481, 0.245, 0.484, 0.142, 0.4, 0.482, 0.288, 0.244, 0.325]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.5052 | Steps: 4 | Val loss: 1.7282 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=77753)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.6045 | Steps: 4 | Val loss: 2.1605 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.2758 | Steps: 4 | Val loss: 6.9885 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.7745 | Steps: 4 | Val loss: 1.9365 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 12:26:32 (running for 00:28:38.11)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (8 PENDING, 3 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.468 |      0.322 |                   55 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.618 |      0.349 |                   47 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.183 |      0.296 |                   29 |
| train_9b9e8_00016 | PENDING    |                    | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=77753)[0m top1: 0.24300373134328357
[2m[36m(func pid=77753)[0m top5: 0.7332089552238806
[2m[36m(func pid=77753)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=77753)[0m f1_macro: 0.20493067421983238
[2m[36m(func pid=77753)[0m f1_weighted: 0.2651226084121599
[2m[36m(func pid=77753)[0m f1_per_class: [0.211, 0.152, 0.229, 0.387, 0.034, 0.334, 0.236, 0.22, 0.035, 0.211]
[2m[36m(func pid=82573)[0m top1: 0.3694029850746269
[2m[36m(func pid=82573)[0m top5: 0.8824626865671642
[2m[36m(func pid=82573)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=82573)[0m f1_macro: 0.32664172295904653
[2m[36m(func pid=82573)[0m f1_weighted: 0.38974640490217116
[2m[36m(func pid=82573)[0m f1_per_class: [0.421, 0.444, 0.226, 0.389, 0.081, 0.432, 0.383, 0.336, 0.238, 0.317]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.34794776119402987
[2m[36m(func pid=89025)[0m top5: 0.8111007462686567
[2m[36m(func pid=89025)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=89025)[0m f1_macro: 0.28855875764482086
[2m[36m(func pid=89025)[0m f1_weighted: 0.3465582868829019
[2m[36m(func pid=89025)[0m f1_per_class: [0.324, 0.387, 0.153, 0.477, 0.143, 0.381, 0.224, 0.302, 0.22, 0.275]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.37966417910447764
[2m[36m(func pid=84655)[0m top5: 0.8833955223880597
[2m[36m(func pid=84655)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=84655)[0m f1_macro: 0.3174359735743547
[2m[36m(func pid=84655)[0m f1_weighted: 0.38907185805260625
[2m[36m(func pid=84655)[0m f1_per_class: [0.344, 0.451, 0.295, 0.279, 0.153, 0.381, 0.516, 0.284, 0.208, 0.263]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.5170 | Steps: 4 | Val loss: 1.7236 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.2967 | Steps: 4 | Val loss: 5.3987 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=82573)[0m top1: 0.37546641791044777
[2m[36m(func pid=82573)[0m top5: 0.8857276119402985
[2m[36m(func pid=82573)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=82573)[0m f1_macro: 0.33746690369450083
[2m[36m(func pid=82573)[0m f1_weighted: 0.4018972009921802
[2m[36m(func pid=82573)[0m f1_per_class: [0.464, 0.439, 0.238, 0.434, 0.075, 0.419, 0.383, 0.335, 0.258, 0.331]
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5901 | Steps: 4 | Val loss: 2.1447 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m top1: 0.43423507462686567
[2m[36m(func pid=89025)[0m top5: 0.8810634328358209
[2m[36m(func pid=89025)[0m f1_micro: 0.43423507462686567
[2m[36m(func pid=89025)[0m f1_macro: 0.34754357495607413
[2m[36m(func pid=89025)[0m f1_weighted: 0.4438843738319945
[2m[36m(func pid=89025)[0m f1_per_class: [0.514, 0.469, 0.419, 0.554, 0.074, 0.308, 0.452, 0.287, 0.188, 0.212]
[2m[36m(func pid=84655)[0m top1: 0.3596082089552239
[2m[36m(func pid=84655)[0m top5: 0.8544776119402985
[2m[36m(func pid=84655)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=84655)[0m f1_macro: 0.3173102075708968
[2m[36m(func pid=84655)[0m f1_weighted: 0.3648286499042542
[2m[36m(func pid=84655)[0m f1_per_class: [0.432, 0.441, 0.348, 0.201, 0.107, 0.348, 0.51, 0.335, 0.222, 0.23]
== Status ==
Current time: 2024-01-07 12:26:37 (running for 00:28:43.43)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.517 |      0.337 |                   57 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.775 |      0.317 |                   48 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  2.276 |      0.289 |                   30 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=96422)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=96422)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=96422)[0m Configuration completed!
[2m[36m(func pid=96422)[0m New optimizer parameters:
[2m[36m(func pid=96422)[0m SGD (
[2m[36m(func pid=96422)[0m Parameter Group 0
[2m[36m(func pid=96422)[0m     dampening: 0
[2m[36m(func pid=96422)[0m     differentiable: False
[2m[36m(func pid=96422)[0m     foreach: None
[2m[36m(func pid=96422)[0m     lr: 0.0001
[2m[36m(func pid=96422)[0m     maximize: False
[2m[36m(func pid=96422)[0m     momentum: 0.99
[2m[36m(func pid=96422)[0m     nesterov: False
[2m[36m(func pid=96422)[0m     weight_decay: 1e-05
[2m[36m(func pid=96422)[0m )
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.4591 | Steps: 4 | Val loss: 1.7131 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 12:26:42 (running for 00:28:48.67)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.459 |      0.347 |                   58 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.59  |      0.317 |                   49 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.297 |      0.348 |                   31 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.39132462686567165
[2m[36m(func pid=82573)[0m top5: 0.8908582089552238
[2m[36m(func pid=82573)[0m f1_micro: 0.39132462686567165
[2m[36m(func pid=82573)[0m f1_macro: 0.346523301448784
[2m[36m(func pid=82573)[0m f1_weighted: 0.4180791102525529
[2m[36m(func pid=82573)[0m f1_per_class: [0.434, 0.443, 0.32, 0.445, 0.079, 0.44, 0.421, 0.33, 0.224, 0.328]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.2957 | Steps: 4 | Val loss: 6.8020 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.6503 | Steps: 4 | Val loss: 2.2931 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9669 | Steps: 4 | Val loss: 2.3296 | Batch size: 32 | lr: 0.0001 | Duration: 4.49s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.5364 | Steps: 4 | Val loss: 1.6853 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=89025)[0m top1: 0.36100746268656714
[2m[36m(func pid=89025)[0m top5: 0.8521455223880597
[2m[36m(func pid=89025)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=89025)[0m f1_macro: 0.3263942983568727
[2m[36m(func pid=89025)[0m f1_weighted: 0.377078159949908
[2m[36m(func pid=89025)[0m f1_per_class: [0.539, 0.468, 0.553, 0.515, 0.118, 0.152, 0.337, 0.219, 0.14, 0.223]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.31529850746268656
[2m[36m(func pid=84655)[0m top5: 0.8367537313432836
[2m[36m(func pid=84655)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=84655)[0m f1_macro: 0.30386814521655403
[2m[36m(func pid=84655)[0m f1_weighted: 0.32585001871403774
[2m[36m(func pid=84655)[0m f1_per_class: [0.438, 0.406, 0.381, 0.217, 0.087, 0.345, 0.386, 0.322, 0.237, 0.219]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=96422)[0m top1: 0.18330223880597016
[2m[36m(func pid=96422)[0m top5: 0.5256529850746269
[2m[36m(func pid=96422)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=96422)[0m f1_macro: 0.10899126315090805
[2m[36m(func pid=96422)[0m f1_weighted: 0.12924024965567651
[2m[36m(func pid=96422)[0m f1_per_class: [0.213, 0.341, 0.0, 0.103, 0.01, 0.274, 0.009, 0.034, 0.0, 0.105]
[2m[36m(func pid=96422)[0m 
== Status ==
Current time: 2024-01-07 12:26:48 (running for 00:28:54.03)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.536 |      0.354 |                   59 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.65  |      0.304 |                   50 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.296 |      0.326 |                   32 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.967 |      0.109 |                    1 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.39972014925373134
[2m[36m(func pid=82573)[0m top5: 0.8931902985074627
[2m[36m(func pid=82573)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=82573)[0m f1_macro: 0.35397733390612707
[2m[36m(func pid=82573)[0m f1_weighted: 0.42413857117686343
[2m[36m(func pid=82573)[0m f1_per_class: [0.459, 0.438, 0.3, 0.477, 0.076, 0.435, 0.408, 0.357, 0.234, 0.356]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.3046 | Steps: 4 | Val loss: 6.3954 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.5404 | Steps: 4 | Val loss: 2.2428 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9707 | Steps: 4 | Val loss: 2.3395 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.6052 | Steps: 4 | Val loss: 1.6874 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=89025)[0m top1: 0.3656716417910448
[2m[36m(func pid=89025)[0m top5: 0.8451492537313433
[2m[36m(func pid=89025)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=89025)[0m f1_macro: 0.3312148494818178
[2m[36m(func pid=89025)[0m f1_weighted: 0.3918927603178149
[2m[36m(func pid=89025)[0m f1_per_class: [0.523, 0.423, 0.488, 0.496, 0.113, 0.23, 0.383, 0.303, 0.19, 0.164]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.32276119402985076
[2m[36m(func pid=84655)[0m top5: 0.8493470149253731
[2m[36m(func pid=84655)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=84655)[0m f1_macro: 0.31157218339900433
[2m[36m(func pid=84655)[0m f1_weighted: 0.33851669465014006
[2m[36m(func pid=84655)[0m f1_per_class: [0.508, 0.425, 0.325, 0.281, 0.103, 0.338, 0.359, 0.314, 0.225, 0.237]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=96422)[0m top1: 0.17257462686567165
[2m[36m(func pid=96422)[0m top5: 0.5083955223880597
[2m[36m(func pid=96422)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=96422)[0m f1_macro: 0.09724473594826819
[2m[36m(func pid=96422)[0m f1_weighted: 0.12307307416024475
[2m[36m(func pid=96422)[0m f1_per_class: [0.159, 0.293, 0.0, 0.108, 0.009, 0.31, 0.006, 0.019, 0.0, 0.068]
[2m[36m(func pid=96422)[0m 
== Status ==
Current time: 2024-01-07 12:26:53 (running for 00:28:59.39)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.605 |      0.343 |                   60 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.54  |      0.312 |                   51 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.305 |      0.331 |                   33 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.971 |      0.097 |                    2 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.39505597014925375
[2m[36m(func pid=82573)[0m top5: 0.882929104477612
[2m[36m(func pid=82573)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=82573)[0m f1_macro: 0.34300482977750774
[2m[36m(func pid=82573)[0m f1_weighted: 0.4151456218560597
[2m[36m(func pid=82573)[0m f1_per_class: [0.372, 0.416, 0.32, 0.488, 0.091, 0.426, 0.388, 0.364, 0.242, 0.323]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.4148 | Steps: 4 | Val loss: 6.9275 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4667 | Steps: 4 | Val loss: 2.1463 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9137 | Steps: 4 | Val loss: 2.3426 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.4127 | Steps: 4 | Val loss: 1.6871 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=89025)[0m top1: 0.36100746268656714
[2m[36m(func pid=89025)[0m top5: 0.8311567164179104
[2m[36m(func pid=89025)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=89025)[0m f1_macro: 0.3241882414540834
[2m[36m(func pid=89025)[0m f1_weighted: 0.37595067844541324
[2m[36m(func pid=89025)[0m f1_per_class: [0.376, 0.465, 0.439, 0.391, 0.076, 0.29, 0.384, 0.327, 0.17, 0.324]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3381529850746269
[2m[36m(func pid=84655)[0m top5: 0.8614738805970149
[2m[36m(func pid=84655)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=84655)[0m f1_macro: 0.3066552545585842
[2m[36m(func pid=84655)[0m f1_weighted: 0.36355408647789256
[2m[36m(func pid=84655)[0m f1_per_class: [0.395, 0.378, 0.286, 0.399, 0.127, 0.359, 0.356, 0.335, 0.228, 0.204]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=96422)[0m top1: 0.1646455223880597
[2m[36m(func pid=96422)[0m top5: 0.5060634328358209
[2m[36m(func pid=96422)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=96422)[0m f1_macro: 0.08912754421639857
[2m[36m(func pid=96422)[0m f1_weighted: 0.12546417920173095
[2m[36m(func pid=96422)[0m f1_per_class: [0.1, 0.274, 0.0, 0.112, 0.0, 0.32, 0.021, 0.031, 0.0, 0.033]
== Status ==
Current time: 2024-01-07 12:26:58 (running for 00:29:04.40)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.605 |      0.343 |                   60 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.467 |      0.307 |                   52 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.415 |      0.324 |                   34 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.914 |      0.089 |                    3 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=82573)[0m top1: 0.40158582089552236
[2m[36m(func pid=82573)[0m top5: 0.882929104477612
[2m[36m(func pid=82573)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=82573)[0m f1_macro: 0.3398025248734349
[2m[36m(func pid=82573)[0m f1_weighted: 0.42393332680003454
[2m[36m(func pid=82573)[0m f1_per_class: [0.33, 0.44, 0.312, 0.486, 0.097, 0.421, 0.417, 0.34, 0.222, 0.333]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.4020 | Steps: 4 | Val loss: 2.0645 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.5360 | Steps: 4 | Val loss: 7.8935 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9809 | Steps: 4 | Val loss: 2.3156 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.4022 | Steps: 4 | Val loss: 1.7058 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=89025)[0m top1: 0.3101679104477612
[2m[36m(func pid=89025)[0m top5: 0.8442164179104478
[2m[36m(func pid=89025)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=89025)[0m f1_macro: 0.3145226543020224
[2m[36m(func pid=89025)[0m f1_weighted: 0.3030395944539787
[2m[36m(func pid=89025)[0m f1_per_class: [0.407, 0.488, 0.444, 0.323, 0.112, 0.289, 0.201, 0.221, 0.167, 0.492]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3605410447761194
[2m[36m(func pid=84655)[0m top5: 0.8684701492537313
[2m[36m(func pid=84655)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=84655)[0m f1_macro: 0.3033661545879262
[2m[36m(func pid=84655)[0m f1_weighted: 0.3924318564589018
[2m[36m(func pid=84655)[0m f1_per_class: [0.312, 0.315, 0.257, 0.448, 0.128, 0.377, 0.442, 0.351, 0.206, 0.197]
[2m[36m(func pid=84655)[0m 
== Status ==
Current time: 2024-01-07 12:27:04 (running for 00:29:10.13)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.413 |      0.34  |                   61 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.402 |      0.303 |                   53 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.536 |      0.315 |                   35 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.981 |      0.081 |                    4 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=96422)[0m top1: 0.1501865671641791
[2m[36m(func pid=96422)[0m top5: 0.5247201492537313
[2m[36m(func pid=96422)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=96422)[0m f1_macro: 0.0806948366105836
[2m[36m(func pid=96422)[0m f1_weighted: 0.12808582492565335
[2m[36m(func pid=96422)[0m f1_per_class: [0.036, 0.24, 0.0, 0.122, 0.012, 0.282, 0.054, 0.06, 0.0, 0.0]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=82573)[0m top1: 0.384794776119403
[2m[36m(func pid=82573)[0m top5: 0.8791977611940298
[2m[36m(func pid=82573)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=82573)[0m f1_macro: 0.329957734375789
[2m[36m(func pid=82573)[0m f1_weighted: 0.40532552476302103
[2m[36m(func pid=82573)[0m f1_per_class: [0.337, 0.403, 0.316, 0.489, 0.089, 0.413, 0.38, 0.308, 0.241, 0.324]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.9489 | Steps: 4 | Val loss: 7.5391 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.4014 | Steps: 4 | Val loss: 1.9953 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9744 | Steps: 4 | Val loss: 2.3108 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.4017 | Steps: 4 | Val loss: 1.6860 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=89025)[0m top1: 0.3246268656716418
[2m[36m(func pid=89025)[0m top5: 0.8390858208955224
[2m[36m(func pid=89025)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=89025)[0m f1_macro: 0.3448278895754843
[2m[36m(func pid=89025)[0m f1_weighted: 0.3260592320272242
[2m[36m(func pid=89025)[0m f1_per_class: [0.529, 0.435, 0.491, 0.509, 0.132, 0.271, 0.126, 0.225, 0.201, 0.531]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.37453358208955223
[2m[36m(func pid=84655)[0m top5: 0.8810634328358209
[2m[36m(func pid=84655)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=84655)[0m f1_macro: 0.3057765960002371
[2m[36m(func pid=84655)[0m f1_weighted: 0.40225179701990477
[2m[36m(func pid=84655)[0m f1_per_class: [0.336, 0.308, 0.263, 0.482, 0.142, 0.38, 0.459, 0.271, 0.211, 0.207]
[2m[36m(func pid=84655)[0m 
== Status ==
Current time: 2024-01-07 12:27:10 (running for 00:29:16.06)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.402 |      0.338 |                   63 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.401 |      0.306 |                   54 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  0.949 |      0.345 |                   36 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.981 |      0.081 |                    4 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3903917910447761
[2m[36m(func pid=82573)[0m top5: 0.8824626865671642
[2m[36m(func pid=82573)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=82573)[0m f1_macro: 0.3375039551031827
[2m[36m(func pid=82573)[0m f1_weighted: 0.408063922569965
[2m[36m(func pid=82573)[0m f1_per_class: [0.41, 0.395, 0.3, 0.503, 0.083, 0.412, 0.368, 0.359, 0.238, 0.308]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=96422)[0m top1: 0.14458955223880596
[2m[36m(func pid=96422)[0m top5: 0.5354477611940298
[2m[36m(func pid=96422)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=96422)[0m f1_macro: 0.09118627252897475
[2m[36m(func pid=96422)[0m f1_weighted: 0.13066400230853817
[2m[36m(func pid=96422)[0m f1_per_class: [0.035, 0.21, 0.108, 0.129, 0.011, 0.288, 0.069, 0.061, 0.0, 0.0]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.7701 | Steps: 4 | Val loss: 7.8205 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.8252 | Steps: 4 | Val loss: 2.1395 | Batch size: 32 | lr: 0.01 | Duration: 3.24s
[2m[36m(func pid=89025)[0m top1: 0.30736940298507465
[2m[36m(func pid=89025)[0m top5: 0.8540111940298507
[2m[36m(func pid=89025)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=89025)[0m f1_macro: 0.3095569165614874
[2m[36m(func pid=89025)[0m f1_weighted: 0.3344165822808402
[2m[36m(func pid=89025)[0m f1_per_class: [0.493, 0.298, 0.456, 0.532, 0.13, 0.241, 0.243, 0.218, 0.12, 0.364]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.4957 | Steps: 4 | Val loss: 1.7058 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=84655)[0m top1: 0.3414179104477612
[2m[36m(func pid=84655)[0m top5: 0.8642723880597015
[2m[36m(func pid=84655)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=84655)[0m f1_macro: 0.2845780512908204
[2m[36m(func pid=84655)[0m f1_weighted: 0.37816837349623433
[2m[36m(func pid=84655)[0m f1_per_class: [0.353, 0.328, 0.154, 0.413, 0.105, 0.378, 0.436, 0.271, 0.193, 0.216]
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9089 | Steps: 4 | Val loss: 2.3048 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=84655)[0m 
== Status ==
Current time: 2024-01-07 12:27:15 (running for 00:29:21.35)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.496 |      0.326 |                   64 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.825 |      0.285 |                   55 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  0.77  |      0.31  |                   37 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.974 |      0.091 |                    5 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3810634328358209
[2m[36m(func pid=82573)[0m top5: 0.8773320895522388
[2m[36m(func pid=82573)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=82573)[0m f1_macro: 0.32639804282631
[2m[36m(func pid=82573)[0m f1_weighted: 0.3984373651607055
[2m[36m(func pid=82573)[0m f1_per_class: [0.366, 0.371, 0.261, 0.504, 0.101, 0.427, 0.348, 0.353, 0.233, 0.299]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=96422)[0m top1: 0.1478544776119403
[2m[36m(func pid=96422)[0m top5: 0.5541044776119403
[2m[36m(func pid=96422)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=96422)[0m f1_macro: 0.09952912263823001
[2m[36m(func pid=96422)[0m f1_weighted: 0.14503453146104175
[2m[36m(func pid=96422)[0m f1_per_class: [0.052, 0.201, 0.108, 0.156, 0.016, 0.286, 0.093, 0.069, 0.012, 0.0]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.5611 | Steps: 4 | Val loss: 6.7685 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.9777 | Steps: 4 | Val loss: 2.1331 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.2749 | Steps: 4 | Val loss: 1.7246 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=89025)[0m top1: 0.3512126865671642
[2m[36m(func pid=89025)[0m top5: 0.8791977611940298
[2m[36m(func pid=89025)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=89025)[0m f1_macro: 0.29672039411000034
[2m[36m(func pid=89025)[0m f1_weighted: 0.37488251402217676
[2m[36m(func pid=89025)[0m f1_per_class: [0.455, 0.307, 0.394, 0.519, 0.126, 0.214, 0.427, 0.068, 0.172, 0.286]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.333955223880597
[2m[36m(func pid=84655)[0m top5: 0.8647388059701493
[2m[36m(func pid=84655)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=84655)[0m f1_macro: 0.29090082458948097
[2m[36m(func pid=84655)[0m f1_weighted: 0.3718109964272458
[2m[36m(func pid=84655)[0m f1_per_class: [0.427, 0.339, 0.191, 0.385, 0.122, 0.34, 0.439, 0.297, 0.188, 0.181]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9288 | Steps: 4 | Val loss: 2.2973 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 12:27:20 (running for 00:29:26.58)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.275 |      0.32  |                   65 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.978 |      0.291 |                   56 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.561 |      0.297 |                   38 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.909 |      0.1   |                    6 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.37173507462686567
[2m[36m(func pid=82573)[0m top5: 0.875
[2m[36m(func pid=82573)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=82573)[0m f1_macro: 0.3204989370491452
[2m[36m(func pid=82573)[0m f1_weighted: 0.3891171471695003
[2m[36m(func pid=82573)[0m f1_per_class: [0.39, 0.363, 0.25, 0.507, 0.108, 0.429, 0.322, 0.339, 0.214, 0.284]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.0203 | Steps: 4 | Val loss: 6.2226 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=96422)[0m top1: 0.14458955223880596
[2m[36m(func pid=96422)[0m top5: 0.570429104477612
[2m[36m(func pid=96422)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=96422)[0m f1_macro: 0.09974508447333563
[2m[36m(func pid=96422)[0m f1_weighted: 0.1447597747601525
[2m[36m(func pid=96422)[0m f1_per_class: [0.048, 0.182, 0.114, 0.176, 0.017, 0.28, 0.085, 0.083, 0.012, 0.0]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4171 | Steps: 4 | Val loss: 1.9884 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.3086 | Steps: 4 | Val loss: 1.7159 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=89025)[0m top1: 0.3833955223880597
[2m[36m(func pid=89025)[0m top5: 0.8642723880597015
[2m[36m(func pid=89025)[0m f1_micro: 0.3833955223880597
[2m[36m(func pid=89025)[0m f1_macro: 0.3167891201840618
[2m[36m(func pid=89025)[0m f1_weighted: 0.40784109124601425
[2m[36m(func pid=89025)[0m f1_per_class: [0.32, 0.37, 0.276, 0.525, 0.111, 0.28, 0.431, 0.293, 0.217, 0.345]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3670708955223881
[2m[36m(func pid=84655)[0m top5: 0.8885261194029851
[2m[36m(func pid=84655)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=84655)[0m f1_macro: 0.3231600255196155
[2m[36m(func pid=84655)[0m f1_weighted: 0.3993543642755054
[2m[36m(func pid=84655)[0m f1_per_class: [0.485, 0.402, 0.26, 0.441, 0.135, 0.329, 0.435, 0.309, 0.206, 0.23]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9069 | Steps: 4 | Val loss: 2.2992 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 12:27:25 (running for 00:29:31.89)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.309 |      0.327 |                   66 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.417 |      0.323 |                   57 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.02  |      0.317 |                   39 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.929 |      0.1   |                    7 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3763992537313433
[2m[36m(func pid=82573)[0m top5: 0.8754664179104478
[2m[36m(func pid=82573)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=82573)[0m f1_macro: 0.32710020416534275
[2m[36m(func pid=82573)[0m f1_weighted: 0.38944196822802624
[2m[36m(func pid=82573)[0m f1_per_class: [0.44, 0.38, 0.233, 0.522, 0.114, 0.426, 0.294, 0.355, 0.202, 0.306]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6060 | Steps: 4 | Val loss: 8.2297 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4139 | Steps: 4 | Val loss: 1.9779 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=96422)[0m top1: 0.1501865671641791
[2m[36m(func pid=96422)[0m top5: 0.5685634328358209
[2m[36m(func pid=96422)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=96422)[0m f1_macro: 0.10750101926263031
[2m[36m(func pid=96422)[0m f1_weighted: 0.15132198742181915
[2m[36m(func pid=96422)[0m f1_per_class: [0.071, 0.173, 0.103, 0.192, 0.028, 0.297, 0.083, 0.118, 0.011, 0.0]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.3779 | Steps: 4 | Val loss: 1.7556 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=89025)[0m top1: 0.34468283582089554
[2m[36m(func pid=89025)[0m top5: 0.7910447761194029
[2m[36m(func pid=89025)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=89025)[0m f1_macro: 0.2785881707819076
[2m[36m(func pid=89025)[0m f1_weighted: 0.31462444878895063
[2m[36m(func pid=89025)[0m f1_per_class: [0.262, 0.277, 0.264, 0.575, 0.116, 0.228, 0.141, 0.311, 0.255, 0.357]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3763992537313433
[2m[36m(func pid=84655)[0m top5: 0.8871268656716418
[2m[36m(func pid=84655)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=84655)[0m f1_macro: 0.33135581083981047
[2m[36m(func pid=84655)[0m f1_weighted: 0.4040158440891369
[2m[36m(func pid=84655)[0m f1_per_class: [0.509, 0.418, 0.248, 0.457, 0.164, 0.322, 0.424, 0.318, 0.208, 0.245]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8782 | Steps: 4 | Val loss: 2.2929 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=82573)[0m top1: 0.3558768656716418
[2m[36m(func pid=82573)[0m top5: 0.8600746268656716
[2m[36m(func pid=82573)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=82573)[0m f1_macro: 0.31913938039053547
[2m[36m(func pid=82573)[0m f1_weighted: 0.36927815066609904
[2m[36m(func pid=82573)[0m f1_per_class: [0.418, 0.379, 0.253, 0.506, 0.116, 0.419, 0.249, 0.354, 0.168, 0.331]
[2m[36m(func pid=82573)[0m 
== Status ==
Current time: 2024-01-07 12:27:31 (running for 00:29:37.21)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.378 |      0.319 |                   67 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.414 |      0.331 |                   58 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  0.606 |      0.279 |                   40 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.907 |      0.108 |                    8 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.5455 | Steps: 4 | Val loss: 6.9559 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7258 | Steps: 4 | Val loss: 1.9689 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=96422)[0m top1: 0.15671641791044777
[2m[36m(func pid=96422)[0m top5: 0.5886194029850746
[2m[36m(func pid=96422)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=96422)[0m f1_macro: 0.11882248598101704
[2m[36m(func pid=96422)[0m f1_weighted: 0.15995769603320023
[2m[36m(func pid=96422)[0m f1_per_class: [0.085, 0.153, 0.154, 0.214, 0.036, 0.312, 0.092, 0.131, 0.011, 0.0]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.3469 | Steps: 4 | Val loss: 1.7431 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=89025)[0m top1: 0.36380597014925375
[2m[36m(func pid=89025)[0m top5: 0.8297574626865671
[2m[36m(func pid=89025)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=89025)[0m f1_macro: 0.29160803524756335
[2m[36m(func pid=89025)[0m f1_weighted: 0.37626598055826516
[2m[36m(func pid=89025)[0m f1_per_class: [0.247, 0.405, 0.165, 0.543, 0.112, 0.259, 0.298, 0.307, 0.248, 0.333]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3843283582089552
[2m[36m(func pid=84655)[0m top5: 0.8819962686567164
[2m[36m(func pid=84655)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=84655)[0m f1_macro: 0.3373265563140345
[2m[36m(func pid=84655)[0m f1_weighted: 0.40457721678599007
[2m[36m(func pid=84655)[0m f1_per_class: [0.523, 0.423, 0.25, 0.492, 0.165, 0.367, 0.369, 0.338, 0.205, 0.24]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8746 | Steps: 4 | Val loss: 2.2863 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 12:27:36 (running for 00:29:42.46)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.347 |      0.321 |                   68 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.726 |      0.337 |                   59 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  2.545 |      0.292 |                   41 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.878 |      0.119 |                    9 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3614738805970149
[2m[36m(func pid=82573)[0m top5: 0.8642723880597015
[2m[36m(func pid=82573)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=82573)[0m f1_macro: 0.3210862086770304
[2m[36m(func pid=82573)[0m f1_weighted: 0.37232356833326047
[2m[36m(func pid=82573)[0m f1_per_class: [0.456, 0.383, 0.222, 0.522, 0.108, 0.416, 0.239, 0.359, 0.18, 0.326]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.2433 | Steps: 4 | Val loss: 7.5150 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4256 | Steps: 4 | Val loss: 2.1092 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=96422)[0m top1: 0.14738805970149255
[2m[36m(func pid=96422)[0m top5: 0.5979477611940298
[2m[36m(func pid=96422)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=96422)[0m f1_macro: 0.11388307135810669
[2m[36m(func pid=96422)[0m f1_weighted: 0.16203162563862689
[2m[36m(func pid=96422)[0m f1_per_class: [0.079, 0.146, 0.133, 0.229, 0.035, 0.276, 0.106, 0.114, 0.021, 0.0]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.3654 | Steps: 4 | Val loss: 1.7293 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=89025)[0m top1: 0.37779850746268656
[2m[36m(func pid=89025)[0m top5: 0.8619402985074627
[2m[36m(func pid=89025)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=89025)[0m f1_macro: 0.2883101044141848
[2m[36m(func pid=89025)[0m f1_weighted: 0.3537184744395999
[2m[36m(func pid=89025)[0m f1_per_class: [0.406, 0.519, 0.127, 0.194, 0.104, 0.138, 0.55, 0.14, 0.205, 0.5]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.35074626865671643
[2m[36m(func pid=84655)[0m top5: 0.8666044776119403
[2m[36m(func pid=84655)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=84655)[0m f1_macro: 0.29605123291949104
[2m[36m(func pid=84655)[0m f1_weighted: 0.3788357287062317
[2m[36m(func pid=84655)[0m f1_per_class: [0.387, 0.397, 0.133, 0.455, 0.12, 0.373, 0.347, 0.33, 0.184, 0.235]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8530 | Steps: 4 | Val loss: 2.2809 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 12:27:41 (running for 00:29:47.72)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.365 |      0.322 |                   69 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.426 |      0.296 |                   60 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.243 |      0.288 |                   42 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.875 |      0.114 |                   10 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.363339552238806
[2m[36m(func pid=82573)[0m top5: 0.8722014925373134
[2m[36m(func pid=82573)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=82573)[0m f1_macro: 0.32170723648290206
[2m[36m(func pid=82573)[0m f1_weighted: 0.3773599077645578
[2m[36m(func pid=82573)[0m f1_per_class: [0.459, 0.37, 0.255, 0.512, 0.117, 0.433, 0.273, 0.323, 0.177, 0.297]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.4002 | Steps: 4 | Val loss: 7.7253 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.7996 | Steps: 4 | Val loss: 2.2498 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=96422)[0m top1: 0.15391791044776118
[2m[36m(func pid=96422)[0m top5: 0.6100746268656716
[2m[36m(func pid=96422)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=96422)[0m f1_macro: 0.11970954572790109
[2m[36m(func pid=96422)[0m f1_weighted: 0.16758123287022575
[2m[36m(func pid=96422)[0m f1_per_class: [0.12, 0.146, 0.105, 0.222, 0.035, 0.299, 0.116, 0.131, 0.023, 0.0]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.4517 | Steps: 4 | Val loss: 1.7405 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=89025)[0m top1: 0.3614738805970149
[2m[36m(func pid=89025)[0m top5: 0.8502798507462687
[2m[36m(func pid=89025)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=89025)[0m f1_macro: 0.2962244916883733
[2m[36m(func pid=89025)[0m f1_weighted: 0.33996792892790545
[2m[36m(func pid=89025)[0m f1_per_class: [0.476, 0.512, 0.125, 0.13, 0.122, 0.152, 0.546, 0.208, 0.183, 0.508]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.31716417910447764
[2m[36m(func pid=84655)[0m top5: 0.84375
[2m[36m(func pid=84655)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=84655)[0m f1_macro: 0.2742749529331735
[2m[36m(func pid=84655)[0m f1_weighted: 0.3496315459691015
[2m[36m(func pid=84655)[0m f1_per_class: [0.339, 0.343, 0.106, 0.419, 0.132, 0.352, 0.327, 0.328, 0.188, 0.209]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8060 | Steps: 4 | Val loss: 2.2840 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 12:27:47 (running for 00:29:53.01)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.452 |      0.316 |                   70 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.8   |      0.274 |                   61 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.4   |      0.296 |                   43 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.853 |      0.12  |                   11 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3558768656716418
[2m[36m(func pid=82573)[0m top5: 0.8656716417910447
[2m[36m(func pid=82573)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=82573)[0m f1_macro: 0.31619187487861145
[2m[36m(func pid=82573)[0m f1_weighted: 0.3719439968203213
[2m[36m(func pid=82573)[0m f1_per_class: [0.424, 0.353, 0.247, 0.501, 0.142, 0.433, 0.278, 0.322, 0.174, 0.288]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.9459 | Steps: 4 | Val loss: 8.1809 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4291 | Steps: 4 | Val loss: 2.0885 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=96422)[0m top1: 0.14925373134328357
[2m[36m(func pid=96422)[0m top5: 0.6114738805970149
[2m[36m(func pid=96422)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=96422)[0m f1_macro: 0.12052219175469034
[2m[36m(func pid=96422)[0m f1_weighted: 0.1676641179435384
[2m[36m(func pid=96422)[0m f1_per_class: [0.127, 0.151, 0.113, 0.229, 0.033, 0.286, 0.111, 0.132, 0.022, 0.0]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.3102 | Steps: 4 | Val loss: 1.7412 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=89025)[0m top1: 0.27052238805970147
[2m[36m(func pid=89025)[0m top5: 0.804570895522388
[2m[36m(func pid=89025)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=89025)[0m f1_macro: 0.2642573108536387
[2m[36m(func pid=89025)[0m f1_weighted: 0.30510019513016085
[2m[36m(func pid=89025)[0m f1_per_class: [0.495, 0.375, 0.073, 0.242, 0.144, 0.267, 0.358, 0.269, 0.172, 0.247]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3628731343283582
[2m[36m(func pid=84655)[0m top5: 0.867070895522388
[2m[36m(func pid=84655)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=84655)[0m f1_macro: 0.29584192890770333
[2m[36m(func pid=84655)[0m f1_weighted: 0.3897238762694603
[2m[36m(func pid=84655)[0m f1_per_class: [0.373, 0.302, 0.147, 0.512, 0.135, 0.382, 0.383, 0.327, 0.19, 0.208]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=82573)[0m top1: 0.3582089552238806
[2m[36m(func pid=82573)[0m top5: 0.8656716417910447
[2m[36m(func pid=82573)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=82573)[0m f1_macro: 0.3169500454466874
[2m[36m(func pid=82573)[0m f1_weighted: 0.3742283434686301
[2m[36m(func pid=82573)[0m f1_per_class: [0.431, 0.348, 0.255, 0.494, 0.153, 0.442, 0.288, 0.349, 0.17, 0.239]
[2m[36m(func pid=82573)[0m 
== Status ==
Current time: 2024-01-07 12:27:52 (running for 00:29:58.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.31  |      0.317 |                   71 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.429 |      0.296 |                   62 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  2.946 |      0.264 |                   44 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.806 |      0.121 |                   12 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.7881 | Steps: 4 | Val loss: 2.2691 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.5565 | Steps: 4 | Val loss: 10.6153 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4688 | Steps: 4 | Val loss: 2.1284 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=96422)[0m top1: 0.15485074626865672
[2m[36m(func pid=96422)[0m top5: 0.6352611940298507
[2m[36m(func pid=96422)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=96422)[0m f1_macro: 0.12536340260959863
[2m[36m(func pid=96422)[0m f1_weighted: 0.17623670178915748
[2m[36m(func pid=96422)[0m f1_per_class: [0.115, 0.153, 0.133, 0.253, 0.033, 0.276, 0.117, 0.148, 0.025, 0.0]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.4081 | Steps: 4 | Val loss: 1.7897 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=89025)[0m top1: 0.22901119402985073
[2m[36m(func pid=89025)[0m top5: 0.7220149253731343
[2m[36m(func pid=89025)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=89025)[0m f1_macro: 0.22810985701389302
[2m[36m(func pid=89025)[0m f1_weighted: 0.22425138253653734
[2m[36m(func pid=89025)[0m f1_per_class: [0.538, 0.126, 0.048, 0.42, 0.128, 0.267, 0.057, 0.282, 0.204, 0.211]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.35027985074626866
[2m[36m(func pid=84655)[0m top5: 0.8656716417910447
[2m[36m(func pid=84655)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=84655)[0m f1_macro: 0.2937176558575125
[2m[36m(func pid=84655)[0m f1_weighted: 0.37776793602209235
[2m[36m(func pid=84655)[0m f1_per_class: [0.418, 0.298, 0.153, 0.495, 0.12, 0.377, 0.362, 0.322, 0.175, 0.217]
[2m[36m(func pid=84655)[0m 
== Status ==
Current time: 2024-01-07 12:27:57 (running for 00:30:03.67)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.408 |      0.298 |                   72 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.469 |      0.294 |                   63 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.557 |      0.228 |                   45 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.788 |      0.125 |                   13 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.34095149253731344
[2m[36m(func pid=82573)[0m top5: 0.8484141791044776
[2m[36m(func pid=82573)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=82573)[0m f1_macro: 0.29768630227490495
[2m[36m(func pid=82573)[0m f1_weighted: 0.3583706774741268
[2m[36m(func pid=82573)[0m f1_per_class: [0.395, 0.331, 0.175, 0.482, 0.12, 0.421, 0.265, 0.364, 0.186, 0.238]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7919 | Steps: 4 | Val loss: 2.2641 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 3.1427 | Steps: 4 | Val loss: 9.7347 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5810 | Steps: 4 | Val loss: 2.1929 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.2196 | Steps: 4 | Val loss: 1.7928 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=96422)[0m top1: 0.15438432835820895
[2m[36m(func pid=96422)[0m top5: 0.6436567164179104
[2m[36m(func pid=96422)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=96422)[0m f1_macro: 0.12777778219405161
[2m[36m(func pid=96422)[0m f1_weighted: 0.17699501032832926
[2m[36m(func pid=96422)[0m f1_per_class: [0.122, 0.158, 0.16, 0.27, 0.027, 0.248, 0.109, 0.162, 0.022, 0.0]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=89025)[0m top1: 0.30363805970149255
[2m[36m(func pid=89025)[0m top5: 0.7504664179104478
[2m[36m(func pid=89025)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=89025)[0m f1_macro: 0.26638770873948187
[2m[36m(func pid=89025)[0m f1_weighted: 0.26352813684657206
[2m[36m(func pid=89025)[0m f1_per_class: [0.505, 0.308, 0.182, 0.487, 0.127, 0.262, 0.022, 0.258, 0.227, 0.286]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3423507462686567
[2m[36m(func pid=84655)[0m top5: 0.8596082089552238
[2m[36m(func pid=84655)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=84655)[0m f1_macro: 0.2974703286088692
[2m[36m(func pid=84655)[0m f1_weighted: 0.37022537739246125
[2m[36m(func pid=84655)[0m f1_per_class: [0.369, 0.302, 0.224, 0.479, 0.153, 0.353, 0.356, 0.336, 0.185, 0.218]
[2m[36m(func pid=84655)[0m 
== Status ==
Current time: 2024-01-07 12:28:03 (running for 00:30:08.96)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.22  |      0.294 |                   73 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.581 |      0.297 |                   64 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  3.143 |      0.266 |                   46 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.792 |      0.128 |                   14 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.33722014925373134
[2m[36m(func pid=82573)[0m top5: 0.847481343283582
[2m[36m(func pid=82573)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=82573)[0m f1_macro: 0.29400408764600094
[2m[36m(func pid=82573)[0m f1_weighted: 0.35314531807778127
[2m[36m(func pid=82573)[0m f1_per_class: [0.38, 0.318, 0.167, 0.47, 0.12, 0.433, 0.261, 0.366, 0.195, 0.229]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7884 | Steps: 4 | Val loss: 2.2500 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.6913 | Steps: 4 | Val loss: 8.6384 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.3649 | Steps: 4 | Val loss: 2.2320 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.2169 | Steps: 4 | Val loss: 1.8127 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=96422)[0m top1: 0.1571828358208955
[2m[36m(func pid=96422)[0m top5: 0.6655783582089553
[2m[36m(func pid=96422)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=96422)[0m f1_macro: 0.12790428939394968
[2m[36m(func pid=96422)[0m f1_weighted: 0.1833398607187469
[2m[36m(func pid=96422)[0m f1_per_class: [0.118, 0.142, 0.148, 0.273, 0.027, 0.245, 0.137, 0.164, 0.024, 0.0]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=89025)[0m top1: 0.2896455223880597
[2m[36m(func pid=89025)[0m top5: 0.7761194029850746
[2m[36m(func pid=89025)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=89025)[0m f1_macro: 0.3110787723611813
[2m[36m(func pid=89025)[0m f1_weighted: 0.2835545459981655
[2m[36m(func pid=89025)[0m f1_per_class: [0.268, 0.464, 0.526, 0.251, 0.091, 0.301, 0.205, 0.254, 0.244, 0.506]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.33955223880597013
[2m[36m(func pid=84655)[0m top5: 0.8540111940298507
[2m[36m(func pid=84655)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=84655)[0m f1_macro: 0.2983811347991423
[2m[36m(func pid=84655)[0m f1_weighted: 0.37143908771790557
[2m[36m(func pid=84655)[0m f1_per_class: [0.383, 0.334, 0.236, 0.44, 0.12, 0.36, 0.378, 0.317, 0.191, 0.224]
[2m[36m(func pid=84655)[0m 
== Status ==
Current time: 2024-01-07 12:28:08 (running for 00:30:14.03)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.321
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 4 RUNNING, 13 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00013 | RUNNING    | 192.168.7.53:82573 | 0.001  |       0.9  |         0.0001 |  1.217 |      0.293 |                   74 |
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.365 |      0.298 |                   65 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.691 |      0.311 |                   47 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.788 |      0.128 |                   15 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.3376865671641791
[2m[36m(func pid=82573)[0m top5: 0.8423507462686567
[2m[36m(func pid=82573)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=82573)[0m f1_macro: 0.2929251570316924
[2m[36m(func pid=82573)[0m f1_weighted: 0.3508039540024577
[2m[36m(func pid=82573)[0m f1_per_class: [0.365, 0.297, 0.182, 0.489, 0.109, 0.416, 0.254, 0.361, 0.219, 0.238]
[2m[36m(func pid=82573)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8237 | Steps: 4 | Val loss: 2.2379 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.4018 | Steps: 4 | Val loss: 9.4219 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.6155 | Steps: 4 | Val loss: 2.2978 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=82573)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.3117 | Steps: 4 | Val loss: 1.7687 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=96422)[0m top1: 0.16511194029850745
[2m[36m(func pid=96422)[0m top5: 0.6772388059701493
[2m[36m(func pid=96422)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=96422)[0m f1_macro: 0.13165862691247604
[2m[36m(func pid=96422)[0m f1_weighted: 0.1946243369604081
[2m[36m(func pid=96422)[0m f1_per_class: [0.12, 0.144, 0.172, 0.288, 0.024, 0.205, 0.175, 0.161, 0.027, 0.0]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=89025)[0m top1: 0.27705223880597013
[2m[36m(func pid=89025)[0m top5: 0.7518656716417911
[2m[36m(func pid=89025)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=89025)[0m f1_macro: 0.2832623371961026
[2m[36m(func pid=89025)[0m f1_weighted: 0.28451732409906183
[2m[36m(func pid=89025)[0m f1_per_class: [0.169, 0.398, 0.769, 0.087, 0.081, 0.274, 0.457, 0.075, 0.23, 0.292]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.333955223880597
[2m[36m(func pid=84655)[0m top5: 0.8456156716417911
[2m[36m(func pid=84655)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=84655)[0m f1_macro: 0.29777150149181614
[2m[36m(func pid=84655)[0m f1_weighted: 0.36668587002534425
[2m[36m(func pid=84655)[0m f1_per_class: [0.373, 0.368, 0.26, 0.392, 0.097, 0.35, 0.388, 0.332, 0.198, 0.219]
[2m[36m(func pid=84655)[0m 
== Status ==
Current time: 2024-01-07 12:28:13 (running for 00:30:19.25)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (7 PENDING, 3 RUNNING, 14 TERMINATED)
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655 | 0.01   |       0.9  |         0.0001 |  0.616 |      0.298 |                   66 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025 | 0.1    |       0.9  |         0.0001 |  1.402 |      0.283 |                   48 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422 | 0.0001 |       0.99 |         1e-05  |  2.824 |      0.132 |                   16 |
| train_9b9e8_00017 | PENDING    |                    | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                    | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                    | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                    | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                    | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                    | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                    | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163 | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543 | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969 | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392 | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840 | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540 | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161 | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180 | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891 | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137 | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
+-------------------+------------+--------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=82573)[0m top1: 0.345615671641791
[2m[36m(func pid=82573)[0m top5: 0.8568097014925373
[2m[36m(func pid=82573)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=82573)[0m f1_macro: 0.3015445594628735
[2m[36m(func pid=82573)[0m f1_weighted: 0.35575066357389257
[2m[36m(func pid=82573)[0m f1_per_class: [0.389, 0.295, 0.229, 0.492, 0.137, 0.442, 0.262, 0.34, 0.189, 0.241]
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.7203 | Steps: 4 | Val loss: 2.2248 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.0796 | Steps: 4 | Val loss: 8.4816 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4264 | Steps: 4 | Val loss: 2.2801 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=96422)[0m top1: 0.17583955223880596
[2m[36m(func pid=96422)[0m top5: 0.6921641791044776
[2m[36m(func pid=96422)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=96422)[0m f1_macro: 0.14824564687024017
[2m[36m(func pid=96422)[0m f1_weighted: 0.20572175816433122
[2m[36m(func pid=96422)[0m f1_per_class: [0.133, 0.165, 0.226, 0.303, 0.029, 0.195, 0.186, 0.156, 0.03, 0.059]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=89025)[0m top1: 0.3101679104477612
[2m[36m(func pid=89025)[0m top5: 0.7831156716417911
[2m[36m(func pid=89025)[0m f1_micro: 0.3101679104477612
[2m[36m(func pid=89025)[0m f1_macro: 0.3219503458314495
[2m[36m(func pid=89025)[0m f1_weighted: 0.31940657020762614
[2m[36m(func pid=89025)[0m f1_per_class: [0.384, 0.421, 0.8, 0.167, 0.08, 0.271, 0.455, 0.156, 0.258, 0.226]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3423507462686567
[2m[36m(func pid=84655)[0m top5: 0.8484141791044776
[2m[36m(func pid=84655)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=84655)[0m f1_macro: 0.31032222484750727
[2m[36m(func pid=84655)[0m f1_weighted: 0.3649842996747922
[2m[36m(func pid=84655)[0m f1_per_class: [0.382, 0.426, 0.321, 0.382, 0.093, 0.344, 0.354, 0.334, 0.231, 0.237]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.1378 | Steps: 4 | Val loss: 8.3551 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.6656 | Steps: 4 | Val loss: 2.2152 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.5746 | Steps: 4 | Val loss: 2.2704 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=101169)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=101169)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=101169)[0m Configuration completed!
[2m[36m(func pid=101169)[0m New optimizer parameters:
[2m[36m(func pid=101169)[0m SGD (
[2m[36m(func pid=101169)[0m Parameter Group 0
[2m[36m(func pid=101169)[0m     dampening: 0
[2m[36m(func pid=101169)[0m     differentiable: False
[2m[36m(func pid=101169)[0m     foreach: None
[2m[36m(func pid=101169)[0m     lr: 0.001
[2m[36m(func pid=101169)[0m     maximize: False
[2m[36m(func pid=101169)[0m     momentum: 0.99
[2m[36m(func pid=101169)[0m     nesterov: False
[2m[36m(func pid=101169)[0m     weight_decay: 1e-05
[2m[36m(func pid=101169)[0m )
[2m[36m(func pid=101169)[0m 
== Status ==
Current time: 2024-01-07 12:28:22 (running for 00:30:28.80)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.32075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.426 |      0.31  |                   67 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  2.138 |      0.326 |                   50 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.72  |      0.148 |                   17 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.29197761194029853
[2m[36m(func pid=89025)[0m top5: 0.8148320895522388
[2m[36m(func pid=89025)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=89025)[0m f1_macro: 0.32599574534532855
[2m[36m(func pid=89025)[0m f1_weighted: 0.3132965319611629
[2m[36m(func pid=89025)[0m f1_per_class: [0.5, 0.395, 0.733, 0.306, 0.128, 0.292, 0.303, 0.182, 0.248, 0.172]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=96422)[0m top1: 0.17817164179104478
[2m[36m(func pid=96422)[0m top5: 0.6893656716417911
[2m[36m(func pid=96422)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=96422)[0m f1_macro: 0.15019771024068734
[2m[36m(func pid=96422)[0m f1_weighted: 0.20287623588747195
[2m[36m(func pid=96422)[0m f1_per_class: [0.147, 0.158, 0.25, 0.313, 0.035, 0.17, 0.177, 0.17, 0.029, 0.053]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=84655)[0m top1: 0.35027985074626866
[2m[36m(func pid=84655)[0m top5: 0.8446828358208955
[2m[36m(func pid=84655)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=84655)[0m f1_macro: 0.3243402006177504
[2m[36m(func pid=84655)[0m f1_weighted: 0.35399460641188796
[2m[36m(func pid=84655)[0m f1_per_class: [0.446, 0.465, 0.356, 0.339, 0.087, 0.332, 0.325, 0.365, 0.246, 0.282]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.7226 | Steps: 4 | Val loss: 9.9901 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7174 | Steps: 4 | Val loss: 2.2072 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0126 | Steps: 4 | Val loss: 2.3095 | Batch size: 32 | lr: 0.001 | Duration: 4.43s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3590 | Steps: 4 | Val loss: 2.1440 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 12:28:28 (running for 00:30:34.10)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.32075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.575 |      0.324 |                   68 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.723 |      0.268 |                   51 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.666 |      0.15  |                   18 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.2728544776119403
[2m[36m(func pid=89025)[0m top5: 0.7541977611940298
[2m[36m(func pid=89025)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=89025)[0m f1_macro: 0.26816158271698715
[2m[36m(func pid=89025)[0m f1_weighted: 0.25665609927116806
[2m[36m(func pid=89025)[0m f1_per_class: [0.558, 0.316, 0.2, 0.396, 0.134, 0.291, 0.065, 0.236, 0.288, 0.197]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=101169)[0m top1: 0.18190298507462688
[2m[36m(func pid=101169)[0m top5: 0.5438432835820896
[2m[36m(func pid=101169)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=101169)[0m f1_macro: 0.11988177502636028
[2m[36m(func pid=101169)[0m f1_weighted: 0.13463983454819103
[2m[36m(func pid=101169)[0m f1_per_class: [0.341, 0.339, 0.0, 0.126, 0.009, 0.24, 0.012, 0.035, 0.0, 0.097]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=96422)[0m top1: 0.18983208955223882
[2m[36m(func pid=96422)[0m top5: 0.691231343283582
[2m[36m(func pid=96422)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=96422)[0m f1_macro: 0.16071049515325142
[2m[36m(func pid=96422)[0m f1_weighted: 0.2138734321629613
[2m[36m(func pid=96422)[0m f1_per_class: [0.144, 0.153, 0.22, 0.336, 0.031, 0.157, 0.192, 0.192, 0.046, 0.137]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3773320895522388
[2m[36m(func pid=84655)[0m top5: 0.8680037313432836
[2m[36m(func pid=84655)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=84655)[0m f1_macro: 0.3419465763805864
[2m[36m(func pid=84655)[0m f1_weighted: 0.3859956419544879
[2m[36m(func pid=84655)[0m f1_per_class: [0.475, 0.474, 0.377, 0.423, 0.107, 0.375, 0.33, 0.372, 0.25, 0.237]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5808 | Steps: 4 | Val loss: 13.1041 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9410 | Steps: 4 | Val loss: 2.3099 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.6948 | Steps: 4 | Val loss: 2.1957 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.7130 | Steps: 4 | Val loss: 1.9963 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 12:28:33 (running for 00:30:39.34)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.32075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.359 |      0.342 |                   69 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  0.581 |      0.217 |                   52 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.717 |      0.161 |                   19 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  3.013 |      0.12  |                    1 |
| train_9b9e8_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.18610074626865672
[2m[36m(func pid=89025)[0m top5: 0.6529850746268657
[2m[36m(func pid=89025)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=89025)[0m f1_macro: 0.21667028362998142
[2m[36m(func pid=89025)[0m f1_weighted: 0.18998028661412303
[2m[36m(func pid=89025)[0m f1_per_class: [0.5, 0.258, 0.039, 0.254, 0.089, 0.289, 0.012, 0.284, 0.249, 0.193]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=101169)[0m top1: 0.18889925373134328
[2m[36m(func pid=101169)[0m top5: 0.5415111940298507
[2m[36m(func pid=101169)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=101169)[0m f1_macro: 0.11965759014387718
[2m[36m(func pid=101169)[0m f1_weighted: 0.14148744865060914
[2m[36m(func pid=101169)[0m f1_per_class: [0.278, 0.326, 0.0, 0.132, 0.0, 0.314, 0.012, 0.029, 0.02, 0.085]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=96422)[0m top1: 0.2019589552238806
[2m[36m(func pid=96422)[0m top5: 0.6940298507462687
[2m[36m(func pid=96422)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=96422)[0m f1_macro: 0.1693326136081192
[2m[36m(func pid=96422)[0m f1_weighted: 0.22265899669378425
[2m[36m(func pid=96422)[0m f1_per_class: [0.155, 0.163, 0.209, 0.362, 0.032, 0.159, 0.183, 0.224, 0.044, 0.162]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=84655)[0m top1: 0.4001865671641791
[2m[36m(func pid=84655)[0m top5: 0.882929104477612
[2m[36m(func pid=84655)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=84655)[0m f1_macro: 0.34788666070482077
[2m[36m(func pid=84655)[0m f1_weighted: 0.4196864801647964
[2m[36m(func pid=84655)[0m f1_per_class: [0.477, 0.457, 0.371, 0.483, 0.121, 0.377, 0.403, 0.349, 0.233, 0.209]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.1487 | Steps: 4 | Val loss: 11.3791 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8827 | Steps: 4 | Val loss: 2.3076 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.6474 | Steps: 4 | Val loss: 2.1923 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.3180 | Steps: 4 | Val loss: 1.8625 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 12:28:38 (running for 00:30:44.69)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.32075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.713 |      0.348 |                   70 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  2.149 |      0.209 |                   53 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.695 |      0.169 |                   20 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  2.941 |      0.12  |                    2 |
| train_9b9e8_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.2234141791044776
[2m[36m(func pid=89025)[0m top5: 0.7192164179104478
[2m[36m(func pid=89025)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=89025)[0m f1_macro: 0.20892351667035153
[2m[36m(func pid=89025)[0m f1_weighted: 0.22078126767507814
[2m[36m(func pid=89025)[0m f1_per_class: [0.436, 0.369, 0.074, 0.323, 0.083, 0.103, 0.069, 0.28, 0.216, 0.137]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=101169)[0m top1: 0.18050373134328357
[2m[36m(func pid=101169)[0m top5: 0.5531716417910447
[2m[36m(func pid=101169)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=101169)[0m f1_macro: 0.13190955669394008
[2m[36m(func pid=101169)[0m f1_weighted: 0.1517806344322734
[2m[36m(func pid=101169)[0m f1_per_class: [0.253, 0.32, 0.093, 0.148, 0.0, 0.255, 0.035, 0.148, 0.03, 0.036]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=96422)[0m top1: 0.19869402985074627
[2m[36m(func pid=96422)[0m top5: 0.7000932835820896
[2m[36m(func pid=96422)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=96422)[0m f1_macro: 0.1722250789823649
[2m[36m(func pid=96422)[0m f1_weighted: 0.2165929795244368
[2m[36m(func pid=96422)[0m f1_per_class: [0.146, 0.122, 0.235, 0.363, 0.033, 0.181, 0.175, 0.225, 0.041, 0.2]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=84655)[0m top1: 0.42024253731343286
[2m[36m(func pid=84655)[0m top5: 0.9090485074626866
[2m[36m(func pid=84655)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=84655)[0m f1_macro: 0.3390138206697581
[2m[36m(func pid=84655)[0m f1_weighted: 0.43896003805195727
[2m[36m(func pid=84655)[0m f1_per_class: [0.395, 0.443, 0.342, 0.426, 0.12, 0.369, 0.549, 0.284, 0.237, 0.225]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.1791 | Steps: 4 | Val loss: 8.9514 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8855 | Steps: 4 | Val loss: 2.2944 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5622 | Steps: 4 | Val loss: 2.1761 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.3512 | Steps: 4 | Val loss: 1.9171 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:28:44 (running for 00:30:49.96)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.32075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.318 |      0.339 |                   71 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  2.179 |      0.291 |                   54 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.647 |      0.172 |                   21 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  2.883 |      0.132 |                    3 |
| train_9b9e8_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.3423507462686567
[2m[36m(func pid=89025)[0m top5: 0.8274253731343284
[2m[36m(func pid=89025)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=89025)[0m f1_macro: 0.2913196690049916
[2m[36m(func pid=89025)[0m f1_weighted: 0.3249043450568649
[2m[36m(func pid=89025)[0m f1_per_class: [0.42, 0.469, 0.595, 0.4, 0.061, 0.032, 0.328, 0.135, 0.246, 0.228]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=101169)[0m top1: 0.16277985074626866
[2m[36m(func pid=101169)[0m top5: 0.5914179104477612
[2m[36m(func pid=101169)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=101169)[0m f1_macro: 0.1328581606103129
[2m[36m(func pid=101169)[0m f1_weighted: 0.14737884229900758
[2m[36m(func pid=101169)[0m f1_per_class: [0.262, 0.302, 0.12, 0.168, 0.015, 0.125, 0.047, 0.217, 0.032, 0.04]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=96422)[0m top1: 0.20848880597014927
[2m[36m(func pid=96422)[0m top5: 0.7178171641791045
[2m[36m(func pid=96422)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=96422)[0m f1_macro: 0.18373605036658797
[2m[36m(func pid=96422)[0m f1_weighted: 0.22294526592412534
[2m[36m(func pid=96422)[0m f1_per_class: [0.154, 0.106, 0.262, 0.379, 0.035, 0.204, 0.179, 0.224, 0.042, 0.252]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=84655)[0m top1: 0.41277985074626866
[2m[36m(func pid=84655)[0m top5: 0.9034514925373134
[2m[36m(func pid=84655)[0m f1_micro: 0.41277985074626866
[2m[36m(func pid=84655)[0m f1_macro: 0.329238891177893
[2m[36m(func pid=84655)[0m f1_weighted: 0.42120902829153034
[2m[36m(func pid=84655)[0m f1_per_class: [0.324, 0.435, 0.371, 0.359, 0.195, 0.393, 0.571, 0.187, 0.205, 0.252]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.8944 | Steps: 4 | Val loss: 8.9936 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.6972 | Steps: 4 | Val loss: 2.2542 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3921 | Steps: 4 | Val loss: 1.9805 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.5431 | Steps: 4 | Val loss: 2.1652 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 12:28:49 (running for 00:30:55.13)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.32075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.351 |      0.329 |                   72 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.894 |      0.29  |                   55 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.562 |      0.184 |                   22 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  2.886 |      0.133 |                    4 |
| train_9b9e8_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.3493470149253731
[2m[36m(func pid=89025)[0m top5: 0.8255597014925373
[2m[36m(func pid=89025)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=89025)[0m f1_macro: 0.2902502307663649
[2m[36m(func pid=89025)[0m f1_weighted: 0.3362806650103456
[2m[36m(func pid=89025)[0m f1_per_class: [0.315, 0.431, 0.741, 0.224, 0.111, 0.062, 0.563, 0.058, 0.225, 0.171]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=101169)[0m top1: 0.17210820895522388
[2m[36m(func pid=101169)[0m top5: 0.6357276119402985
[2m[36m(func pid=101169)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=101169)[0m f1_macro: 0.1490660861233044
[2m[36m(func pid=101169)[0m f1_weighted: 0.1630309498215694
[2m[36m(func pid=101169)[0m f1_per_class: [0.317, 0.285, 0.2, 0.218, 0.029, 0.087, 0.066, 0.246, 0.043, 0.0]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3987873134328358
[2m[36m(func pid=84655)[0m top5: 0.8936567164179104
[2m[36m(func pid=84655)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=84655)[0m f1_macro: 0.33004102978443106
[2m[36m(func pid=84655)[0m f1_weighted: 0.4107414983778821
[2m[36m(func pid=84655)[0m f1_per_class: [0.327, 0.458, 0.347, 0.327, 0.172, 0.382, 0.544, 0.26, 0.192, 0.292]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=96422)[0m top1: 0.21082089552238806
[2m[36m(func pid=96422)[0m top5: 0.7215485074626866
[2m[36m(func pid=96422)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=96422)[0m f1_macro: 0.17736588255248414
[2m[36m(func pid=96422)[0m f1_weighted: 0.22362831670664152
[2m[36m(func pid=96422)[0m f1_per_class: [0.152, 0.115, 0.196, 0.384, 0.036, 0.185, 0.181, 0.214, 0.044, 0.265]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.2675 | Steps: 4 | Val loss: 9.8995 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6732 | Steps: 4 | Val loss: 2.1961 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.5666 | Steps: 4 | Val loss: 2.0747 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.5779 | Steps: 4 | Val loss: 2.1555 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 12:28:54 (running for 00:31:00.50)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.32075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.392 |      0.33  |                   73 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  2.268 |      0.231 |                   56 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.543 |      0.177 |                   23 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  2.697 |      0.149 |                    5 |
| train_9b9e8_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.2719216417910448
[2m[36m(func pid=89025)[0m top5: 0.7644589552238806
[2m[36m(func pid=89025)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=89025)[0m f1_macro: 0.23060855689799
[2m[36m(func pid=89025)[0m f1_weighted: 0.29786416260704807
[2m[36m(func pid=89025)[0m f1_per_class: [0.248, 0.298, 0.19, 0.2, 0.124, 0.211, 0.463, 0.246, 0.155, 0.169]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=101169)[0m top1: 0.1767723880597015
[2m[36m(func pid=101169)[0m top5: 0.699160447761194
[2m[36m(func pid=101169)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=101169)[0m f1_macro: 0.14811009097925593
[2m[36m(func pid=101169)[0m f1_weighted: 0.1727109873383346
[2m[36m(func pid=101169)[0m f1_per_class: [0.316, 0.196, 0.144, 0.309, 0.041, 0.046, 0.079, 0.24, 0.052, 0.057]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=84655)[0m top1: 0.3712686567164179
[2m[36m(func pid=84655)[0m top5: 0.878731343283582
[2m[36m(func pid=84655)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=84655)[0m f1_macro: 0.32517943695320617
[2m[36m(func pid=84655)[0m f1_weighted: 0.3847872289643059
[2m[36m(func pid=84655)[0m f1_per_class: [0.25, 0.468, 0.356, 0.285, 0.212, 0.362, 0.495, 0.29, 0.185, 0.349]
[2m[36m(func pid=84655)[0m 
[2m[36m(func pid=96422)[0m top1: 0.22201492537313433
[2m[36m(func pid=96422)[0m top5: 0.7192164179104478
[2m[36m(func pid=96422)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=96422)[0m f1_macro: 0.1842232938934732
[2m[36m(func pid=96422)[0m f1_weighted: 0.22848917512082711
[2m[36m(func pid=96422)[0m f1_per_class: [0.167, 0.105, 0.2, 0.415, 0.039, 0.187, 0.167, 0.245, 0.038, 0.28]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5972 | Steps: 4 | Val loss: 2.1510 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.4238 | Steps: 4 | Val loss: 11.0611 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=84655)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4850 | Steps: 4 | Val loss: 2.1650 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.4875 | Steps: 4 | Val loss: 2.1446 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 12:28:59 (running for 00:31:05.78)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.32075
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00014 | RUNNING    | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.567 |      0.325 |                   74 |
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  0.424 |      0.227 |                   57 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.578 |      0.184 |                   24 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  2.673 |      0.148 |                    6 |
| train_9b9e8_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.2126865671641791
[2m[36m(func pid=89025)[0m top5: 0.7280783582089553
[2m[36m(func pid=89025)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=89025)[0m f1_macro: 0.22716126473611467
[2m[36m(func pid=89025)[0m f1_weighted: 0.23212971750341993
[2m[36m(func pid=89025)[0m f1_per_class: [0.397, 0.206, 0.066, 0.237, 0.074, 0.298, 0.208, 0.274, 0.171, 0.34]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=101169)[0m top1: 0.20522388059701493
[2m[36m(func pid=101169)[0m top5: 0.7280783582089553
[2m[36m(func pid=101169)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=101169)[0m f1_macro: 0.15388132976064245
[2m[36m(func pid=101169)[0m f1_weighted: 0.20732941355256995
[2m[36m(func pid=101169)[0m f1_per_class: [0.246, 0.22, 0.109, 0.377, 0.054, 0.031, 0.126, 0.238, 0.081, 0.057]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=84655)[0m top1: 0.34328358208955223
[2m[36m(func pid=84655)[0m top5: 0.8628731343283582
[2m[36m(func pid=84655)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=84655)[0m f1_macro: 0.31474521465628175
[2m[36m(func pid=84655)[0m f1_weighted: 0.3663790023694031
[2m[36m(func pid=84655)[0m f1_per_class: [0.315, 0.431, 0.321, 0.328, 0.174, 0.357, 0.412, 0.31, 0.175, 0.325]
[2m[36m(func pid=96422)[0m top1: 0.22807835820895522
[2m[36m(func pid=96422)[0m top5: 0.7299440298507462
[2m[36m(func pid=96422)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=96422)[0m f1_macro: 0.1850567842899426
[2m[36m(func pid=96422)[0m f1_weighted: 0.23731578328002811
[2m[36m(func pid=96422)[0m f1_per_class: [0.175, 0.096, 0.165, 0.415, 0.043, 0.205, 0.197, 0.232, 0.048, 0.275]
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 3.1647 | Steps: 4 | Val loss: 10.4026 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.3456 | Steps: 4 | Val loss: 2.0985 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=101169)[0m top1: 0.22201492537313433
[2m[36m(func pid=101169)[0m top5: 0.7611940298507462
[2m[36m(func pid=101169)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=101169)[0m f1_macro: 0.1746057822721155
[2m[36m(func pid=101169)[0m f1_weighted: 0.22801861111633864
[2m[36m(func pid=101169)[0m f1_per_class: [0.162, 0.209, 0.134, 0.408, 0.063, 0.069, 0.153, 0.253, 0.09, 0.205]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=89025)[0m top1: 0.2621268656716418
[2m[36m(func pid=89025)[0m top5: 0.7532649253731343
[2m[36m(func pid=89025)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=89025)[0m f1_macro: 0.27021391776898157
[2m[36m(func pid=89025)[0m f1_weighted: 0.27676888323873544
[2m[36m(func pid=89025)[0m f1_per_class: [0.43, 0.245, 0.121, 0.424, 0.064, 0.332, 0.136, 0.264, 0.216, 0.471]
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.5106 | Steps: 4 | Val loss: 2.1140 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.2961 | Steps: 4 | Val loss: 2.0541 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=96422)[0m top1: 0.2416044776119403
[2m[36m(func pid=96422)[0m top5: 0.7476679104477612
[2m[36m(func pid=96422)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=96422)[0m f1_macro: 0.18826964330765894
[2m[36m(func pid=96422)[0m f1_weighted: 0.24724750499347414
[2m[36m(func pid=96422)[0m f1_per_class: [0.201, 0.073, 0.177, 0.447, 0.039, 0.212, 0.213, 0.206, 0.05, 0.264]
[2m[36m(func pid=101169)[0m top1: 0.24207089552238806
[2m[36m(func pid=101169)[0m top5: 0.7681902985074627
[2m[36m(func pid=101169)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=101169)[0m f1_macro: 0.1992500684920852
[2m[36m(func pid=101169)[0m f1_weighted: 0.2616180366965484
[2m[36m(func pid=101169)[0m f1_per_class: [0.137, 0.238, 0.128, 0.422, 0.07, 0.076, 0.229, 0.255, 0.093, 0.346]
== Status ==
Current time: 2024-01-07 12:29:05 (running for 00:31:11.09)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  0.424 |      0.227 |                   57 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.487 |      0.185 |                   25 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  2.346 |      0.175 |                    8 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 12:29:10 (running for 00:31:16.55)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  3.165 |      0.27  |                   58 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.487 |      0.185 |                   25 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  2.346 |      0.175 |                    8 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=103447)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=103447)[0m Configuration completed!
[2m[36m(func pid=103447)[0m New optimizer parameters:
[2m[36m(func pid=103447)[0m SGD (
[2m[36m(func pid=103447)[0m Parameter Group 0
[2m[36m(func pid=103447)[0m     dampening: 0
[2m[36m(func pid=103447)[0m     differentiable: False
[2m[36m(func pid=103447)[0m     foreach: None
[2m[36m(func pid=103447)[0m     lr: 0.01
[2m[36m(func pid=103447)[0m     maximize: False
[2m[36m(func pid=103447)[0m     momentum: 0.99
[2m[36m(func pid=103447)[0m     nesterov: False
[2m[36m(func pid=103447)[0m     weight_decay: 1e-05
[2m[36m(func pid=103447)[0m )
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.1973 | Steps: 4 | Val loss: 8.4765 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.1694 | Steps: 4 | Val loss: 2.0235 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.4508 | Steps: 4 | Val loss: 2.1108 | Batch size: 32 | lr: 0.0001 | Duration: 3.28s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9431 | Steps: 4 | Val loss: 2.3185 | Batch size: 32 | lr: 0.01 | Duration: 4.71s
== Status ==
Current time: 2024-01-07 12:29:15 (running for 00:31:21.58)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  3.165 |      0.27  |                   58 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.511 |      0.188 |                   26 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  2.296 |      0.199 |                    9 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.26259328358208955
[2m[36m(func pid=101169)[0m top5: 0.7728544776119403
[2m[36m(func pid=101169)[0m f1_micro: 0.26259328358208955
[2m[36m(func pid=101169)[0m f1_macro: 0.21831370356607574
[2m[36m(func pid=101169)[0m f1_weighted: 0.29098548282603504
[2m[36m(func pid=101169)[0m f1_per_class: [0.144, 0.246, 0.115, 0.412, 0.074, 0.124, 0.309, 0.276, 0.071, 0.413]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=89025)[0m top1: 0.3773320895522388
[2m[36m(func pid=89025)[0m top5: 0.8568097014925373
[2m[36m(func pid=89025)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=89025)[0m f1_macro: 0.32596940834367294
[2m[36m(func pid=89025)[0m f1_weighted: 0.3534453967441638
[2m[36m(func pid=89025)[0m f1_per_class: [0.408, 0.414, 0.177, 0.535, 0.138, 0.31, 0.184, 0.313, 0.232, 0.549]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=96422)[0m top1: 0.25046641791044777
[2m[36m(func pid=96422)[0m top5: 0.7406716417910447
[2m[36m(func pid=96422)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=96422)[0m f1_macro: 0.19741323422154694
[2m[36m(func pid=96422)[0m f1_weighted: 0.24855119540541507
[2m[36m(func pid=96422)[0m f1_per_class: [0.221, 0.073, 0.153, 0.464, 0.039, 0.246, 0.181, 0.22, 0.067, 0.309]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m top1: 0.18330223880597016
[2m[36m(func pid=103447)[0m top5: 0.5223880597014925
[2m[36m(func pid=103447)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=103447)[0m f1_macro: 0.14298163188258858
[2m[36m(func pid=103447)[0m f1_weighted: 0.1515029816162034
[2m[36m(func pid=103447)[0m f1_per_class: [0.309, 0.315, 0.0, 0.125, 0.024, 0.295, 0.033, 0.188, 0.0, 0.141]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.2250 | Steps: 4 | Val loss: 1.9559 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.8239 | Steps: 4 | Val loss: 8.5704 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.4742 | Steps: 4 | Val loss: 2.0898 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7824 | Steps: 4 | Val loss: 2.3784 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:29:21 (running for 00:31:27.17)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.197 |      0.326 |                   59 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.451 |      0.197 |                   27 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  2.225 |      0.25  |                   11 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  2.943 |      0.143 |                    1 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.31203358208955223
[2m[36m(func pid=101169)[0m top5: 0.7971082089552238
[2m[36m(func pid=101169)[0m f1_micro: 0.31203358208955223
[2m[36m(func pid=101169)[0m f1_macro: 0.2502067917082737
[2m[36m(func pid=101169)[0m f1_weighted: 0.34602366166855336
[2m[36m(func pid=101169)[0m f1_per_class: [0.162, 0.281, 0.135, 0.454, 0.078, 0.267, 0.379, 0.275, 0.065, 0.405]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=89025)[0m top1: 0.3871268656716418
[2m[36m(func pid=89025)[0m top5: 0.8768656716417911
[2m[36m(func pid=89025)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=89025)[0m f1_macro: 0.29987239582305125
[2m[36m(func pid=89025)[0m f1_weighted: 0.3648871823708059
[2m[36m(func pid=89025)[0m f1_per_class: [0.388, 0.51, 0.179, 0.476, 0.059, 0.194, 0.282, 0.287, 0.224, 0.4]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=96422)[0m top1: 0.261660447761194
[2m[36m(func pid=96422)[0m top5: 0.7560634328358209
[2m[36m(func pid=96422)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=96422)[0m f1_macro: 0.20107619031617854
[2m[36m(func pid=96422)[0m f1_weighted: 0.26337489121484897
[2m[36m(func pid=96422)[0m f1_per_class: [0.247, 0.091, 0.143, 0.464, 0.055, 0.245, 0.223, 0.214, 0.063, 0.266]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m top1: 0.1021455223880597
[2m[36m(func pid=103447)[0m top5: 0.46408582089552236
[2m[36m(func pid=103447)[0m f1_micro: 0.10214552238805971
[2m[36m(func pid=103447)[0m f1_macro: 0.10962276621375763
[2m[36m(func pid=103447)[0m f1_weighted: 0.0613093957654422
[2m[36m(func pid=103447)[0m f1_per_class: [0.128, 0.011, 0.375, 0.129, 0.068, 0.0, 0.003, 0.252, 0.051, 0.08]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.9364 | Steps: 4 | Val loss: 1.8897 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.2252 | Steps: 4 | Val loss: 9.1027 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.3200 | Steps: 4 | Val loss: 2.0753 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.4628 | Steps: 4 | Val loss: 2.2793 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 12:29:26 (running for 00:31:32.22)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  0.824 |      0.3   |                   60 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.474 |      0.201 |                   28 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.936 |      0.269 |                   12 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  2.782 |      0.11  |                    2 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.3558768656716418
[2m[36m(func pid=101169)[0m top5: 0.8227611940298507
[2m[36m(func pid=101169)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=101169)[0m f1_macro: 0.2692832159077204
[2m[36m(func pid=101169)[0m f1_weighted: 0.38392062054267584
[2m[36m(func pid=101169)[0m f1_per_class: [0.228, 0.347, 0.186, 0.504, 0.065, 0.334, 0.4, 0.263, 0.025, 0.341]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=89025)[0m top1: 0.3675373134328358
[2m[36m(func pid=89025)[0m top5: 0.8684701492537313
[2m[36m(func pid=89025)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=89025)[0m f1_macro: 0.30054822256826136
[2m[36m(func pid=89025)[0m f1_weighted: 0.3618646842114364
[2m[36m(func pid=89025)[0m f1_per_class: [0.382, 0.501, 0.149, 0.371, 0.053, 0.272, 0.35, 0.266, 0.206, 0.456]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=96422)[0m top1: 0.28451492537313433
[2m[36m(func pid=96422)[0m top5: 0.7635261194029851
[2m[36m(func pid=96422)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=96422)[0m f1_macro: 0.21746993220374117
[2m[36m(func pid=96422)[0m f1_weighted: 0.2837440749082045
[2m[36m(func pid=96422)[0m f1_per_class: [0.3, 0.083, 0.152, 0.489, 0.05, 0.27, 0.253, 0.236, 0.087, 0.256]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m top1: 0.19402985074626866
[2m[36m(func pid=103447)[0m top5: 0.5834888059701493
[2m[36m(func pid=103447)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=103447)[0m f1_macro: 0.1787921647695306
[2m[36m(func pid=103447)[0m f1_weighted: 0.15243640571385697
[2m[36m(func pid=103447)[0m f1_per_class: [0.198, 0.0, 0.375, 0.438, 0.074, 0.0, 0.006, 0.252, 0.077, 0.366]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.0097 | Steps: 4 | Val loss: 1.8829 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4959 | Steps: 4 | Val loss: 10.5575 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.0686 | Steps: 4 | Val loss: 2.0260 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3107 | Steps: 4 | Val loss: 2.0540 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 12:29:31 (running for 00:31:37.48)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.225 |      0.301 |                   61 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.32  |      0.217 |                   29 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  2.01  |      0.283 |                   13 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  2.463 |      0.179 |                    3 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.3568097014925373
[2m[36m(func pid=101169)[0m top5: 0.8176305970149254
[2m[36m(func pid=101169)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=101169)[0m f1_macro: 0.28336156539253626
[2m[36m(func pid=101169)[0m f1_weighted: 0.3860356282980373
[2m[36m(func pid=101169)[0m f1_per_class: [0.26, 0.373, 0.212, 0.505, 0.055, 0.345, 0.379, 0.259, 0.068, 0.377]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=89025)[0m top1: 0.27472014925373134
[2m[36m(func pid=89025)[0m top5: 0.8269589552238806
[2m[36m(func pid=89025)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=89025)[0m f1_macro: 0.24564974536369388
[2m[36m(func pid=89025)[0m f1_weighted: 0.28523654324522824
[2m[36m(func pid=89025)[0m f1_per_class: [0.39, 0.475, 0.115, 0.313, 0.163, 0.249, 0.205, 0.158, 0.143, 0.245]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=103447)[0m top1: 0.22108208955223882
[2m[36m(func pid=103447)[0m top5: 0.7686567164179104
[2m[36m(func pid=103447)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=103447)[0m f1_macro: 0.18188641989451776
[2m[36m(func pid=103447)[0m f1_weighted: 0.21219397364468348
[2m[36m(func pid=103447)[0m f1_per_class: [0.117, 0.011, 0.229, 0.44, 0.071, 0.069, 0.174, 0.277, 0.115, 0.315]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m top1: 0.29244402985074625
[2m[36m(func pid=96422)[0m top5: 0.7817164179104478
[2m[36m(func pid=96422)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=96422)[0m f1_macro: 0.2355269377124456
[2m[36m(func pid=96422)[0m f1_weighted: 0.30283497774668916
[2m[36m(func pid=96422)[0m f1_per_class: [0.385, 0.083, 0.15, 0.483, 0.056, 0.278, 0.309, 0.241, 0.119, 0.253]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.9022 | Steps: 4 | Val loss: 1.8035 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.0329 | Steps: 4 | Val loss: 10.4151 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.1674 | Steps: 4 | Val loss: 1.7885 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.3315 | Steps: 4 | Val loss: 2.0466 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 12:29:36 (running for 00:31:42.71)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  0.496 |      0.246 |                   62 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.311 |      0.236 |                   30 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.902 |      0.297 |                   14 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  2.069 |      0.182 |                    4 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.37966417910447764
[2m[36m(func pid=101169)[0m top5: 0.8470149253731343
[2m[36m(func pid=101169)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=101169)[0m f1_macro: 0.2969512898606578
[2m[36m(func pid=101169)[0m f1_weighted: 0.403825556023061
[2m[36m(func pid=101169)[0m f1_per_class: [0.332, 0.425, 0.233, 0.496, 0.054, 0.361, 0.407, 0.271, 0.043, 0.347]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=89025)[0m top1: 0.2537313432835821
[2m[36m(func pid=89025)[0m top5: 0.8339552238805971
[2m[36m(func pid=89025)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=89025)[0m f1_macro: 0.2275848897740385
[2m[36m(func pid=89025)[0m f1_weighted: 0.2838559616529065
[2m[36m(func pid=89025)[0m f1_per_class: [0.373, 0.392, 0.132, 0.32, 0.123, 0.212, 0.263, 0.153, 0.138, 0.171]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=103447)[0m top1: 0.376865671641791
[2m[36m(func pid=103447)[0m top5: 0.8619402985074627
[2m[36m(func pid=103447)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=103447)[0m f1_macro: 0.24631603655796494
[2m[36m(func pid=103447)[0m f1_weighted: 0.3777630532093987
[2m[36m(func pid=103447)[0m f1_per_class: [0.227, 0.145, 0.258, 0.425, 0.103, 0.324, 0.603, 0.076, 0.115, 0.187]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m top1: 0.30130597014925375
[2m[36m(func pid=96422)[0m top5: 0.7868470149253731
[2m[36m(func pid=96422)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=96422)[0m f1_macro: 0.24334610780960997
[2m[36m(func pid=96422)[0m f1_weighted: 0.3056605362157974
[2m[36m(func pid=96422)[0m f1_per_class: [0.41, 0.1, 0.2, 0.498, 0.065, 0.299, 0.283, 0.238, 0.142, 0.197]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.8590 | Steps: 4 | Val loss: 1.7719 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.8031 | Steps: 4 | Val loss: 8.4624 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.6513 | Steps: 4 | Val loss: 1.8267 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=101169)[0m top1: 0.376865671641791
[2m[36m(func pid=101169)[0m top5: 0.8535447761194029
[2m[36m(func pid=101169)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=101169)[0m f1_macro: 0.31447999134639815
[2m[36m(func pid=101169)[0m f1_weighted: 0.40233450737477233
[2m[36m(func pid=101169)[0m f1_per_class: [0.393, 0.417, 0.279, 0.484, 0.051, 0.386, 0.392, 0.292, 0.112, 0.339]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.2962 | Steps: 4 | Val loss: 2.0400 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 12:29:42 (running for 00:31:48.73)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  0.803 |      0.278 |                   64 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.331 |      0.243 |                   31 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.859 |      0.314 |                   15 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  2.167 |      0.246 |                    5 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.3736007462686567
[2m[36m(func pid=89025)[0m top5: 0.8652052238805971
[2m[36m(func pid=89025)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=89025)[0m f1_macro: 0.27783960819980313
[2m[36m(func pid=89025)[0m f1_weighted: 0.38616600743748947
[2m[36m(func pid=89025)[0m f1_per_class: [0.415, 0.326, 0.17, 0.398, 0.123, 0.152, 0.567, 0.228, 0.205, 0.194]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=103447)[0m top1: 0.3344216417910448
[2m[36m(func pid=103447)[0m top5: 0.8768656716417911
[2m[36m(func pid=103447)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=103447)[0m f1_macro: 0.2850579705902906
[2m[36m(func pid=103447)[0m f1_weighted: 0.3693213460269259
[2m[36m(func pid=103447)[0m f1_per_class: [0.465, 0.251, 0.245, 0.396, 0.081, 0.395, 0.454, 0.326, 0.097, 0.14]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.4294 | Steps: 4 | Val loss: 1.7236 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=96422)[0m top1: 0.300839552238806
[2m[36m(func pid=96422)[0m top5: 0.784981343283582
[2m[36m(func pid=96422)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=96422)[0m f1_macro: 0.247448659574306
[2m[36m(func pid=96422)[0m f1_weighted: 0.3079318269701088
[2m[36m(func pid=96422)[0m f1_per_class: [0.428, 0.088, 0.214, 0.489, 0.065, 0.329, 0.295, 0.24, 0.129, 0.198]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.6960 | Steps: 4 | Val loss: 9.1621 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=101169)[0m top1: 0.3829291044776119
[2m[36m(func pid=101169)[0m top5: 0.8680037313432836
[2m[36m(func pid=101169)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=101169)[0m f1_macro: 0.3242983887900796
[2m[36m(func pid=101169)[0m f1_weighted: 0.4070023818014021
[2m[36m(func pid=101169)[0m f1_per_class: [0.471, 0.418, 0.276, 0.449, 0.072, 0.4, 0.427, 0.273, 0.179, 0.278]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.3272 | Steps: 4 | Val loss: 2.0807 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.2562 | Steps: 4 | Val loss: 2.0406 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=89025)[0m top1: 0.32975746268656714
[2m[36m(func pid=89025)[0m top5: 0.8232276119402985
[2m[36m(func pid=89025)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=89025)[0m f1_macro: 0.2553305839060867
[2m[36m(func pid=89025)[0m f1_weighted: 0.345420135371526
[2m[36m(func pid=89025)[0m f1_per_class: [0.338, 0.251, 0.136, 0.349, 0.125, 0.146, 0.525, 0.24, 0.191, 0.253]
== Status ==
Current time: 2024-01-07 12:29:48 (running for 00:31:54.01)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  0.696 |      0.255 |                   65 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.296 |      0.247 |                   32 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.429 |      0.324 |                   16 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.651 |      0.285 |                    6 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=103447)[0m top1: 0.28078358208955223
[2m[36m(func pid=103447)[0m top5: 0.8530783582089553
[2m[36m(func pid=103447)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=103447)[0m f1_macro: 0.2881550259641507
[2m[36m(func pid=103447)[0m f1_weighted: 0.2897149402429531
[2m[36m(func pid=103447)[0m f1_per_class: [0.588, 0.327, 0.224, 0.376, 0.071, 0.395, 0.141, 0.282, 0.261, 0.217]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.6993 | Steps: 4 | Val loss: 1.7056 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=96422)[0m top1: 0.2943097014925373
[2m[36m(func pid=96422)[0m top5: 0.777518656716418
[2m[36m(func pid=96422)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=96422)[0m f1_macro: 0.24357466592985366
[2m[36m(func pid=96422)[0m f1_weighted: 0.3048825512765568
[2m[36m(func pid=96422)[0m f1_per_class: [0.427, 0.073, 0.176, 0.475, 0.066, 0.337, 0.3, 0.255, 0.134, 0.193]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.4986 | Steps: 4 | Val loss: 10.5448 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=101169)[0m top1: 0.38992537313432835
[2m[36m(func pid=101169)[0m top5: 0.8801305970149254
[2m[36m(func pid=101169)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=101169)[0m f1_macro: 0.3364066446538376
[2m[36m(func pid=101169)[0m f1_weighted: 0.4125822994775051
[2m[36m(func pid=101169)[0m f1_per_class: [0.496, 0.421, 0.308, 0.438, 0.086, 0.421, 0.438, 0.295, 0.194, 0.267]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.5712 | Steps: 4 | Val loss: 2.3205 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:29:53 (running for 00:31:59.26)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.499 |      0.237 |                   66 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.256 |      0.244 |                   33 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.699 |      0.336 |                   17 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.327 |      0.288 |                    7 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.2462686567164179
[2m[36m(func pid=89025)[0m top5: 0.7933768656716418
[2m[36m(func pid=89025)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=89025)[0m f1_macro: 0.23701319939055882
[2m[36m(func pid=89025)[0m f1_weighted: 0.26987729064574995
[2m[36m(func pid=89025)[0m f1_per_class: [0.359, 0.231, 0.156, 0.269, 0.133, 0.204, 0.334, 0.255, 0.149, 0.28]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.2566 | Steps: 4 | Val loss: 2.0387 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=103447)[0m top1: 0.27611940298507465
[2m[36m(func pid=103447)[0m top5: 0.8344216417910447
[2m[36m(func pid=103447)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=103447)[0m f1_macro: 0.2839550753474549
[2m[36m(func pid=103447)[0m f1_weighted: 0.26981773527224495
[2m[36m(func pid=103447)[0m f1_per_class: [0.581, 0.368, 0.2, 0.387, 0.093, 0.413, 0.042, 0.26, 0.195, 0.301]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.4271 | Steps: 4 | Val loss: 1.7177 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=96422)[0m top1: 0.29384328358208955
[2m[36m(func pid=96422)[0m top5: 0.7779850746268657
[2m[36m(func pid=96422)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=96422)[0m f1_macro: 0.24470822731815986
[2m[36m(func pid=96422)[0m f1_weighted: 0.301492425627909
[2m[36m(func pid=96422)[0m f1_per_class: [0.448, 0.069, 0.169, 0.485, 0.066, 0.314, 0.286, 0.275, 0.126, 0.209]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.4550 | Steps: 4 | Val loss: 9.0594 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.2075 | Steps: 4 | Val loss: 2.1360 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=101169)[0m top1: 0.36847014925373134
[2m[36m(func pid=101169)[0m top5: 0.8754664179104478
[2m[36m(func pid=101169)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=101169)[0m f1_macro: 0.327937957089775
[2m[36m(func pid=101169)[0m f1_weighted: 0.39292888102698564
[2m[36m(func pid=101169)[0m f1_per_class: [0.505, 0.406, 0.338, 0.404, 0.104, 0.424, 0.416, 0.294, 0.148, 0.24]
[2m[36m(func pid=101169)[0m 
== Status ==
Current time: 2024-01-07 12:29:58 (running for 00:32:04.68)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.455 |      0.266 |                   67 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.257 |      0.245 |                   34 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.427 |      0.328 |                   18 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.571 |      0.284 |                    8 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.2896455223880597
[2m[36m(func pid=89025)[0m top5: 0.8316231343283582
[2m[36m(func pid=89025)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=89025)[0m f1_macro: 0.2657021498322939
[2m[36m(func pid=89025)[0m f1_weighted: 0.31257635966654257
[2m[36m(func pid=89025)[0m f1_per_class: [0.318, 0.307, 0.146, 0.383, 0.106, 0.325, 0.275, 0.261, 0.203, 0.333]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.1716 | Steps: 4 | Val loss: 2.0516 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=103447)[0m top1: 0.32369402985074625
[2m[36m(func pid=103447)[0m top5: 0.8605410447761194
[2m[36m(func pid=103447)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=103447)[0m f1_macro: 0.30611420789320387
[2m[36m(func pid=103447)[0m f1_weighted: 0.3501618325404372
[2m[36m(func pid=103447)[0m f1_per_class: [0.553, 0.326, 0.15, 0.417, 0.135, 0.414, 0.307, 0.292, 0.166, 0.303]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.2920 | Steps: 4 | Val loss: 1.7591 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.0213 | Steps: 4 | Val loss: 7.7494 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=96422)[0m top1: 0.27705223880597013
[2m[36m(func pid=96422)[0m top5: 0.7681902985074627
[2m[36m(func pid=96422)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=96422)[0m f1_macro: 0.23781631115948612
[2m[36m(func pid=96422)[0m f1_weighted: 0.2884011890503659
[2m[36m(func pid=96422)[0m f1_per_class: [0.441, 0.058, 0.156, 0.456, 0.059, 0.31, 0.276, 0.283, 0.13, 0.209]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m top1: 0.3568097014925373
[2m[36m(func pid=101169)[0m top5: 0.8736007462686567
[2m[36m(func pid=101169)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=101169)[0m f1_macro: 0.33273839013474615
[2m[36m(func pid=101169)[0m f1_weighted: 0.3805584734227396
[2m[36m(func pid=101169)[0m f1_per_class: [0.549, 0.41, 0.369, 0.372, 0.106, 0.416, 0.398, 0.292, 0.191, 0.224]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.2699 | Steps: 4 | Val loss: 2.1041 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 12:30:03 (running for 00:32:09.88)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.021 |      0.344 |                   68 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.172 |      0.238 |                   35 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.292 |      0.333 |                   19 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.207 |      0.306 |                    9 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.3843283582089552
[2m[36m(func pid=89025)[0m top5: 0.8861940298507462
[2m[36m(func pid=89025)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=89025)[0m f1_macro: 0.34381335704255545
[2m[36m(func pid=89025)[0m f1_weighted: 0.3913896997888092
[2m[36m(func pid=89025)[0m f1_per_class: [0.371, 0.475, 0.229, 0.45, 0.099, 0.373, 0.327, 0.334, 0.272, 0.508]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.2176 | Steps: 4 | Val loss: 2.0213 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=103447)[0m top1: 0.37453358208955223
[2m[36m(func pid=103447)[0m top5: 0.8871268656716418
[2m[36m(func pid=103447)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=103447)[0m f1_macro: 0.32033062024688064
[2m[36m(func pid=103447)[0m f1_weighted: 0.39863519111614953
[2m[36m(func pid=103447)[0m f1_per_class: [0.519, 0.352, 0.137, 0.404, 0.124, 0.404, 0.483, 0.211, 0.186, 0.383]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.1893 | Steps: 4 | Val loss: 1.7797 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.8666 | Steps: 4 | Val loss: 7.4575 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=96422)[0m top1: 0.29151119402985076
[2m[36m(func pid=96422)[0m top5: 0.7887126865671642
[2m[36m(func pid=96422)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=96422)[0m f1_macro: 0.25213798671537613
[2m[36m(func pid=96422)[0m f1_weighted: 0.29538057889807307
[2m[36m(func pid=96422)[0m f1_per_class: [0.478, 0.058, 0.197, 0.475, 0.071, 0.327, 0.268, 0.297, 0.128, 0.222]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m top1: 0.34654850746268656
[2m[36m(func pid=101169)[0m top5: 0.8740671641791045
[2m[36m(func pid=101169)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=101169)[0m f1_macro: 0.3361548383638014
[2m[36m(func pid=101169)[0m f1_weighted: 0.3704089366314155
[2m[36m(func pid=101169)[0m f1_per_class: [0.562, 0.409, 0.429, 0.349, 0.115, 0.418, 0.382, 0.306, 0.171, 0.22]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.2089 | Steps: 4 | Val loss: 2.1554 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=89025)[0m top1: 0.416044776119403
[2m[36m(func pid=89025)[0m top5: 0.9015858208955224
[2m[36m(func pid=89025)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=89025)[0m f1_macro: 0.3749600056358352
[2m[36m(func pid=89025)[0m f1_weighted: 0.41811665335321807
[2m[36m(func pid=89025)[0m f1_per_class: [0.403, 0.534, 0.476, 0.398, 0.113, 0.326, 0.449, 0.318, 0.245, 0.489]
== Status ==
Current time: 2024-01-07 12:30:09 (running for 00:32:15.24)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.867 |      0.375 |                   69 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.218 |      0.252 |                   36 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.189 |      0.336 |                   20 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.27  |      0.32  |                   10 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.1331 | Steps: 4 | Val loss: 2.0126 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=103447)[0m top1: 0.40578358208955223
[2m[36m(func pid=103447)[0m top5: 0.8969216417910447
[2m[36m(func pid=103447)[0m f1_micro: 0.40578358208955223
[2m[36m(func pid=103447)[0m f1_macro: 0.31746486952271996
[2m[36m(func pid=103447)[0m f1_weighted: 0.40449443752130587
[2m[36m(func pid=103447)[0m f1_per_class: [0.517, 0.437, 0.168, 0.319, 0.165, 0.422, 0.551, 0.105, 0.155, 0.336]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.6606 | Steps: 4 | Val loss: 1.7891 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 5.7240 | Steps: 4 | Val loss: 8.8861 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=96422)[0m top1: 0.3003731343283582
[2m[36m(func pid=96422)[0m top5: 0.7868470149253731
[2m[36m(func pid=96422)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=96422)[0m f1_macro: 0.2617364463228298
[2m[36m(func pid=96422)[0m f1_weighted: 0.31372650035741434
[2m[36m(func pid=96422)[0m f1_per_class: [0.512, 0.071, 0.203, 0.479, 0.068, 0.351, 0.31, 0.278, 0.14, 0.204]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m top1: 0.34048507462686567
[2m[36m(func pid=101169)[0m top5: 0.8656716417910447
[2m[36m(func pid=101169)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=101169)[0m f1_macro: 0.3247479361713004
[2m[36m(func pid=101169)[0m f1_weighted: 0.36863713391391645
[2m[36m(func pid=101169)[0m f1_per_class: [0.519, 0.352, 0.393, 0.374, 0.138, 0.434, 0.386, 0.304, 0.155, 0.192]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 0.7773 | Steps: 4 | Val loss: 2.3944 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=89025)[0m top1: 0.3871268656716418== Status ==
Current time: 2024-01-07 12:30:14 (running for 00:32:20.65)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  5.724 |      0.347 |                   70 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.133 |      0.262 |                   37 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.661 |      0.325 |                   21 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.209 |      0.317 |                   11 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)



[2m[36m(func pid=89025)[0m top5: 0.8782649253731343
[2m[36m(func pid=89025)[0m f1_micro: 0.3871268656716418
[2m[36m(func pid=89025)[0m f1_macro: 0.34687105607589575
[2m[36m(func pid=89025)[0m f1_weighted: 0.3686751636919148
[2m[36m(func pid=89025)[0m f1_per_class: [0.403, 0.525, 0.526, 0.243, 0.074, 0.221, 0.483, 0.282, 0.224, 0.489]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.1430 | Steps: 4 | Val loss: 1.9877 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 1.2556 | Steps: 4 | Val loss: 1.7913 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=103447)[0m top1: 0.4001865671641791
[2m[36m(func pid=103447)[0m top5: 0.8913246268656716
[2m[36m(func pid=103447)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=103447)[0m f1_macro: 0.3129644462822124
[2m[36m(func pid=103447)[0m f1_weighted: 0.3841734445390188
[2m[36m(func pid=103447)[0m f1_per_class: [0.45, 0.477, 0.146, 0.211, 0.173, 0.42, 0.552, 0.165, 0.171, 0.365]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.2249 | Steps: 4 | Val loss: 8.8121 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=101169)[0m top1: 0.34654850746268656
[2m[36m(func pid=101169)[0m top5: 0.8759328358208955
[2m[36m(func pid=101169)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=101169)[0m f1_macro: 0.3302235695976671
[2m[36m(func pid=101169)[0m f1_weighted: 0.3735693881191112
[2m[36m(func pid=101169)[0m f1_per_class: [0.558, 0.374, 0.393, 0.385, 0.118, 0.427, 0.38, 0.3, 0.161, 0.206]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=96422)[0m top1: 0.31716417910447764
[2m[36m(func pid=96422)[0m top5: 0.8064365671641791
[2m[36m(func pid=96422)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=96422)[0m f1_macro: 0.27168767364029606
[2m[36m(func pid=96422)[0m f1_weighted: 0.3396506506651302
[2m[36m(func pid=96422)[0m f1_per_class: [0.554, 0.115, 0.197, 0.485, 0.078, 0.348, 0.37, 0.261, 0.122, 0.188]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.3232 | Steps: 4 | Val loss: 2.5508 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 12:30:19 (running for 00:32:25.83)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  0.225 |      0.313 |                   71 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.143 |      0.272 |                   38 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.256 |      0.33  |                   22 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.777 |      0.313 |                   12 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.3260261194029851
[2m[36m(func pid=89025)[0m top5: 0.8446828358208955
[2m[36m(func pid=89025)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=89025)[0m f1_macro: 0.31307443743393604
[2m[36m(func pid=89025)[0m f1_weighted: 0.3480346906763501
[2m[36m(func pid=89025)[0m f1_per_class: [0.383, 0.42, 0.449, 0.351, 0.085, 0.292, 0.361, 0.256, 0.22, 0.315]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=103447)[0m top1: 0.37779850746268656
[2m[36m(func pid=103447)[0m top5: 0.8922574626865671
[2m[36m(func pid=103447)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=103447)[0m f1_macro: 0.31572463947311513
[2m[36m(func pid=103447)[0m f1_weighted: 0.38370730034377876
[2m[36m(func pid=103447)[0m f1_per_class: [0.432, 0.479, 0.149, 0.284, 0.141, 0.374, 0.476, 0.272, 0.213, 0.339]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.0299 | Steps: 4 | Val loss: 1.7721 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.1200 | Steps: 4 | Val loss: 1.9729 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.7167 | Steps: 4 | Val loss: 10.8195 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=101169)[0m top1: 0.35634328358208955
[2m[36m(func pid=101169)[0m top5: 0.8777985074626866
[2m[36m(func pid=101169)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=101169)[0m f1_macro: 0.3281393140320354
[2m[36m(func pid=101169)[0m f1_weighted: 0.3844389711062748
[2m[36m(func pid=101169)[0m f1_per_class: [0.531, 0.356, 0.304, 0.422, 0.134, 0.429, 0.387, 0.326, 0.181, 0.211]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 0.5853 | Steps: 4 | Val loss: 2.8214 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=96422)[0m top1: 0.3111007462686567
[2m[36m(func pid=96422)[0m top5: 0.8143656716417911
[2m[36m(func pid=96422)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=96422)[0m f1_macro: 0.26806437063718386
[2m[36m(func pid=96422)[0m f1_weighted: 0.3372082575024289
[2m[36m(func pid=96422)[0m f1_per_class: [0.552, 0.145, 0.168, 0.469, 0.084, 0.331, 0.365, 0.27, 0.118, 0.178]
[2m[36m(func pid=96422)[0m 
== Status ==
Current time: 2024-01-07 12:30:25 (running for 00:32:31.23)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  0.717 |      0.267 |                   72 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.12  |      0.268 |                   39 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.03  |      0.328 |                   23 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.323 |      0.316 |                   13 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.25419776119402987
[2m[36m(func pid=89025)[0m top5: 0.7831156716417911
[2m[36m(func pid=89025)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=89025)[0m f1_macro: 0.26700079506066343
[2m[36m(func pid=89025)[0m f1_weighted: 0.2762999735083029
[2m[36m(func pid=89025)[0m f1_per_class: [0.406, 0.209, 0.421, 0.37, 0.063, 0.298, 0.232, 0.226, 0.202, 0.242]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=103447)[0m top1: 0.3414179104477612
[2m[36m(func pid=103447)[0m top5: 0.8731343283582089
[2m[36m(func pid=103447)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=103447)[0m f1_macro: 0.2972171214998703
[2m[36m(func pid=103447)[0m f1_weighted: 0.3688517844924042
[2m[36m(func pid=103447)[0m f1_per_class: [0.444, 0.41, 0.135, 0.405, 0.119, 0.33, 0.368, 0.296, 0.207, 0.257]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.0479 | Steps: 4 | Val loss: 1.7964 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.0663 | Steps: 4 | Val loss: 1.9656 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2646 | Steps: 4 | Val loss: 10.0230 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=101169)[0m top1: 0.35401119402985076
[2m[36m(func pid=101169)[0m top5: 0.8745335820895522
[2m[36m(func pid=101169)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=101169)[0m f1_macro: 0.3212098708935668
[2m[36m(func pid=101169)[0m f1_weighted: 0.3846115859509603
[2m[36m(func pid=101169)[0m f1_per_class: [0.525, 0.376, 0.273, 0.44, 0.12, 0.417, 0.369, 0.315, 0.163, 0.213]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 0.5332 | Steps: 4 | Val loss: 3.3012 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=96422)[0m top1: 0.31343283582089554
[2m[36m(func pid=96422)[0m top5: 0.8166977611940298
[2m[36m(func pid=96422)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=96422)[0m f1_macro: 0.2691552748468117
[2m[36m(func pid=96422)[0m f1_weighted: 0.33713119155834015
[2m[36m(func pid=96422)[0m f1_per_class: [0.523, 0.171, 0.189, 0.479, 0.087, 0.322, 0.344, 0.28, 0.13, 0.168]
[2m[36m(func pid=96422)[0m 
== Status ==
Current time: 2024-01-07 12:30:30 (running for 00:32:36.46)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  2.265 |      0.243 |                   73 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.066 |      0.269 |                   40 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.048 |      0.321 |                   24 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.585 |      0.297 |                   14 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.2733208955223881
[2m[36m(func pid=89025)[0m top5: 0.7905783582089553
[2m[36m(func pid=89025)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=89025)[0m f1_macro: 0.24283141476704925
[2m[36m(func pid=89025)[0m f1_weighted: 0.2962611053000834
[2m[36m(func pid=89025)[0m f1_per_class: [0.353, 0.231, 0.105, 0.375, 0.101, 0.333, 0.273, 0.262, 0.192, 0.204]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=103447)[0m top1: 0.31156716417910446
[2m[36m(func pid=103447)[0m top5: 0.8442164179104478
[2m[36m(func pid=103447)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=103447)[0m f1_macro: 0.27746108986897117
[2m[36m(func pid=103447)[0m f1_weighted: 0.33055530383093096
[2m[36m(func pid=103447)[0m f1_per_class: [0.473, 0.314, 0.131, 0.484, 0.101, 0.302, 0.227, 0.32, 0.217, 0.205]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.1629 | Steps: 4 | Val loss: 1.8228 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.9923 | Steps: 4 | Val loss: 1.9644 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.2765 | Steps: 4 | Val loss: 10.8631 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=101169)[0m top1: 0.3572761194029851
[2m[36m(func pid=101169)[0m top5: 0.8708022388059702
[2m[36m(func pid=101169)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=101169)[0m f1_macro: 0.31602073797688823
[2m[36m(func pid=101169)[0m f1_weighted: 0.3852226688626501
[2m[36m(func pid=101169)[0m f1_per_class: [0.504, 0.388, 0.218, 0.446, 0.112, 0.419, 0.359, 0.32, 0.178, 0.217]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 0.7457 | Steps: 4 | Val loss: 3.6380 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=96422)[0m top1: 0.3180970149253731
[2m[36m(func pid=96422)[0m top5: 0.8171641791044776
[2m[36m(func pid=96422)[0m f1_micro: 0.3180970149253731
[2m[36m(func pid=96422)[0m f1_macro: 0.27229741448259065
[2m[36m(func pid=96422)[0m f1_weighted: 0.3461739159483414
[2m[36m(func pid=96422)[0m f1_per_class: [0.487, 0.166, 0.229, 0.474, 0.082, 0.355, 0.373, 0.267, 0.127, 0.163]
[2m[36m(func pid=96422)[0m 
== Status ==
Current time: 2024-01-07 12:30:35 (running for 00:32:41.51)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.3205
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00015 | RUNNING    | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.276 |      0.264 |                   74 |
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.992 |      0.272 |                   41 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.163 |      0.316 |                   25 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.533 |      0.277 |                   15 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.30363805970149255
[2m[36m(func pid=89025)[0m top5: 0.7835820895522388
[2m[36m(func pid=89025)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=89025)[0m f1_macro: 0.2637913303145248
[2m[36m(func pid=89025)[0m f1_weighted: 0.29165740972351833
[2m[36m(func pid=89025)[0m f1_per_class: [0.346, 0.447, 0.085, 0.099, 0.119, 0.352, 0.376, 0.279, 0.178, 0.355]
[2m[36m(func pid=89025)[0m 
[2m[36m(func pid=103447)[0m top1: 0.30130597014925375
[2m[36m(func pid=103447)[0m top5: 0.8339552238805971
[2m[36m(func pid=103447)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=103447)[0m f1_macro: 0.2655087767382035
[2m[36m(func pid=103447)[0m f1_weighted: 0.3175146901694153
[2m[36m(func pid=103447)[0m f1_per_class: [0.455, 0.221, 0.138, 0.508, 0.085, 0.322, 0.208, 0.339, 0.206, 0.174]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.4706 | Steps: 4 | Val loss: 1.7990 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=89025)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.5505 | Steps: 4 | Val loss: 10.4802 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.0408 | Steps: 4 | Val loss: 1.9501 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=101169)[0m top1: 0.3736007462686567
[2m[36m(func pid=101169)[0m top5: 0.8759328358208955
[2m[36m(func pid=101169)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=101169)[0m f1_macro: 0.32667268757379
[2m[36m(func pid=101169)[0m f1_weighted: 0.3992417388852201
[2m[36m(func pid=101169)[0m f1_per_class: [0.467, 0.402, 0.224, 0.436, 0.147, 0.437, 0.392, 0.366, 0.193, 0.204]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 0.6653 | Steps: 4 | Val loss: 3.7791 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 12:30:40 (running for 00:32:46.81)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (5 PENDING, 3 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.992 |      0.272 |                   41 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.471 |      0.327 |                   26 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.746 |      0.266 |                   16 |
| train_9b9e8_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=89025)[0m top1: 0.33908582089552236
[2m[36m(func pid=89025)[0m top5: 0.8334888059701493
[2m[36m(func pid=89025)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=89025)[0m f1_macro: 0.2686222236060153
[2m[36m(func pid=89025)[0m f1_weighted: 0.31615464737717497
[2m[36m(func pid=89025)[0m f1_per_class: [0.444, 0.463, 0.098, 0.139, 0.054, 0.142, 0.486, 0.266, 0.205, 0.39]
[2m[36m(func pid=96422)[0m top1: 0.324160447761194
[2m[36m(func pid=96422)[0m top5: 0.8208955223880597
[2m[36m(func pid=96422)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=96422)[0m f1_macro: 0.28055878235421056
[2m[36m(func pid=96422)[0m f1_weighted: 0.3544457797290894
[2m[36m(func pid=96422)[0m f1_per_class: [0.528, 0.178, 0.229, 0.478, 0.08, 0.348, 0.387, 0.289, 0.122, 0.167]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.0372 | Steps: 4 | Val loss: 1.7383 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=103447)[0m top1: 0.30363805970149255
[2m[36m(func pid=103447)[0m top5: 0.8390858208955224
[2m[36m(func pid=103447)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=103447)[0m f1_macro: 0.2734777728242147
[2m[36m(func pid=103447)[0m f1_weighted: 0.33144784263109894
[2m[36m(func pid=103447)[0m f1_per_class: [0.52, 0.261, 0.151, 0.5, 0.079, 0.313, 0.245, 0.307, 0.19, 0.168]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.9255 | Steps: 4 | Val loss: 1.9401 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=101169)[0m top1: 0.3983208955223881
[2m[36m(func pid=101169)[0m top5: 0.8889925373134329
[2m[36m(func pid=101169)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=101169)[0m f1_macro: 0.3415219046580681
[2m[36m(func pid=101169)[0m f1_weighted: 0.42223075905437124
[2m[36m(func pid=101169)[0m f1_per_class: [0.461, 0.386, 0.24, 0.484, 0.202, 0.468, 0.421, 0.368, 0.178, 0.209]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 0.9130 | Steps: 4 | Val loss: 3.8097 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=96422)[0m top1: 0.32882462686567165
[2m[36m(func pid=96422)[0m top5: 0.8213619402985075
[2m[36m(func pid=96422)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=96422)[0m f1_macro: 0.2864165850491918
[2m[36m(func pid=96422)[0m f1_weighted: 0.3610240482827158
[2m[36m(func pid=96422)[0m f1_per_class: [0.533, 0.221, 0.224, 0.483, 0.084, 0.353, 0.376, 0.28, 0.146, 0.163]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.1806 | Steps: 4 | Val loss: 1.7699 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=103447)[0m top1: 0.3138992537313433
[2m[36m(func pid=103447)[0m top5: 0.8535447761194029
[2m[36m(func pid=103447)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=103447)[0m f1_macro: 0.28125662718217875
[2m[36m(func pid=103447)[0m f1_weighted: 0.34109402431772057
[2m[36m(func pid=103447)[0m f1_per_class: [0.508, 0.296, 0.171, 0.481, 0.089, 0.326, 0.27, 0.306, 0.193, 0.173]
== Status ==
Current time: 2024-01-07 12:30:46 (running for 00:32:52.80)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.925 |      0.286 |                   43 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.037 |      0.342 |                   27 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.665 |      0.273 |                   17 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=107845)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=107845)[0m Configuration completed!
[2m[36m(func pid=107845)[0m New optimizer parameters:
[2m[36m(func pid=107845)[0m SGD (
[2m[36m(func pid=107845)[0m Parameter Group 0
[2m[36m(func pid=107845)[0m     dampening: 0
[2m[36m(func pid=107845)[0m     differentiable: False
[2m[36m(func pid=107845)[0m     foreach: None
[2m[36m(func pid=107845)[0m     lr: 0.1
[2m[36m(func pid=107845)[0m     maximize: False
[2m[36m(func pid=107845)[0m     momentum: 0.99
[2m[36m(func pid=107845)[0m     nesterov: False
[2m[36m(func pid=107845)[0m     weight_decay: 1e-05
[2m[36m(func pid=107845)[0m )
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.0205 | Steps: 4 | Val loss: 1.9592 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=101169)[0m top1: 0.3969216417910448
[2m[36m(func pid=101169)[0m top5: 0.8861940298507462
[2m[36m(func pid=101169)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=101169)[0m f1_macro: 0.33354046642315016
[2m[36m(func pid=101169)[0m f1_weighted: 0.42065641670060777
[2m[36m(func pid=101169)[0m f1_per_class: [0.4, 0.361, 0.218, 0.499, 0.182, 0.465, 0.418, 0.375, 0.201, 0.217]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 0.4561 | Steps: 4 | Val loss: 3.4913 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 12:30:52 (running for 00:32:58.61)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  2.02  |      0.27  |                   44 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.181 |      0.334 |                   28 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.913 |      0.281 |                   18 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=96422)[0m top1: 0.30923507462686567
[2m[36m(func pid=96422)[0m top5: 0.8143656716417911
[2m[36m(func pid=96422)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=96422)[0m f1_macro: 0.2703885956915396
[2m[36m(func pid=96422)[0m f1_weighted: 0.34282147943898605
[2m[36m(func pid=96422)[0m f1_per_class: [0.496, 0.218, 0.187, 0.468, 0.078, 0.339, 0.341, 0.282, 0.134, 0.161]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.1834 | Steps: 4 | Val loss: 1.7598 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0830 | Steps: 4 | Val loss: 5.1589 | Batch size: 32 | lr: 0.1 | Duration: 4.74s
[2m[36m(func pid=103447)[0m top1: 0.37033582089552236
[2m[36m(func pid=103447)[0m top5: 0.8861940298507462
[2m[36m(func pid=103447)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=103447)[0m f1_macro: 0.30342530413478197
[2m[36m(func pid=103447)[0m f1_weighted: 0.40078764186920013
[2m[36m(func pid=103447)[0m f1_per_class: [0.46, 0.356, 0.186, 0.495, 0.094, 0.351, 0.424, 0.234, 0.234, 0.2]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m top1: 0.40205223880597013
[2m[36m(func pid=101169)[0m top5: 0.8899253731343284
[2m[36m(func pid=101169)[0m f1_micro: 0.4020522388059702
[2m[36m(func pid=101169)[0m f1_macro: 0.3382199882963916
[2m[36m(func pid=101169)[0m f1_weighted: 0.42387127628208354
[2m[36m(func pid=101169)[0m f1_per_class: [0.37, 0.404, 0.265, 0.507, 0.173, 0.449, 0.403, 0.355, 0.237, 0.218]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.9109 | Steps: 4 | Val loss: 1.9339 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=107845)[0m top1: 0.006063432835820896
[2m[36m(func pid=107845)[0m top5: 0.4976679104477612
[2m[36m(func pid=107845)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=107845)[0m f1_macro: 0.0012059369202226345
[2m[36m(func pid=107845)[0m f1_weighted: 7.312117520006647e-05
[2m[36m(func pid=107845)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 0.7196 | Steps: 4 | Val loss: 3.2325 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.2505 | Steps: 4 | Val loss: 1.7486 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 12:30:58 (running for 00:33:04.09)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.911 |      0.28  |                   45 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.183 |      0.338 |                   29 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.456 |      0.303 |                   19 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  3.083 |      0.001 |                    1 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=96422)[0m top1: 0.3208955223880597
[2m[36m(func pid=96422)[0m top5: 0.820429104477612
[2m[36m(func pid=96422)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=96422)[0m f1_macro: 0.2796261570057041
[2m[36m(func pid=96422)[0m f1_weighted: 0.35378391874000714
[2m[36m(func pid=96422)[0m f1_per_class: [0.487, 0.235, 0.22, 0.471, 0.084, 0.344, 0.361, 0.285, 0.147, 0.163]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 4.1849 | Steps: 4 | Val loss: 2.3059 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=101169)[0m top1: 0.4221082089552239
[2m[36m(func pid=101169)[0m top5: 0.8936567164179104
[2m[36m(func pid=101169)[0m f1_micro: 0.4221082089552239
[2m[36m(func pid=101169)[0m f1_macro: 0.34966969443450874
[2m[36m(func pid=101169)[0m f1_weighted: 0.4407555094187089
[2m[36m(func pid=101169)[0m f1_per_class: [0.367, 0.442, 0.289, 0.533, 0.151, 0.437, 0.412, 0.375, 0.253, 0.237]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m top1: 0.4146455223880597
[2m[36m(func pid=103447)[0m top5: 0.9127798507462687
[2m[36m(func pid=103447)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=103447)[0m f1_macro: 0.3190323255250719
[2m[36m(func pid=103447)[0m f1_weighted: 0.4335087968095812
[2m[36m(func pid=103447)[0m f1_per_class: [0.448, 0.362, 0.234, 0.505, 0.149, 0.39, 0.526, 0.124, 0.22, 0.232]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.8069 | Steps: 4 | Val loss: 1.9220 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=107845)[0m top1: 0.20662313432835822
[2m[36m(func pid=107845)[0m top5: 0.867070895522388
[2m[36m(func pid=107845)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=107845)[0m f1_macro: 0.24660480934579138
[2m[36m(func pid=107845)[0m f1_weighted: 0.2125660610070354
[2m[36m(func pid=107845)[0m f1_per_class: [0.127, 0.258, 0.556, 0.142, 0.063, 0.379, 0.18, 0.338, 0.027, 0.396]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.8051 | Steps: 4 | Val loss: 1.7668 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.1575 | Steps: 4 | Val loss: 3.2794 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 12:31:03 (running for 00:33:09.64)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.807 |      0.279 |                   46 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.251 |      0.35  |                   30 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.72  |      0.319 |                   20 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.185 |      0.247 |                    2 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=96422)[0m top1: 0.31716417910447764
[2m[36m(func pid=96422)[0m top5: 0.8218283582089553
[2m[36m(func pid=96422)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=96422)[0m f1_macro: 0.27880336259213095
[2m[36m(func pid=96422)[0m f1_weighted: 0.34384957674942784
[2m[36m(func pid=96422)[0m f1_per_class: [0.453, 0.228, 0.222, 0.463, 0.077, 0.381, 0.322, 0.307, 0.147, 0.189]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.7496 | Steps: 4 | Val loss: 2.9564 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=101169)[0m top1: 0.41744402985074625
[2m[36m(func pid=101169)[0m top5: 0.894589552238806
[2m[36m(func pid=101169)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=101169)[0m f1_macro: 0.3389011666487926
[2m[36m(func pid=101169)[0m f1_weighted: 0.4371355925908188
[2m[36m(func pid=101169)[0m f1_per_class: [0.365, 0.43, 0.28, 0.541, 0.138, 0.425, 0.413, 0.355, 0.216, 0.227]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m top1: 0.4183768656716418
[2m[36m(func pid=103447)[0m top5: 0.9127798507462687
[2m[36m(func pid=103447)[0m f1_micro: 0.4183768656716418
[2m[36m(func pid=103447)[0m f1_macro: 0.32283378858386313
[2m[36m(func pid=103447)[0m f1_weighted: 0.4306525522186444
[2m[36m(func pid=103447)[0m f1_per_class: [0.424, 0.405, 0.268, 0.459, 0.168, 0.382, 0.539, 0.112, 0.238, 0.234]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.9022 | Steps: 4 | Val loss: 1.9193 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=107845)[0m top1: 0.27005597014925375
[2m[36m(func pid=107845)[0m top5: 0.8288246268656716
[2m[36m(func pid=107845)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=107845)[0m f1_macro: 0.28629323511711835
[2m[36m(func pid=107845)[0m f1_weighted: 0.2655582944918375
[2m[36m(func pid=107845)[0m f1_per_class: [0.223, 0.032, 0.636, 0.443, 0.121, 0.388, 0.188, 0.311, 0.125, 0.395]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.0503 | Steps: 4 | Val loss: 1.7533 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.7782 | Steps: 4 | Val loss: 3.3710 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 12:31:09 (running for 00:33:15.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.902 |      0.285 |                   47 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.805 |      0.339 |                   31 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.157 |      0.323 |                   21 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  2.75  |      0.286 |                    3 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=96422)[0m top1: 0.31576492537313433
[2m[36m(func pid=96422)[0m top5: 0.8222947761194029
[2m[36m(func pid=96422)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=96422)[0m f1_macro: 0.2846726395267368
[2m[36m(func pid=96422)[0m f1_weighted: 0.3373346422141635
[2m[36m(func pid=96422)[0m f1_per_class: [0.463, 0.226, 0.255, 0.464, 0.07, 0.39, 0.289, 0.327, 0.157, 0.204]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.6962 | Steps: 4 | Val loss: 5.6269 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=101169)[0m top1: 0.4230410447761194
[2m[36m(func pid=101169)[0m top5: 0.902518656716418
[2m[36m(func pid=101169)[0m f1_micro: 0.4230410447761194
[2m[36m(func pid=101169)[0m f1_macro: 0.34883164316469545
[2m[36m(func pid=101169)[0m f1_weighted: 0.4401412957940591
[2m[36m(func pid=101169)[0m f1_per_class: [0.381, 0.437, 0.295, 0.541, 0.148, 0.419, 0.415, 0.359, 0.236, 0.256]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m top1: 0.4123134328358209
[2m[36m(func pid=103447)[0m top5: 0.9090485074626866
[2m[36m(func pid=103447)[0m f1_micro: 0.4123134328358209
[2m[36m(func pid=103447)[0m f1_macro: 0.3350477599570377
[2m[36m(func pid=103447)[0m f1_weighted: 0.425869463618906
[2m[36m(func pid=103447)[0m f1_per_class: [0.41, 0.417, 0.342, 0.469, 0.158, 0.38, 0.49, 0.198, 0.221, 0.263]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.9164 | Steps: 4 | Val loss: 1.9102 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=107845)[0m top1: 0.22014925373134328
[2m[36m(func pid=107845)[0m top5: 0.7430037313432836
[2m[36m(func pid=107845)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=107845)[0m f1_macro: 0.2615867695908361
[2m[36m(func pid=107845)[0m f1_weighted: 0.22875798189005017
[2m[36m(func pid=107845)[0m f1_per_class: [0.127, 0.016, 0.769, 0.345, 0.075, 0.343, 0.193, 0.281, 0.17, 0.297]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.2003 | Steps: 4 | Val loss: 1.8157 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.5824 | Steps: 4 | Val loss: 3.4530 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=96422)[0m top1: 0.31763059701492535
[2m[36m(func pid=96422)[0m top5: 0.8222947761194029
[2m[36m(func pid=96422)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=96422)[0m f1_macro: 0.2796816742760989
[2m[36m(func pid=96422)[0m f1_weighted: 0.3388413103358334
[2m[36m(func pid=96422)[0m f1_per_class: [0.453, 0.223, 0.242, 0.481, 0.064, 0.356, 0.297, 0.308, 0.168, 0.204]
== Status ==
Current time: 2024-01-07 12:31:15 (running for 00:33:21.03)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.916 |      0.28  |                   48 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.05  |      0.349 |                   32 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.778 |      0.335 |                   22 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  2.696 |      0.262 |                    4 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8820 | Steps: 4 | Val loss: 4.8975 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=101169)[0m top1: 0.4183768656716418
[2m[36m(func pid=101169)[0m top5: 0.8913246268656716
[2m[36m(func pid=101169)[0m f1_micro: 0.4183768656716418
[2m[36m(func pid=101169)[0m f1_macro: 0.3451163248588808
[2m[36m(func pid=101169)[0m f1_weighted: 0.4361763990824588
[2m[36m(func pid=101169)[0m f1_per_class: [0.389, 0.453, 0.274, 0.534, 0.143, 0.387, 0.41, 0.377, 0.229, 0.257]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m top1: 0.4001865671641791
[2m[36m(func pid=103447)[0m top5: 0.9048507462686567
[2m[36m(func pid=103447)[0m f1_micro: 0.4001865671641791
[2m[36m(func pid=103447)[0m f1_macro: 0.346694789636859
[2m[36m(func pid=103447)[0m f1_weighted: 0.41611587921876103
[2m[36m(func pid=103447)[0m f1_per_class: [0.438, 0.408, 0.361, 0.457, 0.176, 0.39, 0.444, 0.315, 0.24, 0.237]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.29757462686567165
[2m[36m(func pid=107845)[0m top5: 0.851679104477612
[2m[36m(func pid=107845)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=107845)[0m f1_macro: 0.2859165646025916
[2m[36m(func pid=107845)[0m f1_weighted: 0.3256181037375723
[2m[36m(func pid=107845)[0m f1_per_class: [0.222, 0.262, 0.706, 0.276, 0.083, 0.385, 0.453, 0.13, 0.161, 0.18]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.8472 | Steps: 4 | Val loss: 1.8983 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.5274 | Steps: 4 | Val loss: 1.8642 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.9735 | Steps: 4 | Val loss: 3.6727 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 12:31:20 (running for 00:33:26.48)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.847 |      0.288 |                   49 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.2   |      0.345 |                   33 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.582 |      0.347 |                   23 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  2.882 |      0.286 |                    5 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=96422)[0m top1: 0.32509328358208955
[2m[36m(func pid=96422)[0m top5: 0.8297574626865671
[2m[36m(func pid=96422)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=96422)[0m f1_macro: 0.28827410648329355
[2m[36m(func pid=96422)[0m f1_weighted: 0.3469988150075684
[2m[36m(func pid=96422)[0m f1_per_class: [0.47, 0.236, 0.255, 0.477, 0.088, 0.367, 0.313, 0.322, 0.156, 0.198]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.9170 | Steps: 4 | Val loss: 6.4301 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=101169)[0m top1: 0.417910447761194
[2m[36m(func pid=101169)[0m top5: 0.8894589552238806
[2m[36m(func pid=101169)[0m f1_micro: 0.417910447761194
[2m[36m(func pid=101169)[0m f1_macro: 0.3389344330751024
[2m[36m(func pid=101169)[0m f1_weighted: 0.4361303120019606
[2m[36m(func pid=101169)[0m f1_per_class: [0.407, 0.453, 0.228, 0.548, 0.115, 0.363, 0.406, 0.358, 0.262, 0.249]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m top1: 0.3917910447761194
[2m[36m(func pid=103447)[0m top5: 0.8950559701492538
[2m[36m(func pid=103447)[0m f1_micro: 0.3917910447761195
[2m[36m(func pid=103447)[0m f1_macro: 0.35048679972661084
[2m[36m(func pid=103447)[0m f1_weighted: 0.4068334890932858
[2m[36m(func pid=103447)[0m f1_per_class: [0.456, 0.403, 0.4, 0.465, 0.165, 0.376, 0.403, 0.358, 0.252, 0.227]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.3031716417910448
[2m[36m(func pid=107845)[0m top5: 0.8111007462686567
[2m[36m(func pid=107845)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=107845)[0m f1_macro: 0.28870853808390395
[2m[36m(func pid=107845)[0m f1_weighted: 0.28405303550430316
[2m[36m(func pid=107845)[0m f1_per_class: [0.512, 0.455, 0.296, 0.372, 0.154, 0.351, 0.08, 0.281, 0.189, 0.197]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.6439 | Steps: 4 | Val loss: 1.8874 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.8982 | Steps: 4 | Val loss: 1.8411 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.9173 | Steps: 4 | Val loss: 4.1107 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.4035 | Steps: 4 | Val loss: 5.8870 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 12:31:26 (running for 00:33:32.20)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.644 |      0.289 |                   50 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.527 |      0.339 |                   34 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.973 |      0.35  |                   24 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  1.917 |      0.289 |                    6 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=96422)[0m top1: 0.33302238805970147
[2m[36m(func pid=96422)[0m top5: 0.8278917910447762
[2m[36m(func pid=96422)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=96422)[0m f1_macro: 0.2889503763047005
[2m[36m(func pid=96422)[0m f1_weighted: 0.354445251279655
[2m[36m(func pid=96422)[0m f1_per_class: [0.469, 0.26, 0.211, 0.49, 0.094, 0.359, 0.314, 0.323, 0.174, 0.196]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m top1: 0.42723880597014924
[2m[36m(func pid=101169)[0m top5: 0.8922574626865671
[2m[36m(func pid=101169)[0m f1_micro: 0.4272388059701493
[2m[36m(func pid=101169)[0m f1_macro: 0.3552356895801503
[2m[36m(func pid=101169)[0m f1_weighted: 0.43958785006467654
[2m[36m(func pid=101169)[0m f1_per_class: [0.489, 0.449, 0.26, 0.564, 0.126, 0.368, 0.391, 0.382, 0.262, 0.261]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m top1: 0.34654850746268656
[2m[36m(func pid=103447)[0m top5: 0.8726679104477612
[2m[36m(func pid=103447)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=103447)[0m f1_macro: 0.3104242858689857
[2m[36m(func pid=103447)[0m f1_weighted: 0.36201942262053427
[2m[36m(func pid=103447)[0m f1_per_class: [0.436, 0.338, 0.295, 0.434, 0.151, 0.321, 0.35, 0.338, 0.237, 0.203]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.40904850746268656
[2m[36m(func pid=107845)[0m top5: 0.9118470149253731
[2m[36m(func pid=107845)[0m f1_micro: 0.40904850746268656
[2m[36m(func pid=107845)[0m f1_macro: 0.34769829522829654
[2m[36m(func pid=107845)[0m f1_weighted: 0.4106117598948497
[2m[36m(func pid=107845)[0m f1_per_class: [0.605, 0.543, 0.144, 0.49, 0.123, 0.374, 0.312, 0.341, 0.209, 0.336]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.8286 | Steps: 4 | Val loss: 1.8757 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.0315 | Steps: 4 | Val loss: 1.8599 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.6498 | Steps: 4 | Val loss: 4.3636 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.9015 | Steps: 4 | Val loss: 7.7084 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 12:31:31 (running for 00:33:37.73)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.829 |      0.294 |                   51 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.898 |      0.355 |                   35 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.917 |      0.31  |                   25 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  3.404 |      0.348 |                    7 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=96422)[0m top1: 0.3400186567164179
[2m[36m(func pid=96422)[0m top5: 0.8348880597014925
[2m[36m(func pid=96422)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=96422)[0m f1_macro: 0.2944405113970561
[2m[36m(func pid=96422)[0m f1_weighted: 0.3687450687337292
[2m[36m(func pid=96422)[0m f1_per_class: [0.459, 0.286, 0.182, 0.489, 0.084, 0.369, 0.341, 0.334, 0.195, 0.206]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m top1: 0.42024253731343286
[2m[36m(func pid=101169)[0m top5: 0.8997201492537313
[2m[36m(func pid=101169)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=101169)[0m f1_macro: 0.35681584267536715
[2m[36m(func pid=101169)[0m f1_weighted: 0.43515400913445423
[2m[36m(func pid=101169)[0m f1_per_class: [0.525, 0.447, 0.267, 0.556, 0.109, 0.38, 0.38, 0.375, 0.251, 0.277]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m top1: 0.3148320895522388
[2m[36m(func pid=103447)[0m top5: 0.8763992537313433
[2m[36m(func pid=103447)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=103447)[0m f1_macro: 0.2912580132905574
[2m[36m(func pid=103447)[0m f1_weighted: 0.33456710143865537
[2m[36m(func pid=103447)[0m f1_per_class: [0.435, 0.31, 0.299, 0.408, 0.162, 0.294, 0.318, 0.321, 0.195, 0.17]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.37593283582089554
[2m[36m(func pid=107845)[0m top5: 0.902518656716418
[2m[36m(func pid=107845)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=107845)[0m f1_macro: 0.2978839138416021
[2m[36m(func pid=107845)[0m f1_weighted: 0.3953251577104875
[2m[36m(func pid=107845)[0m f1_per_class: [0.633, 0.181, 0.086, 0.535, 0.057, 0.216, 0.501, 0.32, 0.147, 0.301]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.1142 | Steps: 4 | Val loss: 1.8538 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.8571 | Steps: 4 | Val loss: 1.8678 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.2415 | Steps: 4 | Val loss: 4.6025 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.7913 | Steps: 4 | Val loss: 11.1941 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 12:31:37 (running for 00:33:43.06)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.829 |      0.294 |                   51 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.114 |      0.349 |                   37 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.65  |      0.291 |                   26 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  1.902 |      0.298 |                    8 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.41091417910447764
[2m[36m(func pid=101169)[0m top5: 0.8964552238805971
[2m[36m(func pid=101169)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=101169)[0m f1_macro: 0.3491775439145703
[2m[36m(func pid=101169)[0m f1_weighted: 0.43439011334737765
[2m[36m(func pid=101169)[0m f1_per_class: [0.557, 0.398, 0.25, 0.545, 0.113, 0.381, 0.42, 0.367, 0.221, 0.239]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=96422)[0m top1: 0.32742537313432835
[2m[36m(func pid=96422)[0m top5: 0.8316231343283582
[2m[36m(func pid=96422)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=96422)[0m f1_macro: 0.2888491538753296
[2m[36m(func pid=96422)[0m f1_weighted: 0.3520038603058962
[2m[36m(func pid=96422)[0m f1_per_class: [0.484, 0.277, 0.174, 0.485, 0.077, 0.376, 0.292, 0.327, 0.187, 0.21]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m top1: 0.3185634328358209
[2m[36m(func pid=103447)[0m top5: 0.8652052238805971
[2m[36m(func pid=103447)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=103447)[0m f1_macro: 0.2841159801895804
[2m[36m(func pid=103447)[0m f1_weighted: 0.3446584989936638
[2m[36m(func pid=103447)[0m f1_per_class: [0.382, 0.289, 0.232, 0.382, 0.171, 0.289, 0.388, 0.369, 0.187, 0.153]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.27845149253731344
[2m[36m(func pid=107845)[0m top5: 0.8549440298507462
[2m[36m(func pid=107845)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=107845)[0m f1_macro: 0.2603170773313691
[2m[36m(func pid=107845)[0m f1_weighted: 0.3020053157846482
[2m[36m(func pid=107845)[0m f1_per_class: [0.605, 0.089, 0.066, 0.306, 0.05, 0.171, 0.466, 0.343, 0.176, 0.331]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.8476 | Steps: 4 | Val loss: 1.8422 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.7552 | Steps: 4 | Val loss: 1.8679 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.3256 | Steps: 4 | Val loss: 4.9050 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.8534 | Steps: 4 | Val loss: 13.0426 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:31:42 (running for 00:33:48.38)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.857 |      0.289 |                   52 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.848 |      0.354 |                   38 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.242 |      0.284 |                   27 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  1.791 |      0.26  |                    9 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.4183768656716418
[2m[36m(func pid=101169)[0m top5: 0.9057835820895522
[2m[36m(func pid=101169)[0m f1_micro: 0.4183768656716418
[2m[36m(func pid=101169)[0m f1_macro: 0.3536093449056621
[2m[36m(func pid=101169)[0m f1_weighted: 0.44168847453891225
[2m[36m(func pid=101169)[0m f1_per_class: [0.533, 0.437, 0.25, 0.535, 0.119, 0.378, 0.435, 0.361, 0.211, 0.277]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=96422)[0m top1: 0.3269589552238806
[2m[36m(func pid=96422)[0m top5: 0.8297574626865671
[2m[36m(func pid=96422)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=96422)[0m f1_macro: 0.2822830210178009
[2m[36m(func pid=96422)[0m f1_weighted: 0.34641001899725155
[2m[36m(func pid=96422)[0m f1_per_class: [0.42, 0.3, 0.169, 0.485, 0.089, 0.368, 0.266, 0.331, 0.192, 0.204]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m top1: 0.32322761194029853
[2m[36m(func pid=103447)[0m top5: 0.8558768656716418
[2m[36m(func pid=103447)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=103447)[0m f1_macro: 0.2856683106200481
[2m[36m(func pid=103447)[0m f1_weighted: 0.3550653358925484
[2m[36m(func pid=103447)[0m f1_per_class: [0.374, 0.285, 0.218, 0.391, 0.163, 0.29, 0.412, 0.396, 0.18, 0.146]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.23880597014925373
[2m[36m(func pid=107845)[0m top5: 0.7882462686567164
[2m[36m(func pid=107845)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=107845)[0m f1_macro: 0.23136310547274216
[2m[36m(func pid=107845)[0m f1_weighted: 0.25586653852948543
[2m[36m(func pid=107845)[0m f1_per_class: [0.468, 0.134, 0.065, 0.168, 0.057, 0.243, 0.396, 0.361, 0.181, 0.24]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.9539 | Steps: 4 | Val loss: 1.9285 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 1.4044 | Steps: 4 | Val loss: 4.9391 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.6441 | Steps: 4 | Val loss: 1.8558 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 5.2765 | Steps: 4 | Val loss: 12.9482 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 12:31:47 (running for 00:33:53.53)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.755 |      0.282 |                   53 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.954 |      0.346 |                   39 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.326 |      0.286 |                   28 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  3.853 |      0.231 |                   10 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.40671641791044777
[2m[36m(func pid=101169)[0m top5: 0.9006529850746269
[2m[36m(func pid=101169)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=101169)[0m f1_macro: 0.3464835039029436
[2m[36m(func pid=101169)[0m f1_weighted: 0.43383048746112085
[2m[36m(func pid=101169)[0m f1_per_class: [0.512, 0.435, 0.205, 0.522, 0.107, 0.389, 0.419, 0.359, 0.224, 0.293]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m top1: 0.32882462686567165
[2m[36m(func pid=103447)[0m top5: 0.8628731343283582
[2m[36m(func pid=103447)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=103447)[0m f1_macro: 0.2805970421563513
[2m[36m(func pid=103447)[0m f1_weighted: 0.3626218955537645
[2m[36m(func pid=103447)[0m f1_per_class: [0.33, 0.294, 0.182, 0.4, 0.143, 0.299, 0.427, 0.384, 0.175, 0.172]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m top1: 0.32742537313432835
[2m[36m(func pid=96422)[0m top5: 0.8344216417910447
[2m[36m(func pid=96422)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=96422)[0m f1_macro: 0.2890254600624351
[2m[36m(func pid=96422)[0m f1_weighted: 0.34427477305677695
[2m[36m(func pid=96422)[0m f1_per_class: [0.488, 0.315, 0.178, 0.483, 0.078, 0.375, 0.245, 0.329, 0.191, 0.21]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=107845)[0m top1: 0.25513059701492535
[2m[36m(func pid=107845)[0m top5: 0.769589552238806
[2m[36m(func pid=107845)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=107845)[0m f1_macro: 0.23335022560819318
[2m[36m(func pid=107845)[0m f1_weighted: 0.245454794284679
[2m[36m(func pid=107845)[0m f1_per_class: [0.289, 0.34, 0.122, 0.036, 0.163, 0.253, 0.369, 0.377, 0.182, 0.202]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.6207 | Steps: 4 | Val loss: 2.0806 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.5401 | Steps: 4 | Val loss: 5.2817 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.5958 | Steps: 4 | Val loss: 1.8474 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.3809 | Steps: 4 | Val loss: 11.5732 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=101169)[0m top1: 0.3810634328358209
[2m[36m(func pid=101169)[0m top5: 0.8805970149253731
[2m[36m(func pid=101169)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=101169)[0m f1_macro: 0.3316505726996574
[2m[36m(func pid=101169)[0m f1_weighted: 0.4137756414039512
[2m[36m(func pid=101169)[0m f1_per_class: [0.538, 0.434, 0.153, 0.499, 0.091, 0.384, 0.383, 0.32, 0.22, 0.295]
[2m[36m(func pid=101169)[0m 
== Status ==
Current time: 2024-01-07 12:31:52 (running for 00:33:58.81)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.644 |      0.289 |                   54 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.621 |      0.332 |                   40 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.404 |      0.281 |                   29 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  5.277 |      0.233 |                   11 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=103447)[0m top1: 0.29617537313432835
[2m[36m(func pid=103447)[0m top5: 0.8484141791044776
[2m[36m(func pid=103447)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=103447)[0m f1_macro: 0.2603600408338605
[2m[36m(func pid=103447)[0m f1_weighted: 0.32411957917766565
[2m[36m(func pid=103447)[0m f1_per_class: [0.235, 0.244, 0.172, 0.333, 0.168, 0.297, 0.395, 0.396, 0.165, 0.2]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m top1: 0.3316231343283582
[2m[36m(func pid=96422)[0m top5: 0.8372201492537313
[2m[36m(func pid=96422)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=96422)[0m f1_macro: 0.2926034652169585
[2m[36m(func pid=96422)[0m f1_weighted: 0.3491145378242595
[2m[36m(func pid=96422)[0m f1_per_class: [0.485, 0.303, 0.198, 0.484, 0.068, 0.385, 0.261, 0.333, 0.21, 0.199]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=107845)[0m top1: 0.3069029850746269
[2m[36m(func pid=107845)[0m top5: 0.8465485074626866
[2m[36m(func pid=107845)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=107845)[0m f1_macro: 0.27699141633105795
[2m[36m(func pid=107845)[0m f1_weighted: 0.3067756287196785
[2m[36m(func pid=107845)[0m f1_per_class: [0.349, 0.428, 0.262, 0.159, 0.225, 0.317, 0.409, 0.198, 0.171, 0.252]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.8134 | Steps: 4 | Val loss: 2.0709 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 0.2588 | Steps: 4 | Val loss: 5.6907 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.7554 | Steps: 4 | Val loss: 1.8545 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.7613 | Steps: 4 | Val loss: 11.3029 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=101169)[0m top1: 0.38199626865671643
[2m[36m(func pid=101169)[0m top5: 0.8922574626865671== Status ==
Current time: 2024-01-07 12:31:58 (running for 00:34:04.01)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.596 |      0.293 |                   55 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.813 |      0.333 |                   41 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.54  |      0.26  |                   30 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  2.381 |      0.277 |                   12 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)



[2m[36m(func pid=101169)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=101169)[0m f1_macro: 0.3325040991030449
[2m[36m(func pid=101169)[0m f1_weighted: 0.41287604498313174
[2m[36m(func pid=101169)[0m f1_per_class: [0.517, 0.452, 0.164, 0.473, 0.103, 0.39, 0.393, 0.315, 0.216, 0.301]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m top1: 0.26119402985074625
[2m[36m(func pid=103447)[0m top5: 0.8171641791044776
[2m[36m(func pid=103447)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=103447)[0m f1_macro: 0.2340936038452333
[2m[36m(func pid=103447)[0m f1_weighted: 0.28665243794947204
[2m[36m(func pid=103447)[0m f1_per_class: [0.179, 0.206, 0.13, 0.255, 0.122, 0.275, 0.379, 0.37, 0.186, 0.239]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m top1: 0.3255597014925373
[2m[36m(func pid=96422)[0m top5: 0.8353544776119403
[2m[36m(func pid=96422)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=96422)[0m f1_macro: 0.28762659850064387
[2m[36m(func pid=96422)[0m f1_weighted: 0.3418211031258281
[2m[36m(func pid=96422)[0m f1_per_class: [0.441, 0.279, 0.189, 0.487, 0.075, 0.384, 0.249, 0.329, 0.237, 0.206]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=107845)[0m top1: 0.35447761194029853
[2m[36m(func pid=107845)[0m top5: 0.875
[2m[36m(func pid=107845)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=107845)[0m f1_macro: 0.3026891410433173
[2m[36m(func pid=107845)[0m f1_weighted: 0.3558014342679864
[2m[36m(func pid=107845)[0m f1_per_class: [0.361, 0.465, 0.44, 0.363, 0.235, 0.325, 0.389, 0.0, 0.202, 0.247]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.7841 | Steps: 4 | Val loss: 2.1576 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.5772 | Steps: 4 | Val loss: 5.8240 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.6268 | Steps: 4 | Val loss: 1.8517 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 4.1658 | Steps: 4 | Val loss: 12.6737 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:32:03 (running for 00:34:09.42)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.755 |      0.288 |                   56 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.784 |      0.328 |                   42 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.259 |      0.234 |                   31 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  1.761 |      0.303 |                   13 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.36613805970149255
[2m[36m(func pid=101169)[0m top5: 0.8838619402985075
[2m[36m(func pid=101169)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=101169)[0m f1_macro: 0.3284576386948979
[2m[36m(func pid=101169)[0m f1_weighted: 0.39614372510137685
[2m[36m(func pid=101169)[0m f1_per_class: [0.552, 0.456, 0.16, 0.44, 0.095, 0.394, 0.363, 0.312, 0.216, 0.297]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m top1: 0.2681902985074627
[2m[36m(func pid=103447)[0m top5: 0.8157649253731343
[2m[36m(func pid=103447)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=103447)[0m f1_macro: 0.24321032518080155
[2m[36m(func pid=103447)[0m f1_weighted: 0.2939703799196914
[2m[36m(func pid=103447)[0m f1_per_class: [0.163, 0.235, 0.152, 0.227, 0.126, 0.292, 0.408, 0.347, 0.191, 0.29]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m top1: 0.32649253731343286
[2m[36m(func pid=96422)[0m top5: 0.8330223880597015
[2m[36m(func pid=96422)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=96422)[0m f1_macro: 0.2901581275766106
[2m[36m(func pid=96422)[0m f1_weighted: 0.3399398933915766
[2m[36m(func pid=96422)[0m f1_per_class: [0.474, 0.283, 0.179, 0.484, 0.073, 0.378, 0.239, 0.352, 0.233, 0.206]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=107845)[0m top1: 0.353544776119403
[2m[36m(func pid=107845)[0m top5: 0.8684701492537313
[2m[36m(func pid=107845)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=107845)[0m f1_macro: 0.31320733121668204
[2m[36m(func pid=107845)[0m f1_weighted: 0.3453849061240273
[2m[36m(func pid=107845)[0m f1_per_class: [0.393, 0.371, 0.595, 0.461, 0.237, 0.341, 0.307, 0.0, 0.199, 0.229]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.4688 | Steps: 4 | Val loss: 2.2342 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.9829 | Steps: 4 | Val loss: 5.9040 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.7628 | Steps: 4 | Val loss: 1.8404 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 10.5709 | Steps: 4 | Val loss: 14.4490 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 12:32:08 (running for 00:34:14.67)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.627 |      0.29  |                   57 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.469 |      0.319 |                   43 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.577 |      0.243 |                   32 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.166 |      0.313 |                   14 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.353544776119403
[2m[36m(func pid=101169)[0m top5: 0.8815298507462687
[2m[36m(func pid=101169)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=101169)[0m f1_macro: 0.3192688268417149
[2m[36m(func pid=101169)[0m f1_weighted: 0.3839712481219052
[2m[36m(func pid=101169)[0m f1_per_class: [0.539, 0.45, 0.143, 0.429, 0.093, 0.4, 0.339, 0.298, 0.212, 0.291]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m top1: 0.27238805970149255
[2m[36m(func pid=103447)[0m top5: 0.8255597014925373
[2m[36m(func pid=103447)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=103447)[0m f1_macro: 0.2544098634385311
[2m[36m(func pid=103447)[0m f1_weighted: 0.29678034107263707
[2m[36m(func pid=103447)[0m f1_per_class: [0.16, 0.275, 0.185, 0.207, 0.107, 0.29, 0.408, 0.366, 0.194, 0.352]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m top1: 0.324160447761194
[2m[36m(func pid=96422)[0m top5: 0.8414179104477612
[2m[36m(func pid=96422)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=96422)[0m f1_macro: 0.29137421533875835
[2m[36m(func pid=96422)[0m f1_weighted: 0.3347385041908079
[2m[36m(func pid=96422)[0m f1_per_class: [0.467, 0.301, 0.222, 0.47, 0.09, 0.383, 0.226, 0.341, 0.212, 0.202]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=107845)[0m top1: 0.3260261194029851
[2m[36m(func pid=107845)[0m top5: 0.840018656716418
[2m[36m(func pid=107845)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=107845)[0m f1_macro: 0.30811923245081657
[2m[36m(func pid=107845)[0m f1_weighted: 0.3193934446647287
[2m[36m(func pid=107845)[0m f1_per_class: [0.451, 0.416, 0.611, 0.429, 0.234, 0.275, 0.234, 0.061, 0.2, 0.17]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.8443 | Steps: 4 | Val loss: 2.3085 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.0017 | Steps: 4 | Val loss: 5.5655 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=101169)[0m top1: 0.34095149253731344
[2m[36m(func pid=101169)[0m top5: 0.8773320895522388
[2m[36m(func pid=101169)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=101169)[0m f1_macro: 0.3131356327810019
[2m[36m(func pid=101169)[0m f1_weighted: 0.36840984060350374
[2m[36m(func pid=101169)[0m f1_per_class: [0.564, 0.454, 0.137, 0.401, 0.094, 0.398, 0.315, 0.278, 0.199, 0.291]
[2m[36m(func pid=101169)[0m 
== Status ==
Current time: 2024-01-07 12:32:13 (running for 00:34:19.79)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.763 |      0.291 |                   58 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.844 |      0.313 |                   44 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.983 |      0.254 |                   33 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 10.571 |      0.308 |                   15 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.6716 | Steps: 4 | Val loss: 1.8297 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.2905 | Steps: 4 | Val loss: 15.8835 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=103447)[0m top1: 0.29850746268656714
[2m[36m(func pid=103447)[0m top5: 0.8404850746268657
[2m[36m(func pid=103447)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=103447)[0m f1_macro: 0.2704857807496824
[2m[36m(func pid=103447)[0m f1_weighted: 0.3256646899207339
[2m[36m(func pid=103447)[0m f1_per_class: [0.222, 0.326, 0.193, 0.307, 0.099, 0.274, 0.386, 0.353, 0.203, 0.342]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m top1: 0.3260261194029851
[2m[36m(func pid=96422)[0m top5: 0.8442164179104478
[2m[36m(func pid=96422)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=96422)[0m f1_macro: 0.2986918669009072
[2m[36m(func pid=96422)[0m f1_weighted: 0.3382217036372179
[2m[36m(func pid=96422)[0m f1_per_class: [0.529, 0.306, 0.238, 0.47, 0.103, 0.393, 0.23, 0.341, 0.183, 0.195]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=107845)[0m top1: 0.30597014925373134
[2m[36m(func pid=107845)[0m top5: 0.8199626865671642
[2m[36m(func pid=107845)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=107845)[0m f1_macro: 0.29566272345126865
[2m[36m(func pid=107845)[0m f1_weighted: 0.3010424870147878
[2m[36m(func pid=107845)[0m f1_per_class: [0.432, 0.382, 0.595, 0.41, 0.18, 0.228, 0.209, 0.171, 0.228, 0.122]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.5237 | Steps: 4 | Val loss: 2.2886 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.5946 | Steps: 4 | Val loss: 5.4340 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 12:32:19 (running for 00:34:24.95)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.672 |      0.299 |                   59 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.524 |      0.314 |                   45 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.002 |      0.27  |                   34 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  2.291 |      0.296 |                   16 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.34375
[2m[36m(func pid=101169)[0m top5: 0.8843283582089553
[2m[36m(func pid=101169)[0m f1_micro: 0.34375
[2m[36m(func pid=101169)[0m f1_macro: 0.31411234768526436
[2m[36m(func pid=101169)[0m f1_weighted: 0.3687289971309869
[2m[36m(func pid=101169)[0m f1_per_class: [0.549, 0.457, 0.149, 0.404, 0.106, 0.402, 0.31, 0.281, 0.193, 0.289]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 3.4729 | Steps: 4 | Val loss: 17.3225 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.5573 | Steps: 4 | Val loss: 1.8524 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=103447)[0m top1: 0.300839552238806
[2m[36m(func pid=103447)[0m top5: 0.8442164179104478
[2m[36m(func pid=103447)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=103447)[0m f1_macro: 0.2727252333613448
[2m[36m(func pid=103447)[0m f1_weighted: 0.32939164856341113
[2m[36m(func pid=103447)[0m f1_per_class: [0.271, 0.33, 0.166, 0.345, 0.083, 0.259, 0.362, 0.366, 0.198, 0.349]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.283115671641791
[2m[36m(func pid=107845)[0m top5: 0.8120335820895522
[2m[36m(func pid=107845)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=107845)[0m f1_macro: 0.282903819742425
[2m[36m(func pid=107845)[0m f1_weighted: 0.289845593587991
[2m[36m(func pid=107845)[0m f1_per_class: [0.443, 0.355, 0.468, 0.372, 0.141, 0.149, 0.221, 0.348, 0.232, 0.1]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.6599 | Steps: 4 | Val loss: 2.3185 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=96422)[0m top1: 0.31296641791044777
[2m[36m(func pid=96422)[0m top5: 0.8372201492537313
[2m[36m(func pid=96422)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=96422)[0m f1_macro: 0.2854038095301168
[2m[36m(func pid=96422)[0m f1_weighted: 0.3253694144069685
[2m[36m(func pid=96422)[0m f1_per_class: [0.471, 0.259, 0.231, 0.462, 0.091, 0.394, 0.224, 0.344, 0.185, 0.193]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.0324 | Steps: 4 | Val loss: 4.8839 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 12:32:24 (running for 00:34:30.09)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.557 |      0.285 |                   60 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.66  |      0.31  |                   46 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.595 |      0.273 |                   35 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  3.473 |      0.283 |                   17 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.33955223880597013
[2m[36m(func pid=101169)[0m top5: 0.878731343283582
[2m[36m(func pid=101169)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=101169)[0m f1_macro: 0.3100380023883998
[2m[36m(func pid=101169)[0m f1_weighted: 0.36596367052083634
[2m[36m(func pid=101169)[0m f1_per_class: [0.549, 0.441, 0.14, 0.432, 0.113, 0.404, 0.284, 0.288, 0.178, 0.271]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.7790 | Steps: 4 | Val loss: 16.5346 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.5367 | Steps: 4 | Val loss: 1.8321 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=103447)[0m top1: 0.355410447761194
[2m[36m(func pid=103447)[0m top5: 0.8740671641791045
[2m[36m(func pid=103447)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=103447)[0m f1_macro: 0.3124510728697052
[2m[36m(func pid=103447)[0m f1_weighted: 0.38094401106753567
[2m[36m(func pid=103447)[0m f1_per_class: [0.32, 0.399, 0.186, 0.376, 0.1, 0.3, 0.438, 0.378, 0.218, 0.409]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.2989738805970149
[2m[36m(func pid=107845)[0m top5: 0.8470149253731343
[2m[36m(func pid=107845)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=107845)[0m f1_macro: 0.28512537230971335
[2m[36m(func pid=107845)[0m f1_weighted: 0.30914295615453546
[2m[36m(func pid=107845)[0m f1_per_class: [0.42, 0.362, 0.343, 0.354, 0.113, 0.19, 0.276, 0.38, 0.274, 0.14]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.7691 | Steps: 4 | Val loss: 2.3226 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=96422)[0m top1: 0.32276119402985076
[2m[36m(func pid=96422)[0m top5: 0.8409514925373134
[2m[36m(func pid=96422)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=96422)[0m f1_macro: 0.2900161477611707
[2m[36m(func pid=96422)[0m f1_weighted: 0.3341262354186109
[2m[36m(func pid=96422)[0m f1_per_class: [0.467, 0.282, 0.235, 0.483, 0.1, 0.395, 0.221, 0.338, 0.188, 0.191]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.8886 | Steps: 4 | Val loss: 4.8648 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:32:29 (running for 00:34:35.35)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.537 |      0.29  |                   61 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.769 |      0.308 |                   47 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.032 |      0.312 |                   36 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  1.779 |      0.285 |                   18 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.34468283582089554
[2m[36m(func pid=101169)[0m top5: 0.8805970149253731
[2m[36m(func pid=101169)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=101169)[0m f1_macro: 0.3080344420410927
[2m[36m(func pid=101169)[0m f1_weighted: 0.36707171231629776
[2m[36m(func pid=101169)[0m f1_per_class: [0.515, 0.455, 0.144, 0.459, 0.11, 0.399, 0.258, 0.286, 0.192, 0.263]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 4.2505 | Steps: 4 | Val loss: 17.4284 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.5765 | Steps: 4 | Val loss: 1.8352 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=103447)[0m top1: 0.3689365671641791
[2m[36m(func pid=103447)[0m top5: 0.8847947761194029
[2m[36m(func pid=103447)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=103447)[0m f1_macro: 0.33222166457121693
[2m[36m(func pid=103447)[0m f1_weighted: 0.3836559281180991
[2m[36m(func pid=103447)[0m f1_per_class: [0.37, 0.456, 0.236, 0.387, 0.112, 0.33, 0.387, 0.365, 0.25, 0.429]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.30550373134328357
[2m[36m(func pid=107845)[0m top5: 0.8563432835820896
[2m[36m(func pid=107845)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=107845)[0m f1_macro: 0.2870802275365802
[2m[36m(func pid=107845)[0m f1_weighted: 0.31244392022371453
[2m[36m(func pid=107845)[0m f1_per_class: [0.411, 0.375, 0.235, 0.214, 0.084, 0.232, 0.393, 0.382, 0.259, 0.286]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.0216 | Steps: 4 | Val loss: 2.2902 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=96422)[0m top1: 0.322294776119403
[2m[36m(func pid=96422)[0m top5: 0.8376865671641791
[2m[36m(func pid=96422)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=96422)[0m f1_macro: 0.288772645350594
[2m[36m(func pid=96422)[0m f1_weighted: 0.3344819657763278
[2m[36m(func pid=96422)[0m f1_per_class: [0.466, 0.301, 0.211, 0.486, 0.092, 0.375, 0.214, 0.337, 0.211, 0.194]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.4073 | Steps: 4 | Val loss: 4.9440 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 12:32:34 (running for 00:34:40.63)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.577 |      0.289 |                   62 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.022 |      0.314 |                   48 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.889 |      0.332 |                   37 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.251 |      0.287 |                   19 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.35634328358208955
[2m[36m(func pid=101169)[0m top5: 0.8889925373134329
[2m[36m(func pid=101169)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=101169)[0m f1_macro: 0.3140836860049121
[2m[36m(func pid=101169)[0m f1_weighted: 0.3787339372865609
[2m[36m(func pid=101169)[0m f1_per_class: [0.531, 0.452, 0.138, 0.471, 0.103, 0.411, 0.282, 0.278, 0.202, 0.272]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.5552 | Steps: 4 | Val loss: 19.1173 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.5624 | Steps: 4 | Val loss: 1.8472 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=103447)[0m top1: 0.3689365671641791
[2m[36m(func pid=103447)[0m top5: 0.8857276119402985
[2m[36m(func pid=103447)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=103447)[0m f1_macro: 0.340401222938447
[2m[36m(func pid=103447)[0m f1_weighted: 0.3717655015264438
[2m[36m(func pid=103447)[0m f1_per_class: [0.437, 0.47, 0.268, 0.413, 0.131, 0.338, 0.306, 0.36, 0.254, 0.427]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.3316231343283582
[2m[36m(func pid=107845)[0m top5: 0.8418843283582089
[2m[36m(func pid=107845)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=107845)[0m f1_macro: 0.29282041872516085
[2m[36m(func pid=107845)[0m f1_weighted: 0.3169906923040292
[2m[36m(func pid=107845)[0m f1_per_class: [0.367, 0.365, 0.181, 0.104, 0.107, 0.236, 0.518, 0.319, 0.34, 0.391]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.6160 | Steps: 4 | Val loss: 2.3282 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=96422)[0m top1: 0.3208955223880597
[2m[36m(func pid=96422)[0m top5: 0.8311567164179104
[2m[36m(func pid=96422)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=96422)[0m f1_macro: 0.2845245869648024
[2m[36m(func pid=96422)[0m f1_weighted: 0.3279271716374647
[2m[36m(func pid=96422)[0m f1_per_class: [0.418, 0.299, 0.209, 0.489, 0.095, 0.387, 0.187, 0.342, 0.221, 0.199]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.5405 | Steps: 4 | Val loss: 5.2282 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 12:32:39 (running for 00:34:45.78)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.562 |      0.285 |                   63 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.616 |      0.305 |                   49 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.407 |      0.34  |                   38 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  1.555 |      0.293 |                   20 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.3414179104477612
[2m[36m(func pid=101169)[0m top5: 0.8843283582089553
[2m[36m(func pid=101169)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=101169)[0m f1_macro: 0.30502620516320755
[2m[36m(func pid=101169)[0m f1_weighted: 0.36638752980054434
[2m[36m(func pid=101169)[0m f1_per_class: [0.515, 0.418, 0.149, 0.467, 0.12, 0.396, 0.273, 0.277, 0.205, 0.231]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 4.5847 | Steps: 4 | Val loss: 21.4112 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=103447)[0m top1: 0.37220149253731344
[2m[36m(func pid=103447)[0m top5: 0.8763992537313433
[2m[36m(func pid=103447)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=103447)[0m f1_macro: 0.3396037017936015
[2m[36m(func pid=103447)[0m f1_weighted: 0.3662301901734065
[2m[36m(func pid=103447)[0m f1_per_class: [0.438, 0.476, 0.28, 0.445, 0.113, 0.342, 0.252, 0.36, 0.253, 0.437]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.5984 | Steps: 4 | Val loss: 1.8404 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=107845)[0m top1: 0.31902985074626866
[2m[36m(func pid=107845)[0m top5: 0.8166977611940298
[2m[36m(func pid=107845)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=107845)[0m f1_macro: 0.2732899174752201
[2m[36m(func pid=107845)[0m f1_weighted: 0.3040197367776036
[2m[36m(func pid=107845)[0m f1_per_class: [0.366, 0.323, 0.132, 0.098, 0.104, 0.212, 0.532, 0.257, 0.28, 0.429]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.7595 | Steps: 4 | Val loss: 2.3929 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=96422)[0m top1: 0.3260261194029851
[2m[36m(func pid=96422)[0m top5: 0.8334888059701493
[2m[36m(func pid=96422)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=96422)[0m f1_macro: 0.28753173516215014
[2m[36m(func pid=96422)[0m f1_weighted: 0.33431333646956646
[2m[36m(func pid=96422)[0m f1_per_class: [0.411, 0.322, 0.222, 0.494, 0.091, 0.377, 0.194, 0.346, 0.224, 0.193]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.4748 | Steps: 4 | Val loss: 5.3921 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=101169)[0m top1: 0.3269589552238806
[2m[36m(func pid=101169)[0m top5: 0.8754664179104478
[2m[36m(func pid=101169)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=101169)[0m f1_macro: 0.29898060053121595
[2m[36m(func pid=101169)[0m f1_weighted: 0.3558657816515961
[2m[36m(func pid=101169)[0m f1_per_class: [0.531, 0.365, 0.131, 0.451, 0.099, 0.403, 0.276, 0.297, 0.201, 0.236]
[2m[36m(func pid=101169)[0m 
== Status ==
Current time: 2024-01-07 12:32:45 (running for 00:34:51.04)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.598 |      0.288 |                   64 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.76  |      0.299 |                   50 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.541 |      0.34  |                   39 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.585 |      0.273 |                   21 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 4.3800 | Steps: 4 | Val loss: 22.9555 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=103447)[0m top1: 0.3670708955223881
[2m[36m(func pid=103447)[0m top5: 0.871268656716418
[2m[36m(func pid=103447)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=103447)[0m f1_macro: 0.3390448518533503
[2m[36m(func pid=103447)[0m f1_weighted: 0.3506813954795766
[2m[36m(func pid=103447)[0m f1_per_class: [0.464, 0.467, 0.342, 0.464, 0.156, 0.34, 0.192, 0.332, 0.257, 0.376]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.6423 | Steps: 4 | Val loss: 1.8363 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6192 | Steps: 4 | Val loss: 2.3605 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=107845)[0m top1: 0.2733208955223881
[2m[36m(func pid=107845)[0m top5: 0.8138992537313433
[2m[36m(func pid=107845)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=107845)[0m f1_macro: 0.25622669893142247
[2m[36m(func pid=107845)[0m f1_weighted: 0.2958775218013668
[2m[36m(func pid=107845)[0m f1_per_class: [0.38, 0.333, 0.087, 0.175, 0.061, 0.15, 0.455, 0.301, 0.171, 0.45]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=96422)[0m top1: 0.32882462686567165
[2m[36m(func pid=96422)[0m top5: 0.8325559701492538
[2m[36m(func pid=96422)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=96422)[0m f1_macro: 0.29129898973591917
[2m[36m(func pid=96422)[0m f1_weighted: 0.3466324539335583
[2m[36m(func pid=96422)[0m f1_per_class: [0.438, 0.308, 0.224, 0.493, 0.092, 0.395, 0.24, 0.337, 0.2, 0.185]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.8705 | Steps: 4 | Val loss: 5.7650 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 12:32:50 (running for 00:34:56.30)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.642 |      0.291 |                   65 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.619 |      0.305 |                   51 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.475 |      0.339 |                   40 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.38  |      0.256 |                   22 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.33908582089552236
[2m[36m(func pid=101169)[0m top5: 0.8815298507462687
[2m[36m(func pid=101169)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=101169)[0m f1_macro: 0.304728063265601
[2m[36m(func pid=101169)[0m f1_weighted: 0.3680195957037479
[2m[36m(func pid=101169)[0m f1_per_class: [0.531, 0.372, 0.151, 0.476, 0.104, 0.388, 0.294, 0.292, 0.217, 0.222]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 6.8718 | Steps: 4 | Val loss: 22.0735 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=103447)[0m top1: 0.35494402985074625
[2m[36m(func pid=103447)[0m top5: 0.8591417910447762
[2m[36m(func pid=103447)[0m f1_micro: 0.35494402985074625
[2m[36m(func pid=103447)[0m f1_macro: 0.32163160408050406
[2m[36m(func pid=103447)[0m f1_weighted: 0.33012182267204804
[2m[36m(func pid=103447)[0m f1_per_class: [0.4, 0.472, 0.317, 0.437, 0.139, 0.347, 0.149, 0.33, 0.245, 0.38]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.6483 | Steps: 4 | Val loss: 1.8335 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.6848 | Steps: 4 | Val loss: 2.2830 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=107845)[0m top1: 0.29524253731343286
[2m[36m(func pid=107845)[0m top5: 0.8484141791044776
[2m[36m(func pid=107845)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=107845)[0m f1_macro: 0.26755885938170626
[2m[36m(func pid=107845)[0m f1_weighted: 0.3137517972257107
[2m[36m(func pid=107845)[0m f1_per_class: [0.354, 0.409, 0.096, 0.239, 0.063, 0.154, 0.408, 0.295, 0.204, 0.452]
[2m[36m(func pid=107845)[0m 
== Status ==
Current time: 2024-01-07 12:32:55 (running for 00:35:01.37)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.648 |      0.288 |                   66 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.619 |      0.305 |                   51 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.871 |      0.322 |                   41 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  6.872 |      0.268 |                   23 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=96422)[0m top1: 0.31949626865671643
[2m[36m(func pid=96422)[0m top5: 0.8372201492537313
[2m[36m(func pid=96422)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=96422)[0m f1_macro: 0.2878382519306034
[2m[36m(func pid=96422)[0m f1_weighted: 0.32322038930822855
[2m[36m(func pid=96422)[0m f1_per_class: [0.453, 0.266, 0.261, 0.487, 0.096, 0.429, 0.18, 0.328, 0.186, 0.192]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m top1: 0.3498134328358209
[2m[36m(func pid=101169)[0m top5: 0.8885261194029851
[2m[36m(func pid=101169)[0m f1_micro: 0.3498134328358209
[2m[36m(func pid=101169)[0m f1_macro: 0.3136503928714045
[2m[36m(func pid=101169)[0m f1_weighted: 0.37923151357190443
[2m[36m(func pid=101169)[0m f1_per_class: [0.538, 0.37, 0.193, 0.485, 0.111, 0.384, 0.322, 0.309, 0.201, 0.223]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.3947 | Steps: 4 | Val loss: 5.7235 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 3.8802 | Steps: 4 | Val loss: 22.4836 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=103447)[0m top1: 0.3605410447761194
[2m[36m(func pid=103447)[0m top5: 0.8684701492537313
[2m[36m(func pid=103447)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=103447)[0m f1_macro: 0.32110575792260465
[2m[36m(func pid=103447)[0m f1_weighted: 0.3417601839717403
[2m[36m(func pid=103447)[0m f1_per_class: [0.412, 0.471, 0.286, 0.453, 0.141, 0.354, 0.176, 0.31, 0.252, 0.358]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5846 | Steps: 4 | Val loss: 2.2310 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=107845)[0m top1: 0.29850746268656714
[2m[36m(func pid=107845)[0m top5: 0.8418843283582089
[2m[36m(func pid=107845)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=107845)[0m f1_macro: 0.2686051108016889
[2m[36m(func pid=107845)[0m f1_weighted: 0.3225571138827254
[2m[36m(func pid=107845)[0m f1_per_class: [0.444, 0.356, 0.127, 0.395, 0.067, 0.214, 0.297, 0.287, 0.245, 0.253]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.4958 | Steps: 4 | Val loss: 1.8176 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 12:33:00 (running for 00:35:06.78)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.648 |      0.288 |                   66 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.585 |      0.322 |                   53 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.395 |      0.321 |                   42 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  3.88  |      0.269 |                   24 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.36380597014925375
[2m[36m(func pid=101169)[0m top5: 0.8927238805970149
[2m[36m(func pid=101169)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=101169)[0m f1_macro: 0.3216401921327253
[2m[36m(func pid=101169)[0m f1_weighted: 0.3977192161691321
[2m[36m(func pid=101169)[0m f1_per_class: [0.505, 0.372, 0.208, 0.48, 0.117, 0.398, 0.38, 0.33, 0.199, 0.227]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=96422)[0m top1: 0.32882462686567165
[2m[36m(func pid=96422)[0m top5: 0.8372201492537313
[2m[36m(func pid=96422)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=96422)[0m f1_macro: 0.2939429313434081
[2m[36m(func pid=96422)[0m f1_weighted: 0.3268198927354755
[2m[36m(func pid=96422)[0m f1_per_class: [0.455, 0.288, 0.279, 0.501, 0.102, 0.421, 0.165, 0.339, 0.192, 0.197]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.7075 | Steps: 4 | Val loss: 5.6278 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.0550 | Steps: 4 | Val loss: 24.3247 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=103447)[0m top1: 0.3591417910447761
[2m[36m(func pid=103447)[0m top5: 0.8647388059701493
[2m[36m(func pid=103447)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=103447)[0m f1_macro: 0.3164450857559324
[2m[36m(func pid=103447)[0m f1_weighted: 0.3491653780054232
[2m[36m(func pid=103447)[0m f1_per_class: [0.448, 0.453, 0.274, 0.467, 0.135, 0.352, 0.201, 0.308, 0.242, 0.286]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.9101 | Steps: 4 | Val loss: 2.2284 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=107845)[0m top1: 0.29244402985074625
[2m[36m(func pid=107845)[0m top5: 0.8134328358208955
[2m[36m(func pid=107845)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=107845)[0m f1_macro: 0.25596525304683565
[2m[36m(func pid=107845)[0m f1_weighted: 0.30680793571725623
[2m[36m(func pid=107845)[0m f1_per_class: [0.41, 0.301, 0.145, 0.467, 0.079, 0.265, 0.195, 0.271, 0.273, 0.153]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.4003 | Steps: 4 | Val loss: 1.8249 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=101169)[0m top1: 0.37220149253731344
[2m[36m(func pid=101169)[0m top5: 0.894589552238806
[2m[36m(func pid=101169)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=101169)[0m f1_macro: 0.33319316049302494
[2m[36m(func pid=101169)[0m f1_weighted: 0.40106374268690304
[2m[36m(func pid=101169)[0m f1_per_class: [0.545, 0.382, 0.24, 0.502, 0.091, 0.414, 0.354, 0.326, 0.224, 0.254]
[2m[36m(func pid=101169)[0m 
== Status ==
Current time: 2024-01-07 12:33:06 (running for 00:35:12.30)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.496 |      0.294 |                   67 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.91  |      0.333 |                   54 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.707 |      0.316 |                   43 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  3.055 |      0.256 |                   25 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.2633 | Steps: 4 | Val loss: 5.2904 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=96422)[0m top1: 0.32882462686567165
[2m[36m(func pid=96422)[0m top5: 0.8344216417910447
[2m[36m(func pid=96422)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=96422)[0m f1_macro: 0.28905671490280377
[2m[36m(func pid=96422)[0m f1_weighted: 0.3306564704150123
[2m[36m(func pid=96422)[0m f1_per_class: [0.42, 0.276, 0.253, 0.503, 0.107, 0.403, 0.194, 0.336, 0.193, 0.207]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 7.3011 | Steps: 4 | Val loss: 26.3049 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=103447)[0m top1: 0.36986940298507465
[2m[36m(func pid=103447)[0m top5: 0.8759328358208955
[2m[36m(func pid=103447)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=103447)[0m f1_macro: 0.3229259011028952
[2m[36m(func pid=103447)[0m f1_weighted: 0.38321484219774476
[2m[36m(func pid=103447)[0m f1_per_class: [0.447, 0.423, 0.283, 0.468, 0.174, 0.323, 0.347, 0.281, 0.258, 0.225]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.4962 | Steps: 4 | Val loss: 2.2468 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=107845)[0m top1: 0.29850746268656714
[2m[36m(func pid=107845)[0m top5: 0.789179104477612
[2m[36m(func pid=107845)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=107845)[0m f1_macro: 0.25282682479305363
[2m[36m(func pid=107845)[0m f1_weighted: 0.2999107959854565
[2m[36m(func pid=107845)[0m f1_per_class: [0.333, 0.328, 0.203, 0.483, 0.122, 0.321, 0.13, 0.28, 0.205, 0.123]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.4920 | Steps: 4 | Val loss: 1.8117 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 12:33:11 (running for 00:35:17.66)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.4   |      0.289 |                   68 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.496 |      0.335 |                   55 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.263 |      0.323 |                   44 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  7.301 |      0.253 |                   26 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.37453358208955223
[2m[36m(func pid=101169)[0m top5: 0.8955223880597015
[2m[36m(func pid=101169)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=101169)[0m f1_macro: 0.33485726780968517
[2m[36m(func pid=101169)[0m f1_weighted: 0.40480344057586604
[2m[36m(func pid=101169)[0m f1_per_class: [0.549, 0.404, 0.24, 0.502, 0.091, 0.411, 0.356, 0.312, 0.225, 0.257]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.1959 | Steps: 4 | Val loss: 5.4075 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 6.9641 | Steps: 4 | Val loss: 31.1319 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=96422)[0m top1: 0.333955223880597
[2m[36m(func pid=96422)[0m top5: 0.8409514925373134
[2m[36m(func pid=96422)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=96422)[0m f1_macro: 0.29284304919691473
[2m[36m(func pid=96422)[0m f1_weighted: 0.3368641586890114
[2m[36m(func pid=96422)[0m f1_per_class: [0.393, 0.27, 0.273, 0.495, 0.124, 0.427, 0.217, 0.341, 0.186, 0.204]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m top1: 0.365205223880597
[2m[36m(func pid=103447)[0m top5: 0.8722014925373134
[2m[36m(func pid=103447)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=103447)[0m f1_macro: 0.3125632853317211
[2m[36m(func pid=103447)[0m f1_weighted: 0.38949399232242576
[2m[36m(func pid=103447)[0m f1_per_class: [0.427, 0.384, 0.241, 0.46, 0.177, 0.299, 0.412, 0.271, 0.269, 0.186]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.283115671641791
[2m[36m(func pid=107845)[0m top5: 0.7444029850746269
[2m[36m(func pid=107845)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=107845)[0m f1_macro: 0.24500784282170035
[2m[36m(func pid=107845)[0m f1_weighted: 0.26877623007857326
[2m[36m(func pid=107845)[0m f1_per_class: [0.251, 0.338, 0.28, 0.42, 0.152, 0.335, 0.078, 0.281, 0.186, 0.13]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.6709 | Steps: 4 | Val loss: 2.2840 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.5265 | Steps: 4 | Val loss: 1.8167 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 12:33:17 (running for 00:35:23.06)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.492 |      0.293 |                   69 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.671 |      0.338 |                   56 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.196 |      0.313 |                   45 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  6.964 |      0.245 |                   27 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.37080223880597013
[2m[36m(func pid=101169)[0m top5: 0.8889925373134329
[2m[36m(func pid=101169)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=101169)[0m f1_macro: 0.33793685373052595
[2m[36m(func pid=101169)[0m f1_weighted: 0.4011919578176492
[2m[36m(func pid=101169)[0m f1_per_class: [0.557, 0.419, 0.296, 0.493, 0.081, 0.391, 0.352, 0.302, 0.229, 0.258]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.0858 | Steps: 4 | Val loss: 5.6542 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.0152 | Steps: 4 | Val loss: 33.9340 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=96422)[0m top1: 0.3376865671641791
[2m[36m(func pid=96422)[0m top5: 0.840018656716418
[2m[36m(func pid=96422)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=96422)[0m f1_macro: 0.28916766280409323
[2m[36m(func pid=96422)[0m f1_weighted: 0.3448207191207478
[2m[36m(func pid=96422)[0m f1_per_class: [0.37, 0.289, 0.229, 0.504, 0.125, 0.419, 0.23, 0.33, 0.2, 0.195]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m top1: 0.3423507462686567
[2m[36m(func pid=103447)[0m top5: 0.8614738805970149
[2m[36m(func pid=103447)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=103447)[0m f1_macro: 0.28789767317103837
[2m[36m(func pid=103447)[0m f1_weighted: 0.3709448336153114
[2m[36m(func pid=103447)[0m f1_per_class: [0.404, 0.289, 0.169, 0.45, 0.193, 0.268, 0.434, 0.249, 0.259, 0.164]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.9125 | Steps: 4 | Val loss: 2.2923 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=107845)[0m top1: 0.26072761194029853
[2m[36m(func pid=107845)[0m top5: 0.7262126865671642
[2m[36m(func pid=107845)[0m f1_micro: 0.26072761194029853
[2m[36m(func pid=107845)[0m f1_macro: 0.2414404445712405
[2m[36m(func pid=107845)[0m f1_weighted: 0.2561246244940972
[2m[36m(func pid=107845)[0m f1_per_class: [0.147, 0.344, 0.312, 0.357, 0.17, 0.333, 0.089, 0.3, 0.229, 0.133]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.5575 | Steps: 4 | Val loss: 1.8328 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 12:33:22 (running for 00:35:28.33)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.526 |      0.289 |                   70 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.912 |      0.325 |                   57 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.086 |      0.288 |                   46 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  2.015 |      0.241 |                   28 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.3614738805970149
[2m[36m(func pid=101169)[0m top5: 0.8880597014925373
[2m[36m(func pid=101169)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=101169)[0m f1_macro: 0.32468300144279755
[2m[36m(func pid=101169)[0m f1_weighted: 0.39954546686944875
[2m[36m(func pid=101169)[0m f1_per_class: [0.526, 0.382, 0.24, 0.479, 0.082, 0.377, 0.39, 0.313, 0.218, 0.24]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.2607 | Steps: 4 | Val loss: 6.1266 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 15.3591 | Steps: 4 | Val loss: 36.9514 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=96422)[0m top1: 0.3358208955223881
[2m[36m(func pid=96422)[0m top5: 0.8274253731343284
[2m[36m(func pid=96422)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=96422)[0m f1_macro: 0.28688386925554865
[2m[36m(func pid=96422)[0m f1_weighted: 0.3371326113565398
[2m[36m(func pid=96422)[0m f1_per_class: [0.371, 0.284, 0.216, 0.507, 0.114, 0.408, 0.204, 0.351, 0.208, 0.205]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m top1: 0.3278917910447761
[2m[36m(func pid=103447)[0m top5: 0.8428171641791045
[2m[36m(func pid=103447)[0m f1_micro: 0.3278917910447761
[2m[36m(func pid=103447)[0m f1_macro: 0.2712264728376418
[2m[36m(func pid=103447)[0m f1_weighted: 0.3563082651233069
[2m[36m(func pid=103447)[0m f1_per_class: [0.425, 0.18, 0.109, 0.443, 0.18, 0.268, 0.458, 0.232, 0.256, 0.161]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.7908 | Steps: 4 | Val loss: 2.2861 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=107845)[0m top1: 0.228544776119403
[2m[36m(func pid=107845)[0m top5: 0.7033582089552238
[2m[36m(func pid=107845)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=107845)[0m f1_macro: 0.235167345117058
[2m[36m(func pid=107845)[0m f1_weighted: 0.22860336170421613
[2m[36m(func pid=107845)[0m f1_per_class: [0.119, 0.325, 0.369, 0.294, 0.17, 0.312, 0.073, 0.289, 0.28, 0.121]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.5559 | Steps: 4 | Val loss: 1.8440 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 12:33:27 (running for 00:35:33.70)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.558 |      0.287 |                   71 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.791 |      0.323 |                   58 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.261 |      0.271 |                   47 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 15.359 |      0.235 |                   29 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.3591417910447761
[2m[36m(func pid=101169)[0m top5: 0.8880597014925373
[2m[36m(func pid=101169)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=101169)[0m f1_macro: 0.3234854899845737
[2m[36m(func pid=101169)[0m f1_weighted: 0.3967804582367589
[2m[36m(func pid=101169)[0m f1_per_class: [0.523, 0.381, 0.25, 0.422, 0.088, 0.363, 0.441, 0.313, 0.207, 0.249]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 0.2167 | Steps: 4 | Val loss: 6.5798 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 3.5301 | Steps: 4 | Val loss: 34.7874 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=96422)[0m top1: 0.31949626865671643
[2m[36m(func pid=96422)[0m top5: 0.835820895522388
[2m[36m(func pid=96422)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=96422)[0m f1_macro: 0.27455901578159553
[2m[36m(func pid=96422)[0m f1_weighted: 0.32291664774665374
[2m[36m(func pid=96422)[0m f1_per_class: [0.319, 0.252, 0.186, 0.482, 0.119, 0.417, 0.199, 0.344, 0.211, 0.215]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m top1: 0.3111007462686567
[2m[36m(func pid=103447)[0m top5: 0.8274253731343284
[2m[36m(func pid=103447)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=103447)[0m f1_macro: 0.2559151797899776
[2m[36m(func pid=103447)[0m f1_weighted: 0.33496136393530623
[2m[36m(func pid=103447)[0m f1_per_class: [0.418, 0.093, 0.089, 0.4, 0.174, 0.274, 0.479, 0.223, 0.248, 0.162]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.7376 | Steps: 4 | Val loss: 2.3062 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=107845)[0m top1: 0.21082089552238806
[2m[36m(func pid=107845)[0m top5: 0.7243470149253731
[2m[36m(func pid=107845)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=107845)[0m f1_macro: 0.22484731335661273
[2m[36m(func pid=107845)[0m f1_weighted: 0.21991600574252032
[2m[36m(func pid=107845)[0m f1_per_class: [0.134, 0.287, 0.444, 0.255, 0.108, 0.271, 0.122, 0.312, 0.183, 0.131]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.5345 | Steps: 4 | Val loss: 1.8589 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 12:33:33 (running for 00:35:38.96)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.556 |      0.275 |                   72 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.738 |      0.322 |                   59 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.217 |      0.256 |                   48 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  3.53  |      0.225 |                   30 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.3582089552238806
[2m[36m(func pid=101169)[0m top5: 0.8815298507462687
[2m[36m(func pid=101169)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=101169)[0m f1_macro: 0.321980867703927
[2m[36m(func pid=101169)[0m f1_weighted: 0.3912679180019541
[2m[36m(func pid=101169)[0m f1_per_class: [0.5, 0.401, 0.255, 0.398, 0.083, 0.35, 0.434, 0.334, 0.222, 0.242]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.8698 | Steps: 4 | Val loss: 7.1249 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.4455 | Steps: 4 | Val loss: 33.4325 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=96422)[0m top1: 0.3199626865671642
[2m[36m(func pid=96422)[0m top5: 0.8297574626865671
[2m[36m(func pid=96422)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=96422)[0m f1_macro: 0.27486990446686
[2m[36m(func pid=96422)[0m f1_weighted: 0.31835382152839076
[2m[36m(func pid=96422)[0m f1_per_class: [0.305, 0.216, 0.214, 0.488, 0.123, 0.432, 0.191, 0.359, 0.211, 0.209]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.6748 | Steps: 4 | Val loss: 2.2999 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=103447)[0m top1: 0.29524253731343286
[2m[36m(func pid=103447)[0m top5: 0.8055037313432836
[2m[36m(func pid=103447)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=103447)[0m f1_macro: 0.24307324107464678
[2m[36m(func pid=103447)[0m f1_weighted: 0.3195769538782659
[2m[36m(func pid=103447)[0m f1_per_class: [0.403, 0.085, 0.08, 0.346, 0.152, 0.255, 0.492, 0.231, 0.239, 0.149]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.22761194029850745
[2m[36m(func pid=107845)[0m top5: 0.7663246268656716
[2m[36m(func pid=107845)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=107845)[0m f1_macro: 0.25366933653875523
[2m[36m(func pid=107845)[0m f1_weighted: 0.2536147605705333
[2m[36m(func pid=107845)[0m f1_per_class: [0.222, 0.308, 0.595, 0.271, 0.08, 0.165, 0.252, 0.262, 0.132, 0.25]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.3747 | Steps: 4 | Val loss: 1.8306 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 12:33:38 (running for 00:35:44.25)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.535 |      0.275 |                   73 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.675 |      0.319 |                   60 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.87  |      0.243 |                   49 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  1.445 |      0.254 |                   31 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.3572761194029851
[2m[36m(func pid=101169)[0m top5: 0.8857276119402985
[2m[36m(func pid=101169)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=101169)[0m f1_macro: 0.3194776047773628
[2m[36m(func pid=101169)[0m f1_weighted: 0.38305229653130995
[2m[36m(func pid=101169)[0m f1_per_class: [0.476, 0.414, 0.276, 0.355, 0.089, 0.343, 0.446, 0.315, 0.224, 0.257]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.8959 | Steps: 4 | Val loss: 7.7749 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.6226 | Steps: 4 | Val loss: 35.3699 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=96422)[0m top1: 0.3292910447761194
[2m[36m(func pid=96422)[0m top5: 0.8381529850746269
[2m[36m(func pid=96422)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=96422)[0m f1_macro: 0.2858135511231405
[2m[36m(func pid=96422)[0m f1_weighted: 0.3256309203974595
[2m[36m(func pid=96422)[0m f1_per_class: [0.33, 0.236, 0.264, 0.501, 0.134, 0.435, 0.19, 0.352, 0.199, 0.216]
[2m[36m(func pid=96422)[0m 
[2m[36m(func pid=103447)[0m top1: 0.27705223880597013
[2m[36m(func pid=103447)[0m top5: 0.7751865671641791
[2m[36m(func pid=103447)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=103447)[0m f1_macro: 0.2280279366962358
[2m[36m(func pid=103447)[0m f1_weighted: 0.30068616802433384
[2m[36m(func pid=103447)[0m f1_per_class: [0.328, 0.082, 0.077, 0.304, 0.151, 0.254, 0.474, 0.239, 0.231, 0.141]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.8739 | Steps: 4 | Val loss: 2.4302 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=107845)[0m top1: 0.2126865671641791
[2m[36m(func pid=107845)[0m top5: 0.7961753731343284
[2m[36m(func pid=107845)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=107845)[0m f1_macro: 0.2549189078464572
[2m[36m(func pid=107845)[0m f1_weighted: 0.2468070806298199
[2m[36m(func pid=107845)[0m f1_per_class: [0.246, 0.237, 0.645, 0.247, 0.107, 0.11, 0.31, 0.276, 0.107, 0.265]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=96422)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.5708 | Steps: 4 | Val loss: 1.8230 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 12:33:43 (running for 00:35:49.55)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00016 | RUNNING    | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.375 |      0.286 |                   74 |
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.874 |      0.309 |                   61 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.896 |      0.228 |                   50 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  1.623 |      0.255 |                   32 |
| train_9b9e8_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.3353544776119403
[2m[36m(func pid=101169)[0m top5: 0.8815298507462687
[2m[36m(func pid=101169)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=101169)[0m f1_macro: 0.3093187708175119
[2m[36m(func pid=101169)[0m f1_weighted: 0.3555493960040062
[2m[36m(func pid=101169)[0m f1_per_class: [0.492, 0.401, 0.282, 0.268, 0.078, 0.351, 0.439, 0.321, 0.204, 0.257]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.3731 | Steps: 4 | Val loss: 7.8190 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 4.3253 | Steps: 4 | Val loss: 35.6196 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=96422)[0m top1: 0.32742537313432835
[2m[36m(func pid=96422)[0m top5: 0.8386194029850746
[2m[36m(func pid=96422)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=96422)[0m f1_macro: 0.2856994607936657
[2m[36m(func pid=96422)[0m f1_weighted: 0.32650637835860885
[2m[36m(func pid=96422)[0m f1_per_class: [0.354, 0.245, 0.267, 0.497, 0.129, 0.423, 0.198, 0.342, 0.194, 0.209]
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.2706 | Steps: 4 | Val loss: 2.3932 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=103447)[0m top1: 0.2681902985074627
[2m[36m(func pid=103447)[0m top5: 0.7723880597014925
[2m[36m(func pid=103447)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=103447)[0m f1_macro: 0.22514603010917708
[2m[36m(func pid=103447)[0m f1_weighted: 0.2950711644362476
[2m[36m(func pid=103447)[0m f1_per_class: [0.32, 0.09, 0.08, 0.32, 0.126, 0.255, 0.434, 0.248, 0.23, 0.147]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.22621268656716417
[2m[36m(func pid=107845)[0m top5: 0.8036380597014925
[2m[36m(func pid=107845)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=107845)[0m f1_macro: 0.27062600302807305
[2m[36m(func pid=107845)[0m f1_weighted: 0.2633247811212488
[2m[36m(func pid=107845)[0m f1_per_class: [0.321, 0.259, 0.645, 0.241, 0.086, 0.114, 0.354, 0.251, 0.108, 0.327]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=101169)[0m top1: 0.34375
[2m[36m(func pid=101169)[0m top5: 0.8801305970149254
[2m[36m(func pid=101169)[0m f1_micro: 0.34375
[2m[36m(func pid=101169)[0m f1_macro: 0.3040138794772273
[2m[36m(func pid=101169)[0m f1_weighted: 0.35993964717396376
[2m[36m(func pid=101169)[0m f1_per_class: [0.465, 0.407, 0.267, 0.241, 0.094, 0.355, 0.48, 0.313, 0.191, 0.227]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.5420 | Steps: 4 | Val loss: 7.6509 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 3.1055 | Steps: 4 | Val loss: 31.1982 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5993 | Steps: 4 | Val loss: 2.3812 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=103447)[0m top1: 0.2667910447761194
[2m[36m(func pid=103447)[0m top5: 0.777518656716418
[2m[36m(func pid=103447)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=103447)[0m f1_macro: 0.2296442166929169
[2m[36m(func pid=103447)[0m f1_weighted: 0.2918287758132178
[2m[36m(func pid=103447)[0m f1_per_class: [0.321, 0.109, 0.106, 0.357, 0.14, 0.266, 0.372, 0.264, 0.208, 0.153]
[2m[36m(func pid=107845)[0m top1: 0.2756529850746269
[2m[36m(func pid=107845)[0m top5: 0.8166977611940298
[2m[36m(func pid=107845)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=107845)[0m f1_macro: 0.28447986150326293
[2m[36m(func pid=107845)[0m f1_weighted: 0.307028725168636
[2m[36m(func pid=107845)[0m f1_per_class: [0.314, 0.299, 0.588, 0.275, 0.077, 0.182, 0.423, 0.229, 0.14, 0.317]
[2m[36m(func pid=101169)[0m top1: 0.35074626865671643
[2m[36m(func pid=101169)[0m top5: 0.8805970149253731
[2m[36m(func pid=101169)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=101169)[0m f1_macro: 0.3063890486444035
[2m[36m(func pid=101169)[0m f1_weighted: 0.3718991752907149
[2m[36m(func pid=101169)[0m f1_per_class: [0.453, 0.407, 0.239, 0.306, 0.1, 0.344, 0.462, 0.314, 0.217, 0.221]
== Status ==
Current time: 2024-01-07 12:33:48 (running for 00:35:54.88)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.271 |      0.304 |                   62 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.373 |      0.225 |                   51 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.325 |      0.271 |                   33 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 12:33:54 (running for 00:36:00.92)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.599 |      0.306 |                   63 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.373 |      0.225 |                   51 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.325 |      0.271 |                   33 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=115877)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=115877)[0m Configuration completed!
[2m[36m(func pid=115877)[0m New optimizer parameters:
[2m[36m(func pid=115877)[0m SGD (
[2m[36m(func pid=115877)[0m Parameter Group 0
[2m[36m(func pid=115877)[0m     dampening: 0
[2m[36m(func pid=115877)[0m     differentiable: False
[2m[36m(func pid=115877)[0m     foreach: None
[2m[36m(func pid=115877)[0m     lr: 0.0001
[2m[36m(func pid=115877)[0m     maximize: False
[2m[36m(func pid=115877)[0m     momentum: 0.9
[2m[36m(func pid=115877)[0m     nesterov: False
[2m[36m(func pid=115877)[0m     weight_decay: 1e-05
[2m[36m(func pid=115877)[0m )
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.6238 | Steps: 4 | Val loss: 2.3483 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.7996 | Steps: 4 | Val loss: 7.3897 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 7.8110 | Steps: 4 | Val loss: 27.7734 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0255 | Steps: 4 | Val loss: 2.3266 | Batch size: 32 | lr: 0.0001 | Duration: 4.82s
== Status ==
Current time: 2024-01-07 12:34:00 (running for 00:36:05.95)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.599 |      0.306 |                   63 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.542 |      0.23  |                   52 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  3.105 |      0.284 |                   34 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.36100746268656714
[2m[36m(func pid=101169)[0m top5: 0.8885261194029851
[2m[36m(func pid=101169)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=101169)[0m f1_macro: 0.30723571341332856
[2m[36m(func pid=101169)[0m f1_weighted: 0.37748635095028127
[2m[36m(func pid=101169)[0m f1_per_class: [0.4, 0.424, 0.226, 0.298, 0.114, 0.343, 0.479, 0.327, 0.225, 0.237]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m top1: 0.3204291044776119
[2m[36m(func pid=107845)[0m top5: 0.8428171641791045
[2m[36m(func pid=107845)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=107845)[0m f1_macro: 0.320895930825453
[2m[36m(func pid=107845)[0m f1_weighted: 0.34937439767362505
[2m[36m(func pid=107845)[0m f1_per_class: [0.333, 0.399, 0.72, 0.327, 0.088, 0.242, 0.433, 0.209, 0.165, 0.293]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=103447)[0m top1: 0.2751865671641791
[2m[36m(func pid=103447)[0m top5: 0.7863805970149254
[2m[36m(func pid=103447)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=103447)[0m f1_macro: 0.23855709070148984
[2m[36m(func pid=103447)[0m f1_weighted: 0.2958154989035136
[2m[36m(func pid=103447)[0m f1_per_class: [0.307, 0.153, 0.123, 0.398, 0.12, 0.287, 0.309, 0.271, 0.23, 0.186]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=115877)[0m top1: 0.1912313432835821
[2m[36m(func pid=115877)[0m top5: 0.5256529850746269
[2m[36m(func pid=115877)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=115877)[0m f1_macro: 0.11692158035854956
[2m[36m(func pid=115877)[0m f1_weighted: 0.13107546114020496
[2m[36m(func pid=115877)[0m f1_per_class: [0.242, 0.324, 0.0, 0.099, 0.011, 0.32, 0.012, 0.012, 0.0, 0.148]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4636 | Steps: 4 | Val loss: 2.3687 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 5.8324 | Steps: 4 | Val loss: 27.7590 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.9365 | Steps: 4 | Val loss: 7.1474 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9703 | Steps: 4 | Val loss: 2.3314 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 12:34:05 (running for 00:36:11.81)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.464 |      0.297 |                   65 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.8   |      0.239 |                   53 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  7.811 |      0.321 |                   35 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  3.025 |      0.117 |                    1 |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.35401119402985076
[2m[36m(func pid=101169)[0m top5: 0.882929104477612
[2m[36m(func pid=101169)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=101169)[0m f1_macro: 0.2969302619245561
[2m[36m(func pid=101169)[0m f1_weighted: 0.3710680909195157
[2m[36m(func pid=101169)[0m f1_per_class: [0.379, 0.423, 0.194, 0.283, 0.098, 0.335, 0.478, 0.323, 0.227, 0.23]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m top1: 0.3400186567164179
[2m[36m(func pid=107845)[0m top5: 0.8582089552238806
[2m[36m(func pid=107845)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=107845)[0m f1_macro: 0.32434175721980346
[2m[36m(func pid=107845)[0m f1_weighted: 0.35455510435753923
[2m[36m(func pid=107845)[0m f1_per_class: [0.345, 0.45, 0.643, 0.33, 0.114, 0.307, 0.393, 0.194, 0.192, 0.274]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=103447)[0m top1: 0.3031716417910448
[2m[36m(func pid=103447)[0m top5: 0.8017723880597015
[2m[36m(func pid=103447)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=103447)[0m f1_macro: 0.2649722590015682
[2m[36m(func pid=103447)[0m f1_weighted: 0.31560001356077966
[2m[36m(func pid=103447)[0m f1_per_class: [0.321, 0.255, 0.193, 0.462, 0.106, 0.301, 0.241, 0.297, 0.248, 0.226]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=115877)[0m top1: 0.1837686567164179
[2m[36m(func pid=115877)[0m top5: 0.5172574626865671
[2m[36m(func pid=115877)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=115877)[0m f1_macro: 0.09562849030699369
[2m[36m(func pid=115877)[0m f1_weighted: 0.1267383035832844
[2m[36m(func pid=115877)[0m f1_per_class: [0.092, 0.312, 0.0, 0.104, 0.01, 0.325, 0.009, 0.02, 0.0, 0.083]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.4182 | Steps: 4 | Val loss: 2.3580 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 4.3816 | Steps: 4 | Val loss: 30.1193 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.8077 | Steps: 4 | Val loss: 6.9998 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:34:11 (running for 00:36:17.08)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.418 |      0.301 |                   66 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.936 |      0.265 |                   54 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  5.832 |      0.324 |                   36 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.97  |      0.096 |                    2 |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.35867537313432835
[2m[36m(func pid=101169)[0m top5: 0.8819962686567164
[2m[36m(func pid=101169)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=101169)[0m f1_macro: 0.30142986514196746
[2m[36m(func pid=101169)[0m f1_weighted: 0.37657738618258185
[2m[36m(func pid=101169)[0m f1_per_class: [0.404, 0.429, 0.194, 0.296, 0.102, 0.345, 0.477, 0.315, 0.225, 0.227]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m top1: 0.3353544776119403
[2m[36m(func pid=107845)[0m top5: 0.8479477611940298
[2m[36m(func pid=107845)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=107845)[0m f1_macro: 0.31659904807646644
[2m[36m(func pid=107845)[0m f1_weighted: 0.3417226431831549
[2m[36m(func pid=107845)[0m f1_per_class: [0.332, 0.444, 0.524, 0.379, 0.214, 0.361, 0.284, 0.213, 0.229, 0.186]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9822 | Steps: 4 | Val loss: 2.3445 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=103447)[0m top1: 0.32322761194029853
[2m[36m(func pid=103447)[0m top5: 0.8106343283582089
[2m[36m(func pid=103447)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=103447)[0m f1_macro: 0.2860874810513464
[2m[36m(func pid=103447)[0m f1_weighted: 0.3322667903612785
[2m[36m(func pid=103447)[0m f1_per_class: [0.333, 0.341, 0.324, 0.479, 0.12, 0.295, 0.233, 0.298, 0.219, 0.219]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=115877)[0m top1: 0.1623134328358209
[2m[36m(func pid=115877)[0m top5: 0.49906716417910446
[2m[36m(func pid=115877)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=115877)[0m f1_macro: 0.08408029982052519
[2m[36m(func pid=115877)[0m f1_weighted: 0.11997508949858783
[2m[36m(func pid=115877)[0m f1_per_class: [0.063, 0.27, 0.0, 0.105, 0.017, 0.311, 0.018, 0.026, 0.0, 0.031]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.4238 | Steps: 4 | Val loss: 2.3154 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 3.7635 | Steps: 4 | Val loss: 36.7448 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.0246 | Steps: 4 | Val loss: 7.2315 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 12:34:16 (running for 00:36:22.14)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.424 |      0.31  |                   67 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.808 |      0.286 |                   55 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.382 |      0.317 |                   37 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.982 |      0.084 |                    3 |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.3675373134328358
[2m[36m(func pid=101169)[0m top5: 0.8871268656716418
[2m[36m(func pid=101169)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=101169)[0m f1_macro: 0.30982243474011223
[2m[36m(func pid=101169)[0m f1_weighted: 0.38672671759938887
[2m[36m(func pid=101169)[0m f1_per_class: [0.417, 0.441, 0.184, 0.325, 0.13, 0.365, 0.47, 0.302, 0.225, 0.238]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m top1: 0.3111007462686567
[2m[36m(func pid=107845)[0m top5: 0.8125
[2m[36m(func pid=107845)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=107845)[0m f1_macro: 0.29605045554379916
[2m[36m(func pid=107845)[0m f1_weighted: 0.30533692422685776
[2m[36m(func pid=107845)[0m f1_per_class: [0.403, 0.417, 0.406, 0.364, 0.174, 0.401, 0.167, 0.251, 0.255, 0.123]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9544 | Steps: 4 | Val loss: 2.3229 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=103447)[0m top1: 0.32975746268656714
[2m[36m(func pid=103447)[0m top5: 0.8157649253731343
[2m[36m(func pid=103447)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=103447)[0m f1_macro: 0.3073599670553272
[2m[36m(func pid=103447)[0m f1_weighted: 0.3297348205458192
[2m[36m(func pid=103447)[0m f1_per_class: [0.353, 0.415, 0.5, 0.471, 0.079, 0.276, 0.187, 0.326, 0.209, 0.257]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.0562 | Steps: 4 | Val loss: 2.3603 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=115877)[0m top1: 0.15858208955223882
[2m[36m(func pid=115877)[0m top5: 0.5209888059701493
[2m[36m(func pid=115877)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=115877)[0m f1_macro: 0.08475186706569714
[2m[36m(func pid=115877)[0m f1_weighted: 0.13040029579277568
[2m[36m(func pid=115877)[0m f1_per_class: [0.024, 0.262, 0.0, 0.133, 0.014, 0.284, 0.038, 0.065, 0.0, 0.028]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 7.6274 | Steps: 4 | Val loss: 47.5969 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.3987 | Steps: 4 | Val loss: 7.2833 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=101169)[0m top1: 0.3619402985074627
[2m[36m(func pid=101169)[0m top5: 0.8782649253731343
[2m[36m(func pid=101169)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=101169)[0m f1_macro: 0.2991570408576571
[2m[36m(func pid=101169)[0m f1_weighted: 0.3806961539698636
[2m[36m(func pid=101169)[0m f1_per_class: [0.374, 0.443, 0.169, 0.308, 0.112, 0.374, 0.464, 0.313, 0.226, 0.209]
== Status ==
Current time: 2024-01-07 12:34:21 (running for 00:36:27.69)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  1.056 |      0.299 |                   68 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.025 |      0.307 |                   56 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  3.763 |      0.296 |                   38 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.954 |      0.085 |                    4 |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m top1: 0.2658582089552239
[2m[36m(func pid=107845)[0m top5: 0.75
[2m[36m(func pid=107845)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=107845)[0m f1_macro: 0.2458206230182903
[2m[36m(func pid=107845)[0m f1_weighted: 0.25620842992668336
[2m[36m(func pid=107845)[0m f1_per_class: [0.468, 0.348, 0.255, 0.342, 0.0, 0.405, 0.065, 0.277, 0.22, 0.078]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=103447)[0m top1: 0.33488805970149255
[2m[36m(func pid=103447)[0m top5: 0.8255597014925373
[2m[36m(func pid=103447)[0m f1_micro: 0.33488805970149255
[2m[36m(func pid=103447)[0m f1_macro: 0.33092876810332245
[2m[36m(func pid=103447)[0m f1_weighted: 0.32437768820529156
[2m[36m(func pid=103447)[0m f1_per_class: [0.363, 0.434, 0.667, 0.458, 0.09, 0.262, 0.17, 0.31, 0.226, 0.33]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9750 | Steps: 4 | Val loss: 2.3190 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.6699 | Steps: 4 | Val loss: 2.4104 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=115877)[0m top1: 0.1478544776119403
[2m[36m(func pid=115877)[0m top5: 0.5279850746268657
[2m[36m(func pid=115877)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=115877)[0m f1_macro: 0.08584355675646202
[2m[36m(func pid=115877)[0m f1_weighted: 0.13191146457008318
[2m[36m(func pid=115877)[0m f1_per_class: [0.036, 0.214, 0.051, 0.146, 0.007, 0.279, 0.059, 0.066, 0.0, 0.0]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.9878 | Steps: 4 | Val loss: 7.3585 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 9.3734 | Steps: 4 | Val loss: 54.7667 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 12:34:27 (running for 00:36:32.99)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.67  |      0.291 |                   69 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  2.399 |      0.331 |                   57 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  7.627 |      0.246 |                   39 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.975 |      0.086 |                    5 |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.3493470149253731
[2m[36m(func pid=101169)[0m top5: 0.8773320895522388
[2m[36m(func pid=101169)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=101169)[0m f1_macro: 0.29140868261695707
[2m[36m(func pid=101169)[0m f1_weighted: 0.3753409336536589
[2m[36m(func pid=101169)[0m f1_per_class: [0.355, 0.441, 0.135, 0.345, 0.101, 0.366, 0.417, 0.31, 0.233, 0.21]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m top1: 0.24953358208955223
[2m[36m(func pid=107845)[0m top5: 0.7131529850746269
[2m[36m(func pid=107845)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=107845)[0m f1_macro: 0.24587150872191915
[2m[36m(func pid=107845)[0m f1_weighted: 0.24127524025134395
[2m[36m(func pid=107845)[0m f1_per_class: [0.557, 0.256, 0.226, 0.376, 0.091, 0.398, 0.033, 0.296, 0.163, 0.062]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=103447)[0m top1: 0.34095149253731344
[2m[36m(func pid=103447)[0m top5: 0.8213619402985075
[2m[36m(func pid=103447)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=103447)[0m f1_macro: 0.3380369188027238
[2m[36m(func pid=103447)[0m f1_weighted: 0.324665264793563
[2m[36m(func pid=103447)[0m f1_per_class: [0.368, 0.43, 0.69, 0.45, 0.089, 0.232, 0.184, 0.333, 0.239, 0.366]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.9673 | Steps: 4 | Val loss: 2.3054 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4441 | Steps: 4 | Val loss: 2.4138 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 22.9516 | Steps: 4 | Val loss: 60.4464 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.2005 | Steps: 4 | Val loss: 7.9483 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=115877)[0m top1: 0.15858208955223882
[2m[36m(func pid=115877)[0m top5: 0.5480410447761194
[2m[36m(func pid=115877)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=115877)[0m f1_macro: 0.08816072763866376
[2m[36m(func pid=115877)[0m f1_weighted: 0.13878899393301117
[2m[36m(func pid=115877)[0m f1_per_class: [0.018, 0.222, 0.05, 0.157, 0.008, 0.3, 0.06, 0.067, 0.0, 0.0]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:34:32 (running for 00:36:38.19)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.444 |      0.295 |                   70 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.988 |      0.338 |                   58 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  9.373 |      0.246 |                   40 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.967 |      0.088 |                    6 |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.34841417910447764
[2m[36m(func pid=101169)[0m top5: 0.8736007462686567
[2m[36m(func pid=101169)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=101169)[0m f1_macro: 0.29458623029619285
[2m[36m(func pid=101169)[0m f1_weighted: 0.3730528063015026
[2m[36m(func pid=101169)[0m f1_per_class: [0.34, 0.444, 0.141, 0.376, 0.117, 0.367, 0.374, 0.341, 0.223, 0.224]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=103447)[0m top1: 0.33115671641791045
[2m[36m(func pid=103447)[0m top5: 0.8199626865671642
[2m[36m(func pid=103447)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=103447)[0m f1_macro: 0.34062338769225575
[2m[36m(func pid=103447)[0m f1_weighted: 0.30641278455398285
[2m[36m(func pid=103447)[0m f1_per_class: [0.397, 0.427, 0.741, 0.392, 0.082, 0.188, 0.19, 0.334, 0.245, 0.41]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.23600746268656717
[2m[36m(func pid=107845)[0m top5: 0.6753731343283582
[2m[36m(func pid=107845)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=107845)[0m f1_macro: 0.2323933127185501
[2m[36m(func pid=107845)[0m f1_weighted: 0.23059212774460827
[2m[36m(func pid=107845)[0m f1_per_class: [0.532, 0.251, 0.16, 0.368, 0.059, 0.373, 0.015, 0.319, 0.186, 0.061]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9504 | Steps: 4 | Val loss: 2.3162 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4391 | Steps: 4 | Val loss: 2.3881 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 4.1204 | Steps: 4 | Val loss: 56.2785 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.3623 | Steps: 4 | Val loss: 7.8014 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=115877)[0m top1: 0.1501865671641791
[2m[36m(func pid=115877)[0m top5: 0.5307835820895522
[2m[36m(func pid=115877)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=115877)[0m f1_macro: 0.08850940845831554
[2m[36m(func pid=115877)[0m f1_weighted: 0.13156591122286188
[2m[36m(func pid=115877)[0m f1_per_class: [0.017, 0.218, 0.087, 0.137, 0.015, 0.284, 0.062, 0.064, 0.0, 0.0]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:34:37 (running for 00:36:43.43)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.439 |      0.298 |                   71 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.201 |      0.341 |                   59 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 22.952 |      0.232 |                   41 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.95  |      0.089 |                    7 |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.35447761194029853
[2m[36m(func pid=101169)[0m top5: 0.878731343283582
[2m[36m(func pid=101169)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=101169)[0m f1_macro: 0.2976620109907242
[2m[36m(func pid=101169)[0m f1_weighted: 0.37697776091998175
[2m[36m(func pid=101169)[0m f1_per_class: [0.32, 0.439, 0.158, 0.412, 0.113, 0.378, 0.348, 0.368, 0.225, 0.216]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m top1: 0.2691231343283582
[2m[36m(func pid=107845)[0m top5: 0.6893656716417911
[2m[36m(func pid=107845)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=107845)[0m f1_macro: 0.24751499618479503
[2m[36m(func pid=107845)[0m f1_weighted: 0.25614135221312595
[2m[36m(func pid=107845)[0m f1_per_class: [0.484, 0.364, 0.122, 0.386, 0.105, 0.346, 0.028, 0.31, 0.228, 0.103]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=103447)[0m top1: 0.3400186567164179
[2m[36m(func pid=103447)[0m top5: 0.8171641791044776
[2m[36m(func pid=103447)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=103447)[0m f1_macro: 0.33318844037785456
[2m[36m(func pid=103447)[0m f1_weighted: 0.32115879189698504
[2m[36m(func pid=103447)[0m f1_per_class: [0.441, 0.419, 0.625, 0.414, 0.093, 0.193, 0.225, 0.328, 0.239, 0.356]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9478 | Steps: 4 | Val loss: 2.3099 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.5357 | Steps: 4 | Val loss: 2.3881 | Batch size: 32 | lr: 0.001 | Duration: 2.71s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 15.5207 | Steps: 4 | Val loss: 52.7948 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.5675 | Steps: 4 | Val loss: 7.0985 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=115877)[0m top1: 0.1525186567164179
[2m[36m(func pid=115877)[0m top5: 0.5410447761194029
[2m[36m(func pid=115877)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=115877)[0m f1_macro: 0.090612550975707
[2m[36m(func pid=115877)[0m f1_weighted: 0.13620250187180288
[2m[36m(func pid=115877)[0m f1_per_class: [0.031, 0.222, 0.075, 0.14, 0.008, 0.286, 0.07, 0.073, 0.0, 0.0]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:34:42 (running for 00:36:48.62)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.536 |      0.301 |                   72 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.362 |      0.333 |                   60 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.12  |      0.248 |                   42 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.948 |      0.091 |                    8 |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.3596082089552239
[2m[36m(func pid=101169)[0m top5: 0.8796641791044776
[2m[36m(func pid=101169)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=101169)[0m f1_macro: 0.3009575579719829
[2m[36m(func pid=101169)[0m f1_weighted: 0.38372684226704545
[2m[36m(func pid=101169)[0m f1_per_class: [0.327, 0.432, 0.155, 0.435, 0.135, 0.367, 0.355, 0.375, 0.223, 0.204]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m top1: 0.2896455223880597
[2m[36m(func pid=107845)[0m top5: 0.7000932835820896
[2m[36m(func pid=107845)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=107845)[0m f1_macro: 0.25123995009187794
[2m[36m(func pid=107845)[0m f1_weighted: 0.27516043023289133
[2m[36m(func pid=107845)[0m f1_per_class: [0.463, 0.42, 0.132, 0.406, 0.072, 0.327, 0.057, 0.277, 0.2, 0.158]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=103447)[0m top1: 0.37173507462686567
[2m[36m(func pid=103447)[0m top5: 0.8339552238805971
[2m[36m(func pid=103447)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=103447)[0m f1_macro: 0.33185575375478743
[2m[36m(func pid=103447)[0m f1_weighted: 0.3618627888399464
[2m[36m(func pid=103447)[0m f1_per_class: [0.411, 0.416, 0.522, 0.502, 0.067, 0.199, 0.283, 0.324, 0.259, 0.337]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.9216 | Steps: 4 | Val loss: 2.2990 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.4633 | Steps: 4 | Val loss: 2.4552 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 3.8417 | Steps: 4 | Val loss: 48.3566 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.4383 | Steps: 4 | Val loss: 6.7325 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=115877)[0m top1: 0.1553171641791045
[2m[36m(func pid=115877)[0m top5: 0.5527052238805971
[2m[36m(func pid=115877)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=115877)[0m f1_macro: 0.09303166161497907
[2m[36m(func pid=115877)[0m f1_weighted: 0.13627568352251868
[2m[36m(func pid=115877)[0m f1_per_class: [0.03, 0.236, 0.07, 0.137, 0.017, 0.292, 0.061, 0.073, 0.014, 0.0]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:34:47 (running for 00:36:53.80)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.463 |      0.29  |                   73 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.567 |      0.332 |                   61 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 15.521 |      0.251 |                   43 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.922 |      0.093 |                    9 |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.3451492537313433
[2m[36m(func pid=101169)[0m top5: 0.8773320895522388
[2m[36m(func pid=101169)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=101169)[0m f1_macro: 0.29016144377485226
[2m[36m(func pid=101169)[0m f1_weighted: 0.3671720958370977
[2m[36m(func pid=101169)[0m f1_per_class: [0.295, 0.429, 0.138, 0.435, 0.118, 0.372, 0.302, 0.383, 0.21, 0.22]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m top1: 0.28078358208955223
[2m[36m(func pid=107845)[0m top5: 0.7299440298507462
[2m[36m(func pid=107845)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=107845)[0m f1_macro: 0.24731808508256367
[2m[36m(func pid=107845)[0m f1_weighted: 0.288557763399964
[2m[36m(func pid=107845)[0m f1_per_class: [0.444, 0.447, 0.118, 0.396, 0.057, 0.197, 0.145, 0.299, 0.159, 0.21]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=103447)[0m top1: 0.37919776119402987
[2m[36m(func pid=103447)[0m top5: 0.8386194029850746
[2m[36m(func pid=103447)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=103447)[0m f1_macro: 0.3153760975264547
[2m[36m(func pid=103447)[0m f1_weighted: 0.38018574917360076
[2m[36m(func pid=103447)[0m f1_per_class: [0.386, 0.402, 0.371, 0.526, 0.066, 0.241, 0.322, 0.318, 0.266, 0.255]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.9621 | Steps: 4 | Val loss: 2.2992 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.2545 | Steps: 4 | Val loss: 2.4656 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 9.8430 | Steps: 4 | Val loss: 46.2237 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.4159 | Steps: 4 | Val loss: 6.6008 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=115877)[0m top1: 0.15485074626865672
[2m[36m(func pid=115877)[0m top5: 0.558768656716418
[2m[36m(func pid=115877)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=115877)[0m f1_macro: 0.09486788587823423
[2m[36m(func pid=115877)[0m f1_weighted: 0.14053357376786557
[2m[36m(func pid=115877)[0m f1_per_class: [0.047, 0.224, 0.06, 0.149, 0.016, 0.292, 0.069, 0.078, 0.013, 0.0]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:34:53 (running for 00:36:58.95)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.32
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00017 | RUNNING    | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.463 |      0.29  |                   73 |
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.438 |      0.315 |                   62 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  3.842 |      0.247 |                   44 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.962 |      0.095 |                   10 |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=101169)[0m top1: 0.34328358208955223
[2m[36m(func pid=101169)[0m top5: 0.8745335820895522
[2m[36m(func pid=101169)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=101169)[0m f1_macro: 0.28524881494032095
[2m[36m(func pid=101169)[0m f1_weighted: 0.3700489402536113
[2m[36m(func pid=101169)[0m f1_per_class: [0.3, 0.384, 0.127, 0.465, 0.128, 0.368, 0.318, 0.357, 0.201, 0.205]
[2m[36m(func pid=101169)[0m 
[2m[36m(func pid=107845)[0m top1: 0.2891791044776119
[2m[36m(func pid=107845)[0m top5: 0.769589552238806
[2m[36m(func pid=107845)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=107845)[0m f1_macro: 0.23864484148870563
[2m[36m(func pid=107845)[0m f1_weighted: 0.3064790217578079
[2m[36m(func pid=107845)[0m f1_per_class: [0.4, 0.478, 0.135, 0.391, 0.058, 0.113, 0.251, 0.188, 0.136, 0.237]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=103447)[0m top1: 0.3894589552238806
[2m[36m(func pid=103447)[0m top5: 0.8372201492537313
[2m[36m(func pid=103447)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=103447)[0m f1_macro: 0.31930472747765704
[2m[36m(func pid=103447)[0m f1_weighted: 0.39369517317549585
[2m[36m(func pid=103447)[0m f1_per_class: [0.384, 0.334, 0.317, 0.544, 0.071, 0.267, 0.371, 0.344, 0.302, 0.258]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=101169)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.6201 | Steps: 4 | Val loss: 2.4626 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9034 | Steps: 4 | Val loss: 2.2903 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 7.3142 | Steps: 4 | Val loss: 43.4991 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.8000 | Steps: 4 | Val loss: 6.7174 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=101169)[0m top1: 0.3451492537313433
[2m[36m(func pid=101169)[0m top5: 0.8754664179104478
[2m[36m(func pid=101169)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=101169)[0m f1_macro: 0.28992491402100246
[2m[36m(func pid=101169)[0m f1_weighted: 0.3721215175594193
[2m[36m(func pid=101169)[0m f1_per_class: [0.32, 0.363, 0.129, 0.456, 0.145, 0.382, 0.338, 0.352, 0.205, 0.21]
== Status ==
Current time: 2024-01-07 12:34:58 (running for 00:37:04.26)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.31875
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 PENDING, 3 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.416 |      0.319 |                   63 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  9.843 |      0.239 |                   45 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.962 |      0.095 |                   10 |
| train_9b9e8_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=115877)[0m top1: 0.15485074626865672
[2m[36m(func pid=115877)[0m top5: 0.5750932835820896
[2m[36m(func pid=115877)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=115877)[0m f1_macro: 0.09661662000150169
[2m[36m(func pid=115877)[0m f1_weighted: 0.14842993839504875
[2m[36m(func pid=115877)[0m f1_per_class: [0.028, 0.209, 0.086, 0.167, 0.008, 0.281, 0.092, 0.081, 0.013, 0.0]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=103447)[0m top1: 0.37406716417910446
[2m[36m(func pid=103447)[0m top5: 0.8325559701492538
[2m[36m(func pid=103447)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=103447)[0m f1_macro: 0.2903556793582023
[2m[36m(func pid=103447)[0m f1_weighted: 0.38089706356683595
[2m[36m(func pid=103447)[0m f1_per_class: [0.326, 0.207, 0.222, 0.538, 0.103, 0.308, 0.408, 0.329, 0.248, 0.215]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.30223880597014924
[2m[36m(func pid=107845)[0m top5: 0.8027052238805971
[2m[36m(func pid=107845)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=107845)[0m f1_macro: 0.24089768830603592
[2m[36m(func pid=107845)[0m f1_weighted: 0.33601836476839353
[2m[36m(func pid=107845)[0m f1_per_class: [0.4, 0.454, 0.144, 0.388, 0.07, 0.092, 0.382, 0.156, 0.13, 0.192]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8852 | Steps: 4 | Val loss: 2.2867 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 10.1947 | Steps: 4 | Val loss: 40.9975 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4703 | Steps: 4 | Val loss: 6.7373 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=115877)[0m top1: 0.15205223880597016
[2m[36m(func pid=115877)[0m top5: 0.590018656716418
[2m[36m(func pid=115877)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=115877)[0m f1_macro: 0.0956414082526151
[2m[36m(func pid=115877)[0m f1_weighted: 0.14444263515495134
[2m[36m(func pid=115877)[0m f1_per_class: [0.024, 0.191, 0.073, 0.164, 0.017, 0.293, 0.085, 0.095, 0.014, 0.0]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=103447)[0m top1: 0.3666044776119403
[2m[36m(func pid=103447)[0m top5: 0.8288246268656716
[2m[36m(func pid=103447)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=103447)[0m f1_macro: 0.27774370130650355
[2m[36m(func pid=103447)[0m f1_weighted: 0.3796262455233281
[2m[36m(func pid=103447)[0m f1_per_class: [0.317, 0.163, 0.16, 0.532, 0.142, 0.301, 0.445, 0.317, 0.208, 0.192]
[2m[36m(func pid=107845)[0m top1: 0.31763059701492535
[2m[36m(func pid=107845)[0m top5: 0.8213619402985075
[2m[36m(func pid=107845)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=107845)[0m f1_macro: 0.24223222182092868
[2m[36m(func pid=107845)[0m f1_weighted: 0.34935620449717913
[2m[36m(func pid=107845)[0m f1_per_class: [0.364, 0.396, 0.154, 0.439, 0.098, 0.062, 0.424, 0.176, 0.126, 0.184]
== Status ==
Current time: 2024-01-07 12:35:04 (running for 00:37:10.37)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.31875
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.8   |      0.29  |                   64 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  7.314 |      0.241 |                   46 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.885 |      0.096 |                   12 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=119092)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=119092)[0m Configuration completed!
[2m[36m(func pid=119092)[0m New optimizer parameters:
[2m[36m(func pid=119092)[0m SGD (
[2m[36m(func pid=119092)[0m Parameter Group 0
[2m[36m(func pid=119092)[0m     dampening: 0
[2m[36m(func pid=119092)[0m     differentiable: False
[2m[36m(func pid=119092)[0m     foreach: None
[2m[36m(func pid=119092)[0m     lr: 0.001
[2m[36m(func pid=119092)[0m     maximize: False
[2m[36m(func pid=119092)[0m     momentum: 0.9
[2m[36m(func pid=119092)[0m     nesterov: False
[2m[36m(func pid=119092)[0m     weight_decay: 1e-05
[2m[36m(func pid=119092)[0m )
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8629 | Steps: 4 | Val loss: 2.2778 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.3948 | Steps: 4 | Val loss: 7.1186 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.2719 | Steps: 4 | Val loss: 40.5751 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 12:35:09 (running for 00:37:15.90)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.31875
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.47  |      0.278 |                   65 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 10.195 |      0.242 |                   47 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.863 |      0.104 |                   13 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=115877)[0m top1: 0.16791044776119404
[2m[36m(func pid=115877)[0m top5: 0.6021455223880597
[2m[36m(func pid=115877)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=115877)[0m f1_macro: 0.10407343281803934
[2m[36m(func pid=115877)[0m f1_weighted: 0.16373405371755542
[2m[36m(func pid=115877)[0m f1_per_class: [0.035, 0.216, 0.07, 0.212, 0.016, 0.305, 0.086, 0.085, 0.014, 0.0]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9385 | Steps: 4 | Val loss: 2.3233 | Batch size: 32 | lr: 0.001 | Duration: 4.42s
[2m[36m(func pid=103447)[0m top1: 0.3460820895522388
[2m[36m(func pid=103447)[0m top5: 0.820429104477612
[2m[36m(func pid=103447)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=103447)[0m f1_macro: 0.2636209182396568
[2m[36m(func pid=103447)[0m f1_weighted: 0.3690058597607511
[2m[36m(func pid=103447)[0m f1_per_class: [0.317, 0.119, 0.128, 0.48, 0.145, 0.297, 0.491, 0.298, 0.208, 0.152]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.31949626865671643
[2m[36m(func pid=107845)[0m top5: 0.840018656716418
[2m[36m(func pid=107845)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=107845)[0m f1_macro: 0.24413094426951734
[2m[36m(func pid=107845)[0m f1_weighted: 0.351186209450347
[2m[36m(func pid=107845)[0m f1_per_class: [0.351, 0.341, 0.202, 0.444, 0.101, 0.085, 0.447, 0.194, 0.1, 0.177]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8712 | Steps: 4 | Val loss: 2.2763 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=119092)[0m top1: 0.18050373134328357
[2m[36m(func pid=119092)[0m top5: 0.5359141791044776
[2m[36m(func pid=119092)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=119092)[0m f1_macro: 0.12041047494063033
[2m[36m(func pid=119092)[0m f1_weighted: 0.13018161899525751
[2m[36m(func pid=119092)[0m f1_per_class: [0.33, 0.342, 0.0, 0.098, 0.008, 0.261, 0.012, 0.044, 0.0, 0.109]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 8.1421 | Steps: 4 | Val loss: 41.1584 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.6156 | Steps: 4 | Val loss: 7.3177 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 12:35:15 (running for 00:37:21.53)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.31875
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.395 |      0.264 |                   66 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  1.272 |      0.244 |                   48 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.871 |      0.109 |                   14 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.939 |      0.12  |                    1 |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=115877)[0m top1: 0.17397388059701493
[2m[36m(func pid=115877)[0m top5: 0.59375
[2m[36m(func pid=115877)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=115877)[0m f1_macro: 0.10893138237342048
[2m[36m(func pid=115877)[0m f1_weighted: 0.16981646313031015
[2m[36m(func pid=115877)[0m f1_per_class: [0.058, 0.227, 0.061, 0.218, 0.017, 0.31, 0.091, 0.094, 0.014, 0.0]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9567 | Steps: 4 | Val loss: 2.3299 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=103447)[0m top1: 0.324160447761194
[2m[36m(func pid=103447)[0m top5: 0.820429104477612
[2m[36m(func pid=103447)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=103447)[0m f1_macro: 0.2587556007030979
[2m[36m(func pid=103447)[0m f1_weighted: 0.3499361535060808
[2m[36m(func pid=103447)[0m f1_per_class: [0.345, 0.141, 0.098, 0.386, 0.18, 0.3, 0.504, 0.273, 0.212, 0.148]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.32509328358208955
[2m[36m(func pid=107845)[0m top5: 0.8348880597014925
[2m[36m(func pid=107845)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=107845)[0m f1_macro: 0.25019168961908445
[2m[36m(func pid=107845)[0m f1_weighted: 0.35332218654563985
[2m[36m(func pid=107845)[0m f1_per_class: [0.348, 0.257, 0.23, 0.445, 0.143, 0.113, 0.485, 0.225, 0.091, 0.164]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m top1: 0.18190298507462688
[2m[36m(func pid=119092)[0m top5: 0.523320895522388
[2m[36m(func pid=119092)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=119092)[0m f1_macro: 0.1120189237568524
[2m[36m(func pid=119092)[0m f1_weighted: 0.13019967822054884
[2m[36m(func pid=119092)[0m f1_per_class: [0.256, 0.311, 0.0, 0.108, 0.008, 0.336, 0.003, 0.02, 0.0, 0.078]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8602 | Steps: 4 | Val loss: 2.2701 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6234 | Steps: 4 | Val loss: 7.6549 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 8.0808 | Steps: 4 | Val loss: 43.2575 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9919 | Steps: 4 | Val loss: 2.3189 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 12:35:21 (running for 00:37:27.25)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.31875
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.616 |      0.259 |                   67 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  8.142 |      0.25  |                   49 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.86  |      0.102 |                   15 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.957 |      0.112 |                    2 |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=115877)[0m top1: 0.17164179104477612
[2m[36m(func pid=115877)[0m top5: 0.6049440298507462
[2m[36m(func pid=115877)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=115877)[0m f1_macro: 0.10182419091288866
[2m[36m(func pid=115877)[0m f1_weighted: 0.1682086350831684
[2m[36m(func pid=115877)[0m f1_per_class: [0.037, 0.206, 0.053, 0.231, 0.01, 0.316, 0.09, 0.076, 0.0, 0.0]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=107845)[0m top1: 0.29850746268656714
[2m[36m(func pid=107845)[0m top5: 0.8246268656716418
[2m[36m(func pid=107845)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=107845)[0m f1_macro: 0.24520834609838654
[2m[36m(func pid=107845)[0m f1_weighted: 0.3292919978102293
[2m[36m(func pid=107845)[0m f1_per_class: [0.298, 0.276, 0.211, 0.315, 0.146, 0.145, 0.491, 0.303, 0.094, 0.173]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=103447)[0m top1: 0.3111007462686567
[2m[36m(func pid=103447)[0m top5: 0.8115671641791045
[2m[36m(func pid=103447)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=103447)[0m f1_macro: 0.25932683407156903
[2m[36m(func pid=103447)[0m f1_weighted: 0.3322075044751175
[2m[36m(func pid=103447)[0m f1_per_class: [0.313, 0.177, 0.135, 0.338, 0.179, 0.314, 0.463, 0.261, 0.239, 0.175]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=119092)[0m top1: 0.17863805970149255
[2m[36m(func pid=119092)[0m top5: 0.5363805970149254
[2m[36m(func pid=119092)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=119092)[0m f1_macro: 0.11862907031753964
[2m[36m(func pid=119092)[0m f1_weighted: 0.13482698191167286
[2m[36m(func pid=119092)[0m f1_per_class: [0.256, 0.285, 0.059, 0.115, 0.021, 0.33, 0.024, 0.028, 0.032, 0.036]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8884 | Steps: 4 | Val loss: 2.2516 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.0502 | Steps: 4 | Val loss: 8.3496 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6239 | Steps: 4 | Val loss: 45.6744 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8751 | Steps: 4 | Val loss: 2.3038 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 12:35:27 (running for 00:37:33.00)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.31875
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  2.623 |      0.259 |                   68 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  8.081 |      0.245 |                   50 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.888 |      0.112 |                   16 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.992 |      0.119 |                    3 |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=115877)[0m top1: 0.18516791044776118
[2m[36m(func pid=115877)[0m top5: 0.6408582089552238
[2m[36m(func pid=115877)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=115877)[0m f1_macro: 0.11162747754457614
[2m[36m(func pid=115877)[0m f1_weighted: 0.17854808570915653
[2m[36m(func pid=115877)[0m f1_per_class: [0.043, 0.237, 0.056, 0.228, 0.019, 0.32, 0.102, 0.076, 0.034, 0.0]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=107845)[0m top1: 0.291044776119403
[2m[36m(func pid=107845)[0m top5: 0.8036380597014925
[2m[36m(func pid=107845)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=107845)[0m f1_macro: 0.24927987762849096
[2m[36m(func pid=107845)[0m f1_weighted: 0.2986053431838349
[2m[36m(func pid=107845)[0m f1_per_class: [0.302, 0.298, 0.292, 0.163, 0.123, 0.172, 0.494, 0.348, 0.115, 0.186]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=103447)[0m top1: 0.2826492537313433
[2m[36m(func pid=103447)[0m top5: 0.7896455223880597
[2m[36m(func pid=103447)[0m f1_micro: 0.2826492537313433
[2m[36m(func pid=103447)[0m f1_macro: 0.24362722906974574
[2m[36m(func pid=103447)[0m f1_weighted: 0.2949390744997396
[2m[36m(func pid=103447)[0m f1_per_class: [0.337, 0.207, 0.124, 0.2, 0.162, 0.31, 0.454, 0.246, 0.234, 0.162]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=119092)[0m top1: 0.18003731343283583
[2m[36m(func pid=119092)[0m top5: 0.5489738805970149
[2m[36m(func pid=119092)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=119092)[0m f1_macro: 0.13471202265557528
[2m[36m(func pid=119092)[0m f1_weighted: 0.1483005893660893
[2m[36m(func pid=119092)[0m f1_per_class: [0.253, 0.278, 0.14, 0.121, 0.036, 0.332, 0.057, 0.071, 0.033, 0.027]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.9041 | Steps: 4 | Val loss: 2.2564 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.4946 | Steps: 4 | Val loss: 9.1934 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 6.9461 | Steps: 4 | Val loss: 50.9414 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8585 | Steps: 4 | Val loss: 2.2684 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 12:35:32 (running for 00:37:38.60)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.31875
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.05  |      0.244 |                   69 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  0.624 |      0.249 |                   51 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.904 |      0.112 |                   17 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.875 |      0.135 |                    4 |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=115877)[0m top1: 0.18563432835820895
[2m[36m(func pid=115877)[0m top5: 0.625
[2m[36m(func pid=115877)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=115877)[0m f1_macro: 0.11224278280694606
[2m[36m(func pid=115877)[0m f1_weighted: 0.18049956048105587
[2m[36m(func pid=115877)[0m f1_per_class: [0.041, 0.251, 0.051, 0.217, 0.018, 0.333, 0.109, 0.068, 0.034, 0.0]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=103447)[0m top1: 0.26119402985074625
[2m[36m(func pid=103447)[0m top5: 0.7444029850746269
[2m[36m(func pid=103447)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=103447)[0m f1_macro: 0.23156070232227471
[2m[36m(func pid=103447)[0m f1_weighted: 0.25529033292011294
[2m[36m(func pid=103447)[0m f1_per_class: [0.343, 0.218, 0.141, 0.072, 0.152, 0.303, 0.438, 0.227, 0.258, 0.164]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.2756529850746269
[2m[36m(func pid=107845)[0m top5: 0.7756529850746269
[2m[36m(func pid=107845)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=107845)[0m f1_macro: 0.23790912275562381
[2m[36m(func pid=107845)[0m f1_weighted: 0.26172021638725124
[2m[36m(func pid=107845)[0m f1_per_class: [0.263, 0.309, 0.338, 0.042, 0.091, 0.204, 0.463, 0.364, 0.127, 0.178]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m top1: 0.19029850746268656
[2m[36m(func pid=119092)[0m top5: 0.5932835820895522
[2m[36m(func pid=119092)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=119092)[0m f1_macro: 0.14498019897659314
[2m[36m(func pid=119092)[0m f1_weighted: 0.1674623934384625
[2m[36m(func pid=119092)[0m f1_per_class: [0.243, 0.275, 0.1, 0.179, 0.035, 0.341, 0.057, 0.124, 0.012, 0.084]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8930 | Steps: 4 | Val loss: 2.2528 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.3080 | Steps: 4 | Val loss: 10.5387 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 7.4689 | Steps: 4 | Val loss: 50.4790 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7781 | Steps: 4 | Val loss: 2.2514 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=115877)[0m top1: 0.19029850746268656
[2m[36m(func pid=115877)[0m top5: 0.6347947761194029
[2m[36m(func pid=115877)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=115877)[0m f1_macro: 0.11415587829145397
[2m[36m(func pid=115877)[0m f1_weighted: 0.1872342103230688
[2m[36m(func pid=115877)[0m f1_per_class: [0.043, 0.262, 0.056, 0.229, 0.01, 0.327, 0.116, 0.07, 0.029, 0.0]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:35:38 (running for 00:37:44.41)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.31875
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.495 |      0.232 |                   70 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  6.946 |      0.238 |                   52 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.893 |      0.114 |                   18 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.858 |      0.145 |                    5 |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=103447)[0m top1: 0.24067164179104478
[2m[36m(func pid=103447)[0m top5: 0.6735074626865671
[2m[36m(func pid=103447)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=103447)[0m f1_macro: 0.2212520195980124
[2m[36m(func pid=103447)[0m f1_weighted: 0.22458591960322663
[2m[36m(func pid=103447)[0m f1_per_class: [0.352, 0.201, 0.141, 0.023, 0.156, 0.315, 0.386, 0.236, 0.227, 0.176]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=107845)[0m top1: 0.28031716417910446
[2m[36m(func pid=107845)[0m top5: 0.7854477611940298
[2m[36m(func pid=107845)[0m f1_micro: 0.28031716417910446
[2m[36m(func pid=107845)[0m f1_macro: 0.24964342350038624
[2m[36m(func pid=107845)[0m f1_weighted: 0.26224210895872885
[2m[36m(func pid=107845)[0m f1_per_class: [0.261, 0.323, 0.387, 0.054, 0.1, 0.25, 0.426, 0.356, 0.142, 0.197]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m top1: 0.19636194029850745
[2m[36m(func pid=119092)[0m top5: 0.6217350746268657
[2m[36m(func pid=119092)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=119092)[0m f1_macro: 0.15361881124864735
[2m[36m(func pid=119092)[0m f1_weighted: 0.1851198624027145
[2m[36m(func pid=119092)[0m f1_per_class: [0.244, 0.283, 0.069, 0.203, 0.024, 0.353, 0.07, 0.202, 0.012, 0.076]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.3736 | Steps: 4 | Val loss: 11.4939 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.8469 | Steps: 4 | Val loss: 2.2503 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 7.3334 | Steps: 4 | Val loss: 48.3368 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7242 | Steps: 4 | Val loss: 2.2021 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 12:35:43 (running for 00:37:49.89)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.31875
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.308 |      0.221 |                   71 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  7.469 |      0.25  |                   53 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.893 |      0.114 |                   18 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.778 |      0.154 |                    6 |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=103447)[0m top1: 0.22621268656716417
[2m[36m(func pid=103447)[0m top5: 0.6450559701492538
[2m[36m(func pid=103447)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=103447)[0m f1_macro: 0.23048263257044427
[2m[36m(func pid=103447)[0m f1_weighted: 0.2057961896522483
[2m[36m(func pid=103447)[0m f1_per_class: [0.429, 0.209, 0.243, 0.01, 0.198, 0.311, 0.331, 0.223, 0.185, 0.166]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=119092)[0m top1: 0.21361940298507462
[2m[36m(func pid=119092)[0m top5: 0.6963619402985075
[2m[36m(func pid=119092)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=119092)[0m f1_macro: 0.1701963126321944
[2m[36m(func pid=119092)[0m f1_weighted: 0.21576157138485988
[2m[36m(func pid=119092)[0m f1_per_class: [0.247, 0.265, 0.074, 0.277, 0.029, 0.371, 0.106, 0.187, 0.026, 0.119]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m top1: 0.18330223880597016
[2m[36m(func pid=115877)[0m top5: 0.6399253731343284
[2m[36m(func pid=115877)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=115877)[0m f1_macro: 0.11195203740830466
[2m[36m(func pid=115877)[0m f1_weighted: 0.18293479887547975
[2m[36m(func pid=115877)[0m f1_per_class: [0.051, 0.254, 0.053, 0.232, 0.011, 0.308, 0.11, 0.075, 0.027, 0.0]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=107845)[0m top1: 0.3045708955223881
[2m[36m(func pid=107845)[0m top5: 0.7975746268656716
[2m[36m(func pid=107845)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=107845)[0m f1_macro: 0.270876821062368
[2m[36m(func pid=107845)[0m f1_weighted: 0.2839173540960162
[2m[36m(func pid=107845)[0m f1_per_class: [0.308, 0.376, 0.429, 0.069, 0.113, 0.28, 0.435, 0.368, 0.152, 0.178]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.2440 | Steps: 4 | Val loss: 12.0644 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6617 | Steps: 4 | Val loss: 2.1806 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 10.3611 | Steps: 4 | Val loss: 44.4726 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.8399 | Steps: 4 | Val loss: 2.2523 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 12:35:49 (running for 00:37:55.17)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.31875
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.374 |      0.23  |                   72 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  7.333 |      0.271 |                   54 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.847 |      0.112 |                   19 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.724 |      0.17  |                    7 |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=103447)[0m top1: 0.2178171641791045
[2m[36m(func pid=103447)[0m top5: 0.6497201492537313
[2m[36m(func pid=103447)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=103447)[0m f1_macro: 0.23836209153949697
[2m[36m(func pid=103447)[0m f1_weighted: 0.19962930784866403
[2m[36m(func pid=103447)[0m f1_per_class: [0.462, 0.254, 0.31, 0.01, 0.182, 0.301, 0.289, 0.211, 0.162, 0.205]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=119092)[0m top1: 0.2271455223880597
[2m[36m(func pid=119092)[0m top5: 0.7243470149253731
[2m[36m(func pid=119092)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=119092)[0m f1_macro: 0.19120665618136456
[2m[36m(func pid=119092)[0m f1_weighted: 0.2377345982743412
[2m[36m(func pid=119092)[0m f1_per_class: [0.327, 0.271, 0.053, 0.309, 0.021, 0.378, 0.129, 0.226, 0.026, 0.171]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m top1: 0.3316231343283582
[2m[36m(func pid=107845)[0m top5: 0.8227611940298507
[2m[36m(func pid=107845)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=107845)[0m f1_macro: 0.2950468075983591
[2m[36m(func pid=107845)[0m f1_weighted: 0.32653893676596074
[2m[36m(func pid=107845)[0m f1_per_class: [0.361, 0.41, 0.456, 0.17, 0.135, 0.276, 0.46, 0.376, 0.154, 0.151]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m top1: 0.18423507462686567
[2m[36m(func pid=115877)[0m top5: 0.6389925373134329
[2m[36m(func pid=115877)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=115877)[0m f1_macro: 0.11494948011972925
[2m[36m(func pid=115877)[0m f1_weighted: 0.18250451635573986
[2m[36m(func pid=115877)[0m f1_per_class: [0.026, 0.262, 0.053, 0.227, 0.01, 0.323, 0.098, 0.103, 0.027, 0.021]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7201 | Steps: 4 | Val loss: 12.0626 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6650 | Steps: 4 | Val loss: 2.1555 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.3970 | Steps: 4 | Val loss: 40.0887 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.8345 | Steps: 4 | Val loss: 2.2535 | Batch size: 32 | lr: 0.0001 | Duration: 3.27s
== Status ==
Current time: 2024-01-07 12:35:54 (running for 00:38:00.66)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.31875
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.244 |      0.238 |                   73 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 10.361 |      0.295 |                   55 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.84  |      0.115 |                   20 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.665 |      0.204 |                    9 |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=103447)[0m top1: 0.20615671641791045
[2m[36m(func pid=103447)[0m top5: 0.6674440298507462
[2m[36m(func pid=103447)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=103447)[0m f1_macro: 0.23188557066996665
[2m[36m(func pid=103447)[0m f1_weighted: 0.1919089823033742
[2m[36m(func pid=103447)[0m f1_per_class: [0.4, 0.258, 0.356, 0.017, 0.152, 0.301, 0.26, 0.213, 0.141, 0.221]
[2m[36m(func pid=103447)[0m 
[2m[36m(func pid=119092)[0m top1: 0.22574626865671643
[2m[36m(func pid=119092)[0m top5: 0.7453358208955224
[2m[36m(func pid=119092)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=119092)[0m f1_macro: 0.2039756699571506
[2m[36m(func pid=119092)[0m f1_weighted: 0.2467142505810324
[2m[36m(func pid=119092)[0m f1_per_class: [0.362, 0.288, 0.051, 0.3, 0.023, 0.328, 0.169, 0.232, 0.043, 0.243]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m top1: 0.3591417910447761
[2m[36m(func pid=107845)[0m top5: 0.855410447761194
[2m[36m(func pid=107845)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=107845)[0m f1_macro: 0.31902551725051254
[2m[36m(func pid=107845)[0m f1_weighted: 0.36505525534168204
[2m[36m(func pid=107845)[0m f1_per_class: [0.397, 0.422, 0.49, 0.26, 0.132, 0.295, 0.487, 0.381, 0.153, 0.174]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m top1: 0.18936567164179105
[2m[36m(func pid=115877)[0m top5: 0.6268656716417911
[2m[36m(func pid=115877)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=115877)[0m f1_macro: 0.11673365232477165
[2m[36m(func pid=115877)[0m f1_weighted: 0.1868697571413352
[2m[36m(func pid=115877)[0m f1_per_class: [0.068, 0.252, 0.047, 0.236, 0.021, 0.325, 0.11, 0.095, 0.014, 0.0]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=103447)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.0341 | Steps: 4 | Val loss: 11.8087 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.5480 | Steps: 4 | Val loss: 2.1297 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.8835 | Steps: 4 | Val loss: 38.8754 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.8929 | Steps: 4 | Val loss: 2.2503 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 12:36:00 (running for 00:38:06.03)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.31875
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00018 | RUNNING    | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  0.72  |      0.232 |                   74 |
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  0.397 |      0.319 |                   56 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.835 |      0.117 |                   21 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.548 |      0.221 |                   10 |
| train_9b9e8_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.2555970149253731
[2m[36m(func pid=119092)[0m top5: 0.7607276119402985
[2m[36m(func pid=119092)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=119092)[0m f1_macro: 0.22067217031502318
[2m[36m(func pid=119092)[0m f1_weighted: 0.27951403361621296
[2m[36m(func pid=119092)[0m f1_per_class: [0.351, 0.277, 0.055, 0.312, 0.027, 0.412, 0.241, 0.232, 0.057, 0.242]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=103447)[0m top1: 0.19636194029850745
[2m[36m(func pid=103447)[0m top5: 0.6721082089552238
[2m[36m(func pid=103447)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=103447)[0m f1_macro: 0.22405104974714685
[2m[36m(func pid=103447)[0m f1_weighted: 0.1808019486178707
[2m[36m(func pid=103447)[0m f1_per_class: [0.327, 0.238, 0.377, 0.023, 0.143, 0.292, 0.235, 0.215, 0.138, 0.253]
[2m[36m(func pid=107845)[0m top1: 0.3805970149253731
[2m[36m(func pid=107845)[0m top5: 0.867070895522388
[2m[36m(func pid=107845)[0m f1_micro: 0.3805970149253731
[2m[36m(func pid=107845)[0m f1_macro: 0.3395334966977739
[2m[36m(func pid=107845)[0m f1_weighted: 0.39363101336303313
[2m[36m(func pid=107845)[0m f1_per_class: [0.444, 0.443, 0.6, 0.333, 0.125, 0.293, 0.511, 0.307, 0.152, 0.187]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m top1: 0.1935634328358209
[2m[36m(func pid=115877)[0m top5: 0.6310634328358209
[2m[36m(func pid=115877)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=115877)[0m f1_macro: 0.12069091650452039
[2m[36m(func pid=115877)[0m f1_weighted: 0.18890705508442526
[2m[36m(func pid=115877)[0m f1_per_class: [0.076, 0.267, 0.053, 0.24, 0.022, 0.323, 0.101, 0.112, 0.013, 0.0]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.5705 | Steps: 4 | Val loss: 2.1223 | Batch size: 32 | lr: 0.001 | Duration: 3.22s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 6.0122 | Steps: 4 | Val loss: 39.9060 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.8309 | Steps: 4 | Val loss: 2.2506 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
[2m[36m(func pid=119092)[0m top1: 0.2490671641791045
[2m[36m(func pid=119092)[0m top5: 0.7551305970149254
[2m[36m(func pid=119092)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=119092)[0m f1_macro: 0.2205996239654123
[2m[36m(func pid=119092)[0m f1_weighted: 0.264325037812836
[2m[36m(func pid=119092)[0m f1_per_class: [0.347, 0.274, 0.05, 0.285, 0.027, 0.429, 0.209, 0.224, 0.07, 0.29]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m top1: 0.3880597014925373
[2m[36m(func pid=107845)[0m top5: 0.8610074626865671
[2m[36m(func pid=107845)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=107845)[0m f1_macro: 0.35263950674608563
[2m[36m(func pid=107845)[0m f1_weighted: 0.41225973474127436
[2m[36m(func pid=107845)[0m f1_per_class: [0.48, 0.451, 0.65, 0.416, 0.107, 0.312, 0.489, 0.266, 0.148, 0.208]
[2m[36m(func pid=115877)[0m top1: 0.18889925373134328
[2m[36m(func pid=115877)[0m top5: 0.636660447761194
[2m[36m(func pid=115877)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=115877)[0m f1_macro: 0.1224214959149819
[2m[36m(func pid=115877)[0m f1_weighted: 0.18054316308227103
[2m[36m(func pid=115877)[0m f1_per_class: [0.081, 0.26, 0.069, 0.215, 0.023, 0.336, 0.094, 0.111, 0.014, 0.021]
== Status ==
Current time: 2024-01-07 12:36:05 (running for 00:38:11.69)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  0.883 |      0.34  |                   57 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.893 |      0.121 |                   22 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.571 |      0.221 |                   11 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=121943)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=121943)[0m Configuration completed!
[2m[36m(func pid=121943)[0m New optimizer parameters:
[2m[36m(func pid=121943)[0m SGD (
[2m[36m(func pid=121943)[0m Parameter Group 0
[2m[36m(func pid=121943)[0m     dampening: 0
[2m[36m(func pid=121943)[0m     differentiable: False
[2m[36m(func pid=121943)[0m     foreach: None
[2m[36m(func pid=121943)[0m     lr: 0.01
[2m[36m(func pid=121943)[0m     maximize: False
[2m[36m(func pid=121943)[0m     momentum: 0.9
[2m[36m(func pid=121943)[0m     nesterov: False
[2m[36m(func pid=121943)[0m     weight_decay: 1e-05
[2m[36m(func pid=121943)[0m )
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5287 | Steps: 4 | Val loss: 2.0887 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 12:36:11 (running for 00:38:16.98)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  6.012 |      0.353 |                   58 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.831 |      0.122 |                   23 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.529 |      0.226 |                   12 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.25419776119402987
[2m[36m(func pid=119092)[0m top5: 0.78125
[2m[36m(func pid=119092)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=119092)[0m f1_macro: 0.2261548079734665
[2m[36m(func pid=119092)[0m f1_weighted: 0.27060899079661355
[2m[36m(func pid=119092)[0m f1_per_class: [0.366, 0.248, 0.051, 0.302, 0.047, 0.394, 0.241, 0.224, 0.061, 0.329]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 17.1386 | Steps: 4 | Val loss: 44.9513 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.8442 | Steps: 4 | Val loss: 2.2378 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9752 | Steps: 4 | Val loss: 2.3055 | Batch size: 32 | lr: 0.01 | Duration: 4.62s
[2m[36m(func pid=107845)[0m top1: 0.3568097014925373
[2m[36m(func pid=107845)[0m top5: 0.835820895522388
[2m[36m(func pid=107845)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=107845)[0m f1_macro: 0.3322517747952259
[2m[36m(func pid=107845)[0m f1_weighted: 0.3869626451728713
[2m[36m(func pid=107845)[0m f1_per_class: [0.475, 0.437, 0.55, 0.389, 0.099, 0.298, 0.449, 0.247, 0.128, 0.25]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.4071 | Steps: 4 | Val loss: 2.0409 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=115877)[0m top1: 0.19916044776119404
[2m[36m(func pid=115877)[0m top5: 0.659981343283582
[2m[36m(func pid=115877)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=115877)[0m f1_macro: 0.12786085681082268
[2m[36m(func pid=115877)[0m f1_weighted: 0.1981244375695667
[2m[36m(func pid=115877)[0m f1_per_class: [0.075, 0.262, 0.076, 0.23, 0.023, 0.349, 0.138, 0.08, 0.026, 0.02]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m top1: 0.19449626865671643
[2m[36m(func pid=121943)[0m top5: 0.5377798507462687
[2m[36m(func pid=121943)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=121943)[0m f1_macro: 0.15630615511439205
[2m[36m(func pid=121943)[0m f1_weighted: 0.15184943191696937
[2m[36m(func pid=121943)[0m f1_per_class: [0.477, 0.331, 0.031, 0.13, 0.024, 0.316, 0.027, 0.037, 0.0, 0.191]
[2m[36m(func pid=121943)[0m 
== Status ==
Current time: 2024-01-07 12:36:16 (running for 00:38:22.39)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 17.139 |      0.332 |                   59 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.844 |      0.128 |                   24 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.407 |      0.241 |                   13 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  2.975 |      0.156 |                    1 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.2933768656716418
[2m[36m(func pid=119092)[0m top5: 0.8125
[2m[36m(func pid=119092)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=119092)[0m f1_macro: 0.24063176123797367
[2m[36m(func pid=119092)[0m f1_weighted: 0.30397154415627176
[2m[36m(func pid=119092)[0m f1_per_class: [0.364, 0.247, 0.07, 0.391, 0.054, 0.396, 0.27, 0.209, 0.087, 0.32]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.1841 | Steps: 4 | Val loss: 49.0362 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.8203 | Steps: 4 | Val loss: 2.2374 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.7997 | Steps: 4 | Val loss: 2.2366 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.4003 | Steps: 4 | Val loss: 2.0228 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=107845)[0m top1: 0.3358208955223881
[2m[36m(func pid=107845)[0m top5: 0.8185634328358209
[2m[36m(func pid=107845)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=107845)[0m f1_macro: 0.32152264693720883
[2m[36m(func pid=107845)[0m f1_weighted: 0.3732070019401259
[2m[36m(func pid=107845)[0m f1_per_class: [0.436, 0.434, 0.578, 0.402, 0.084, 0.31, 0.396, 0.229, 0.125, 0.222]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m top1: 0.197294776119403
[2m[36m(func pid=115877)[0m top5: 0.6595149253731343
[2m[36m(func pid=115877)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=115877)[0m f1_macro: 0.12695414772549524
[2m[36m(func pid=115877)[0m f1_weighted: 0.2006136714846793
[2m[36m(func pid=115877)[0m f1_per_class: [0.068, 0.24, 0.07, 0.244, 0.012, 0.341, 0.146, 0.104, 0.026, 0.02]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m top1: 0.1515858208955224
[2m[36m(func pid=121943)[0m top5: 0.6492537313432836
[2m[36m(func pid=121943)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=121943)[0m f1_macro: 0.15842684094218712
[2m[36m(func pid=121943)[0m f1_weighted: 0.17342230004167883
[2m[36m(func pid=121943)[0m f1_per_class: [0.481, 0.005, 0.023, 0.142, 0.0, 0.387, 0.228, 0.151, 0.0, 0.167]
[2m[36m(func pid=121943)[0m 
== Status ==
Current time: 2024-01-07 12:36:21 (running for 00:38:27.45)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  1.184 |      0.322 |                   60 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.82  |      0.127 |                   25 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.4   |      0.235 |                   14 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  2.8   |      0.158 |                    2 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.31156716417910446
[2m[36m(func pid=119092)[0m top5: 0.8152985074626866
[2m[36m(func pid=119092)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=119092)[0m f1_macro: 0.23494855369870468
[2m[36m(func pid=119092)[0m f1_weighted: 0.313264602016963
[2m[36m(func pid=119092)[0m f1_per_class: [0.347, 0.206, 0.078, 0.459, 0.051, 0.397, 0.268, 0.185, 0.075, 0.282]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 10.0551 | Steps: 4 | Val loss: 54.1568 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.8034 | Steps: 4 | Val loss: 2.2338 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.3515 | Steps: 4 | Val loss: 1.9987 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.6131 | Steps: 4 | Val loss: 2.1372 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=107845)[0m top1: 0.3064365671641791
[2m[36m(func pid=107845)[0m top5: 0.7905783582089553
[2m[36m(func pid=107845)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=107845)[0m f1_macro: 0.2974911330260023
[2m[36m(func pid=107845)[0m f1_weighted: 0.34407437417393105
[2m[36m(func pid=107845)[0m f1_per_class: [0.473, 0.36, 0.448, 0.405, 0.067, 0.285, 0.346, 0.226, 0.136, 0.226]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m top1: 0.20149253731343283
[2m[36m(func pid=115877)[0m top5: 0.6627798507462687
[2m[36m(func pid=115877)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=115877)[0m f1_macro: 0.1259040421416368
[2m[36m(func pid=115877)[0m f1_weighted: 0.20608881818026936
[2m[36m(func pid=115877)[0m f1_per_class: [0.062, 0.246, 0.07, 0.262, 0.0, 0.341, 0.148, 0.084, 0.027, 0.019]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:36:26 (running for 00:38:32.48)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 10.055 |      0.297 |                   61 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.803 |      0.126 |                   26 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.352 |      0.251 |                   15 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  2.8   |      0.158 |                    2 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.33861940298507465
[2m[36m(func pid=119092)[0m top5: 0.824160447761194
[2m[36m(func pid=119092)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=119092)[0m f1_macro: 0.25099094184786963
[2m[36m(func pid=119092)[0m f1_weighted: 0.331289654418828
[2m[36m(func pid=119092)[0m f1_per_class: [0.355, 0.184, 0.108, 0.505, 0.08, 0.407, 0.294, 0.163, 0.085, 0.329]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m top1: 0.23274253731343283
[2m[36m(func pid=121943)[0m top5: 0.6972947761194029
[2m[36m(func pid=121943)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=121943)[0m f1_macro: 0.18709749842927795
[2m[36m(func pid=121943)[0m f1_weighted: 0.21440815794316065
[2m[36m(func pid=121943)[0m f1_per_class: [0.188, 0.0, 0.05, 0.388, 0.097, 0.381, 0.111, 0.371, 0.0, 0.286]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 4.1427 | Steps: 4 | Val loss: 58.5826 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.8247 | Steps: 4 | Val loss: 2.2077 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.2884 | Steps: 4 | Val loss: 1.9928 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=107845)[0m top1: 0.27238805970149255
[2m[36m(func pid=107845)[0m top5: 0.7761194029850746
[2m[36m(func pid=107845)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=107845)[0m f1_macro: 0.26976329269107496
[2m[36m(func pid=107845)[0m f1_weighted: 0.311346308420841
[2m[36m(func pid=107845)[0m f1_per_class: [0.44, 0.351, 0.317, 0.362, 0.056, 0.3, 0.28, 0.242, 0.127, 0.222]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.2644 | Steps: 4 | Val loss: 2.0126 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=115877)[0m top1: 0.2126865671641791
[2m[36m(func pid=115877)[0m top5: 0.707089552238806
[2m[36m(func pid=115877)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=115877)[0m f1_macro: 0.1316573590825223
[2m[36m(func pid=115877)[0m f1_weighted: 0.2157192006310684
[2m[36m(func pid=115877)[0m f1_per_class: [0.056, 0.292, 0.06, 0.262, 0.011, 0.331, 0.155, 0.095, 0.032, 0.023]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:36:31 (running for 00:38:37.86)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.143 |      0.27  |                   62 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.825 |      0.132 |                   27 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.288 |      0.239 |                   16 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  2.613 |      0.187 |                    3 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.32975746268656714
[2m[36m(func pid=119092)[0m top5: 0.816231343283582
[2m[36m(func pid=119092)[0m f1_micro: 0.32975746268656714
[2m[36m(func pid=119092)[0m f1_macro: 0.23886693184198493
[2m[36m(func pid=119092)[0m f1_weighted: 0.31844648711659107
[2m[36m(func pid=119092)[0m f1_per_class: [0.289, 0.131, 0.125, 0.505, 0.072, 0.397, 0.287, 0.181, 0.084, 0.317]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m top1: 0.3208955223880597
[2m[36m(func pid=121943)[0m top5: 0.7411380597014925
[2m[36m(func pid=121943)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=121943)[0m f1_macro: 0.26650305754848674
[2m[36m(func pid=121943)[0m f1_weighted: 0.24948181489414378
[2m[36m(func pid=121943)[0m f1_per_class: [0.387, 0.005, 0.364, 0.541, 0.067, 0.355, 0.054, 0.362, 0.15, 0.38]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 13.5729 | Steps: 4 | Val loss: 61.5361 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.8040 | Steps: 4 | Val loss: 2.2097 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.3579 | Steps: 4 | Val loss: 1.9747 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=107845)[0m top1: 0.26492537313432835
[2m[36m(func pid=107845)[0m top5: 0.7560634328358209
[2m[36m(func pid=107845)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=107845)[0m f1_macro: 0.2626731746390941
[2m[36m(func pid=107845)[0m f1_weighted: 0.29933041523049175
[2m[36m(func pid=107845)[0m f1_per_class: [0.392, 0.375, 0.257, 0.333, 0.06, 0.32, 0.236, 0.312, 0.14, 0.202]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 1.9739 | Steps: 4 | Val loss: 1.8187 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=115877)[0m top1: 0.22154850746268656
[2m[36m(func pid=115877)[0m top5: 0.7052238805970149
[2m[36m(func pid=115877)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=115877)[0m f1_macro: 0.14142424707925577
[2m[36m(func pid=115877)[0m f1_weighted: 0.22657979317848198
[2m[36m(func pid=115877)[0m f1_per_class: [0.108, 0.273, 0.068, 0.281, 0.012, 0.345, 0.175, 0.099, 0.03, 0.024]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:36:37 (running for 00:38:43.13)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 13.573 |      0.263 |                   63 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.804 |      0.141 |                   28 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.358 |      0.246 |                   17 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  2.264 |      0.267 |                    4 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3376865671641791
[2m[36m(func pid=119092)[0m top5: 0.8129664179104478
[2m[36m(func pid=119092)[0m f1_micro: 0.3376865671641791
[2m[36m(func pid=119092)[0m f1_macro: 0.2464142618056469
[2m[36m(func pid=119092)[0m f1_weighted: 0.32804979764850023
[2m[36m(func pid=119092)[0m f1_per_class: [0.247, 0.113, 0.147, 0.511, 0.066, 0.407, 0.317, 0.185, 0.119, 0.353]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 11.7840 | Steps: 4 | Val loss: 64.7402 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=121943)[0m top1: 0.3582089552238806
[2m[36m(func pid=121943)[0m top5: 0.8479477611940298
[2m[36m(func pid=121943)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=121943)[0m f1_macro: 0.3361285637852912
[2m[36m(func pid=121943)[0m f1_weighted: 0.33199702602130027
[2m[36m(func pid=121943)[0m f1_per_class: [0.526, 0.222, 0.585, 0.542, 0.066, 0.405, 0.169, 0.384, 0.168, 0.294]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.8144 | Steps: 4 | Val loss: 2.2048 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.3318 | Steps: 4 | Val loss: 1.9730 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=107845)[0m top1: 0.2574626865671642
[2m[36m(func pid=107845)[0m top5: 0.7374067164179104
[2m[36m(func pid=107845)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=107845)[0m f1_macro: 0.24863191507822288
[2m[36m(func pid=107845)[0m f1_weighted: 0.28412604459254054
[2m[36m(func pid=107845)[0m f1_per_class: [0.4, 0.369, 0.15, 0.293, 0.068, 0.346, 0.219, 0.295, 0.148, 0.198]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 1.7046 | Steps: 4 | Val loss: 1.6564 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=115877)[0m top1: 0.2234141791044776
[2m[36m(func pid=115877)[0m top5: 0.7122201492537313
[2m[36m(func pid=115877)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=115877)[0m f1_macro: 0.1399378836434534
[2m[36m(func pid=115877)[0m f1_weighted: 0.23370320999400787
[2m[36m(func pid=115877)[0m f1_per_class: [0.089, 0.308, 0.057, 0.278, 0.0, 0.344, 0.186, 0.084, 0.032, 0.021]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:36:42 (running for 00:38:48.45)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 11.784 |      0.249 |                   64 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.814 |      0.14  |                   29 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.358 |      0.246 |                   17 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.974 |      0.336 |                    5 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3381529850746269
[2m[36m(func pid=119092)[0m top5: 0.8185634328358209
[2m[36m(func pid=119092)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=119092)[0m f1_macro: 0.258479899550459
[2m[36m(func pid=119092)[0m f1_weighted: 0.3353229907217391
[2m[36m(func pid=119092)[0m f1_per_class: [0.245, 0.139, 0.162, 0.504, 0.066, 0.413, 0.322, 0.213, 0.137, 0.384]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 7.1359 | Steps: 4 | Val loss: 65.8027 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=121943)[0m top1: 0.425839552238806
[2m[36m(func pid=121943)[0m top5: 0.902518656716418
[2m[36m(func pid=121943)[0m f1_micro: 0.42583955223880593
[2m[36m(func pid=121943)[0m f1_macro: 0.3500709892034284
[2m[36m(func pid=121943)[0m f1_weighted: 0.4435344768545318
[2m[36m(func pid=121943)[0m f1_per_class: [0.478, 0.39, 0.369, 0.536, 0.137, 0.429, 0.474, 0.26, 0.167, 0.26]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.8115 | Steps: 4 | Val loss: 2.2098 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.1719 | Steps: 4 | Val loss: 1.9864 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=107845)[0m top1: 0.25466417910447764
[2m[36m(func pid=107845)[0m top5: 0.7168843283582089
[2m[36m(func pid=107845)[0m f1_micro: 0.25466417910447764
[2m[36m(func pid=107845)[0m f1_macro: 0.23267442290778434
[2m[36m(func pid=107845)[0m f1_weighted: 0.2834707054162704
[2m[36m(func pid=107845)[0m f1_per_class: [0.354, 0.348, 0.107, 0.3, 0.075, 0.351, 0.235, 0.254, 0.148, 0.155]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.5211 | Steps: 4 | Val loss: 1.6660 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 12:36:47 (running for 00:38:53.85)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  7.136 |      0.233 |                   65 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.814 |      0.14  |                   29 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.172 |      0.263 |                   19 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.705 |      0.35  |                    6 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3306902985074627
[2m[36m(func pid=119092)[0m top5: 0.804570895522388
[2m[36m(func pid=119092)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=119092)[0m f1_macro: 0.2632304693660561
[2m[36m(func pid=119092)[0m f1_weighted: 0.33543135921046996
[2m[36m(func pid=119092)[0m f1_per_class: [0.261, 0.178, 0.191, 0.508, 0.063, 0.424, 0.283, 0.264, 0.124, 0.336]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m top1: 0.21361940298507462
[2m[36m(func pid=115877)[0m top5: 0.7066231343283582
[2m[36m(func pid=115877)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=115877)[0m f1_macro: 0.13853575114950556
[2m[36m(func pid=115877)[0m f1_weighted: 0.2184238201985095
[2m[36m(func pid=115877)[0m f1_per_class: [0.087, 0.286, 0.057, 0.278, 0.012, 0.355, 0.136, 0.121, 0.031, 0.022]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 7.2351 | Steps: 4 | Val loss: 66.4637 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=121943)[0m top1: 0.42024253731343286
[2m[36m(func pid=121943)[0m top5: 0.8796641791044776
[2m[36m(func pid=121943)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=121943)[0m f1_macro: 0.31673750671572054
[2m[36m(func pid=121943)[0m f1_weighted: 0.4287136629288087
[2m[36m(func pid=121943)[0m f1_per_class: [0.377, 0.2, 0.198, 0.561, 0.189, 0.461, 0.499, 0.309, 0.197, 0.178]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.2157 | Steps: 4 | Val loss: 1.9919 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.8017 | Steps: 4 | Val loss: 2.2212 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=107845)[0m top1: 0.2462686567164179
[2m[36m(func pid=107845)[0m top5: 0.7215485074626866
[2m[36m(func pid=107845)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=107845)[0m f1_macro: 0.22605481572082642
[2m[36m(func pid=107845)[0m f1_weighted: 0.27639526575970225
[2m[36m(func pid=107845)[0m f1_per_class: [0.349, 0.222, 0.076, 0.304, 0.114, 0.368, 0.272, 0.258, 0.168, 0.13]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 1.4048 | Steps: 4 | Val loss: 1.7395 | Batch size: 32 | lr: 0.01 | Duration: 3.18s
[2m[36m(func pid=119092)[0m top1: 0.31716417910447764
[2m[36m(func pid=119092)[0m top5: 0.800839552238806
[2m[36m(func pid=119092)[0m f1_micro: 0.31716417910447764
[2m[36m(func pid=119092)[0m f1_macro: 0.26023254195836343
[2m[36m(func pid=119092)[0m f1_weighted: 0.3300659570761083
[2m[36m(func pid=119092)[0m f1_per_class: [0.262, 0.188, 0.18, 0.488, 0.054, 0.411, 0.278, 0.285, 0.147, 0.31]
[2m[36m(func pid=119092)[0m 
== Status ==
Current time: 2024-01-07 12:36:53 (running for 00:38:59.21)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  7.235 |      0.226 |                   66 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.812 |      0.139 |                   30 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.216 |      0.26  |                   20 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.521 |      0.317 |                    7 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=115877)[0m top1: 0.2126865671641791
[2m[36m(func pid=115877)[0m top5: 0.6968283582089553
[2m[36m(func pid=115877)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=115877)[0m f1_macro: 0.14022532156860726
[2m[36m(func pid=115877)[0m f1_weighted: 0.2188135696334976
[2m[36m(func pid=115877)[0m f1_per_class: [0.107, 0.283, 0.055, 0.263, 0.011, 0.364, 0.149, 0.117, 0.029, 0.025]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 19.3517 | Steps: 4 | Val loss: 66.9779 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=121943)[0m top1: 0.3824626865671642
[2m[36m(func pid=121943)[0m top5: 0.8614738805970149
[2m[36m(func pid=121943)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=121943)[0m f1_macro: 0.2860912847715113
[2m[36m(func pid=121943)[0m f1_weighted: 0.39089490084177014
[2m[36m(func pid=121943)[0m f1_per_class: [0.29, 0.132, 0.122, 0.563, 0.157, 0.483, 0.4, 0.366, 0.162, 0.185]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.1046 | Steps: 4 | Val loss: 1.9895 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=107845)[0m top1: 0.23880597014925373
[2m[36m(func pid=107845)[0m top5: 0.7215485074626866
[2m[36m(func pid=107845)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=107845)[0m f1_macro: 0.2250133550485626
[2m[36m(func pid=107845)[0m f1_weighted: 0.2735217913819778
[2m[36m(func pid=107845)[0m f1_per_class: [0.397, 0.228, 0.062, 0.24, 0.087, 0.367, 0.314, 0.281, 0.159, 0.116]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.7754 | Steps: 4 | Val loss: 2.2139 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 12:36:58 (running for 00:39:04.38)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 19.352 |      0.225 |                   67 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.802 |      0.14  |                   31 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.105 |      0.261 |                   21 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.405 |      0.286 |                    8 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3125
[2m[36m(func pid=119092)[0m top5: 0.8078358208955224
[2m[36m(func pid=119092)[0m f1_micro: 0.3125
[2m[36m(func pid=119092)[0m f1_macro: 0.2611314939187964
[2m[36m(func pid=119092)[0m f1_weighted: 0.3284421304083325
[2m[36m(func pid=119092)[0m f1_per_class: [0.297, 0.154, 0.187, 0.476, 0.053, 0.411, 0.301, 0.283, 0.158, 0.291]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.3819 | Steps: 4 | Val loss: 1.7607 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=115877)[0m top1: 0.21735074626865672
[2m[36m(func pid=115877)[0m top5: 0.7052238805970149
[2m[36m(func pid=115877)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=115877)[0m f1_macro: 0.14329744813648718
[2m[36m(func pid=115877)[0m f1_weighted: 0.22318341345426088
[2m[36m(func pid=115877)[0m f1_per_class: [0.093, 0.299, 0.058, 0.257, 0.01, 0.366, 0.157, 0.132, 0.035, 0.026]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 4.0131 | Steps: 4 | Val loss: 63.4028 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.1401 | Steps: 4 | Val loss: 1.9714 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=121943)[0m top1: 0.3572761194029851
[2m[36m(func pid=121943)[0m top5: 0.8656716417910447
[2m[36m(func pid=121943)[0m f1_micro: 0.35727611940298515
[2m[36m(func pid=121943)[0m f1_macro: 0.2924452131480362
[2m[36m(func pid=121943)[0m f1_weighted: 0.3934857944322639
[2m[36m(func pid=121943)[0m f1_per_class: [0.316, 0.295, 0.144, 0.427, 0.108, 0.42, 0.463, 0.35, 0.21, 0.192]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=107845)[0m top1: 0.2555970149253731
[2m[36m(func pid=107845)[0m top5: 0.7458022388059702
[2m[36m(func pid=107845)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=107845)[0m f1_macro: 0.23877877809455533
[2m[36m(func pid=107845)[0m f1_weighted: 0.28843698203850276
[2m[36m(func pid=107845)[0m f1_per_class: [0.438, 0.208, 0.074, 0.19, 0.112, 0.386, 0.403, 0.325, 0.158, 0.094]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.8345 | Steps: 4 | Val loss: 2.2184 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 12:37:03 (running for 00:39:09.45)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.013 |      0.239 |                   68 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.775 |      0.143 |                   32 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.14  |      0.263 |                   22 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.382 |      0.292 |                    9 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.31949626865671643
[2m[36m(func pid=119092)[0m top5: 0.8101679104477612
[2m[36m(func pid=119092)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=119092)[0m f1_macro: 0.263378433798317
[2m[36m(func pid=119092)[0m f1_weighted: 0.33452009211562755
[2m[36m(func pid=119092)[0m f1_per_class: [0.321, 0.143, 0.222, 0.492, 0.056, 0.404, 0.313, 0.292, 0.152, 0.238]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 1.1811 | Steps: 4 | Val loss: 1.6979 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=115877)[0m top1: 0.21641791044776118
[2m[36m(func pid=115877)[0m top5: 0.6907649253731343
[2m[36m(func pid=115877)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=115877)[0m f1_macro: 0.1434704867735381
[2m[36m(func pid=115877)[0m f1_weighted: 0.21991904028243062
[2m[36m(func pid=115877)[0m f1_per_class: [0.11, 0.291, 0.067, 0.285, 0.012, 0.34, 0.131, 0.144, 0.03, 0.023]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 12.0315 | Steps: 4 | Val loss: 57.3393 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.1107 | Steps: 4 | Val loss: 1.9497 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=121943)[0m top1: 0.3931902985074627
[2m[36m(func pid=121943)[0m top5: 0.8833955223880597
[2m[36m(func pid=121943)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=121943)[0m f1_macro: 0.3388396655227416
[2m[36m(func pid=121943)[0m f1_weighted: 0.41746034307847085
[2m[36m(func pid=121943)[0m f1_per_class: [0.471, 0.439, 0.293, 0.368, 0.081, 0.41, 0.51, 0.298, 0.239, 0.28]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=107845)[0m top1: 0.2943097014925373
[2m[36m(func pid=107845)[0m top5: 0.7723880597014925
[2m[36m(func pid=107845)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=107845)[0m f1_macro: 0.25271936104087517
[2m[36m(func pid=107845)[0m f1_weighted: 0.32167846152627794
[2m[36m(func pid=107845)[0m f1_per_class: [0.433, 0.264, 0.088, 0.2, 0.096, 0.371, 0.476, 0.34, 0.154, 0.105]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.7903 | Steps: 4 | Val loss: 2.2229 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 12:37:09 (running for 00:39:14.97)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 12.031 |      0.253 |                   69 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.834 |      0.143 |                   33 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.111 |      0.281 |                   23 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.181 |      0.339 |                   10 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.33115671641791045
[2m[36m(func pid=119092)[0m top5: 0.8208955223880597
[2m[36m(func pid=119092)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=119092)[0m f1_macro: 0.2808461959356937
[2m[36m(func pid=119092)[0m f1_weighted: 0.34489401053277147
[2m[36m(func pid=119092)[0m f1_per_class: [0.437, 0.143, 0.267, 0.517, 0.06, 0.39, 0.321, 0.305, 0.139, 0.231]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.2741 | Steps: 4 | Val loss: 1.7632 | Batch size: 32 | lr: 0.01 | Duration: 3.37s
[2m[36m(func pid=115877)[0m top1: 0.21082089552238806
[2m[36m(func pid=115877)[0m top5: 0.679570895522388
[2m[36m(func pid=115877)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=115877)[0m f1_macro: 0.14113442867607884
[2m[36m(func pid=115877)[0m f1_weighted: 0.2174918610076726
[2m[36m(func pid=115877)[0m f1_per_class: [0.118, 0.274, 0.062, 0.287, 0.012, 0.343, 0.133, 0.133, 0.028, 0.022]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 18.4230 | Steps: 4 | Val loss: 59.0396 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.0030 | Steps: 4 | Val loss: 1.9392 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=121943)[0m top1: 0.3670708955223881
[2m[36m(func pid=121943)[0m top5: 0.8736007462686567
[2m[36m(func pid=121943)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=121943)[0m f1_macro: 0.3517355379752327
[2m[36m(func pid=121943)[0m f1_weighted: 0.3656933255840206
[2m[36m(func pid=121943)[0m f1_per_class: [0.56, 0.456, 0.353, 0.261, 0.089, 0.392, 0.408, 0.372, 0.238, 0.389]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=107845)[0m top1: 0.3003731343283582
[2m[36m(func pid=107845)[0m top5: 0.7714552238805971
[2m[36m(func pid=107845)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=107845)[0m f1_macro: 0.25593255166592277
[2m[36m(func pid=107845)[0m f1_weighted: 0.31627739046410736
[2m[36m(func pid=107845)[0m f1_per_class: [0.453, 0.284, 0.122, 0.161, 0.121, 0.314, 0.5, 0.35, 0.16, 0.094]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.7625 | Steps: 4 | Val loss: 2.2217 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 12:37:14 (running for 00:39:20.24)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 18.423 |      0.256 |                   70 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.79  |      0.141 |                   34 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.003 |      0.29  |                   24 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.274 |      0.352 |                   11 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.33302238805970147
[2m[36m(func pid=119092)[0m top5: 0.8274253731343284
[2m[36m(func pid=119092)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=119092)[0m f1_macro: 0.2896213980250388
[2m[36m(func pid=119092)[0m f1_weighted: 0.35088902759427737
[2m[36m(func pid=119092)[0m f1_per_class: [0.475, 0.159, 0.3, 0.501, 0.06, 0.389, 0.343, 0.31, 0.144, 0.215]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 12.0035 | Steps: 4 | Val loss: 57.9645 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.2473 | Steps: 4 | Val loss: 1.8818 | Batch size: 32 | lr: 0.01 | Duration: 3.14s
[2m[36m(func pid=115877)[0m top1: 0.21361940298507462
[2m[36m(func pid=115877)[0m top5: 0.6865671641791045
[2m[36m(func pid=115877)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=115877)[0m f1_macro: 0.14473328562469218
[2m[36m(func pid=115877)[0m f1_weighted: 0.21899674557260115
[2m[36m(func pid=115877)[0m f1_per_class: [0.141, 0.287, 0.07, 0.29, 0.013, 0.332, 0.129, 0.134, 0.028, 0.023]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.0046 | Steps: 4 | Val loss: 1.9207 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=107845)[0m top1: 0.3111007462686567
[2m[36m(func pid=107845)[0m top5: 0.7756529850746269
[2m[36m(func pid=107845)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=107845)[0m f1_macro: 0.26627087101624086
[2m[36m(func pid=107845)[0m f1_weighted: 0.3174790106433723
[2m[36m(func pid=107845)[0m f1_per_class: [0.426, 0.369, 0.183, 0.163, 0.148, 0.246, 0.477, 0.344, 0.183, 0.124]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=121943)[0m top1: 0.3362873134328358
[2m[36m(func pid=121943)[0m top5: 0.8418843283582089
[2m[36m(func pid=121943)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=121943)[0m f1_macro: 0.32270355210736446
[2m[36m(func pid=121943)[0m f1_weighted: 0.36003396757394324
[2m[36m(func pid=121943)[0m f1_per_class: [0.505, 0.421, 0.276, 0.427, 0.062, 0.392, 0.265, 0.363, 0.235, 0.28]
[2m[36m(func pid=121943)[0m 
== Status ==
Current time: 2024-01-07 12:37:19 (running for 00:39:25.41)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 12.003 |      0.266 |                   71 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.763 |      0.145 |                   35 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.005 |      0.284 |                   25 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.247 |      0.323 |                   12 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3344216417910448
[2m[36m(func pid=119092)[0m top5: 0.8344216417910447
[2m[36m(func pid=119092)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=119092)[0m f1_macro: 0.2840178335561244
[2m[36m(func pid=119092)[0m f1_weighted: 0.3494286548264926
[2m[36m(func pid=119092)[0m f1_per_class: [0.463, 0.149, 0.242, 0.506, 0.059, 0.392, 0.337, 0.327, 0.14, 0.227]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.8058 | Steps: 4 | Val loss: 2.2081 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 3.7913 | Steps: 4 | Val loss: 57.6126 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 1.1453 | Steps: 4 | Val loss: 2.0152 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=115877)[0m top1: 0.21455223880597016
[2m[36m(func pid=115877)[0m top5: 0.7066231343283582
[2m[36m(func pid=115877)[0m f1_micro: 0.21455223880597016
[2m[36m(func pid=115877)[0m f1_macro: 0.14479629769081223
[2m[36m(func pid=115877)[0m f1_weighted: 0.22469226056361546
[2m[36m(func pid=115877)[0m f1_per_class: [0.114, 0.306, 0.058, 0.279, 0.011, 0.332, 0.146, 0.149, 0.031, 0.023]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.0672 | Steps: 4 | Val loss: 1.9299 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 12:37:24 (running for 00:39:30.54)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  3.791 |      0.301 |                   72 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.806 |      0.145 |                   36 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.005 |      0.284 |                   25 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.247 |      0.323 |                   12 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.3246268656716418
[2m[36m(func pid=107845)[0m top5: 0.804570895522388
[2m[36m(func pid=107845)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=107845)[0m f1_macro: 0.3011993447128567
[2m[36m(func pid=107845)[0m f1_weighted: 0.321473051077341
[2m[36m(func pid=107845)[0m f1_per_class: [0.455, 0.416, 0.4, 0.182, 0.27, 0.146, 0.479, 0.326, 0.146, 0.192]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m top1: 0.33302238805970147
[2m[36m(func pid=119092)[0m top5: 0.8232276119402985
[2m[36m(func pid=119092)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=119092)[0m f1_macro: 0.2756761250050937
[2m[36m(func pid=119092)[0m f1_weighted: 0.3468710265861039
[2m[36m(func pid=119092)[0m f1_per_class: [0.384, 0.138, 0.233, 0.508, 0.068, 0.396, 0.334, 0.331, 0.154, 0.21]
[2m[36m(func pid=121943)[0m top1: 0.30970149253731344
[2m[36m(func pid=121943)[0m top5: 0.8050373134328358
[2m[36m(func pid=121943)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=121943)[0m f1_macro: 0.2790832890291485
[2m[36m(func pid=121943)[0m f1_weighted: 0.3194804545948304
[2m[36m(func pid=121943)[0m f1_per_class: [0.434, 0.232, 0.267, 0.516, 0.081, 0.384, 0.175, 0.339, 0.219, 0.145]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.7371 | Steps: 4 | Val loss: 2.2046 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 17.2020 | Steps: 4 | Val loss: 60.6649 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.9984 | Steps: 4 | Val loss: 1.9022 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 1.3069 | Steps: 4 | Val loss: 2.0700 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=115877)[0m top1: 0.21688432835820895
[2m[36m(func pid=115877)[0m top5: 0.7168843283582089
[2m[36m(func pid=115877)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=115877)[0m f1_macro: 0.14505038892905722
[2m[36m(func pid=115877)[0m f1_weighted: 0.2337764667302923
[2m[36m(func pid=115877)[0m f1_per_class: [0.097, 0.292, 0.058, 0.303, 0.012, 0.32, 0.168, 0.15, 0.029, 0.022]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:37:30 (running for 00:39:35.98)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 17.202 |      0.304 |                   73 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.737 |      0.145 |                   37 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.067 |      0.276 |                   26 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.145 |      0.279 |                   13 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.30830223880597013
[2m[36m(func pid=107845)[0m top5: 0.816231343283582
[2m[36m(func pid=107845)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=107845)[0m f1_macro: 0.3038993227575566
[2m[36m(func pid=107845)[0m f1_weighted: 0.3099169066187449
[2m[36m(func pid=107845)[0m f1_per_class: [0.495, 0.447, 0.565, 0.17, 0.074, 0.116, 0.44, 0.343, 0.122, 0.267]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m top1: 0.3493470149253731
[2m[36m(func pid=119092)[0m top5: 0.8325559701492538
[2m[36m(func pid=119092)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=119092)[0m f1_macro: 0.28206898309573497
[2m[36m(func pid=119092)[0m f1_weighted: 0.3567390013940517
[2m[36m(func pid=119092)[0m f1_per_class: [0.4, 0.123, 0.233, 0.528, 0.082, 0.42, 0.35, 0.313, 0.155, 0.217]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m top1: 0.302705223880597
[2m[36m(func pid=121943)[0m top5: 0.8092350746268657
[2m[36m(func pid=121943)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=121943)[0m f1_macro: 0.26237647768841627
[2m[36m(func pid=121943)[0m f1_weighted: 0.3193426274681463
[2m[36m(func pid=121943)[0m f1_per_class: [0.228, 0.174, 0.229, 0.469, 0.117, 0.411, 0.244, 0.385, 0.242, 0.124]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.7590 | Steps: 4 | Val loss: 2.1924 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 16.8646 | Steps: 4 | Val loss: 57.2498 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.9847 | Steps: 4 | Val loss: 1.8709 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 1.2758 | Steps: 4 | Val loss: 1.8427 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
[2m[36m(func pid=115877)[0m top1: 0.23274253731343283
[2m[36m(func pid=115877)[0m top5: 0.7327425373134329
[2m[36m(func pid=115877)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=115877)[0m f1_macro: 0.15309413461675644
[2m[36m(func pid=115877)[0m f1_weighted: 0.247777190485116
[2m[36m(func pid=115877)[0m f1_per_class: [0.124, 0.3, 0.059, 0.323, 0.0, 0.35, 0.18, 0.14, 0.033, 0.022]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:37:35 (running for 00:39:41.41)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 17.202 |      0.304 |                   73 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.759 |      0.153 |                   38 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.985 |      0.294 |                   28 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.307 |      0.262 |                   14 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.3218283582089552
[2m[36m(func pid=107845)[0m top5: 0.8306902985074627
[2m[36m(func pid=107845)[0m f1_micro: 0.3218283582089552
[2m[36m(func pid=107845)[0m f1_macro: 0.32567480917403746
[2m[36m(func pid=107845)[0m f1_weighted: 0.3275137261242523
[2m[36m(func pid=107845)[0m f1_per_class: [0.44, 0.487, 0.703, 0.206, 0.074, 0.134, 0.439, 0.321, 0.121, 0.333]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m top1: 0.3619402985074627
[2m[36m(func pid=119092)[0m top5: 0.8423507462686567
[2m[36m(func pid=119092)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=119092)[0m f1_macro: 0.2943294829154802
[2m[36m(func pid=119092)[0m f1_weighted: 0.3706291448139332
[2m[36m(func pid=119092)[0m f1_per_class: [0.428, 0.171, 0.235, 0.531, 0.087, 0.44, 0.353, 0.331, 0.158, 0.209]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m top1: 0.35447761194029853
[2m[36m(func pid=121943)[0m top5: 0.8619402985074627
[2m[36m(func pid=121943)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=121943)[0m f1_macro: 0.29303379364375276
[2m[36m(func pid=121943)[0m f1_weighted: 0.38630058661129457
[2m[36m(func pid=121943)[0m f1_per_class: [0.256, 0.305, 0.268, 0.353, 0.171, 0.448, 0.505, 0.302, 0.191, 0.131]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.7580 | Steps: 4 | Val loss: 2.2001 | Batch size: 32 | lr: 0.0001 | Duration: 3.22s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.0104 | Steps: 4 | Val loss: 1.8848 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 9.6699 | Steps: 4 | Val loss: 54.6336 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.0955 | Steps: 4 | Val loss: 1.6410 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=115877)[0m top1: 0.22388059701492538
[2m[36m(func pid=115877)[0m top5: 0.7145522388059702
[2m[36m(func pid=115877)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=115877)[0m f1_macro: 0.14939566902520102
[2m[36m(func pid=115877)[0m f1_weighted: 0.2379072264647129
[2m[36m(func pid=115877)[0m f1_per_class: [0.114, 0.283, 0.052, 0.317, 0.012, 0.36, 0.159, 0.144, 0.031, 0.022]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:37:40 (running for 00:39:46.57)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.3175
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 16.865 |      0.326 |                   74 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.758 |      0.149 |                   39 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  2.01  |      0.286 |                   29 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.276 |      0.293 |                   15 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3521455223880597
[2m[36m(func pid=119092)[0m top5: 0.8409514925373134
[2m[36m(func pid=119092)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=119092)[0m f1_macro: 0.2859137363660548
[2m[36m(func pid=119092)[0m f1_weighted: 0.3668836493911552
[2m[36m(func pid=119092)[0m f1_per_class: [0.384, 0.243, 0.202, 0.524, 0.085, 0.386, 0.331, 0.325, 0.139, 0.241]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m top1: 0.322294776119403
[2m[36m(func pid=107845)[0m top5: 0.8451492537313433
[2m[36m(func pid=107845)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=107845)[0m f1_macro: 0.33681456193062165
[2m[36m(func pid=107845)[0m f1_weighted: 0.34307669204157565
[2m[36m(func pid=107845)[0m f1_per_class: [0.448, 0.495, 0.667, 0.287, 0.125, 0.181, 0.394, 0.303, 0.115, 0.353]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=121943)[0m top1: 0.42677238805970147
[2m[36m(func pid=121943)[0m top5: 0.9104477611940298
[2m[36m(func pid=121943)[0m f1_micro: 0.42677238805970147
[2m[36m(func pid=121943)[0m f1_macro: 0.34719021930508015
[2m[36m(func pid=121943)[0m f1_weighted: 0.4260968647192609
[2m[36m(func pid=121943)[0m f1_per_class: [0.46, 0.47, 0.296, 0.34, 0.214, 0.44, 0.548, 0.26, 0.173, 0.269]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.7271 | Steps: 4 | Val loss: 2.2010 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 1.8847 | Steps: 4 | Val loss: 1.8792 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 2.3203 | Steps: 4 | Val loss: 49.5648 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.1960 | Steps: 4 | Val loss: 1.6709 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=115877)[0m top1: 0.22061567164179105
[2m[36m(func pid=115877)[0m top5: 0.710820895522388
[2m[36m(func pid=115877)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=115877)[0m f1_macro: 0.14636991639695401
[2m[36m(func pid=115877)[0m f1_weighted: 0.2307536062815891
[2m[36m(func pid=115877)[0m f1_per_class: [0.126, 0.293, 0.05, 0.3, 0.013, 0.356, 0.149, 0.127, 0.029, 0.022]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:37:45 (running for 00:39:51.66)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  9.67  |      0.337 |                   75 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.727 |      0.146 |                   40 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.885 |      0.287 |                   30 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.095 |      0.347 |                   16 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.34048507462686567
[2m[36m(func pid=119092)[0m top5: 0.8442164179104478
[2m[36m(func pid=119092)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=119092)[0m f1_macro: 0.28690817723990963
[2m[36m(func pid=119092)[0m f1_weighted: 0.35414901949520955
[2m[36m(func pid=119092)[0m f1_per_class: [0.351, 0.306, 0.186, 0.479, 0.097, 0.427, 0.278, 0.335, 0.143, 0.268]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m top1: 0.3596082089552239
[2m[36m(func pid=107845)[0m top5: 0.8698694029850746
[2m[36m(func pid=107845)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=107845)[0m f1_macro: 0.37098195797714334
[2m[36m(func pid=107845)[0m f1_weighted: 0.3851319200308111
[2m[36m(func pid=107845)[0m f1_per_class: [0.439, 0.519, 0.714, 0.406, 0.17, 0.22, 0.39, 0.303, 0.131, 0.418]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=121943)[0m top1: 0.4207089552238806
[2m[36m(func pid=121943)[0m top5: 0.9039179104477612
[2m[36m(func pid=121943)[0m f1_micro: 0.4207089552238806
[2m[36m(func pid=121943)[0m f1_macro: 0.344560836209067
[2m[36m(func pid=121943)[0m f1_weighted: 0.4323183631781396
[2m[36m(func pid=121943)[0m f1_per_class: [0.374, 0.504, 0.181, 0.414, 0.231, 0.444, 0.475, 0.295, 0.19, 0.338]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.8114 | Steps: 4 | Val loss: 2.2024 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.9182 | Steps: 4 | Val loss: 1.8636 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 5.7605 | Steps: 4 | Val loss: 44.9164 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 12:37:50 (running for 00:39:56.82)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  2.32  |      0.371 |                   76 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.727 |      0.146 |                   40 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.918 |      0.301 |                   31 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.196 |      0.345 |                   17 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.34701492537313433
[2m[36m(func pid=119092)[0m top5: 0.8470149253731343
[2m[36m(func pid=119092)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=119092)[0m f1_macro: 0.3009397669744248
[2m[36m(func pid=119092)[0m f1_weighted: 0.35858182446599546
[2m[36m(func pid=119092)[0m f1_per_class: [0.418, 0.356, 0.216, 0.48, 0.085, 0.417, 0.261, 0.331, 0.149, 0.297]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m top1: 0.2234141791044776
[2m[36m(func pid=115877)[0m top5: 0.7112873134328358
[2m[36m(func pid=115877)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=115877)[0m f1_macro: 0.15155160963459416
[2m[36m(func pid=115877)[0m f1_weighted: 0.23685391592414803
[2m[36m(func pid=115877)[0m f1_per_class: [0.109, 0.291, 0.086, 0.305, 0.025, 0.361, 0.164, 0.126, 0.027, 0.02]
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.0518 | Steps: 4 | Val loss: 1.7261 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=107845)[0m top1: 0.40298507462686567
[2m[36m(func pid=107845)[0m top5: 0.8666044776119403
[2m[36m(func pid=107845)[0m f1_micro: 0.40298507462686567
[2m[36m(func pid=107845)[0m f1_macro: 0.3969415449065272
[2m[36m(func pid=107845)[0m f1_weighted: 0.4280593098091336
[2m[36m(func pid=107845)[0m f1_per_class: [0.426, 0.506, 0.692, 0.496, 0.182, 0.293, 0.42, 0.328, 0.155, 0.471]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=121943)[0m top1: 0.3880597014925373
[2m[36m(func pid=121943)[0m top5: 0.8927238805970149
[2m[36m(func pid=121943)[0m f1_micro: 0.3880597014925373
[2m[36m(func pid=121943)[0m f1_macro: 0.3368664128199276
[2m[36m(func pid=121943)[0m f1_weighted: 0.40584386657854943
[2m[36m(func pid=121943)[0m f1_per_class: [0.435, 0.476, 0.159, 0.463, 0.22, 0.413, 0.363, 0.3, 0.197, 0.344]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 1.9186 | Steps: 4 | Val loss: 1.8374 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.7435 | Steps: 4 | Val loss: 2.1951 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 9.2339 | Steps: 4 | Val loss: 43.2971 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=119092)[0m top1: 0.36007462686567165
[2m[36m(func pid=119092)[0m top5: 0.855410447761194
[2m[36m(func pid=119092)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=119092)[0m f1_macro: 0.3099414670077104
[2m[36m(func pid=119092)[0m f1_weighted: 0.3708053237917835
[2m[36m(func pid=119092)[0m f1_per_class: [0.424, 0.366, 0.238, 0.489, 0.075, 0.431, 0.28, 0.337, 0.156, 0.304]
[2m[36m(func pid=119092)[0m 
== Status ==
Current time: 2024-01-07 12:37:56 (running for 00:40:02.24)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  5.76  |      0.397 |                   77 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.811 |      0.152 |                   41 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.919 |      0.31  |                   32 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.052 |      0.337 |                   18 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=115877)[0m top1: 0.23227611940298507
[2m[36m(func pid=115877)[0m top5: 0.7094216417910447
[2m[36m(func pid=115877)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=115877)[0m f1_macro: 0.1565498961325182
[2m[36m(func pid=115877)[0m f1_weighted: 0.24457175610296378
[2m[36m(func pid=115877)[0m f1_per_class: [0.12, 0.281, 0.089, 0.341, 0.025, 0.36, 0.159, 0.144, 0.026, 0.022]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.2557 | Steps: 4 | Val loss: 1.8665 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=107845)[0m top1: 0.41744402985074625
[2m[36m(func pid=107845)[0m top5: 0.8698694029850746
[2m[36m(func pid=107845)[0m f1_micro: 0.41744402985074625
[2m[36m(func pid=107845)[0m f1_macro: 0.4001745075244113
[2m[36m(func pid=107845)[0m f1_weighted: 0.43689994026768436
[2m[36m(func pid=107845)[0m f1_per_class: [0.389, 0.501, 0.72, 0.527, 0.165, 0.306, 0.42, 0.322, 0.172, 0.48]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.8753 | Steps: 4 | Val loss: 1.8151 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=121943)[0m top1: 0.35074626865671643
[2m[36m(func pid=121943)[0m top5: 0.8572761194029851
[2m[36m(func pid=121943)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=121943)[0m f1_macro: 0.3149680902050086
[2m[36m(func pid=121943)[0m f1_weighted: 0.3584500445522678
[2m[36m(func pid=121943)[0m f1_per_class: [0.312, 0.466, 0.162, 0.36, 0.174, 0.424, 0.298, 0.364, 0.182, 0.407]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.7094 | Steps: 4 | Val loss: 2.1951 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 8.1974 | Steps: 4 | Val loss: 42.7748 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 12:38:01 (running for 00:40:07.54)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  9.234 |      0.4   |                   78 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.744 |      0.157 |                   42 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.875 |      0.318 |                   33 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.256 |      0.315 |                   19 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.36986940298507465
[2m[36m(func pid=119092)[0m top5: 0.8605410447761194
[2m[36m(func pid=119092)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=119092)[0m f1_macro: 0.31830603350004977
[2m[36m(func pid=119092)[0m f1_weighted: 0.38906764779625735
[2m[36m(func pid=119092)[0m f1_per_class: [0.476, 0.367, 0.235, 0.48, 0.081, 0.415, 0.35, 0.354, 0.146, 0.279]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m top1: 0.2271455223880597
[2m[36m(func pid=115877)[0m top5: 0.7145522388059702
[2m[36m(func pid=115877)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=115877)[0m f1_macro: 0.15649822456669676
[2m[36m(func pid=115877)[0m f1_weighted: 0.23718911366026724
[2m[36m(func pid=115877)[0m f1_per_class: [0.131, 0.269, 0.103, 0.343, 0.0, 0.328, 0.145, 0.168, 0.024, 0.054]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=107845)[0m top1: 0.43656716417910446
[2m[36m(func pid=107845)[0m top5: 0.8759328358208955
[2m[36m(func pid=107845)[0m f1_micro: 0.43656716417910446
[2m[36m(func pid=107845)[0m f1_macro: 0.4185577313350211
[2m[36m(func pid=107845)[0m f1_weighted: 0.4491499912732992
[2m[36m(func pid=107845)[0m f1_per_class: [0.374, 0.492, 0.72, 0.551, 0.202, 0.334, 0.421, 0.351, 0.219, 0.522]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.1666 | Steps: 4 | Val loss: 2.0190 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.7834 | Steps: 4 | Val loss: 1.8032 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=121943)[0m top1: 0.3269589552238806
[2m[36m(func pid=121943)[0m top5: 0.8334888059701493
[2m[36m(func pid=121943)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=121943)[0m f1_macro: 0.302285172885863
[2m[36m(func pid=121943)[0m f1_weighted: 0.3073526601891069
[2m[36m(func pid=121943)[0m f1_per_class: [0.398, 0.487, 0.168, 0.206, 0.127, 0.416, 0.258, 0.341, 0.215, 0.407]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 14.8519 | Steps: 4 | Val loss: 46.2851 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.7304 | Steps: 4 | Val loss: 2.1989 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 12:38:07 (running for 00:40:12.98)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  8.197 |      0.419 |                   79 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.709 |      0.156 |                   43 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.783 |      0.325 |                   34 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.167 |      0.302 |                   20 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3694029850746269
[2m[36m(func pid=119092)[0m top5: 0.867070895522388
[2m[36m(func pid=119092)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=119092)[0m f1_macro: 0.32546876045875717
[2m[36m(func pid=119092)[0m f1_weighted: 0.39012012103167903
[2m[36m(func pid=119092)[0m f1_per_class: [0.531, 0.349, 0.24, 0.477, 0.075, 0.424, 0.356, 0.342, 0.196, 0.263]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m top1: 0.43236940298507465
[2m[36m(func pid=107845)[0m top5: 0.8549440298507462
[2m[36m(func pid=107845)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=107845)[0m f1_macro: 0.41453670694773026
[2m[36m(func pid=107845)[0m f1_weighted: 0.4459050189912837
[2m[36m(func pid=107845)[0m f1_per_class: [0.364, 0.468, 0.75, 0.553, 0.138, 0.38, 0.403, 0.372, 0.218, 0.5]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m top1: 0.2248134328358209
[2m[36m(func pid=115877)[0m top5: 0.7103544776119403
[2m[36m(func pid=115877)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=115877)[0m f1_macro: 0.15812413470473347
[2m[36m(func pid=115877)[0m f1_weighted: 0.22895824567157003
[2m[36m(func pid=115877)[0m f1_per_class: [0.159, 0.259, 0.118, 0.346, 0.0, 0.331, 0.119, 0.164, 0.012, 0.074]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.0668 | Steps: 4 | Val loss: 1.9528 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.8188 | Steps: 4 | Val loss: 1.8123 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 4.8034 | Steps: 4 | Val loss: 50.2909 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=121943)[0m top1: 0.33302238805970147
[2m[36m(func pid=121943)[0m top5: 0.8498134328358209
[2m[36m(func pid=121943)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=121943)[0m f1_macro: 0.31476511017216197
[2m[36m(func pid=121943)[0m f1_weighted: 0.3370911406218071
[2m[36m(func pid=121943)[0m f1_per_class: [0.491, 0.477, 0.213, 0.385, 0.12, 0.381, 0.208, 0.334, 0.191, 0.346]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.7670 | Steps: 4 | Val loss: 2.1997 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 12:38:12 (running for 00:40:18.27)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 14.852 |      0.415 |                   80 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.73  |      0.158 |                   44 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.819 |      0.312 |                   35 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.067 |      0.315 |                   21 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3530783582089552
[2m[36m(func pid=119092)[0m top5: 0.8591417910447762
[2m[36m(func pid=119092)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=119092)[0m f1_macro: 0.3121570228257135
[2m[36m(func pid=119092)[0m f1_weighted: 0.3740341852435237
[2m[36m(func pid=119092)[0m f1_per_class: [0.48, 0.349, 0.222, 0.442, 0.083, 0.418, 0.344, 0.337, 0.181, 0.265]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m top1: 0.40951492537313433
[2m[36m(func pid=107845)[0m top5: 0.8390858208955224
[2m[36m(func pid=107845)[0m f1_micro: 0.40951492537313433
[2m[36m(func pid=107845)[0m f1_macro: 0.4076496128005534
[2m[36m(func pid=107845)[0m f1_weighted: 0.4288042409647649
[2m[36m(func pid=107845)[0m f1_per_class: [0.391, 0.454, 0.75, 0.522, 0.106, 0.363, 0.388, 0.368, 0.225, 0.511]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m top1: 0.23367537313432835
[2m[36m(func pid=115877)[0m top5: 0.6921641791044776
[2m[36m(func pid=115877)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=115877)[0m f1_macro: 0.17122989363185273
[2m[36m(func pid=115877)[0m f1_weighted: 0.23620122582326125
[2m[36m(func pid=115877)[0m f1_per_class: [0.174, 0.298, 0.101, 0.352, 0.015, 0.327, 0.109, 0.182, 0.023, 0.132]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 0.7027 | Steps: 4 | Val loss: 1.8264 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.8948 | Steps: 4 | Val loss: 1.7760 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 19.7978 | Steps: 4 | Val loss: 52.9456 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=121943)[0m top1: 0.37406716417910446
[2m[36m(func pid=121943)[0m top5: 0.8736007462686567
[2m[36m(func pid=121943)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=121943)[0m f1_macro: 0.32555119799308463
[2m[36m(func pid=121943)[0m f1_weighted: 0.3935401216809185
[2m[36m(func pid=121943)[0m f1_per_class: [0.495, 0.416, 0.182, 0.531, 0.124, 0.384, 0.294, 0.362, 0.186, 0.282]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.7250 | Steps: 4 | Val loss: 2.1946 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 12:38:17 (running for 00:40:23.48)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.803 |      0.408 |                   81 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.767 |      0.171 |                   45 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.895 |      0.334 |                   36 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.703 |      0.326 |                   22 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.37966417910447764
[2m[36m(func pid=119092)[0m top5: 0.867070895522388
[2m[36m(func pid=119092)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=119092)[0m f1_macro: 0.3339595472143213
[2m[36m(func pid=119092)[0m f1_weighted: 0.39761918001256064
[2m[36m(func pid=119092)[0m f1_per_class: [0.483, 0.37, 0.282, 0.459, 0.093, 0.448, 0.378, 0.345, 0.191, 0.289]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m top1: 0.394589552238806
[2m[36m(func pid=107845)[0m top5: 0.8264925373134329
[2m[36m(func pid=107845)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=107845)[0m f1_macro: 0.4100201660917075
[2m[36m(func pid=107845)[0m f1_weighted: 0.42004356062973325
[2m[36m(func pid=107845)[0m f1_per_class: [0.44, 0.439, 0.8, 0.525, 0.08, 0.356, 0.368, 0.349, 0.213, 0.531]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m top1: 0.2355410447761194
[2m[36m(func pid=115877)[0m top5: 0.6949626865671642
[2m[36m(func pid=115877)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=115877)[0m f1_macro: 0.16710600634235787
[2m[36m(func pid=115877)[0m f1_weighted: 0.24099377629793242
[2m[36m(func pid=115877)[0m f1_per_class: [0.188, 0.282, 0.104, 0.363, 0.015, 0.342, 0.125, 0.144, 0.036, 0.072]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 0.7437 | Steps: 4 | Val loss: 1.7548 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.9341 | Steps: 4 | Val loss: 1.7958 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 17.6794 | Steps: 4 | Val loss: 56.9940 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=121943)[0m top1: 0.4099813432835821
[2m[36m(func pid=121943)[0m top5: 0.8922574626865671
[2m[36m(func pid=121943)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=121943)[0m f1_macro: 0.33459685061041927
[2m[36m(func pid=121943)[0m f1_weighted: 0.44057770540520963
[2m[36m(func pid=121943)[0m f1_per_class: [0.496, 0.435, 0.143, 0.541, 0.114, 0.379, 0.444, 0.318, 0.165, 0.312]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m top1: 0.36007462686567165
[2m[36m(func pid=119092)[0m top5: 0.863339552238806
[2m[36m(func pid=119092)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=119092)[0m f1_macro: 0.3197338898213758
[2m[36m(func pid=119092)[0m f1_weighted: 0.3799199280587801
[2m[36m(func pid=119092)[0m f1_per_class: [0.5, 0.337, 0.238, 0.428, 0.099, 0.442, 0.372, 0.338, 0.187, 0.257]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7227 | Steps: 4 | Val loss: 2.1969 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 12:38:24 (running for 00:40:30.05)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 17.679 |      0.389 |                   83 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.725 |      0.167 |                   46 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.934 |      0.32  |                   37 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.744 |      0.335 |                   23 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.37593283582089554
[2m[36m(func pid=107845)[0m top5: 0.8097014925373134
[2m[36m(func pid=107845)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=107845)[0m f1_macro: 0.38932613420905193
[2m[36m(func pid=107845)[0m f1_weighted: 0.40537319853651727
[2m[36m(func pid=107845)[0m f1_per_class: [0.429, 0.454, 0.75, 0.499, 0.065, 0.342, 0.344, 0.356, 0.206, 0.449]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m top1: 0.23460820895522388
[2m[36m(func pid=115877)[0m top5: 0.6935634328358209
[2m[36m(func pid=115877)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=115877)[0m f1_macro: 0.1680989232001116
[2m[36m(func pid=115877)[0m f1_weighted: 0.23697468383199086
[2m[36m(func pid=115877)[0m f1_per_class: [0.201, 0.301, 0.1, 0.345, 0.015, 0.342, 0.112, 0.169, 0.025, 0.07]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.6295 | Steps: 4 | Val loss: 1.7928 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 0.7693 | Steps: 4 | Val loss: 1.7769 | Batch size: 32 | lr: 0.01 | Duration: 3.20s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 3.6162 | Steps: 4 | Val loss: 60.5584 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=119092)[0m top1: 0.36427238805970147
[2m[36m(func pid=119092)[0m top5: 0.8610074626865671
[2m[36m(func pid=119092)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=119092)[0m f1_macro: 0.31811244946773626
[2m[36m(func pid=119092)[0m f1_weighted: 0.3862383989403119
[2m[36m(func pid=119092)[0m f1_per_class: [0.457, 0.342, 0.238, 0.442, 0.116, 0.443, 0.382, 0.335, 0.17, 0.257]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m top1: 0.4085820895522388
[2m[36m(func pid=121943)[0m top5: 0.8908582089552238
[2m[36m(func pid=121943)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=121943)[0m f1_macro: 0.3324513416963215
[2m[36m(func pid=121943)[0m f1_weighted: 0.4370735849374289
[2m[36m(func pid=121943)[0m f1_per_class: [0.46, 0.455, 0.149, 0.497, 0.128, 0.43, 0.449, 0.278, 0.203, 0.275]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.6996 | Steps: 4 | Val loss: 2.2093 | Batch size: 32 | lr: 0.0001 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 12:38:29 (running for 00:40:35.58)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  3.616 |      0.385 |                   84 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.723 |      0.168 |                   47 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.629 |      0.318 |                   38 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.769 |      0.332 |                   24 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.3605410447761194
[2m[36m(func pid=107845)[0m top5: 0.7975746268656716
[2m[36m(func pid=107845)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=107845)[0m f1_macro: 0.38521275110787245
[2m[36m(func pid=107845)[0m f1_weighted: 0.38718966304482005
[2m[36m(func pid=107845)[0m f1_per_class: [0.413, 0.446, 0.75, 0.448, 0.063, 0.352, 0.33, 0.362, 0.212, 0.476]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.8066 | Steps: 4 | Val loss: 1.8279 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=115877)[0m top1: 0.2271455223880597
[2m[36m(func pid=115877)[0m top5: 0.6809701492537313
[2m[36m(func pid=115877)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=115877)[0m f1_macro: 0.16456442941857766
[2m[36m(func pid=115877)[0m f1_weighted: 0.23026975340701403
[2m[36m(func pid=115877)[0m f1_per_class: [0.183, 0.272, 0.09, 0.34, 0.025, 0.336, 0.109, 0.202, 0.026, 0.062]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 0.9367 | Steps: 4 | Val loss: 1.8025 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 7.6535 | Steps: 4 | Val loss: 63.7256 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=119092)[0m top1: 0.33955223880597013
[2m[36m(func pid=119092)[0m top5: 0.8549440298507462
[2m[36m(func pid=119092)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=119092)[0m f1_macro: 0.3029938119861983
[2m[36m(func pid=119092)[0m f1_weighted: 0.3614310747615736
[2m[36m(func pid=119092)[0m f1_per_class: [0.437, 0.306, 0.245, 0.421, 0.105, 0.439, 0.344, 0.341, 0.157, 0.237]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m top1: 0.39972014925373134
[2m[36m(func pid=121943)[0m top5: 0.8885261194029851
[2m[36m(func pid=121943)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=121943)[0m f1_macro: 0.3112368244137499
[2m[36m(func pid=121943)[0m f1_weighted: 0.4275203690098033
[2m[36m(func pid=121943)[0m f1_per_class: [0.354, 0.419, 0.149, 0.494, 0.132, 0.414, 0.466, 0.222, 0.187, 0.274]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.7398 | Steps: 4 | Val loss: 2.2103 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 12:38:35 (running for 00:40:40.97)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  7.654 |      0.365 |                   85 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.7   |      0.165 |                   48 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.807 |      0.303 |                   39 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.937 |      0.311 |                   25 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.34654850746268656
[2m[36m(func pid=107845)[0m top5: 0.7803171641791045
[2m[36m(func pid=107845)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=107845)[0m f1_macro: 0.3650518960914477
[2m[36m(func pid=107845)[0m f1_weighted: 0.36504787067222605
[2m[36m(func pid=107845)[0m f1_per_class: [0.403, 0.437, 0.692, 0.401, 0.068, 0.355, 0.309, 0.359, 0.198, 0.429]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.7507 | Steps: 4 | Val loss: 1.8823 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=115877)[0m top1: 0.22527985074626866
[2m[36m(func pid=115877)[0m top5: 0.6791044776119403
[2m[36m(func pid=115877)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=115877)[0m f1_macro: 0.16674264883300122
[2m[36m(func pid=115877)[0m f1_weighted: 0.22880493975527802
[2m[36m(func pid=115877)[0m f1_per_class: [0.171, 0.272, 0.092, 0.331, 0.025, 0.352, 0.106, 0.197, 0.027, 0.094]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.1196 | Steps: 4 | Val loss: 1.9035 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 16.1623 | Steps: 4 | Val loss: 66.8343 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=119092)[0m top1: 0.30783582089552236
[2m[36m(func pid=119092)[0m top5: 0.8372201492537313
[2m[36m(func pid=119092)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=119092)[0m f1_macro: 0.2771420898876977
[2m[36m(func pid=119092)[0m f1_weighted: 0.33257037791207467
[2m[36m(func pid=119092)[0m f1_per_class: [0.378, 0.257, 0.185, 0.404, 0.087, 0.418, 0.303, 0.354, 0.15, 0.237]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m top1: 0.37453358208955223
[2m[36m(func pid=121943)[0m top5: 0.8680037313432836
[2m[36m(func pid=121943)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=121943)[0m f1_macro: 0.31491805065021033
[2m[36m(func pid=121943)[0m f1_weighted: 0.4079450336103092
[2m[36m(func pid=121943)[0m f1_per_class: [0.364, 0.41, 0.164, 0.478, 0.083, 0.401, 0.402, 0.327, 0.199, 0.322]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.7426 | Steps: 4 | Val loss: 2.2170 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 12:38:40 (running for 00:40:46.37)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 16.162 |      0.344 |                   86 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.74  |      0.167 |                   49 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.751 |      0.277 |                   40 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.12  |      0.315 |                   26 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.34048507462686567
[2m[36m(func pid=107845)[0m top5: 0.7625932835820896
[2m[36m(func pid=107845)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=107845)[0m f1_macro: 0.3438764592866581
[2m[36m(func pid=107845)[0m f1_weighted: 0.35127379377825496
[2m[36m(func pid=107845)[0m f1_per_class: [0.418, 0.439, 0.6, 0.395, 0.067, 0.337, 0.274, 0.375, 0.22, 0.313]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.7296 | Steps: 4 | Val loss: 1.8813 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=115877)[0m top1: 0.22434701492537312
[2m[36m(func pid=115877)[0m top5: 0.6707089552238806
[2m[36m(func pid=115877)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=115877)[0m f1_macro: 0.1646338878128095
[2m[36m(func pid=115877)[0m f1_weighted: 0.22965313220085198
[2m[36m(func pid=115877)[0m f1_per_class: [0.165, 0.276, 0.093, 0.331, 0.011, 0.347, 0.106, 0.233, 0.0, 0.083]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 0.7046 | Steps: 4 | Val loss: 1.8961 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 17.9422 | Steps: 4 | Val loss: 65.0200 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=119092)[0m top1: 0.31949626865671643
[2m[36m(func pid=119092)[0m top5: 0.8353544776119403
[2m[36m(func pid=119092)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=119092)[0m f1_macro: 0.2855530510136142
[2m[36m(func pid=119092)[0m f1_weighted: 0.34881544814317095
[2m[36m(func pid=119092)[0m f1_per_class: [0.398, 0.237, 0.186, 0.417, 0.091, 0.439, 0.347, 0.356, 0.143, 0.242]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m top1: 0.37919776119402987
[2m[36m(func pid=121943)[0m top5: 0.8717350746268657
[2m[36m(func pid=121943)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=121943)[0m f1_macro: 0.3336270799105086
[2m[36m(func pid=121943)[0m f1_weighted: 0.3961239155315702
[2m[36m(func pid=121943)[0m f1_per_class: [0.452, 0.49, 0.194, 0.363, 0.087, 0.396, 0.415, 0.318, 0.221, 0.4]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.6738 | Steps: 4 | Val loss: 2.2223 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 12:38:45 (running for 00:40:51.68)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 17.942 |      0.324 |                   87 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.743 |      0.165 |                   50 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.73  |      0.286 |                   41 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.705 |      0.334 |                   27 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.3302238805970149
[2m[36m(func pid=107845)[0m top5: 0.761660447761194
[2m[36m(func pid=107845)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=107845)[0m f1_macro: 0.3240271012032384
[2m[36m(func pid=107845)[0m f1_weighted: 0.34176891030092316
[2m[36m(func pid=107845)[0m f1_per_class: [0.386, 0.425, 0.541, 0.4, 0.071, 0.309, 0.265, 0.355, 0.229, 0.26]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.6280 | Steps: 4 | Val loss: 1.8656 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=115877)[0m top1: 0.21735074626865672
[2m[36m(func pid=115877)[0m top5: 0.6595149253731343
[2m[36m(func pid=115877)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=115877)[0m f1_macro: 0.1707125216307621
[2m[36m(func pid=115877)[0m f1_weighted: 0.22379829065950266
[2m[36m(func pid=115877)[0m f1_per_class: [0.183, 0.282, 0.086, 0.303, 0.021, 0.337, 0.108, 0.23, 0.023, 0.135]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.1018 | Steps: 4 | Val loss: 1.9260 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 7.0789 | Steps: 4 | Val loss: 61.7572 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=119092)[0m top1: 0.3199626865671642
[2m[36m(func pid=119092)[0m top5: 0.8451492537313433
[2m[36m(func pid=119092)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=119092)[0m f1_macro: 0.28945754367014265
[2m[36m(func pid=119092)[0m f1_weighted: 0.35350647158932247
[2m[36m(func pid=119092)[0m f1_per_class: [0.435, 0.248, 0.205, 0.428, 0.096, 0.414, 0.358, 0.332, 0.139, 0.24]
[2m[36m(func pid=119092)[0m 
== Status ==
Current time: 2024-01-07 12:38:51 (running for 00:40:56.97)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 17.942 |      0.324 |                   87 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.674 |      0.171 |                   51 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.628 |      0.289 |                   42 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.102 |      0.332 |                   28 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=121943)[0m top1: 0.37080223880597013
[2m[36m(func pid=121943)[0m top5: 0.8572761194029851
[2m[36m(func pid=121943)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=121943)[0m f1_macro: 0.33155838470157406
[2m[36m(func pid=121943)[0m f1_weighted: 0.39231034953459304
[2m[36m(func pid=121943)[0m f1_per_class: [0.508, 0.484, 0.185, 0.452, 0.094, 0.341, 0.335, 0.349, 0.239, 0.328]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=107845)[0m top1: 0.34095149253731344
[2m[36m(func pid=107845)[0m top5: 0.7817164179104478
[2m[36m(func pid=107845)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=107845)[0m f1_macro: 0.3159888068068619
[2m[36m(func pid=107845)[0m f1_weighted: 0.3598965959837954
[2m[36m(func pid=107845)[0m f1_per_class: [0.415, 0.398, 0.386, 0.43, 0.07, 0.302, 0.316, 0.335, 0.292, 0.217]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.7514 | Steps: 4 | Val loss: 2.2087 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.7203 | Steps: 4 | Val loss: 1.8690 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=115877)[0m top1: 0.22201492537313433
[2m[36m(func pid=115877)[0m top5: 0.6823694029850746
[2m[36m(func pid=115877)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=115877)[0m f1_macro: 0.17262110903586275
[2m[36m(func pid=115877)[0m f1_weighted: 0.23536724339787587
[2m[36m(func pid=115877)[0m f1_per_class: [0.186, 0.28, 0.077, 0.322, 0.021, 0.337, 0.135, 0.204, 0.022, 0.143]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 7.7537 | Steps: 4 | Val loss: 59.2443 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 0.6419 | Steps: 4 | Val loss: 1.9811 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=119092)[0m top1: 0.3260261194029851
[2m[36m(func pid=119092)[0m top5: 0.8372201492537313
[2m[36m(func pid=119092)[0m f1_micro: 0.3260261194029851
[2m[36m(func pid=119092)[0m f1_macro: 0.2857879356401467
[2m[36m(func pid=119092)[0m f1_weighted: 0.3600823963939114
[2m[36m(func pid=119092)[0m f1_per_class: [0.402, 0.248, 0.167, 0.457, 0.095, 0.408, 0.357, 0.34, 0.134, 0.25]
[2m[36m(func pid=119092)[0m 
== Status ==
Current time: 2024-01-07 12:38:56 (running for 00:41:02.37)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  7.754 |      0.315 |                   89 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.751 |      0.173 |                   52 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.72  |      0.286 |                   43 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.102 |      0.332 |                   28 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.34888059701492535
[2m[36m(func pid=107845)[0m top5: 0.789179104477612
[2m[36m(func pid=107845)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=107845)[0m f1_macro: 0.31520352312383854
[2m[36m(func pid=107845)[0m f1_weighted: 0.37326923171666415
[2m[36m(func pid=107845)[0m f1_per_class: [0.472, 0.377, 0.306, 0.46, 0.085, 0.3, 0.341, 0.35, 0.293, 0.169]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=121943)[0m top1: 0.34468283582089554
[2m[36m(func pid=121943)[0m top5: 0.8572761194029851
[2m[36m(func pid=121943)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=121943)[0m f1_macro: 0.31398832342999977
[2m[36m(func pid=121943)[0m f1_weighted: 0.36304432957609944
[2m[36m(func pid=121943)[0m f1_per_class: [0.505, 0.397, 0.216, 0.492, 0.105, 0.358, 0.248, 0.342, 0.22, 0.256]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.7175 | Steps: 4 | Val loss: 2.2036 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.6762 | Steps: 4 | Val loss: 1.8447 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=115877)[0m top1: 0.23227611940298507
[2m[36m(func pid=115877)[0m top5: 0.6865671641791045
[2m[36m(func pid=115877)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=115877)[0m f1_macro: 0.18005315548240675
[2m[36m(func pid=115877)[0m f1_weighted: 0.24315137577947116
[2m[36m(func pid=115877)[0m f1_per_class: [0.211, 0.283, 0.07, 0.347, 0.012, 0.358, 0.122, 0.224, 0.031, 0.143]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=119092)[0m top1: 0.33255597014925375
[2m[36m(func pid=119092)[0m top5: 0.8498134328358209
[2m[36m(func pid=119092)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=119092)[0m f1_macro: 0.289348526835661
[2m[36m(func pid=119092)[0m f1_weighted: 0.36449665652920943
[2m[36m(func pid=119092)[0m f1_per_class: [0.363, 0.296, 0.156, 0.428, 0.11, 0.429, 0.366, 0.322, 0.146, 0.277]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 6.7537 | Steps: 4 | Val loss: 58.3605 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 0.9102 | Steps: 4 | Val loss: 1.9168 | Batch size: 32 | lr: 0.01 | Duration: 3.25s
== Status ==
Current time: 2024-01-07 12:39:02 (running for 00:41:07.99)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  6.754 |      0.305 |                   90 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.718 |      0.18  |                   53 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.676 |      0.289 |                   44 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.642 |      0.314 |                   29 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.3591417910447761
[2m[36m(func pid=107845)[0m top5: 0.7989738805970149
[2m[36m(func pid=107845)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=107845)[0m f1_macro: 0.30503390240381956
[2m[36m(func pid=107845)[0m f1_weighted: 0.3837895998081397
[2m[36m(func pid=107845)[0m f1_per_class: [0.483, 0.32, 0.202, 0.514, 0.082, 0.269, 0.371, 0.365, 0.282, 0.163]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.7244 | Steps: 4 | Val loss: 2.1925 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.6830 | Steps: 4 | Val loss: 1.8429 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=121943)[0m top1: 0.36473880597014924
[2m[36m(func pid=121943)[0m top5: 0.8694029850746269
[2m[36m(func pid=121943)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=121943)[0m f1_macro: 0.33207133312938186
[2m[36m(func pid=121943)[0m f1_weighted: 0.3844892383482079
[2m[36m(func pid=121943)[0m f1_per_class: [0.522, 0.426, 0.257, 0.522, 0.122, 0.353, 0.274, 0.349, 0.198, 0.297]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m top1: 0.333955223880597
[2m[36m(func pid=119092)[0m top5: 0.8540111940298507
[2m[36m(func pid=119092)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=119092)[0m f1_macro: 0.28907696890675716
[2m[36m(func pid=119092)[0m f1_weighted: 0.3666001584614826
[2m[36m(func pid=119092)[0m f1_per_class: [0.316, 0.344, 0.171, 0.394, 0.105, 0.429, 0.383, 0.3, 0.159, 0.29]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 5.7768 | Steps: 4 | Val loss: 58.5732 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=115877)[0m top1: 0.23414179104477612
[2m[36m(func pid=115877)[0m top5: 0.7061567164179104
[2m[36m(func pid=115877)[0m f1_micro: 0.23414179104477612
[2m[36m(func pid=115877)[0m f1_macro: 0.1762968104789647
[2m[36m(func pid=115877)[0m f1_weighted: 0.24811799713373478
[2m[36m(func pid=115877)[0m f1_per_class: [0.225, 0.289, 0.08, 0.365, 0.014, 0.324, 0.141, 0.174, 0.031, 0.12]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.0469 | Steps: 4 | Val loss: 1.8308 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 12:39:07 (running for 00:41:13.45)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  5.777 |      0.301 |                   91 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.724 |      0.176 |                   54 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.683 |      0.289 |                   45 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.91  |      0.332 |                   30 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.35261194029850745
[2m[36m(func pid=107845)[0m top5: 0.8106343283582089
[2m[36m(func pid=107845)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=107845)[0m f1_macro: 0.30079689851107216
[2m[36m(func pid=107845)[0m f1_weighted: 0.3766367077035456
[2m[36m(func pid=107845)[0m f1_per_class: [0.51, 0.308, 0.172, 0.518, 0.094, 0.265, 0.352, 0.366, 0.272, 0.152]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.6618 | Steps: 4 | Val loss: 1.8127 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.6386 | Steps: 4 | Val loss: 2.1899 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=121943)[0m top1: 0.39925373134328357
[2m[36m(func pid=121943)[0m top5: 0.8843283582089553
[2m[36m(func pid=121943)[0m f1_micro: 0.3992537313432836
[2m[36m(func pid=121943)[0m f1_macro: 0.36053767575413864
[2m[36m(func pid=121943)[0m f1_weighted: 0.4229795388365421
[2m[36m(func pid=121943)[0m f1_per_class: [0.568, 0.519, 0.217, 0.488, 0.15, 0.388, 0.365, 0.337, 0.188, 0.385]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m top1: 0.36007462686567165
[2m[36m(func pid=119092)[0m top5: 0.8684701492537313
[2m[36m(func pid=119092)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=119092)[0m f1_macro: 0.3044137835783861
[2m[36m(func pid=119092)[0m f1_weighted: 0.39084504645906987
[2m[36m(func pid=119092)[0m f1_per_class: [0.312, 0.395, 0.168, 0.37, 0.117, 0.43, 0.449, 0.336, 0.17, 0.298]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 4.2169 | Steps: 4 | Val loss: 58.8852 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=115877)[0m top1: 0.23833955223880596
[2m[36m(func pid=115877)[0m top5: 0.7103544776119403
[2m[36m(func pid=115877)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=115877)[0m f1_macro: 0.1809107429854098
[2m[36m(func pid=115877)[0m f1_weighted: 0.25156848024207884
[2m[36m(func pid=115877)[0m f1_per_class: [0.207, 0.294, 0.101, 0.351, 0.015, 0.362, 0.149, 0.17, 0.032, 0.128]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 0.9850 | Steps: 4 | Val loss: 1.9126 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 12:39:12 (running for 00:41:18.83)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.217 |      0.286 |                   92 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.639 |      0.181 |                   55 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.662 |      0.304 |                   46 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.047 |      0.361 |                   31 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.35401119402985076
[2m[36m(func pid=107845)[0m top5: 0.8097014925373134
[2m[36m(func pid=107845)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=107845)[0m f1_macro: 0.28617043454669605
[2m[36m(func pid=107845)[0m f1_weighted: 0.3775426409267917
[2m[36m(func pid=107845)[0m f1_per_class: [0.438, 0.286, 0.12, 0.529, 0.108, 0.27, 0.367, 0.351, 0.238, 0.155]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.5727 | Steps: 4 | Val loss: 1.7854 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.6939 | Steps: 4 | Val loss: 2.1773 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=121943)[0m top1: 0.37453358208955223
[2m[36m(func pid=121943)[0m top5: 0.8647388059701493
[2m[36m(func pid=121943)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=121943)[0m f1_macro: 0.3261290324901419
[2m[36m(func pid=121943)[0m f1_weighted: 0.3973450488577915
[2m[36m(func pid=121943)[0m f1_per_class: [0.372, 0.49, 0.198, 0.423, 0.187, 0.398, 0.372, 0.326, 0.178, 0.317]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m top1: 0.37546641791044777
[2m[36m(func pid=119092)[0m top5: 0.8736007462686567
[2m[36m(func pid=119092)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=119092)[0m f1_macro: 0.31375578918978925
[2m[36m(func pid=119092)[0m f1_weighted: 0.40373591498699746
[2m[36m(func pid=119092)[0m f1_per_class: [0.341, 0.421, 0.179, 0.376, 0.123, 0.434, 0.47, 0.321, 0.175, 0.298]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 13.8002 | Steps: 4 | Val loss: 60.3320 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=115877)[0m top1: 0.25046641791044777
[2m[36m(func pid=115877)[0m top5: 0.7378731343283582
[2m[36m(func pid=115877)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=115877)[0m f1_macro: 0.1870511925007494
[2m[36m(func pid=115877)[0m f1_weighted: 0.271530272355647
[2m[36m(func pid=115877)[0m f1_per_class: [0.18, 0.309, 0.098, 0.359, 0.039, 0.361, 0.202, 0.159, 0.05, 0.114]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 0.7853 | Steps: 4 | Val loss: 2.1576 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 12:39:18 (running for 00:41:24.15)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 13.8   |      0.256 |                   93 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.694 |      0.187 |                   56 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.573 |      0.314 |                   47 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.985 |      0.326 |                   32 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.324160447761194
[2m[36m(func pid=107845)[0m top5: 0.8120335820895522
[2m[36m(func pid=107845)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=107845)[0m f1_macro: 0.2556145922680408
[2m[36m(func pid=107845)[0m f1_weighted: 0.35236809238561917
[2m[36m(func pid=107845)[0m f1_per_class: [0.321, 0.297, 0.093, 0.495, 0.087, 0.208, 0.347, 0.333, 0.218, 0.158]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.4635 | Steps: 4 | Val loss: 1.7736 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.6792 | Steps: 4 | Val loss: 2.1829 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=121943)[0m top1: 0.3064365671641791
[2m[36m(func pid=121943)[0m top5: 0.8409514925373134
[2m[36m(func pid=121943)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=121943)[0m f1_macro: 0.2833580127335148
[2m[36m(func pid=121943)[0m f1_weighted: 0.3458712089216724
[2m[36m(func pid=121943)[0m f1_per_class: [0.134, 0.334, 0.207, 0.363, 0.232, 0.403, 0.356, 0.341, 0.193, 0.271]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m top1: 0.36847014925373134
[2m[36m(func pid=119092)[0m top5: 0.8782649253731343
[2m[36m(func pid=119092)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=119092)[0m f1_macro: 0.3178218741683919
[2m[36m(func pid=119092)[0m f1_weighted: 0.3952333231845519
[2m[36m(func pid=119092)[0m f1_per_class: [0.4, 0.416, 0.187, 0.364, 0.083, 0.412, 0.453, 0.346, 0.191, 0.326]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 4.2814 | Steps: 4 | Val loss: 63.1583 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=115877)[0m top1: 0.24207089552238806
[2m[36m(func pid=115877)[0m top5: 0.7220149253731343
[2m[36m(func pid=115877)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=115877)[0m f1_macro: 0.18327231981213782
[2m[36m(func pid=115877)[0m f1_weighted: 0.2570881932097626
[2m[36m(func pid=115877)[0m f1_per_class: [0.186, 0.285, 0.097, 0.351, 0.036, 0.366, 0.171, 0.173, 0.037, 0.131]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.8100 | Steps: 4 | Val loss: 2.2646 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 12:39:23 (running for 00:41:29.55)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.281 |      0.254 |                   94 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.679 |      0.183 |                   57 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.463 |      0.318 |                   48 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.785 |      0.283 |                   33 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.30550373134328357
[2m[36m(func pid=107845)[0m top5: 0.7924440298507462
[2m[36m(func pid=107845)[0m f1_micro: 0.30550373134328357
[2m[36m(func pid=107845)[0m f1_macro: 0.2537439009655521
[2m[36m(func pid=107845)[0m f1_weighted: 0.3344876746745057
[2m[36m(func pid=107845)[0m f1_per_class: [0.312, 0.36, 0.072, 0.413, 0.075, 0.205, 0.324, 0.332, 0.255, 0.189]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.5659 | Steps: 4 | Val loss: 1.7604 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.6732 | Steps: 4 | Val loss: 2.1805 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=121943)[0m top1: 0.28591417910447764
[2m[36m(func pid=121943)[0m top5: 0.8311567164179104
[2m[36m(func pid=121943)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=121943)[0m f1_macro: 0.26170757965405855
[2m[36m(func pid=121943)[0m f1_weighted: 0.32543093537013795
[2m[36m(func pid=121943)[0m f1_per_class: [0.133, 0.234, 0.192, 0.371, 0.143, 0.386, 0.343, 0.362, 0.206, 0.249]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m top1: 0.3787313432835821
[2m[36m(func pid=119092)[0m top5: 0.8754664179104478
[2m[36m(func pid=119092)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=119092)[0m f1_macro: 0.3261382434675632
[2m[36m(func pid=119092)[0m f1_weighted: 0.40036867425111305
[2m[36m(func pid=119092)[0m f1_per_class: [0.429, 0.455, 0.186, 0.354, 0.09, 0.432, 0.447, 0.34, 0.195, 0.333]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 9.4303 | Steps: 4 | Val loss: 65.9396 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=115877)[0m top1: 0.23694029850746268
[2m[36m(func pid=115877)[0m top5: 0.722481343283582
[2m[36m(func pid=115877)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=115877)[0m f1_macro: 0.17679281728667148
[2m[36m(func pid=115877)[0m f1_weighted: 0.2560330639027768
[2m[36m(func pid=115877)[0m f1_per_class: [0.188, 0.281, 0.098, 0.363, 0.022, 0.336, 0.172, 0.169, 0.032, 0.107]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 0.7173 | Steps: 4 | Val loss: 2.2251 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 12:39:28 (running for 00:41:34.85)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  9.43  |      0.263 |                   95 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.673 |      0.177 |                   58 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.566 |      0.326 |                   49 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.81  |      0.262 |                   34 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.314365671641791
[2m[36m(func pid=107845)[0m top5: 0.7873134328358209
[2m[36m(func pid=107845)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=107845)[0m f1_macro: 0.26284102503435414
[2m[36m(func pid=107845)[0m f1_weighted: 0.32884083140645165
[2m[36m(func pid=107845)[0m f1_per_class: [0.316, 0.433, 0.077, 0.362, 0.086, 0.181, 0.317, 0.321, 0.274, 0.262]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.5055 | Steps: 4 | Val loss: 1.7891 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.6333 | Steps: 4 | Val loss: 2.1831 | Batch size: 32 | lr: 0.0001 | Duration: 3.23s
[2m[36m(func pid=121943)[0m top1: 0.300839552238806
[2m[36m(func pid=121943)[0m top5: 0.8260261194029851
[2m[36m(func pid=121943)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=121943)[0m f1_macro: 0.26329597206794003
[2m[36m(func pid=121943)[0m f1_weighted: 0.3413099131661423
[2m[36m(func pid=121943)[0m f1_per_class: [0.207, 0.29, 0.179, 0.411, 0.074, 0.361, 0.34, 0.335, 0.18, 0.256]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m top1: 0.363339552238806
[2m[36m(func pid=119092)[0m top5: 0.8652052238805971
[2m[36m(func pid=119092)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=119092)[0m f1_macro: 0.31998911389629836
[2m[36m(func pid=119092)[0m f1_weighted: 0.3855524676676231
[2m[36m(func pid=119092)[0m f1_per_class: [0.414, 0.441, 0.189, 0.338, 0.081, 0.432, 0.419, 0.352, 0.192, 0.341]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 10.5111 | Steps: 4 | Val loss: 70.9714 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=115877)[0m top1: 0.24207089552238806
[2m[36m(func pid=115877)[0m top5: 0.7201492537313433
[2m[36m(func pid=115877)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=115877)[0m f1_macro: 0.18373643429362924
[2m[36m(func pid=115877)[0m f1_weighted: 0.2577970820434861
[2m[36m(func pid=115877)[0m f1_per_class: [0.176, 0.249, 0.105, 0.392, 0.032, 0.36, 0.154, 0.191, 0.044, 0.135]
[2m[36m(func pid=115877)[0m 
== Status ==
Current time: 2024-01-07 12:39:34 (running for 00:41:40.16)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 10.511 |      0.263 |                   96 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.633 |      0.184 |                   59 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.505 |      0.32  |                   50 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.717 |      0.263 |                   35 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.30130597014925375
[2m[36m(func pid=107845)[0m top5: 0.7677238805970149
[2m[36m(func pid=107845)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=107845)[0m f1_macro: 0.2631196528219526
[2m[36m(func pid=107845)[0m f1_weighted: 0.30054567213000355
[2m[36m(func pid=107845)[0m f1_per_class: [0.41, 0.433, 0.088, 0.27, 0.12, 0.176, 0.307, 0.306, 0.262, 0.26]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.4869 | Steps: 4 | Val loss: 1.7866 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 0.7295 | Steps: 4 | Val loss: 2.0259 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=119092)[0m top1: 0.355410447761194
[2m[36m(func pid=119092)[0m top5: 0.8638059701492538
[2m[36m(func pid=119092)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=119092)[0m f1_macro: 0.3163748434269972
[2m[36m(func pid=119092)[0m f1_weighted: 0.37589807216225196
[2m[36m(func pid=119092)[0m f1_per_class: [0.425, 0.436, 0.183, 0.359, 0.079, 0.415, 0.372, 0.373, 0.205, 0.318]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.6756 | Steps: 4 | Val loss: 2.1838 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=121943)[0m top1: 0.35774253731343286
[2m[36m(func pid=121943)[0m top5: 0.8535447761194029
[2m[36m(func pid=121943)[0m f1_micro: 0.35774253731343286
[2m[36m(func pid=121943)[0m f1_macro: 0.3024864464491365
[2m[36m(func pid=121943)[0m f1_weighted: 0.3970911394399263
[2m[36m(func pid=121943)[0m f1_per_class: [0.366, 0.329, 0.2, 0.485, 0.071, 0.349, 0.426, 0.336, 0.203, 0.26]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 15.6235 | Steps: 4 | Val loss: 73.8237 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=115877)[0m top1: 0.23787313432835822
[2m[36m(func pid=115877)[0m top5: 0.7206156716417911
[2m[36m(func pid=115877)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=115877)[0m f1_macro: 0.1829670083310529
[2m[36m(func pid=115877)[0m f1_weighted: 0.2539758704773057
[2m[36m(func pid=115877)[0m f1_per_class: [0.198, 0.256, 0.108, 0.384, 0.039, 0.353, 0.15, 0.177, 0.03, 0.134]
== Status ==
Current time: 2024-01-07 12:39:39 (running for 00:41:45.53)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 10.511 |      0.263 |                   96 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.676 |      0.183 |                   60 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.487 |      0.316 |                   51 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.73  |      0.302 |                   36 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.2957089552238806
[2m[36m(func pid=107845)[0m top5: 0.7369402985074627
[2m[36m(func pid=107845)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=107845)[0m f1_macro: 0.2597669904339147
[2m[36m(func pid=107845)[0m f1_weighted: 0.28225782010413936
[2m[36m(func pid=107845)[0m f1_per_class: [0.413, 0.43, 0.096, 0.192, 0.143, 0.168, 0.321, 0.315, 0.254, 0.267]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.5717 | Steps: 4 | Val loss: 1.7879 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 0.6348 | Steps: 4 | Val loss: 1.9057 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=119092)[0m top1: 0.3512126865671642
[2m[36m(func pid=119092)[0m top5: 0.8708022388059702
[2m[36m(func pid=119092)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=119092)[0m f1_macro: 0.31390531740217764
[2m[36m(func pid=119092)[0m f1_weighted: 0.37387226813765584
[2m[36m(func pid=119092)[0m f1_per_class: [0.42, 0.428, 0.173, 0.361, 0.075, 0.407, 0.371, 0.374, 0.205, 0.326]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 30.2373 | Steps: 4 | Val loss: 72.0697 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.6009 | Steps: 4 | Val loss: 2.1775 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=121943)[0m top1: 0.38899253731343286
[2m[36m(func pid=121943)[0m top5: 0.8773320895522388
[2m[36m(func pid=121943)[0m f1_micro: 0.38899253731343286
[2m[36m(func pid=121943)[0m f1_macro: 0.3227825464149737
[2m[36m(func pid=121943)[0m f1_weighted: 0.42387524020417666
[2m[36m(func pid=121943)[0m f1_per_class: [0.449, 0.394, 0.194, 0.496, 0.085, 0.372, 0.463, 0.277, 0.202, 0.296]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.5020 | Steps: 4 | Val loss: 1.7508 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 12:39:45 (running for 00:41:51.09)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  | 30.237 |      0.27  |                   98 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.676 |      0.183 |                   60 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.572 |      0.314 |                   52 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.635 |      0.323 |                   37 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.3283582089552239
[2m[36m(func pid=107845)[0m top5: 0.7518656716417911
[2m[36m(func pid=107845)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=107845)[0m f1_macro: 0.269640654803482
[2m[36m(func pid=107845)[0m f1_weighted: 0.305034625439594
[2m[36m(func pid=107845)[0m f1_per_class: [0.368, 0.429, 0.131, 0.147, 0.095, 0.194, 0.427, 0.34, 0.26, 0.307]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m top1: 0.24113805970149255
[2m[36m(func pid=115877)[0m top5: 0.7271455223880597
[2m[36m(func pid=115877)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=115877)[0m f1_macro: 0.1846310277889166
[2m[36m(func pid=115877)[0m f1_weighted: 0.2562166035985184
[2m[36m(func pid=115877)[0m f1_per_class: [0.197, 0.276, 0.132, 0.378, 0.029, 0.339, 0.155, 0.181, 0.04, 0.118]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 0.5590 | Steps: 4 | Val loss: 1.8786 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=119092)[0m top1: 0.37173507462686567
[2m[36m(func pid=119092)[0m top5: 0.8801305970149254
[2m[36m(func pid=119092)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=119092)[0m f1_macro: 0.32404837799603037
[2m[36m(func pid=119092)[0m f1_weighted: 0.3948567648175972
[2m[36m(func pid=119092)[0m f1_per_class: [0.432, 0.446, 0.179, 0.418, 0.084, 0.412, 0.375, 0.362, 0.224, 0.308]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 3.7661 | Steps: 4 | Val loss: 72.2014 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.6658 | Steps: 4 | Val loss: 2.1774 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=121943)[0m top1: 0.3843283582089552
[2m[36m(func pid=121943)[0m top5: 0.8861940298507462
[2m[36m(func pid=121943)[0m f1_micro: 0.3843283582089552
[2m[36m(func pid=121943)[0m f1_macro: 0.3244917940742057
[2m[36m(func pid=121943)[0m f1_weighted: 0.4173631597696592
[2m[36m(func pid=121943)[0m f1_per_class: [0.515, 0.384, 0.187, 0.491, 0.106, 0.377, 0.445, 0.28, 0.201, 0.257]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.6194 | Steps: 4 | Val loss: 1.7411 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 12:39:50 (running for 00:41:56.49)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00019 | RUNNING    | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  3.766 |      0.258 |                   99 |
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.601 |      0.185 |                   61 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.502 |      0.324 |                   53 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.559 |      0.324 |                   38 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.33115671641791045
[2m[36m(func pid=107845)[0m top5: 0.7430037313432836
[2m[36m(func pid=107845)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=107845)[0m f1_macro: 0.25839081200805936
[2m[36m(func pid=107845)[0m f1_weighted: 0.307004937650112
[2m[36m(func pid=107845)[0m f1_per_class: [0.303, 0.423, 0.167, 0.126, 0.053, 0.215, 0.459, 0.313, 0.275, 0.251]
[2m[36m(func pid=107845)[0m 
[2m[36m(func pid=115877)[0m top1: 0.24440298507462688
[2m[36m(func pid=115877)[0m top5: 0.7322761194029851
[2m[36m(func pid=115877)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=115877)[0m f1_macro: 0.19026691562272408
[2m[36m(func pid=115877)[0m f1_weighted: 0.2578237556972829
[2m[36m(func pid=115877)[0m f1_per_class: [0.2, 0.269, 0.136, 0.362, 0.03, 0.381, 0.165, 0.159, 0.058, 0.143]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 0.8893 | Steps: 4 | Val loss: 1.8612 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=119092)[0m top1: 0.3736007462686567
[2m[36m(func pid=119092)[0m top5: 0.8824626865671642
[2m[36m(func pid=119092)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=119092)[0m f1_macro: 0.321418194664349
[2m[36m(func pid=119092)[0m f1_weighted: 0.40209765352637256
[2m[36m(func pid=119092)[0m f1_per_class: [0.431, 0.419, 0.17, 0.432, 0.078, 0.415, 0.403, 0.345, 0.255, 0.266]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=107845)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 4.0565 | Steps: 4 | Val loss: 73.7964 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.6411 | Steps: 4 | Val loss: 2.1764 | Batch size: 32 | lr: 0.0001 | Duration: 3.20s
[2m[36m(func pid=121943)[0m top1: 0.3908582089552239
[2m[36m(func pid=121943)[0m top5: 0.8908582089552238
[2m[36m(func pid=121943)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=121943)[0m f1_macro: 0.3349949905379467
[2m[36m(func pid=121943)[0m f1_weighted: 0.42345071429577924
[2m[36m(func pid=121943)[0m f1_per_class: [0.552, 0.403, 0.172, 0.504, 0.114, 0.368, 0.431, 0.336, 0.217, 0.253]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.5384 | Steps: 4 | Val loss: 1.7402 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 12:39:55 (running for 00:42:01.80)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 PENDING, 3 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.666 |      0.19  |                   62 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.619 |      0.321 |                   54 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.889 |      0.335 |                   39 |
| train_9b9e8_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=107845)[0m top1: 0.3306902985074627
[2m[36m(func pid=107845)[0m top5: 0.7332089552238806
[2m[36m(func pid=107845)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=107845)[0m f1_macro: 0.2534698750214607
[2m[36m(func pid=107845)[0m f1_weighted: 0.3085365131741473
[2m[36m(func pid=107845)[0m f1_per_class: [0.231, 0.338, 0.216, 0.134, 0.091, 0.263, 0.497, 0.299, 0.25, 0.216]
[2m[36m(func pid=115877)[0m top1: 0.24347014925373134
[2m[36m(func pid=115877)[0m top5: 0.7402052238805971
[2m[36m(func pid=115877)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=115877)[0m f1_macro: 0.1896386212038797
[2m[36m(func pid=115877)[0m f1_weighted: 0.25891537670834736
[2m[36m(func pid=115877)[0m f1_per_class: [0.19, 0.277, 0.108, 0.359, 0.05, 0.371, 0.17, 0.166, 0.052, 0.153]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 0.9227 | Steps: 4 | Val loss: 1.8686 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=119092)[0m top1: 0.3773320895522388
[2m[36m(func pid=119092)[0m top5: 0.8801305970149254
[2m[36m(func pid=119092)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=119092)[0m f1_macro: 0.32452011394507496
[2m[36m(func pid=119092)[0m f1_weighted: 0.4082527198199224
[2m[36m(func pid=119092)[0m f1_per_class: [0.454, 0.396, 0.209, 0.448, 0.084, 0.424, 0.419, 0.357, 0.216, 0.238]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6298 | Steps: 4 | Val loss: 2.1639 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=121943)[0m top1: 0.3726679104477612
[2m[36m(func pid=121943)[0m top5: 0.8894589552238806
[2m[36m(func pid=121943)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=121943)[0m f1_macro: 0.3255320496563415
[2m[36m(func pid=121943)[0m f1_weighted: 0.40697280241296047
[2m[36m(func pid=121943)[0m f1_per_class: [0.578, 0.354, 0.181, 0.488, 0.116, 0.356, 0.429, 0.315, 0.201, 0.238]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.5816 | Steps: 4 | Val loss: 1.7470 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=115877)[0m top1: 0.2560634328358209
[2m[36m(func pid=115877)[0m top5: 0.7448694029850746
[2m[36m(func pid=115877)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=115877)[0m f1_macro: 0.19396858077868664
[2m[36m(func pid=115877)[0m f1_weighted: 0.2713824726871911
[2m[36m(func pid=115877)[0m f1_per_class: [0.205, 0.24, 0.144, 0.386, 0.031, 0.377, 0.206, 0.157, 0.06, 0.132]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=119092)[0m top1: 0.37173507462686567
[2m[36m(func pid=119092)[0m top5: 0.8731343283582089
[2m[36m(func pid=119092)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=119092)[0m f1_macro: 0.31297381056756973
[2m[36m(func pid=119092)[0m f1_weighted: 0.40506552351868885
[2m[36m(func pid=119092)[0m f1_per_class: [0.432, 0.359, 0.178, 0.444, 0.085, 0.409, 0.444, 0.347, 0.219, 0.213]
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 0.5598 | Steps: 4 | Val loss: 1.9340 | Batch size: 32 | lr: 0.01 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 12:40:02 (running for 00:42:08.38)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.63  |      0.194 |                   64 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.538 |      0.325 |                   55 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.923 |      0.326 |                   40 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=132175)[0m Configuration completed!
[2m[36m(func pid=132175)[0m New optimizer parameters:
[2m[36m(func pid=132175)[0m SGD (
[2m[36m(func pid=132175)[0m Parameter Group 0
[2m[36m(func pid=132175)[0m     dampening: 0
[2m[36m(func pid=132175)[0m     differentiable: False
[2m[36m(func pid=132175)[0m     foreach: None
[2m[36m(func pid=132175)[0m     lr: 0.1
[2m[36m(func pid=132175)[0m     maximize: False
[2m[36m(func pid=132175)[0m     momentum: 0.9
[2m[36m(func pid=132175)[0m     nesterov: False
[2m[36m(func pid=132175)[0m     weight_decay: 1e-05
[2m[36m(func pid=132175)[0m )
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6065 | Steps: 4 | Val loss: 2.1642 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=121943)[0m top1: 0.3558768656716418
[2m[36m(func pid=121943)[0m top5: 0.8763992537313433
[2m[36m(func pid=121943)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=121943)[0m f1_macro: 0.3076214080967834
[2m[36m(func pid=121943)[0m f1_weighted: 0.3870865480795241
[2m[36m(func pid=121943)[0m f1_per_class: [0.476, 0.324, 0.21, 0.479, 0.124, 0.323, 0.41, 0.3, 0.19, 0.24]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.5217 | Steps: 4 | Val loss: 1.7432 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 12:40:08 (running for 00:42:14.00)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.606 |      0.197 |                   65 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.582 |      0.313 |                   56 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.56  |      0.308 |                   41 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=115877)[0m top1: 0.25886194029850745
[2m[36m(func pid=115877)[0m top5: 0.7425373134328358
[2m[36m(func pid=115877)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=115877)[0m f1_macro: 0.19720389513998765
[2m[36m(func pid=115877)[0m f1_weighted: 0.2683447309697586
[2m[36m(func pid=115877)[0m f1_per_class: [0.216, 0.254, 0.154, 0.395, 0.034, 0.386, 0.172, 0.169, 0.058, 0.133]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.5984 | Steps: 4 | Val loss: 1.9609 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0111 | Steps: 4 | Val loss: 2.8383 | Batch size: 32 | lr: 0.1 | Duration: 4.93s
[2m[36m(func pid=119092)[0m top1: 0.3726679104477612
[2m[36m(func pid=119092)[0m top5: 0.8703358208955224
[2m[36m(func pid=119092)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=119092)[0m f1_macro: 0.3052942308105304
[2m[36m(func pid=119092)[0m f1_weighted: 0.40338277822007046
[2m[36m(func pid=119092)[0m f1_per_class: [0.387, 0.339, 0.186, 0.434, 0.093, 0.394, 0.475, 0.304, 0.223, 0.218]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.6535 | Steps: 4 | Val loss: 2.1733 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=121943)[0m top1: 0.3628731343283582
[2m[36m(func pid=121943)[0m top5: 0.8605410447761194
[2m[36m(func pid=121943)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=121943)[0m f1_macro: 0.3107083966254822
[2m[36m(func pid=121943)[0m f1_weighted: 0.3887488426974016
[2m[36m(func pid=121943)[0m f1_per_class: [0.34, 0.442, 0.214, 0.417, 0.138, 0.344, 0.403, 0.304, 0.193, 0.312]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=132175)[0m top1: 0.16324626865671643
[2m[36m(func pid=132175)[0m top5: 0.511660447761194
[2m[36m(func pid=132175)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=132175)[0m f1_macro: 0.13415082956622273
[2m[36m(func pid=132175)[0m f1_weighted: 0.06402050297289129
[2m[36m(func pid=132175)[0m f1_per_class: [0.126, 0.32, 0.846, 0.0, 0.033, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.5689 | Steps: 4 | Val loss: 1.7360 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 12:40:13 (running for 00:42:19.79)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.653 |      0.192 |                   66 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.522 |      0.305 |                   57 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.598 |      0.311 |                   42 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  3.011 |      0.134 |                    1 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=115877)[0m top1: 0.24860074626865672
[2m[36m(func pid=115877)[0m top5: 0.7336753731343284
[2m[36m(func pid=115877)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=115877)[0m f1_macro: 0.19243291523247702
[2m[36m(func pid=115877)[0m f1_weighted: 0.26052325801811405
[2m[36m(func pid=115877)[0m f1_per_class: [0.208, 0.263, 0.122, 0.381, 0.03, 0.357, 0.161, 0.194, 0.056, 0.153]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.6150 | Steps: 4 | Val loss: 2.0255 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=119092)[0m top1: 0.38152985074626866
[2m[36m(func pid=119092)[0m top5: 0.867070895522388
[2m[36m(func pid=119092)[0m f1_micro: 0.3815298507462687
[2m[36m(func pid=119092)[0m f1_macro: 0.3083075180771049
[2m[36m(func pid=119092)[0m f1_weighted: 0.4120733359791565
[2m[36m(func pid=119092)[0m f1_per_class: [0.393, 0.326, 0.205, 0.47, 0.099, 0.381, 0.483, 0.313, 0.198, 0.215]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.6012 | Steps: 4 | Val loss: 2.4909 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.6031 | Steps: 4 | Val loss: 2.1744 | Batch size: 32 | lr: 0.0001 | Duration: 3.12s
[2m[36m(func pid=121943)[0m top1: 0.3712686567164179
[2m[36m(func pid=121943)[0m top5: 0.8460820895522388
[2m[36m(func pid=121943)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=121943)[0m f1_macro: 0.32599505995177686
[2m[36m(func pid=121943)[0m f1_weighted: 0.3770659229388831
[2m[36m(func pid=121943)[0m f1_per_class: [0.257, 0.508, 0.224, 0.296, 0.169, 0.389, 0.411, 0.347, 0.211, 0.447]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=132175)[0m top1: 0.31902985074626866
[2m[36m(func pid=132175)[0m top5: 0.7588619402985075
[2m[36m(func pid=132175)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=132175)[0m f1_macro: 0.2568962111660039
[2m[36m(func pid=132175)[0m f1_weighted: 0.32645746766899525
[2m[36m(func pid=132175)[0m f1_per_class: [0.15, 0.0, 0.4, 0.391, 0.081, 0.308, 0.503, 0.346, 0.076, 0.314]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.4144 | Steps: 4 | Val loss: 1.7421 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 12:40:19 (running for 00:42:25.52)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.603 |      0.194 |                   67 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.569 |      0.308 |                   58 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.615 |      0.326 |                   43 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.601 |      0.257 |                    2 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=115877)[0m top1: 0.25093283582089554
[2m[36m(func pid=115877)[0m top5: 0.7234141791044776
[2m[36m(func pid=115877)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=115877)[0m f1_macro: 0.194153425392463
[2m[36m(func pid=115877)[0m f1_weighted: 0.26063840845694375
[2m[36m(func pid=115877)[0m f1_per_class: [0.215, 0.249, 0.12, 0.395, 0.037, 0.357, 0.156, 0.197, 0.042, 0.173]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=119092)[0m top1: 0.37966417910447764
[2m[36m(func pid=119092)[0m top5: 0.8628731343283582
[2m[36m(func pid=119092)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=119092)[0m f1_macro: 0.30909627101259896
[2m[36m(func pid=119092)[0m f1_weighted: 0.4108820488458485
[2m[36m(func pid=119092)[0m f1_per_class: [0.393, 0.323, 0.193, 0.493, 0.095, 0.37, 0.456, 0.347, 0.207, 0.214]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 0.5460 | Steps: 4 | Val loss: 2.1633 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.5949 | Steps: 4 | Val loss: 2.9977 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.5931 | Steps: 4 | Val loss: 2.1780 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=121943)[0m top1: 0.365205223880597
[2m[36m(func pid=121943)[0m top5: 0.835820895522388
[2m[36m(func pid=121943)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=121943)[0m f1_macro: 0.3261751558411418
[2m[36m(func pid=121943)[0m f1_weighted: 0.3559042730742117
[2m[36m(func pid=121943)[0m f1_per_class: [0.2, 0.501, 0.229, 0.197, 0.187, 0.405, 0.427, 0.345, 0.247, 0.523]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.5144 | Steps: 4 | Val loss: 1.7407 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=132175)[0m top1: 0.2933768656716418
[2m[36m(func pid=132175)[0m top5: 0.7574626865671642
[2m[36m(func pid=132175)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=132175)[0m f1_macro: 0.28041005127430785
[2m[36m(func pid=132175)[0m f1_weighted: 0.27911633747788234
[2m[36m(func pid=132175)[0m f1_per_class: [0.483, 0.005, 0.429, 0.514, 0.143, 0.385, 0.169, 0.34, 0.15, 0.186]
[2m[36m(func pid=132175)[0m 
== Status ==
Current time: 2024-01-07 12:40:25 (running for 00:42:31.22)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.593 |      0.192 |                   68 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.414 |      0.309 |                   59 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.546 |      0.326 |                   44 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.595 |      0.28  |                    3 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=115877)[0m top1: 0.2462686567164179
[2m[36m(func pid=115877)[0m top5: 0.7164179104477612
[2m[36m(func pid=115877)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=115877)[0m f1_macro: 0.19215354896662812
[2m[36m(func pid=115877)[0m f1_weighted: 0.25761142753573546
[2m[36m(func pid=115877)[0m f1_per_class: [0.201, 0.253, 0.109, 0.386, 0.03, 0.353, 0.152, 0.213, 0.039, 0.186]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=119092)[0m top1: 0.37779850746268656
[2m[36m(func pid=119092)[0m top5: 0.8740671641791045
[2m[36m(func pid=119092)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=119092)[0m f1_macro: 0.30397966646518504
[2m[36m(func pid=119092)[0m f1_weighted: 0.40559799268638863
[2m[36m(func pid=119092)[0m f1_per_class: [0.38, 0.289, 0.169, 0.51, 0.103, 0.373, 0.444, 0.34, 0.201, 0.23]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.6921 | Steps: 4 | Val loss: 2.0566 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 1.8481 | Steps: 4 | Val loss: 3.2011 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.6252 | Steps: 4 | Val loss: 2.1707 | Batch size: 32 | lr: 0.0001 | Duration: 3.17s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.6365 | Steps: 4 | Val loss: 1.7341 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=121943)[0m top1: 0.3787313432835821
[2m[36m(func pid=121943)[0m top5: 0.8512126865671642
[2m[36m(func pid=121943)[0m f1_micro: 0.3787313432835821
[2m[36m(func pid=121943)[0m f1_macro: 0.32317252757486087
[2m[36m(func pid=121943)[0m f1_weighted: 0.3929759527365237
[2m[36m(func pid=121943)[0m f1_per_class: [0.218, 0.521, 0.198, 0.335, 0.185, 0.383, 0.434, 0.3, 0.215, 0.442]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=132175)[0m top1: 0.29244402985074625
[2m[36m(func pid=132175)[0m top5: 0.7989738805970149
[2m[36m(func pid=132175)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=132175)[0m f1_macro: 0.2614396999536333
[2m[36m(func pid=132175)[0m f1_weighted: 0.264974266618019
[2m[36m(func pid=132175)[0m f1_per_class: [0.282, 0.455, 0.216, 0.023, 0.062, 0.129, 0.42, 0.382, 0.154, 0.492]
[2m[36m(func pid=132175)[0m 
== Status ==
Current time: 2024-01-07 12:40:30 (running for 00:42:36.72)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.593 |      0.192 |                   68 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.636 |      0.304 |                   61 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.692 |      0.323 |                   45 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.848 |      0.261 |                    4 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3773320895522388
[2m[36m(func pid=119092)[0m top5: 0.8833955223880597
[2m[36m(func pid=119092)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=119092)[0m f1_macro: 0.304318740958303
[2m[36m(func pid=119092)[0m f1_weighted: 0.4020426607521544
[2m[36m(func pid=119092)[0m f1_per_class: [0.412, 0.279, 0.158, 0.524, 0.097, 0.394, 0.419, 0.32, 0.194, 0.246]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m top1: 0.24580223880597016
[2m[36m(func pid=115877)[0m top5: 0.7285447761194029
[2m[36m(func pid=115877)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=115877)[0m f1_macro: 0.191395430222198
[2m[36m(func pid=115877)[0m f1_weighted: 0.2581535471547144
[2m[36m(func pid=115877)[0m f1_per_class: [0.185, 0.268, 0.134, 0.374, 0.03, 0.377, 0.152, 0.187, 0.054, 0.154]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.2440 | Steps: 4 | Val loss: 3.1306 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 0.6024 | Steps: 4 | Val loss: 2.0460 | Batch size: 32 | lr: 0.01 | Duration: 3.16s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.3497 | Steps: 4 | Val loss: 1.7052 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.5910 | Steps: 4 | Val loss: 2.1688 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=132175)[0m top1: 0.36427238805970147
[2m[36m(func pid=132175)[0m top5: 0.8791977611940298
[2m[36m(func pid=132175)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=132175)[0m f1_macro: 0.3096365552769102
[2m[36m(func pid=132175)[0m f1_weighted: 0.3631554466333797
[2m[36m(func pid=132175)[0m f1_per_class: [0.443, 0.181, 0.25, 0.572, 0.055, 0.317, 0.315, 0.351, 0.174, 0.438]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.373134328358209
[2m[36m(func pid=121943)[0m top5: 0.8488805970149254
[2m[36m(func pid=121943)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=121943)[0m f1_macro: 0.3070735104068804
[2m[36m(func pid=121943)[0m f1_weighted: 0.40304093421828474
[2m[36m(func pid=121943)[0m f1_per_class: [0.325, 0.369, 0.177, 0.516, 0.141, 0.383, 0.389, 0.304, 0.201, 0.267]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m top1: 0.39225746268656714
[2m[36m(func pid=119092)[0m top5: 0.8875932835820896
[2m[36m(func pid=119092)[0m f1_micro: 0.39225746268656714
[2m[36m(func pid=119092)[0m f1_macro: 0.31597784956003755
[2m[36m(func pid=119092)[0m f1_weighted: 0.41118636201066533
[2m[36m(func pid=119092)[0m f1_per_class: [0.437, 0.282, 0.185, 0.541, 0.112, 0.404, 0.425, 0.315, 0.213, 0.247]
[2m[36m(func pid=119092)[0m 
== Status ==
Current time: 2024-01-07 12:40:35 (running for 00:42:41.91)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.625 |      0.191 |                   69 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.35  |      0.316 |                   62 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.602 |      0.307 |                   46 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.244 |      0.31  |                    5 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=115877)[0m top1: 0.24953358208955223
[2m[36m(func pid=115877)[0m top5: 0.7318097014925373
[2m[36m(func pid=115877)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=115877)[0m f1_macro: 0.1944541137581243
[2m[36m(func pid=115877)[0m f1_weighted: 0.2604655149736495
[2m[36m(func pid=115877)[0m f1_per_class: [0.193, 0.252, 0.132, 0.388, 0.041, 0.372, 0.156, 0.188, 0.064, 0.159]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.3405 | Steps: 4 | Val loss: 5.4704 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.5568 | Steps: 4 | Val loss: 2.2125 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.5104 | Steps: 4 | Val loss: 1.6703 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.5809 | Steps: 4 | Val loss: 2.1615 | Batch size: 32 | lr: 0.0001 | Duration: 3.14s
[2m[36m(func pid=121943)[0m top1: 0.33908582089552236
[2m[36m(func pid=121943)[0m top5: 0.8283582089552238
[2m[36m(func pid=121943)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=121943)[0m f1_macro: 0.2896294475080369
[2m[36m(func pid=121943)[0m f1_weighted: 0.35977360652583285
[2m[36m(func pid=121943)[0m f1_per_class: [0.444, 0.254, 0.161, 0.526, 0.107, 0.364, 0.295, 0.334, 0.217, 0.192]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=132175)[0m top1: 0.21082089552238806
[2m[36m(func pid=132175)[0m top5: 0.707089552238806
[2m[36m(func pid=132175)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=132175)[0m f1_macro: 0.1981692088363021
[2m[36m(func pid=132175)[0m f1_weighted: 0.2057529637562033
[2m[36m(func pid=132175)[0m f1_per_class: [0.244, 0.005, 0.12, 0.406, 0.108, 0.371, 0.057, 0.345, 0.096, 0.23]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m top1: 0.3987873134328358== Status ==
Current time: 2024-01-07 12:40:41 (running for 00:42:47.23)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.591 |      0.194 |                   70 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.51  |      0.323 |                   63 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.557 |      0.29  |                   47 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.341 |      0.198 |                    6 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)



[2m[36m(func pid=119092)[0m top5: 0.8955223880597015
[2m[36m(func pid=119092)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=119092)[0m f1_macro: 0.322938058510447
[2m[36m(func pid=119092)[0m f1_weighted: 0.4178133871128858
[2m[36m(func pid=119092)[0m f1_per_class: [0.441, 0.288, 0.203, 0.535, 0.111, 0.411, 0.442, 0.336, 0.198, 0.263]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m top1: 0.26399253731343286
[2m[36m(func pid=115877)[0m top5: 0.7416044776119403
[2m[36m(func pid=115877)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=115877)[0m f1_macro: 0.20394709642199138
[2m[36m(func pid=115877)[0m f1_weighted: 0.27633164206468214
[2m[36m(func pid=115877)[0m f1_per_class: [0.186, 0.277, 0.145, 0.405, 0.04, 0.41, 0.166, 0.176, 0.072, 0.163]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.1795 | Steps: 4 | Val loss: 2.2969 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 1.8927 | Steps: 4 | Val loss: 6.4983 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.4857 | Steps: 4 | Val loss: 1.6795 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.6142 | Steps: 4 | Val loss: 2.1705 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=121943)[0m top1: 0.32322761194029853
[2m[36m(func pid=121943)[0m top5: 0.8190298507462687
[2m[36m(func pid=121943)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=121943)[0m f1_macro: 0.2778904862188861
[2m[36m(func pid=121943)[0m f1_weighted: 0.3379893642928658
[2m[36m(func pid=121943)[0m f1_per_class: [0.435, 0.201, 0.183, 0.533, 0.117, 0.382, 0.247, 0.307, 0.21, 0.164]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=132175)[0m top1: 0.2178171641791045
[2m[36m(func pid=132175)[0m top5: 0.6124067164179104
[2m[36m(func pid=132175)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=132175)[0m f1_macro: 0.16676371156847308
[2m[36m(func pid=132175)[0m f1_weighted: 0.21158200276659025
[2m[36m(func pid=132175)[0m f1_per_class: [0.171, 0.104, 0.161, 0.0, 0.087, 0.197, 0.494, 0.237, 0.119, 0.097]
[2m[36m(func pid=132175)[0m 
== Status ==
Current time: 2024-01-07 12:40:46 (running for 00:42:52.61)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.581 |      0.204 |                   71 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.486 |      0.319 |                   64 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  1.18  |      0.278 |                   48 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.893 |      0.167 |                    7 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.394589552238806
[2m[36m(func pid=119092)[0m top5: 0.8931902985074627
[2m[36m(func pid=119092)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=119092)[0m f1_macro: 0.31896527887317105
[2m[36m(func pid=119092)[0m f1_weighted: 0.4159033056845374
[2m[36m(func pid=119092)[0m f1_per_class: [0.444, 0.312, 0.194, 0.527, 0.097, 0.421, 0.434, 0.293, 0.206, 0.262]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m top1: 0.24953358208955223
[2m[36m(func pid=115877)[0m top5: 0.7341417910447762
[2m[36m(func pid=115877)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=115877)[0m f1_macro: 0.19748358912991096
[2m[36m(func pid=115877)[0m f1_weighted: 0.26421062636480347
[2m[36m(func pid=115877)[0m f1_per_class: [0.176, 0.257, 0.173, 0.38, 0.035, 0.388, 0.17, 0.182, 0.056, 0.157]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9831 | Steps: 4 | Val loss: 4.6378 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 0.5829 | Steps: 4 | Val loss: 2.2784 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.5048 | Steps: 4 | Val loss: 1.6945 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.5989 | Steps: 4 | Val loss: 2.1625 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=132175)[0m top1: 0.31576492537313433
[2m[36m(func pid=132175)[0m top5: 0.784981343283582
[2m[36m(func pid=132175)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=132175)[0m f1_macro: 0.29606213955775273
[2m[36m(func pid=132175)[0m f1_weighted: 0.30456985651851015
[2m[36m(func pid=132175)[0m f1_per_class: [0.377, 0.498, 0.407, 0.126, 0.128, 0.177, 0.435, 0.209, 0.169, 0.436]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.322294776119403
[2m[36m(func pid=121943)[0m top5: 0.8316231343283582
[2m[36m(func pid=121943)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=121943)[0m f1_macro: 0.29551629315740524
[2m[36m(func pid=121943)[0m f1_weighted: 0.33193191398854555
[2m[36m(func pid=121943)[0m f1_per_class: [0.481, 0.366, 0.2, 0.459, 0.128, 0.383, 0.19, 0.321, 0.233, 0.194]
[2m[36m(func pid=121943)[0m 
== Status ==
Current time: 2024-01-07 12:40:51 (running for 00:42:57.75)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.614 |      0.197 |                   72 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.505 |      0.309 |                   65 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.583 |      0.296 |                   49 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.983 |      0.296 |                    8 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.38526119402985076
[2m[36m(func pid=119092)[0m top5: 0.8936567164179104
[2m[36m(func pid=119092)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=119092)[0m f1_macro: 0.3093095563880706
[2m[36m(func pid=119092)[0m f1_weighted: 0.4067567861035459
[2m[36m(func pid=119092)[0m f1_per_class: [0.423, 0.295, 0.181, 0.512, 0.091, 0.416, 0.437, 0.253, 0.221, 0.265]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m top1: 0.25699626865671643
[2m[36m(func pid=115877)[0m top5: 0.7360074626865671
[2m[36m(func pid=115877)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=115877)[0m f1_macro: 0.20195487813695506
[2m[36m(func pid=115877)[0m f1_weighted: 0.26627205037619656
[2m[36m(func pid=115877)[0m f1_per_class: [0.183, 0.227, 0.175, 0.398, 0.035, 0.414, 0.164, 0.187, 0.058, 0.179]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 1.6477 | Steps: 4 | Val loss: 3.0355 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 0.4977 | Steps: 4 | Val loss: 2.2263 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.4083 | Steps: 4 | Val loss: 1.6514 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.5650 | Steps: 4 | Val loss: 2.1598 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 12:40:57 (running for 00:43:02.99)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.599 |      0.202 |                   73 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.408 |      0.326 |                   66 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.583 |      0.296 |                   49 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.983 |      0.296 |                    8 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.40671641791044777
[2m[36m(func pid=119092)[0m top5: 0.9039179104477612
[2m[36m(func pid=119092)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=119092)[0m f1_macro: 0.3258657989028435
[2m[36m(func pid=119092)[0m f1_weighted: 0.42829638714779433
[2m[36m(func pid=119092)[0m f1_per_class: [0.488, 0.353, 0.218, 0.513, 0.09, 0.412, 0.478, 0.214, 0.225, 0.267]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m top1: 0.42024253731343286
[2m[36m(func pid=132175)[0m top5: 0.9160447761194029
[2m[36m(func pid=132175)[0m f1_micro: 0.42024253731343286
[2m[36m(func pid=132175)[0m f1_macro: 0.42827365818589913
[2m[36m(func pid=132175)[0m f1_weighted: 0.443640097420485
[2m[36m(func pid=132175)[0m f1_per_class: [0.479, 0.492, 0.815, 0.522, 0.104, 0.321, 0.432, 0.305, 0.247, 0.566]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.32649253731343286
[2m[36m(func pid=121943)[0m top5: 0.8376865671641791
[2m[36m(func pid=121943)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=121943)[0m f1_macro: 0.29896295500698844
[2m[36m(func pid=121943)[0m f1_weighted: 0.3426798749528518
[2m[36m(func pid=121943)[0m f1_per_class: [0.481, 0.388, 0.198, 0.407, 0.133, 0.382, 0.266, 0.299, 0.233, 0.202]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m top1: 0.2537313432835821
[2m[36m(func pid=115877)[0m top5: 0.7341417910447762
[2m[36m(func pid=115877)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=115877)[0m f1_macro: 0.20518135782301705
[2m[36m(func pid=115877)[0m f1_weighted: 0.26232230483676955
[2m[36m(func pid=115877)[0m f1_per_class: [0.223, 0.231, 0.15, 0.397, 0.031, 0.393, 0.152, 0.195, 0.062, 0.218]
[2m[36m(func pid=115877)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.4218 | Steps: 4 | Val loss: 1.6413 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.1063 | Steps: 4 | Val loss: 4.4030 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 0.6932 | Steps: 4 | Val loss: 2.1299 | Batch size: 32 | lr: 0.01 | Duration: 3.29s
== Status ==
Current time: 2024-01-07 12:41:02 (running for 00:43:08.20)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00020 | RUNNING    | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.565 |      0.205 |                   74 |
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.422 |      0.317 |                   67 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.498 |      0.299 |                   50 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.648 |      0.428 |                    9 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.40625
[2m[36m(func pid=119092)[0m top5: 0.9029850746268657
[2m[36m(func pid=119092)[0m f1_micro: 0.40625
[2m[36m(func pid=119092)[0m f1_macro: 0.3168724419118442
[2m[36m(func pid=119092)[0m f1_weighted: 0.42614353323098103
[2m[36m(func pid=119092)[0m f1_per_class: [0.441, 0.337, 0.241, 0.502, 0.099, 0.417, 0.5, 0.2, 0.173, 0.259]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=115877)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.5958 | Steps: 4 | Val loss: 2.1617 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=132175)[0m top1: 0.3558768656716418
[2m[36m(func pid=132175)[0m top5: 0.8736007462686567
[2m[36m(func pid=132175)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=132175)[0m f1_macro: 0.36264555464382564
[2m[36m(func pid=132175)[0m f1_weighted: 0.33580869742832703
[2m[36m(func pid=132175)[0m f1_per_class: [0.459, 0.291, 0.774, 0.57, 0.116, 0.322, 0.146, 0.347, 0.206, 0.395]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.35261194029850745
[2m[36m(func pid=121943)[0m top5: 0.8535447761194029
[2m[36m(func pid=121943)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=121943)[0m f1_macro: 0.3184005232770548
[2m[36m(func pid=121943)[0m f1_weighted: 0.37755639975848504
[2m[36m(func pid=121943)[0m f1_per_class: [0.532, 0.379, 0.243, 0.462, 0.129, 0.4, 0.325, 0.306, 0.239, 0.169]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=115877)[0m top1: 0.2579291044776119
[2m[36m(func pid=115877)[0m top5: 0.726679104477612
[2m[36m(func pid=115877)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=115877)[0m f1_macro: 0.2083650223217582
[2m[36m(func pid=115877)[0m f1_weighted: 0.2665593508818075
[2m[36m(func pid=115877)[0m f1_per_class: [0.219, 0.28, 0.15, 0.402, 0.043, 0.367, 0.143, 0.19, 0.076, 0.215]
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.4895 | Steps: 4 | Val loss: 1.6780 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 1.7052 | Steps: 4 | Val loss: 5.1591 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.7964 | Steps: 4 | Val loss: 1.9873 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 12:41:07 (running for 00:43:13.46)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.49  |      0.309 |                   68 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.693 |      0.318 |                   51 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.106 |      0.363 |                   10 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.39738805970149255
[2m[36m(func pid=119092)[0m top5: 0.8997201492537313
[2m[36m(func pid=119092)[0m f1_micro: 0.39738805970149255
[2m[36m(func pid=119092)[0m f1_macro: 0.309141123168778
[2m[36m(func pid=119092)[0m f1_weighted: 0.42256223164870416
[2m[36m(func pid=119092)[0m f1_per_class: [0.389, 0.337, 0.209, 0.49, 0.088, 0.41, 0.5, 0.222, 0.19, 0.257]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m top1: 0.33861940298507465
[2m[36m(func pid=132175)[0m top5: 0.7957089552238806
[2m[36m(func pid=132175)[0m f1_micro: 0.33861940298507465
[2m[36m(func pid=132175)[0m f1_macro: 0.29618551835884166
[2m[36m(func pid=132175)[0m f1_weighted: 0.32812689891406654
[2m[36m(func pid=132175)[0m f1_per_class: [0.3, 0.482, 0.329, 0.154, 0.134, 0.308, 0.455, 0.19, 0.192, 0.418]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.37966417910447764
[2m[36m(func pid=121943)[0m top5: 0.8731343283582089
[2m[36m(func pid=121943)[0m f1_micro: 0.37966417910447764
[2m[36m(func pid=121943)[0m f1_macro: 0.32778953682077033
[2m[36m(func pid=121943)[0m f1_weighted: 0.39906965525943916
[2m[36m(func pid=121943)[0m f1_per_class: [0.492, 0.368, 0.239, 0.509, 0.128, 0.408, 0.353, 0.323, 0.253, 0.206]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.5043 | Steps: 4 | Val loss: 1.6854 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 1.0251 | Steps: 4 | Val loss: 7.4240 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 0.5964 | Steps: 4 | Val loss: 2.0271 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 12:41:12 (running for 00:43:18.92)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.504 |      0.309 |                   69 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.796 |      0.328 |                   52 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.705 |      0.296 |                   11 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3903917910447761
[2m[36m(func pid=119092)[0m top5: 0.8987873134328358
[2m[36m(func pid=119092)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=119092)[0m f1_macro: 0.30904115073268945
[2m[36m(func pid=119092)[0m f1_weighted: 0.4172859384576872
[2m[36m(func pid=119092)[0m f1_per_class: [0.359, 0.322, 0.222, 0.48, 0.082, 0.391, 0.501, 0.249, 0.217, 0.268]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m top1: 0.22294776119402984
[2m[36m(func pid=132175)[0m top5: 0.6702425373134329
[2m[36m(func pid=132175)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=132175)[0m f1_macro: 0.1866586022938334
[2m[36m(func pid=132175)[0m f1_weighted: 0.19898293003577638
[2m[36m(func pid=132175)[0m f1_per_class: [0.175, 0.448, 0.087, 0.045, 0.0, 0.069, 0.238, 0.302, 0.144, 0.359]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.3829291044776119
[2m[36m(func pid=121943)[0m top5: 0.8638059701492538
[2m[36m(func pid=121943)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=121943)[0m f1_macro: 0.31845942893798884
[2m[36m(func pid=121943)[0m f1_weighted: 0.39648836924543257
[2m[36m(func pid=121943)[0m f1_per_class: [0.449, 0.337, 0.182, 0.541, 0.119, 0.388, 0.341, 0.324, 0.27, 0.234]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.3622 | Steps: 4 | Val loss: 1.6889 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.9929 | Steps: 4 | Val loss: 4.8000 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.5089 | Steps: 4 | Val loss: 2.0955 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 12:41:18 (running for 00:43:24.21)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.362 |      0.307 |                   70 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.596 |      0.318 |                   53 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.025 |      0.187 |                   12 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.386660447761194
[2m[36m(func pid=119092)[0m top5: 0.8903917910447762
[2m[36m(func pid=119092)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=119092)[0m f1_macro: 0.3072858580554255
[2m[36m(func pid=119092)[0m f1_weighted: 0.4136144450070191
[2m[36m(func pid=119092)[0m f1_per_class: [0.391, 0.323, 0.235, 0.461, 0.078, 0.387, 0.51, 0.234, 0.194, 0.259]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m top1: 0.36007462686567165
[2m[36m(func pid=132175)[0m top5: 0.7971082089552238
[2m[36m(func pid=132175)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=132175)[0m f1_macro: 0.2741493772433512
[2m[36m(func pid=132175)[0m f1_weighted: 0.36060074596890007
[2m[36m(func pid=132175)[0m f1_per_class: [0.25, 0.443, 0.168, 0.333, 0.095, 0.116, 0.47, 0.342, 0.26, 0.266]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.386660447761194
[2m[36m(func pid=121943)[0m top5: 0.8526119402985075
[2m[36m(func pid=121943)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=121943)[0m f1_macro: 0.32734283597539726
[2m[36m(func pid=121943)[0m f1_weighted: 0.4084834694322725
[2m[36m(func pid=121943)[0m f1_per_class: [0.424, 0.42, 0.143, 0.52, 0.117, 0.391, 0.351, 0.328, 0.269, 0.311]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.5335 | Steps: 4 | Val loss: 1.6523 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 3.3443 | Steps: 4 | Val loss: 3.8458 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.5489 | Steps: 4 | Val loss: 2.2855 | Batch size: 32 | lr: 0.01 | Duration: 3.28s
== Status ==
Current time: 2024-01-07 12:41:23 (running for 00:43:29.42)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.533 |      0.312 |                   71 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.509 |      0.327 |                   54 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.993 |      0.274 |                   13 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3983208955223881
[2m[36m(func pid=119092)[0m top5: 0.8973880597014925
[2m[36m(func pid=119092)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=119092)[0m f1_macro: 0.31187500627987014
[2m[36m(func pid=119092)[0m f1_weighted: 0.4214865117754041
[2m[36m(func pid=119092)[0m f1_per_class: [0.414, 0.332, 0.235, 0.488, 0.094, 0.398, 0.504, 0.232, 0.169, 0.253]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m top1: 0.4253731343283582
[2m[36m(func pid=132175)[0m top5: 0.9085820895522388
[2m[36m(func pid=132175)[0m f1_micro: 0.4253731343283582
[2m[36m(func pid=132175)[0m f1_macro: 0.40255726495981836
[2m[36m(func pid=132175)[0m f1_weighted: 0.43857055096124725
[2m[36m(func pid=132175)[0m f1_per_class: [0.536, 0.346, 0.815, 0.534, 0.179, 0.359, 0.468, 0.375, 0.255, 0.157]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.33675373134328357
[2m[36m(func pid=121943)[0m top5: 0.8507462686567164
[2m[36m(func pid=121943)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=121943)[0m f1_macro: 0.3034583560590188
[2m[36m(func pid=121943)[0m f1_weighted: 0.3675677262163911
[2m[36m(func pid=121943)[0m f1_per_class: [0.419, 0.435, 0.084, 0.35, 0.102, 0.388, 0.374, 0.304, 0.237, 0.341]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.3206 | Steps: 4 | Val loss: 1.6460 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.0977 | Steps: 4 | Val loss: 8.0186 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 12:41:28 (running for 00:43:34.79)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.321 |      0.332 |                   72 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.549 |      0.303 |                   55 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  3.344 |      0.403 |                   14 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.4085820895522388
[2m[36m(func pid=119092)[0m top5: 0.9015858208955224
[2m[36m(func pid=119092)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=119092)[0m f1_macro: 0.33213555272230666
[2m[36m(func pid=119092)[0m f1_weighted: 0.4314051244851072
[2m[36m(func pid=119092)[0m f1_per_class: [0.44, 0.338, 0.27, 0.496, 0.1, 0.428, 0.498, 0.287, 0.195, 0.269]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 0.5443 | Steps: 4 | Val loss: 2.3714 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=132175)[0m top1: 0.2513992537313433
[2m[36m(func pid=132175)[0m top5: 0.6175373134328358
[2m[36m(func pid=132175)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=132175)[0m f1_macro: 0.321201414187365
[2m[36m(func pid=132175)[0m f1_weighted: 0.23134114278020773
[2m[36m(func pid=132175)[0m f1_per_class: [0.575, 0.319, 0.8, 0.323, 0.088, 0.286, 0.015, 0.366, 0.258, 0.182]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.4193 | Steps: 4 | Val loss: 1.6370 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=121943)[0m top1: 0.314365671641791
[2m[36m(func pid=121943)[0m top5: 0.840018656716418
[2m[36m(func pid=121943)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=121943)[0m f1_macro: 0.28766088963055275
[2m[36m(func pid=121943)[0m f1_weighted: 0.35169704136812924
[2m[36m(func pid=121943)[0m f1_per_class: [0.357, 0.383, 0.064, 0.285, 0.111, 0.391, 0.411, 0.337, 0.224, 0.313]
[2m[36m(func pid=121943)[0m 
== Status ==
Current time: 2024-01-07 12:41:34 (running for 00:43:40.17)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.419 |      0.335 |                   73 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.544 |      0.288 |                   56 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.098 |      0.321 |                   15 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.4118470149253731
[2m[36m(func pid=119092)[0m top5: 0.8973880597014925
[2m[36m(func pid=119092)[0m f1_micro: 0.4118470149253731
[2m[36m(func pid=119092)[0m f1_macro: 0.33494299598733346
[2m[36m(func pid=119092)[0m f1_weighted: 0.4356018129834729
[2m[36m(func pid=119092)[0m f1_per_class: [0.47, 0.404, 0.224, 0.504, 0.108, 0.417, 0.471, 0.283, 0.187, 0.282]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 3.4857 | Steps: 4 | Val loss: 8.9804 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5679 | Steps: 4 | Val loss: 2.3831 | Batch size: 32 | lr: 0.01 | Duration: 3.28s
[2m[36m(func pid=132175)[0m top1: 0.2751865671641791
[2m[36m(func pid=132175)[0m top5: 0.6263992537313433
[2m[36m(func pid=132175)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=132175)[0m f1_macro: 0.3402747879808493
[2m[36m(func pid=132175)[0m f1_weighted: 0.22479984609197487
[2m[36m(func pid=132175)[0m f1_per_class: [0.579, 0.422, 0.741, 0.209, 0.086, 0.348, 0.021, 0.329, 0.185, 0.483]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.5320 | Steps: 4 | Val loss: 1.6648 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=121943)[0m top1: 0.32136194029850745
[2m[36m(func pid=121943)[0m top5: 0.8302238805970149
[2m[36m(func pid=121943)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=121943)[0m f1_macro: 0.2859174109389126
[2m[36m(func pid=121943)[0m f1_weighted: 0.35944295696914746
[2m[36m(func pid=121943)[0m f1_per_class: [0.335, 0.392, 0.094, 0.307, 0.09, 0.383, 0.422, 0.311, 0.203, 0.321]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m top1: 0.41091417910447764
[2m[36m(func pid=119092)[0m top5: 0.8908582089552238
[2m[36m(func pid=119092)[0m f1_micro: 0.4109141791044776
[2m[36m(func pid=119092)[0m f1_macro: 0.33773914459527193
[2m[36m(func pid=119092)[0m f1_weighted: 0.4340172564617453
[2m[36m(func pid=119092)[0m f1_per_class: [0.414, 0.423, 0.226, 0.502, 0.117, 0.424, 0.445, 0.329, 0.215, 0.282]
[2m[36m(func pid=119092)[0m 
== Status ==
Current time: 2024-01-07 12:41:39 (running for 00:43:45.44)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.532 |      0.338 |                   74 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.568 |      0.286 |                   57 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  3.486 |      0.34  |                   16 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 1.5845 | Steps: 4 | Val loss: 6.1986 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.6391 | Steps: 4 | Val loss: 2.2393 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.3426 | Steps: 4 | Val loss: 1.6730 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=132175)[0m top1: 0.3414179104477612
[2m[36m(func pid=132175)[0m top5: 0.8120335820895522
[2m[36m(func pid=132175)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=132175)[0m f1_macro: 0.30278723130164265
[2m[36m(func pid=132175)[0m f1_weighted: 0.34898796592027337
[2m[36m(func pid=132175)[0m f1_per_class: [0.364, 0.437, 0.444, 0.258, 0.066, 0.291, 0.461, 0.127, 0.291, 0.289]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.34654850746268656
[2m[36m(func pid=121943)[0m top5: 0.8558768656716418
[2m[36m(func pid=121943)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=121943)[0m f1_macro: 0.2966512457569288
[2m[36m(func pid=121943)[0m f1_weighted: 0.38721482868101686
[2m[36m(func pid=121943)[0m f1_per_class: [0.344, 0.416, 0.115, 0.39, 0.107, 0.36, 0.444, 0.258, 0.17, 0.362]
[2m[36m(func pid=121943)[0m 
== Status ==
Current time: 2024-01-07 12:41:44 (running for 00:43:50.86)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.343 |      0.334 |                   75 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.639 |      0.297 |                   58 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.584 |      0.303 |                   17 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3983208955223881
[2m[36m(func pid=119092)[0m top5: 0.8969216417910447
[2m[36m(func pid=119092)[0m f1_micro: 0.3983208955223881
[2m[36m(func pid=119092)[0m f1_macro: 0.33375516884077344
[2m[36m(func pid=119092)[0m f1_weighted: 0.42322163284464326
[2m[36m(func pid=119092)[0m f1_per_class: [0.436, 0.428, 0.213, 0.465, 0.112, 0.417, 0.443, 0.324, 0.208, 0.291]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 1.9567 | Steps: 4 | Val loss: 5.8235 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 0.4841 | Steps: 4 | Val loss: 2.0660 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.2481 | Steps: 4 | Val loss: 1.6680 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=132175)[0m top1: 0.34794776119402987
[2m[36m(func pid=132175)[0m top5: 0.8055037313432836
[2m[36m(func pid=132175)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=132175)[0m f1_macro: 0.28050857015941866
[2m[36m(func pid=132175)[0m f1_weighted: 0.3653209626708584
[2m[36m(func pid=132175)[0m f1_per_class: [0.258, 0.43, 0.1, 0.263, 0.117, 0.339, 0.494, 0.215, 0.274, 0.315]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.3894589552238806
[2m[36m(func pid=121943)[0m top5: 0.8838619402985075
[2m[36m(func pid=121943)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=121943)[0m f1_macro: 0.32464637799135077
[2m[36m(func pid=121943)[0m f1_weighted: 0.40891889801948916
[2m[36m(func pid=121943)[0m f1_per_class: [0.36, 0.5, 0.215, 0.398, 0.131, 0.337, 0.47, 0.215, 0.187, 0.435]
[2m[36m(func pid=121943)[0m 
== Status ==
Current time: 2024-01-07 12:41:50 (running for 00:43:56.45)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.248 |      0.347 |                   76 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.484 |      0.325 |                   59 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.957 |      0.281 |                   18 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.40531716417910446
[2m[36m(func pid=119092)[0m top5: 0.8992537313432836
[2m[36m(func pid=119092)[0m f1_micro: 0.40531716417910446
[2m[36m(func pid=119092)[0m f1_macro: 0.3468439070233263
[2m[36m(func pid=119092)[0m f1_weighted: 0.4287508929940427
[2m[36m(func pid=119092)[0m f1_per_class: [0.508, 0.443, 0.226, 0.468, 0.121, 0.441, 0.433, 0.338, 0.203, 0.288]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 1.5330 | Steps: 4 | Val loss: 7.6840 | Batch size: 32 | lr: 0.1 | Duration: 3.31s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.5800 | Steps: 4 | Val loss: 2.0163 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.4476 | Steps: 4 | Val loss: 1.6725 | Batch size: 32 | lr: 0.001 | Duration: 2.69s
[2m[36m(func pid=132175)[0m top1: 0.2667910447761194
[2m[36m(func pid=132175)[0m top5: 0.7196828358208955
[2m[36m(func pid=132175)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=132175)[0m f1_macro: 0.23988849420904007
[2m[36m(func pid=132175)[0m f1_weighted: 0.25725305527290226
[2m[36m(func pid=132175)[0m f1_per_class: [0.324, 0.195, 0.071, 0.458, 0.139, 0.36, 0.066, 0.296, 0.199, 0.291]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.40625
[2m[36m(func pid=121943)[0m top5: 0.8810634328358209
[2m[36m(func pid=121943)[0m f1_micro: 0.40625
[2m[36m(func pid=121943)[0m f1_macro: 0.3492138751352051
[2m[36m(func pid=121943)[0m f1_weighted: 0.4207214879138023
[2m[36m(func pid=121943)[0m f1_per_class: [0.36, 0.515, 0.273, 0.42, 0.128, 0.359, 0.45, 0.295, 0.211, 0.482]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m top1: 0.4025186567164179
[2m[36m(func pid=119092)[0m top5: 0.8917910447761194
[2m[36m(func pid=119092)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=119092)[0m f1_macro: 0.34447216394169217
[2m[36m(func pid=119092)[0m f1_weighted: 0.4239804232931896
[2m[36m(func pid=119092)[0m f1_per_class: [0.507, 0.444, 0.218, 0.468, 0.125, 0.446, 0.416, 0.338, 0.202, 0.28]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.1113 | Steps: 4 | Val loss: 10.8594 | Batch size: 32 | lr: 0.1 | Duration: 3.23s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.2579 | Steps: 4 | Val loss: 1.9029 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.3213 | Steps: 4 | Val loss: 1.6873 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 12:42:00 (running for 00:44:06.42)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.448 |      0.344 |                   77 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.58  |      0.349 |                   60 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.533 |      0.24  |                   19 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.17164179104477612
[2m[36m(func pid=132175)[0m top5: 0.6977611940298507
[2m[36m(func pid=132175)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=132175)[0m f1_macro: 0.23859083604599324
[2m[36m(func pid=132175)[0m f1_weighted: 0.1565184231977198
[2m[36m(func pid=132175)[0m f1_per_class: [0.536, 0.112, 0.353, 0.265, 0.174, 0.194, 0.024, 0.167, 0.146, 0.414]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.43050373134328357
[2m[36m(func pid=121943)[0m top5: 0.8861940298507462
[2m[36m(func pid=121943)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=121943)[0m f1_macro: 0.3768279592409809
[2m[36m(func pid=121943)[0m f1_weighted: 0.4400115401155238
[2m[36m(func pid=121943)[0m f1_per_class: [0.441, 0.529, 0.338, 0.482, 0.124, 0.378, 0.427, 0.321, 0.228, 0.5]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m top1: 0.3908582089552239
[2m[36m(func pid=119092)[0m top5: 0.8875932835820896
[2m[36m(func pid=119092)[0m f1_micro: 0.3908582089552239
[2m[36m(func pid=119092)[0m f1_macro: 0.33986586967398896
[2m[36m(func pid=119092)[0m f1_weighted: 0.4126950836856465
[2m[36m(func pid=119092)[0m f1_per_class: [0.512, 0.446, 0.236, 0.456, 0.118, 0.432, 0.391, 0.358, 0.194, 0.256]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.3973 | Steps: 4 | Val loss: 1.7290 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.6191 | Steps: 4 | Val loss: 1.9596 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.3154 | Steps: 4 | Val loss: 6.1155 | Batch size: 32 | lr: 0.1 | Duration: 3.31s
== Status ==
Current time: 2024-01-07 12:42:05 (running for 00:44:11.78)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.321 |      0.34  |                   78 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.258 |      0.377 |                   61 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.111 |      0.239 |                   20 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.36380597014925375
[2m[36m(func pid=119092)[0m top5: 0.8717350746268657
[2m[36m(func pid=119092)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=119092)[0m f1_macro: 0.31381532675280804
[2m[36m(func pid=119092)[0m f1_weighted: 0.39063821982902835
[2m[36m(func pid=119092)[0m f1_per_class: [0.446, 0.376, 0.175, 0.444, 0.098, 0.434, 0.379, 0.323, 0.217, 0.246]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m top1: 0.31156716417910446
[2m[36m(func pid=132175)[0m top5: 0.8041044776119403
[2m[36m(func pid=132175)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=132175)[0m f1_macro: 0.3110871341164005
[2m[36m(func pid=132175)[0m f1_weighted: 0.30598580737985603
[2m[36m(func pid=132175)[0m f1_per_class: [0.262, 0.389, 0.49, 0.271, 0.235, 0.388, 0.269, 0.326, 0.204, 0.277]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.41044776119402987
[2m[36m(func pid=121943)[0m top5: 0.8801305970149254
[2m[36m(func pid=121943)[0m f1_micro: 0.41044776119402987
[2m[36m(func pid=121943)[0m f1_macro: 0.3552041996140541
[2m[36m(func pid=121943)[0m f1_weighted: 0.4214183156476161
[2m[36m(func pid=121943)[0m f1_per_class: [0.413, 0.491, 0.279, 0.478, 0.139, 0.396, 0.391, 0.316, 0.216, 0.433]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.2606 | Steps: 4 | Val loss: 1.7268 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.9925 | Steps: 4 | Val loss: 6.4737 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 0.5470 | Steps: 4 | Val loss: 2.0552 | Batch size: 32 | lr: 0.01 | Duration: 3.24s
== Status ==
Current time: 2024-01-07 12:42:11 (running for 00:44:17.56)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.397 |      0.314 |                   79 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.619 |      0.355 |                   62 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.315 |      0.311 |                   21 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.36847014925373134
[2m[36m(func pid=119092)[0m top5: 0.8740671641791045
[2m[36m(func pid=119092)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=119092)[0m f1_macro: 0.31578826055374626
[2m[36m(func pid=119092)[0m f1_weighted: 0.39531634807599514
[2m[36m(func pid=119092)[0m f1_per_class: [0.444, 0.401, 0.176, 0.468, 0.103, 0.421, 0.365, 0.311, 0.201, 0.267]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m top1: 0.3810634328358209
[2m[36m(func pid=121943)[0m top5: 0.8740671641791045
[2m[36m(func pid=121943)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=121943)[0m f1_macro: 0.33048892713669287
[2m[36m(func pid=121943)[0m f1_weighted: 0.3991748000211886
[2m[36m(func pid=121943)[0m f1_per_class: [0.364, 0.45, 0.24, 0.465, 0.136, 0.379, 0.365, 0.324, 0.206, 0.376]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=132175)[0m top1: 0.3414179104477612
[2m[36m(func pid=132175)[0m top5: 0.8255597014925373
[2m[36m(func pid=132175)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=132175)[0m f1_macro: 0.30238897331391873
[2m[36m(func pid=132175)[0m f1_weighted: 0.3434871896415659
[2m[36m(func pid=132175)[0m f1_per_class: [0.168, 0.43, 0.541, 0.209, 0.109, 0.404, 0.448, 0.312, 0.054, 0.35]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.1482 | Steps: 4 | Val loss: 1.6744 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.2841 | Steps: 4 | Val loss: 6.5668 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.5597 | Steps: 4 | Val loss: 2.1398 | Batch size: 32 | lr: 0.01 | Duration: 3.29s
== Status ==
Current time: 2024-01-07 12:42:17 (running for 00:44:23.08)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.148 |      0.336 |                   81 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.547 |      0.33  |                   63 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.993 |      0.302 |                   22 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3978544776119403
[2m[36m(func pid=119092)[0m top5: 0.8903917910447762
[2m[36m(func pid=119092)[0m f1_micro: 0.3978544776119403
[2m[36m(func pid=119092)[0m f1_macro: 0.3363548741363671
[2m[36m(func pid=119092)[0m f1_weighted: 0.4226448403831505
[2m[36m(func pid=119092)[0m f1_per_class: [0.437, 0.398, 0.224, 0.499, 0.121, 0.447, 0.41, 0.345, 0.222, 0.26]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m top1: 0.3125
[2m[36m(func pid=132175)[0m top5: 0.8451492537313433
[2m[36m(func pid=132175)[0m f1_micro: 0.3125
[2m[36m(func pid=132175)[0m f1_macro: 0.33268856326177626
[2m[36m(func pid=132175)[0m f1_weighted: 0.3413976594980783
[2m[36m(func pid=132175)[0m f1_per_class: [0.351, 0.318, 0.5, 0.422, 0.055, 0.344, 0.29, 0.31, 0.294, 0.444]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.3619402985074627
[2m[36m(func pid=121943)[0m top5: 0.8642723880597015
[2m[36m(func pid=121943)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=121943)[0m f1_macro: 0.3157779154120613
[2m[36m(func pid=121943)[0m f1_weighted: 0.3855417112715536
[2m[36m(func pid=121943)[0m f1_per_class: [0.391, 0.387, 0.222, 0.482, 0.116, 0.393, 0.338, 0.321, 0.196, 0.312]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 1.4166 | Steps: 4 | Val loss: 1.6415 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 1.1360 | Steps: 4 | Val loss: 5.5299 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 0.4672 | Steps: 4 | Val loss: 2.3265 | Batch size: 32 | lr: 0.01 | Duration: 3.29s
== Status ==
Current time: 2024-01-07 12:42:22 (running for 00:44:28.61)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.417 |      0.343 |                   82 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.56  |      0.316 |                   64 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.284 |      0.333 |                   23 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.4076492537313433
[2m[36m(func pid=119092)[0m top5: 0.8978544776119403
[2m[36m(func pid=119092)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=119092)[0m f1_macro: 0.3430535784054115
[2m[36m(func pid=119092)[0m f1_weighted: 0.431618910398057
[2m[36m(func pid=119092)[0m f1_per_class: [0.469, 0.374, 0.23, 0.523, 0.117, 0.433, 0.431, 0.359, 0.227, 0.267]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m top1: 0.37546641791044777
[2m[36m(func pid=132175)[0m top5: 0.8722014925373134
[2m[36m(func pid=132175)[0m f1_micro: 0.3754664179104477
[2m[36m(func pid=132175)[0m f1_macro: 0.3659242705121431
[2m[36m(func pid=132175)[0m f1_weighted: 0.39016376371599737
[2m[36m(func pid=132175)[0m f1_per_class: [0.577, 0.395, 0.381, 0.481, 0.098, 0.394, 0.327, 0.307, 0.225, 0.475]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.3185634328358209
[2m[36m(func pid=121943)[0m top5: 0.8484141791044776
[2m[36m(func pid=121943)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=121943)[0m f1_macro: 0.29051737199039057
[2m[36m(func pid=121943)[0m f1_weighted: 0.342149280624645
[2m[36m(func pid=121943)[0m f1_per_class: [0.372, 0.316, 0.202, 0.411, 0.098, 0.39, 0.307, 0.304, 0.191, 0.314]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 1.3300 | Steps: 4 | Val loss: 1.6710 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 1.2371 | Steps: 4 | Val loss: 7.7073 | Batch size: 32 | lr: 0.1 | Duration: 3.24s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.9477 | Steps: 4 | Val loss: 2.3898 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 12:42:28 (running for 00:44:34.03)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.33  |      0.332 |                   83 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.467 |      0.291 |                   65 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.136 |      0.366 |                   24 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.39505597014925375
[2m[36m(func pid=119092)[0m top5: 0.8871268656716418
[2m[36m(func pid=119092)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=119092)[0m f1_macro: 0.33161840772273943
[2m[36m(func pid=119092)[0m f1_weighted: 0.4216353480899788
[2m[36m(func pid=119092)[0m f1_per_class: [0.479, 0.322, 0.224, 0.515, 0.113, 0.437, 0.439, 0.35, 0.206, 0.23]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m top1: 0.300839552238806
[2m[36m(func pid=132175)[0m top5: 0.7532649253731343
[2m[36m(func pid=132175)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=132175)[0m f1_macro: 0.28838214648014915
[2m[36m(func pid=132175)[0m f1_weighted: 0.29649783683326264
[2m[36m(func pid=132175)[0m f1_per_class: [0.5, 0.431, 0.156, 0.114, 0.144, 0.349, 0.379, 0.27, 0.17, 0.372]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.310634328358209
[2m[36m(func pid=121943)[0m top5: 0.8456156716417911
[2m[36m(func pid=121943)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=121943)[0m f1_macro: 0.28636009400486423
[2m[36m(func pid=121943)[0m f1_weighted: 0.33863820761495983
[2m[36m(func pid=121943)[0m f1_per_class: [0.365, 0.365, 0.197, 0.32, 0.087, 0.375, 0.364, 0.274, 0.19, 0.328]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 1.2752 | Steps: 4 | Val loss: 1.6887 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.9045 | Steps: 4 | Val loss: 9.4481 | Batch size: 32 | lr: 0.1 | Duration: 3.29s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 0.5961 | Steps: 4 | Val loss: 2.3258 | Batch size: 32 | lr: 0.01 | Duration: 3.24s
== Status ==
Current time: 2024-01-07 12:42:33 (running for 00:44:39.55)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.275 |      0.323 |                   84 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.948 |      0.286 |                   66 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.237 |      0.288 |                   25 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.38526119402985076
[2m[36m(func pid=119092)[0m top5: 0.8852611940298507
[2m[36m(func pid=119092)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=119092)[0m f1_macro: 0.32337449002964036
[2m[36m(func pid=119092)[0m f1_weighted: 0.4126866332651494
[2m[36m(func pid=119092)[0m f1_per_class: [0.453, 0.322, 0.215, 0.508, 0.109, 0.425, 0.425, 0.339, 0.205, 0.234]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m top1: 0.2677238805970149
[2m[36m(func pid=132175)[0m top5: 0.7234141791044776
[2m[36m(func pid=132175)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=132175)[0m f1_macro: 0.2643664099169388
[2m[36m(func pid=132175)[0m f1_weighted: 0.28851058559701154
[2m[36m(func pid=132175)[0m f1_per_class: [0.511, 0.333, 0.041, 0.033, 0.148, 0.361, 0.495, 0.212, 0.149, 0.36]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.32322761194029853
[2m[36m(func pid=121943)[0m top5: 0.8563432835820896
[2m[36m(func pid=121943)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=121943)[0m f1_macro: 0.2777161403074987
[2m[36m(func pid=121943)[0m f1_weighted: 0.3447711233694796
[2m[36m(func pid=121943)[0m f1_per_class: [0.354, 0.323, 0.166, 0.246, 0.084, 0.349, 0.491, 0.267, 0.202, 0.296]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.2232 | Steps: 4 | Val loss: 1.6799 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.3920 | Steps: 4 | Val loss: 6.0506 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 12:42:39 (running for 00:44:45.03)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.223 |      0.328 |                   85 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.596 |      0.278 |                   67 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.904 |      0.264 |                   26 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.38759328358208955
[2m[36m(func pid=119092)[0m top5: 0.8885261194029851
[2m[36m(func pid=119092)[0m f1_micro: 0.38759328358208955
[2m[36m(func pid=119092)[0m f1_macro: 0.32817473114280565
[2m[36m(func pid=119092)[0m f1_weighted: 0.412256048418324
[2m[36m(func pid=119092)[0m f1_per_class: [0.479, 0.33, 0.22, 0.516, 0.115, 0.411, 0.411, 0.349, 0.216, 0.235]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 0.5032 | Steps: 4 | Val loss: 2.1441 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=132175)[0m top1: 0.34048507462686567
[2m[36m(func pid=132175)[0m top5: 0.8302238805970149
[2m[36m(func pid=132175)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=132175)[0m f1_macro: 0.2964879723247018
[2m[36m(func pid=132175)[0m f1_weighted: 0.38694766966529043
[2m[36m(func pid=132175)[0m f1_per_class: [0.428, 0.35, 0.073, 0.395, 0.156, 0.345, 0.47, 0.318, 0.159, 0.273]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.3516791044776119
[2m[36m(func pid=121943)[0m top5: 0.8833955223880597
[2m[36m(func pid=121943)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=121943)[0m f1_macro: 0.29483602381797425
[2m[36m(func pid=121943)[0m f1_weighted: 0.37425242894713834
[2m[36m(func pid=121943)[0m f1_per_class: [0.38, 0.33, 0.178, 0.326, 0.116, 0.35, 0.509, 0.269, 0.187, 0.304]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.1319 | Steps: 4 | Val loss: 1.7028 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=119092)[0m top1: 0.37919776119402987
[2m[36m(func pid=119092)[0m top5: 0.8805970149253731
[2m[36m(func pid=119092)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=119092)[0m f1_macro: 0.32574238877920014
[2m[36m(func pid=119092)[0m f1_weighted: 0.4051173549963679
[2m[36m(func pid=119092)[0m f1_per_class: [0.493, 0.329, 0.203, 0.512, 0.1, 0.414, 0.388, 0.355, 0.216, 0.247]
[2m[36m(func pid=119092)[0m 
== Status ==
Current time: 2024-01-07 12:42:44 (running for 00:44:50.19)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.132 |      0.326 |                   86 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.503 |      0.295 |                   68 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.392 |      0.296 |                   27 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 0.6556 | Steps: 4 | Val loss: 6.3250 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 0.3563 | Steps: 4 | Val loss: 2.0488 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=132175)[0m top1: 0.3460820895522388
[2m[36m(func pid=132175)[0m top5: 0.8176305970149254
[2m[36m(func pid=132175)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=132175)[0m f1_macro: 0.28534573030392374
[2m[36m(func pid=132175)[0m f1_weighted: 0.3259489225210631
[2m[36m(func pid=132175)[0m f1_per_class: [0.412, 0.108, 0.22, 0.564, 0.211, 0.338, 0.245, 0.308, 0.195, 0.253]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 1.3028 | Steps: 4 | Val loss: 1.7151 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=121943)[0m top1: 0.37080223880597013
[2m[36m(func pid=121943)[0m top5: 0.8973880597014925
[2m[36m(func pid=121943)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=121943)[0m f1_macro: 0.31818118884088814
[2m[36m(func pid=121943)[0m f1_weighted: 0.395901657157493
[2m[36m(func pid=121943)[0m f1_per_class: [0.408, 0.42, 0.187, 0.369, 0.129, 0.363, 0.476, 0.284, 0.193, 0.353]
[2m[36m(func pid=121943)[0m 
== Status ==
Current time: 2024-01-07 12:42:49 (running for 00:44:55.41)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.303 |      0.32  |                   87 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.356 |      0.318 |                   69 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.656 |      0.285 |                   28 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.37779850746268656
[2m[36m(func pid=119092)[0m top5: 0.8722014925373134
[2m[36m(func pid=119092)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=119092)[0m f1_macro: 0.320379978164039
[2m[36m(func pid=119092)[0m f1_weighted: 0.4024519479868444
[2m[36m(func pid=119092)[0m f1_per_class: [0.458, 0.334, 0.172, 0.512, 0.095, 0.417, 0.379, 0.341, 0.227, 0.267]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.9082 | Steps: 4 | Val loss: 5.9000 | Batch size: 32 | lr: 0.1 | Duration: 3.23s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 0.4768 | Steps: 4 | Val loss: 2.0469 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 1.2662 | Steps: 4 | Val loss: 1.6976 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=132175)[0m top1: 0.35634328358208955
[2m[36m(func pid=132175)[0m top5: 0.8577425373134329
[2m[36m(func pid=132175)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=132175)[0m f1_macro: 0.3046651676896025
[2m[36m(func pid=132175)[0m f1_weighted: 0.33462164346142986
[2m[36m(func pid=132175)[0m f1_per_class: [0.492, 0.267, 0.282, 0.559, 0.177, 0.361, 0.178, 0.275, 0.186, 0.268]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.3689365671641791
[2m[36m(func pid=121943)[0m top5: 0.8913246268656716
[2m[36m(func pid=121943)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=121943)[0m f1_macro: 0.32465818546073866
[2m[36m(func pid=121943)[0m f1_weighted: 0.3906407992097873
[2m[36m(func pid=121943)[0m f1_per_class: [0.382, 0.462, 0.236, 0.431, 0.133, 0.357, 0.376, 0.292, 0.196, 0.381]
[2m[36m(func pid=121943)[0m 
== Status ==
Current time: 2024-01-07 12:42:54 (running for 00:45:00.61)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.266 |      0.319 |                   88 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.477 |      0.325 |                   70 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.908 |      0.305 |                   29 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.37453358208955223
[2m[36m(func pid=119092)[0m top5: 0.8777985074626866
[2m[36m(func pid=119092)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=119092)[0m f1_macro: 0.3186002606788716
[2m[36m(func pid=119092)[0m f1_weighted: 0.4006815492167756
[2m[36m(func pid=119092)[0m f1_per_class: [0.47, 0.346, 0.179, 0.501, 0.099, 0.421, 0.377, 0.343, 0.205, 0.245]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.8878 | Steps: 4 | Val loss: 6.2031 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 0.4837 | Steps: 4 | Val loss: 2.1188 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 1.5236 | Steps: 4 | Val loss: 1.7221 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=132175)[0m top1: 0.36380597014925375
[2m[36m(func pid=132175)[0m top5: 0.855410447761194
[2m[36m(func pid=132175)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=132175)[0m f1_macro: 0.2900196493806566
[2m[36m(func pid=132175)[0m f1_weighted: 0.3872334234578272
[2m[36m(func pid=132175)[0m f1_per_class: [0.562, 0.409, 0.103, 0.43, 0.11, 0.384, 0.425, 0.075, 0.2, 0.202]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.3666044776119403
[2m[36m(func pid=121943)[0m top5: 0.878731343283582
[2m[36m(func pid=121943)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=121943)[0m f1_macro: 0.3271393369381516
[2m[36m(func pid=121943)[0m f1_weighted: 0.37609884914590264
[2m[36m(func pid=121943)[0m f1_per_class: [0.37, 0.475, 0.232, 0.382, 0.153, 0.385, 0.348, 0.323, 0.2, 0.404]
[2m[36m(func pid=121943)[0m 
== Status ==
Current time: 2024-01-07 12:42:59 (running for 00:45:05.75)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.524 |      0.315 |                   89 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.484 |      0.327 |                   71 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.888 |      0.29  |                   30 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3694029850746269
[2m[36m(func pid=119092)[0m top5: 0.8684701492537313
[2m[36m(func pid=119092)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=119092)[0m f1_macro: 0.3149447339912931
[2m[36m(func pid=119092)[0m f1_weighted: 0.3977448710516001
[2m[36m(func pid=119092)[0m f1_per_class: [0.44, 0.385, 0.149, 0.467, 0.109, 0.413, 0.38, 0.337, 0.225, 0.243]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.7655 | Steps: 4 | Val loss: 10.4331 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 0.4448 | Steps: 4 | Val loss: 2.3911 | Batch size: 32 | lr: 0.01 | Duration: 3.22s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 1.4576 | Steps: 4 | Val loss: 1.7202 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=132175)[0m top1: 0.2560634328358209
[2m[36m(func pid=132175)[0m top5: 0.6581156716417911
[2m[36m(func pid=132175)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=132175)[0m f1_macro: 0.23418020673993767
[2m[36m(func pid=132175)[0m f1_weighted: 0.2560662241118599
[2m[36m(func pid=132175)[0m f1_per_class: [0.537, 0.402, 0.073, 0.051, 0.077, 0.308, 0.37, 0.103, 0.198, 0.223]
[2m[36m(func pid=132175)[0m 
== Status ==
Current time: 2024-01-07 12:43:04 (running for 00:45:10.89)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.458 |      0.317 |                   90 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.484 |      0.327 |                   71 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.766 |      0.234 |                   31 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3712686567164179
[2m[36m(func pid=119092)[0m top5: 0.875
[2m[36m(func pid=119092)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=119092)[0m f1_macro: 0.3171817978405627
[2m[36m(func pid=119092)[0m f1_weighted: 0.4043487176914222
[2m[36m(func pid=119092)[0m f1_per_class: [0.467, 0.378, 0.141, 0.454, 0.107, 0.443, 0.408, 0.349, 0.191, 0.234]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m top1: 0.34281716417910446
[2m[36m(func pid=121943)[0m top5: 0.8460820895522388
[2m[36m(func pid=121943)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=121943)[0m f1_macro: 0.3112824122465399
[2m[36m(func pid=121943)[0m f1_weighted: 0.3283901197838895
[2m[36m(func pid=121943)[0m f1_per_class: [0.333, 0.498, 0.197, 0.293, 0.157, 0.374, 0.257, 0.333, 0.246, 0.424]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.2322 | Steps: 4 | Val loss: 10.4101 | Batch size: 32 | lr: 0.1 | Duration: 3.25s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 1.3117 | Steps: 4 | Val loss: 1.7124 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.5427 | Steps: 4 | Val loss: 2.3241 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 12:43:10 (running for 00:45:16.06)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.458 |      0.317 |                   90 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.445 |      0.311 |                   72 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.232 |      0.261 |                   32 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.23367537313432835
[2m[36m(func pid=132175)[0m top5: 0.6613805970149254
[2m[36m(func pid=132175)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=132175)[0m f1_macro: 0.2610516069539915
[2m[36m(func pid=132175)[0m f1_weighted: 0.21856167069305107
[2m[36m(func pid=132175)[0m f1_per_class: [0.479, 0.39, 0.174, 0.072, 0.07, 0.289, 0.202, 0.265, 0.161, 0.507]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m top1: 0.37173507462686567
[2m[36m(func pid=119092)[0m top5: 0.8740671641791045
[2m[36m(func pid=119092)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=119092)[0m f1_macro: 0.323420030049853
[2m[36m(func pid=119092)[0m f1_weighted: 0.4036870616655676
[2m[36m(func pid=119092)[0m f1_per_class: [0.512, 0.408, 0.142, 0.424, 0.1, 0.425, 0.417, 0.36, 0.199, 0.247]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m top1: 0.3400186567164179
[2m[36m(func pid=121943)[0m top5: 0.8577425373134329
[2m[36m(func pid=121943)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=121943)[0m f1_macro: 0.3067974910532806
[2m[36m(func pid=121943)[0m f1_weighted: 0.33635388000461275
[2m[36m(func pid=121943)[0m f1_per_class: [0.369, 0.473, 0.208, 0.32, 0.157, 0.367, 0.28, 0.315, 0.233, 0.344]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 1.0127 | Steps: 4 | Val loss: 6.5376 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 1.3078 | Steps: 4 | Val loss: 1.7285 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.7201 | Steps: 4 | Val loss: 2.3590 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 12:43:15 (running for 00:45:21.54)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.312 |      0.323 |                   91 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.543 |      0.307 |                   73 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.013 |      0.354 |                   33 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.355410447761194
[2m[36m(func pid=132175)[0m top5: 0.816231343283582
[2m[36m(func pid=132175)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=132175)[0m f1_macro: 0.35414500396109444
[2m[36m(func pid=132175)[0m f1_weighted: 0.3529403157489657
[2m[36m(func pid=132175)[0m f1_per_class: [0.477, 0.454, 0.387, 0.443, 0.118, 0.337, 0.22, 0.365, 0.213, 0.528]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m top1: 0.36473880597014924
[2m[36m(func pid=119092)[0m top5: 0.8791977611940298
[2m[36m(func pid=119092)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=119092)[0m f1_macro: 0.3228457753617591
[2m[36m(func pid=119092)[0m f1_weighted: 0.3913110937070981
[2m[36m(func pid=119092)[0m f1_per_class: [0.517, 0.434, 0.143, 0.386, 0.107, 0.413, 0.4, 0.352, 0.212, 0.266]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=121943)[0m top1: 0.3246268656716418
[2m[36m(func pid=121943)[0m top5: 0.8526119402985075
[2m[36m(func pid=121943)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=121943)[0m f1_macro: 0.29990375982128253
[2m[36m(func pid=121943)[0m f1_weighted: 0.33403186065425805
[2m[36m(func pid=121943)[0m f1_per_class: [0.42, 0.428, 0.205, 0.328, 0.101, 0.354, 0.295, 0.325, 0.221, 0.322]
[2m[36m(func pid=121943)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 1.3869 | Steps: 4 | Val loss: 1.7463 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 0.3297 | Steps: 4 | Val loss: 6.2665 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
[2m[36m(func pid=121943)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.4167 | Steps: 4 | Val loss: 2.2155 | Batch size: 32 | lr: 0.01 | Duration: 3.34s
== Status ==
Current time: 2024-01-07 12:43:21 (running for 00:45:27.03)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.32075
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.387 |      0.309 |                   93 |
| train_9b9e8_00022 | RUNNING    | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.72  |      0.3   |                   74 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.013 |      0.354 |                   33 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3591417910447761
[2m[36m(func pid=119092)[0m top5: 0.8656716417910447
[2m[36m(func pid=119092)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=119092)[0m f1_macro: 0.30946103893363913
[2m[36m(func pid=119092)[0m f1_weighted: 0.3856495114139003
[2m[36m(func pid=119092)[0m f1_per_class: [0.4, 0.434, 0.153, 0.386, 0.111, 0.402, 0.395, 0.337, 0.215, 0.262]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m top1: 0.39505597014925375
[2m[36m(func pid=132175)[0m top5: 0.8708022388059702
[2m[36m(func pid=132175)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=132175)[0m f1_macro: 0.37523749052347855
[2m[36m(func pid=132175)[0m f1_weighted: 0.3465655270225734
[2m[36m(func pid=132175)[0m f1_per_class: [0.509, 0.471, 0.649, 0.522, 0.089, 0.109, 0.186, 0.381, 0.232, 0.604]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=121943)[0m top1: 0.34421641791044777
[2m[36m(func pid=121943)[0m top5: 0.8754664179104478
[2m[36m(func pid=121943)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=121943)[0m f1_macro: 0.30998949421104355
[2m[36m(func pid=121943)[0m f1_weighted: 0.37265655139778814
[2m[36m(func pid=121943)[0m f1_per_class: [0.446, 0.386, 0.213, 0.389, 0.124, 0.358, 0.392, 0.33, 0.195, 0.267]
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 1.2998 | Steps: 4 | Val loss: 1.7722 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.4657 | Steps: 4 | Val loss: 5.0970 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=119092)[0m top1: 0.35634328358208955
[2m[36m(func pid=119092)[0m top5: 0.8568097014925373
[2m[36m(func pid=119092)[0m f1_micro: 0.3563432835820895
[2m[36m(func pid=119092)[0m f1_macro: 0.30926060811732564
[2m[36m(func pid=119092)[0m f1_weighted: 0.38129855032441706
[2m[36m(func pid=119092)[0m f1_per_class: [0.355, 0.437, 0.155, 0.395, 0.108, 0.4, 0.366, 0.372, 0.23, 0.277]
[2m[36m(func pid=119092)[0m 
== Status ==
Current time: 2024-01-07 12:43:26 (running for 00:45:32.38)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3205
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.3   |      0.309 |                   94 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.33  |      0.375 |                   34 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.4337686567164179
[2m[36m(func pid=132175)[0m top5: 0.9043843283582089
[2m[36m(func pid=132175)[0m f1_micro: 0.4337686567164179
[2m[36m(func pid=132175)[0m f1_macro: 0.3810676090024125
[2m[36m(func pid=132175)[0m f1_weighted: 0.4221021464643366
[2m[36m(func pid=132175)[0m f1_per_class: [0.418, 0.431, 0.6, 0.569, 0.15, 0.296, 0.363, 0.38, 0.218, 0.384]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 1.3169 | Steps: 4 | Val loss: 1.7443 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.0372 | Steps: 4 | Val loss: 6.3580 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 12:43:31 (running for 00:45:37.59)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3205
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.317 |      0.316 |                   95 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.466 |      0.381 |                   35 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3670708955223881
[2m[36m(func pid=119092)[0m top5: 0.8624067164179104
[2m[36m(func pid=119092)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=119092)[0m f1_macro: 0.31640477017816343
[2m[36m(func pid=119092)[0m f1_weighted: 0.39258831082509776
[2m[36m(func pid=119092)[0m f1_per_class: [0.373, 0.437, 0.164, 0.427, 0.111, 0.404, 0.37, 0.379, 0.218, 0.282]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m top1: 0.3568097014925373
[2m[36m(func pid=132175)[0m top5: 0.8512126865671642
[2m[36m(func pid=132175)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=132175)[0m f1_macro: 0.31648693958363416
[2m[36m(func pid=132175)[0m f1_weighted: 0.3654726158157594
[2m[36m(func pid=132175)[0m f1_per_class: [0.268, 0.231, 0.522, 0.344, 0.188, 0.243, 0.538, 0.364, 0.228, 0.24]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 1.1816 | Steps: 4 | Val loss: 1.7648 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.0094 | Steps: 4 | Val loss: 6.3351 | Batch size: 32 | lr: 0.1 | Duration: 3.20s
[2m[36m(func pid=119092)[0m top1: 0.3591417910447761
[2m[36m(func pid=119092)[0m top5: 0.8596082089552238
[2m[36m(func pid=119092)[0m f1_micro: 0.3591417910447761
[2m[36m(func pid=119092)[0m f1_macro: 0.3123072269265829
[2m[36m(func pid=119092)[0m f1_weighted: 0.3848873034338441
[2m[36m(func pid=119092)[0m f1_per_class: [0.391, 0.393, 0.174, 0.459, 0.102, 0.403, 0.34, 0.373, 0.222, 0.265]
[2m[36m(func pid=119092)[0m 
== Status ==
Current time: 2024-01-07 12:43:37 (running for 00:45:43.07)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3205
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.182 |      0.312 |                   96 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.037 |      0.316 |                   36 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.3614738805970149
[2m[36m(func pid=132175)[0m top5: 0.8418843283582089
[2m[36m(func pid=132175)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=132175)[0m f1_macro: 0.3049327486555915
[2m[36m(func pid=132175)[0m f1_weighted: 0.38247921491104464
[2m[36m(func pid=132175)[0m f1_per_class: [0.196, 0.304, 0.491, 0.374, 0.148, 0.217, 0.55, 0.338, 0.216, 0.216]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 1.2758 | Steps: 4 | Val loss: 1.7559 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.8859 | Steps: 4 | Val loss: 5.2627 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 12:43:42 (running for 00:45:48.29)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3205
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.276 |      0.315 |                   97 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.009 |      0.305 |                   37 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.36100746268656714
[2m[36m(func pid=119092)[0m top5: 0.8726679104477612
[2m[36m(func pid=119092)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=119092)[0m f1_macro: 0.31475901145779717
[2m[36m(func pid=119092)[0m f1_weighted: 0.38723958476260967
[2m[36m(func pid=119092)[0m f1_per_class: [0.404, 0.351, 0.186, 0.473, 0.101, 0.41, 0.355, 0.386, 0.204, 0.278]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m top1: 0.43236940298507465
[2m[36m(func pid=132175)[0m top5: 0.8815298507462687
[2m[36m(func pid=132175)[0m f1_micro: 0.43236940298507465
[2m[36m(func pid=132175)[0m f1_macro: 0.35412320451629
[2m[36m(func pid=132175)[0m f1_weighted: 0.43847469126505406
[2m[36m(func pid=132175)[0m f1_per_class: [0.454, 0.433, 0.433, 0.555, 0.122, 0.18, 0.484, 0.346, 0.253, 0.281]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 1.1143 | Steps: 4 | Val loss: 1.7377 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.7557 | Steps: 4 | Val loss: 7.1606 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 12:43:47 (running for 00:45:53.52)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3205
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.114 |      0.325 |                   98 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.886 |      0.354 |                   38 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.3666044776119403
[2m[36m(func pid=119092)[0m top5: 0.8768656716417911
[2m[36m(func pid=119092)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=119092)[0m f1_macro: 0.32511872140268816
[2m[36m(func pid=119092)[0m f1_weighted: 0.39396043841994305
[2m[36m(func pid=119092)[0m f1_per_class: [0.465, 0.377, 0.22, 0.478, 0.103, 0.406, 0.358, 0.372, 0.188, 0.284]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m top1: 0.365205223880597
[2m[36m(func pid=132175)[0m top5: 0.8260261194029851
[2m[36m(func pid=132175)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=132175)[0m f1_macro: 0.3460615755732598
[2m[36m(func pid=132175)[0m f1_weighted: 0.3409016731405802
[2m[36m(func pid=132175)[0m f1_per_class: [0.543, 0.486, 0.49, 0.518, 0.065, 0.093, 0.176, 0.369, 0.245, 0.475]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 1.2272 | Steps: 4 | Val loss: 1.7574 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 12:43:52 (running for 00:45:58.80)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.3205
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00021 | RUNNING    | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.227 |      0.32  |                   99 |
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.756 |      0.346 |                   39 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=119092)[0m top1: 0.365205223880597
[2m[36m(func pid=119092)[0m top5: 0.871268656716418
[2m[36m(func pid=119092)[0m f1_micro: 0.365205223880597
[2m[36m(func pid=119092)[0m f1_macro: 0.3198185841744246
[2m[36m(func pid=119092)[0m f1_weighted: 0.3959696158023712
[2m[36m(func pid=119092)[0m f1_per_class: [0.437, 0.369, 0.198, 0.469, 0.095, 0.407, 0.379, 0.367, 0.199, 0.275]
[2m[36m(func pid=119092)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.9906 | Steps: 4 | Val loss: 8.3236 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=132175)[0m top1: 0.26119402985074625
[2m[36m(func pid=132175)[0m top5: 0.7751865671641791
[2m[36m(func pid=132175)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=132175)[0m f1_macro: 0.2780533733565519
[2m[36m(func pid=132175)[0m f1_weighted: 0.2692261116103061
[2m[36m(func pid=132175)[0m f1_per_class: [0.526, 0.394, 0.371, 0.345, 0.082, 0.135, 0.172, 0.244, 0.224, 0.288]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=119092)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 1.2801 | Steps: 4 | Val loss: 1.7326 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.8795 | Steps: 4 | Val loss: 8.3616 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=119092)[0m top1: 0.37220149253731344
[2m[36m(func pid=119092)[0m top5: 0.8782649253731343
[2m[36m(func pid=119092)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=119092)[0m f1_macro: 0.3214798569298719
[2m[36m(func pid=119092)[0m f1_weighted: 0.4040787240616653
[2m[36m(func pid=119092)[0m f1_per_class: [0.456, 0.379, 0.174, 0.483, 0.099, 0.403, 0.393, 0.347, 0.195, 0.286]
== Status ==
Current time: 2024-01-07 12:43:58 (running for 00:46:04.55)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.991 |      0.278 |                   40 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.29850746268656714
[2m[36m(func pid=132175)[0m top5: 0.7285447761194029
[2m[36m(func pid=132175)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=132175)[0m f1_macro: 0.26258183680650965
[2m[36m(func pid=132175)[0m f1_weighted: 0.2867065461503922
[2m[36m(func pid=132175)[0m f1_per_class: [0.301, 0.257, 0.329, 0.111, 0.219, 0.258, 0.494, 0.288, 0.18, 0.188]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 0.6015 | Steps: 4 | Val loss: 7.6352 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 12:44:05 (running for 00:46:11.89)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.879 |      0.263 |                   41 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.3269589552238806
[2m[36m(func pid=132175)[0m top5: 0.7677238805970149
[2m[36m(func pid=132175)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=132175)[0m f1_macro: 0.2720691597323063
[2m[36m(func pid=132175)[0m f1_weighted: 0.34118816229505716
[2m[36m(func pid=132175)[0m f1_per_class: [0.329, 0.243, 0.268, 0.266, 0.184, 0.326, 0.519, 0.275, 0.178, 0.133]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 0.8134 | Steps: 4 | Val loss: 7.5001 | Batch size: 32 | lr: 0.1 | Duration: 3.26s
== Status ==
Current time: 2024-01-07 12:44:11 (running for 00:46:17.81)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.601 |      0.272 |                   42 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.33675373134328357
[2m[36m(func pid=132175)[0m top5: 0.7905783582089553
[2m[36m(func pid=132175)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=132175)[0m f1_macro: 0.2739694932732704
[2m[36m(func pid=132175)[0m f1_weighted: 0.3614999894357712
[2m[36m(func pid=132175)[0m f1_per_class: [0.408, 0.326, 0.167, 0.304, 0.085, 0.278, 0.52, 0.266, 0.183, 0.204]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.1016 | Steps: 4 | Val loss: 8.0630 | Batch size: 32 | lr: 0.1 | Duration: 3.37s
== Status ==
Current time: 2024-01-07 12:44:17 (running for 00:46:23.75)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.813 |      0.274 |                   43 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.30363805970149255
[2m[36m(func pid=132175)[0m top5: 0.7905783582089553
[2m[36m(func pid=132175)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=132175)[0m f1_macro: 0.2715250193108464
[2m[36m(func pid=132175)[0m f1_weighted: 0.33638459884627836
[2m[36m(func pid=132175)[0m f1_per_class: [0.462, 0.355, 0.1, 0.38, 0.077, 0.159, 0.377, 0.318, 0.183, 0.304]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 0.2081 | Steps: 4 | Val loss: 9.3387 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 12:44:23 (running for 00:46:29.70)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.102 |      0.272 |                   44 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.3003731343283582
[2m[36m(func pid=132175)[0m top5: 0.7257462686567164
[2m[36m(func pid=132175)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=132175)[0m f1_macro: 0.27308590732180643
[2m[36m(func pid=132175)[0m f1_weighted: 0.28354786753946065
[2m[36m(func pid=132175)[0m f1_per_class: [0.5, 0.42, 0.197, 0.438, 0.097, 0.281, 0.057, 0.323, 0.209, 0.21]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.1037 | Steps: 4 | Val loss: 9.5212 | Batch size: 32 | lr: 0.1 | Duration: 3.32s
== Status ==
Current time: 2024-01-07 12:44:29 (running for 00:46:35.28)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.208 |      0.273 |                   45 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.31529850746268656
[2m[36m(func pid=132175)[0m top5: 0.7164179104477612
[2m[36m(func pid=132175)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=132175)[0m f1_macro: 0.283511078359211
[2m[36m(func pid=132175)[0m f1_weighted: 0.28807508854175684
[2m[36m(func pid=132175)[0m f1_per_class: [0.411, 0.418, 0.295, 0.452, 0.151, 0.359, 0.03, 0.337, 0.222, 0.16]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 0.4921 | Steps: 4 | Val loss: 8.7261 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 12:44:35 (running for 00:46:41.31)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.104 |      0.284 |                   46 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.30970149253731344
[2m[36m(func pid=132175)[0m top5: 0.7569962686567164
[2m[36m(func pid=132175)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=132175)[0m f1_macro: 0.2829114964381909
[2m[36m(func pid=132175)[0m f1_weighted: 0.3052232821916066
[2m[36m(func pid=132175)[0m f1_per_class: [0.219, 0.368, 0.361, 0.435, 0.196, 0.368, 0.135, 0.351, 0.24, 0.155]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.3324 | Steps: 4 | Val loss: 8.3277 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 12:44:40 (running for 00:46:46.91)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.492 |      0.283 |                   47 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.28777985074626866
[2m[36m(func pid=132175)[0m top5: 0.7751865671641791
[2m[36m(func pid=132175)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=132175)[0m f1_macro: 0.25552683156051137
[2m[36m(func pid=132175)[0m f1_weighted: 0.31395092781644873
[2m[36m(func pid=132175)[0m f1_per_class: [0.165, 0.325, 0.255, 0.373, 0.126, 0.376, 0.267, 0.282, 0.219, 0.167]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.6912 | Steps: 4 | Val loss: 8.9582 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 12:44:46 (running for 00:46:52.68)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.332 |      0.256 |                   48 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.269589552238806
[2m[36m(func pid=132175)[0m top5: 0.761660447761194
[2m[36m(func pid=132175)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=132175)[0m f1_macro: 0.26446508101361343
[2m[36m(func pid=132175)[0m f1_weighted: 0.29803744554603595
[2m[36m(func pid=132175)[0m f1_per_class: [0.347, 0.305, 0.333, 0.187, 0.068, 0.297, 0.423, 0.271, 0.149, 0.263]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.3225 | Steps: 4 | Val loss: 9.9838 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 12:44:52 (running for 00:46:58.60)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.691 |      0.264 |                   49 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.2728544776119403
[2m[36m(func pid=132175)[0m top5: 0.7854477611940298
[2m[36m(func pid=132175)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=132175)[0m f1_macro: 0.32244299860785264
[2m[36m(func pid=132175)[0m f1_weighted: 0.30303748492351
[2m[36m(func pid=132175)[0m f1_per_class: [0.5, 0.298, 0.429, 0.107, 0.036, 0.368, 0.456, 0.306, 0.208, 0.515]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.0151 | Steps: 4 | Val loss: 7.9238 | Batch size: 32 | lr: 0.1 | Duration: 3.23s
== Status ==
Current time: 2024-01-07 12:44:58 (running for 00:47:04.54)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.322 |      0.322 |                   50 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.3530783582089552
[2m[36m(func pid=132175)[0m top5: 0.8306902985074627
[2m[36m(func pid=132175)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=132175)[0m f1_macro: 0.33640191155853355
[2m[36m(func pid=132175)[0m f1_weighted: 0.33371141285617234
[2m[36m(func pid=132175)[0m f1_per_class: [0.537, 0.49, 0.191, 0.177, 0.141, 0.368, 0.378, 0.303, 0.256, 0.523]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 0.4590 | Steps: 4 | Val loss: 7.0195 | Batch size: 32 | lr: 0.1 | Duration: 3.31s
== Status ==
Current time: 2024-01-07 12:45:04 (running for 00:47:10.28)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.015 |      0.336 |                   51 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.36380597014925375
[2m[36m(func pid=132175)[0m top5: 0.8591417910447762
[2m[36m(func pid=132175)[0m f1_micro: 0.3638059701492538
[2m[36m(func pid=132175)[0m f1_macro: 0.3325890499600265
[2m[36m(func pid=132175)[0m f1_weighted: 0.3639443413704467
[2m[36m(func pid=132175)[0m f1_per_class: [0.484, 0.506, 0.202, 0.295, 0.163, 0.374, 0.377, 0.253, 0.221, 0.451]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.6462 | Steps: 4 | Val loss: 6.4455 | Batch size: 32 | lr: 0.1 | Duration: 3.40s
== Status ==
Current time: 2024-01-07 12:45:10 (running for 00:47:16.39)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.459 |      0.333 |                   52 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.36800373134328357
[2m[36m(func pid=132175)[0m top5: 0.8754664179104478
[2m[36m(func pid=132175)[0m f1_micro: 0.3680037313432836
[2m[36m(func pid=132175)[0m f1_macro: 0.3428028617656612
[2m[36m(func pid=132175)[0m f1_weighted: 0.3893128564119076
[2m[36m(func pid=132175)[0m f1_per_class: [0.506, 0.468, 0.361, 0.444, 0.178, 0.318, 0.367, 0.259, 0.187, 0.339]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 0.7466 | Steps: 4 | Val loss: 7.2180 | Batch size: 32 | lr: 0.1 | Duration: 3.22s
== Status ==
Current time: 2024-01-07 12:45:16 (running for 00:47:22.69)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.646 |      0.343 |                   53 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.34328358208955223
[2m[36m(func pid=132175)[0m top5: 0.8325559701492538
[2m[36m(func pid=132175)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=132175)[0m f1_macro: 0.2988675745157645
[2m[36m(func pid=132175)[0m f1_weighted: 0.36263652998451823
[2m[36m(func pid=132175)[0m f1_per_class: [0.268, 0.304, 0.37, 0.481, 0.168, 0.279, 0.362, 0.315, 0.189, 0.253]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 0.1799 | Steps: 4 | Val loss: 10.4127 | Batch size: 32 | lr: 0.1 | Duration: 3.25s
== Status ==
Current time: 2024-01-07 12:45:22 (running for 00:47:28.42)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.747 |      0.299 |                   54 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.22014925373134328
[2m[36m(func pid=132175)[0m top5: 0.7714552238805971
[2m[36m(func pid=132175)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=132175)[0m f1_macro: 0.21004657902258628
[2m[36m(func pid=132175)[0m f1_weighted: 0.2572878853191547
[2m[36m(func pid=132175)[0m f1_per_class: [0.086, 0.175, 0.303, 0.334, 0.139, 0.213, 0.297, 0.111, 0.226, 0.216]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.1127 | Steps: 4 | Val loss: 9.9326 | Batch size: 32 | lr: 0.1 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 12:45:28 (running for 00:47:34.12)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.18  |      0.21  |                   55 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.2392723880597015
[2m[36m(func pid=132175)[0m top5: 0.7402052238805971
[2m[36m(func pid=132175)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=132175)[0m f1_macro: 0.2184625966976656
[2m[36m(func pid=132175)[0m f1_weighted: 0.27131433550189377
[2m[36m(func pid=132175)[0m f1_per_class: [0.115, 0.234, 0.231, 0.365, 0.112, 0.234, 0.252, 0.212, 0.26, 0.17]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 0.5211 | Steps: 4 | Val loss: 9.8907 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 12:45:33 (running for 00:47:39.63)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.113 |      0.218 |                   56 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.25046641791044777
[2m[36m(func pid=132175)[0m top5: 0.7271455223880597
[2m[36m(func pid=132175)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=132175)[0m f1_macro: 0.23069700431398515
[2m[36m(func pid=132175)[0m f1_weighted: 0.27490254092751776
[2m[36m(func pid=132175)[0m f1_per_class: [0.254, 0.25, 0.149, 0.383, 0.07, 0.256, 0.208, 0.315, 0.197, 0.226]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 0.4809 | Steps: 4 | Val loss: 10.5697 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 12:45:39 (running for 00:47:45.69)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.521 |      0.231 |                   57 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.23600746268656717
[2m[36m(func pid=132175)[0m top5: 0.7290111940298507
[2m[36m(func pid=132175)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=132175)[0m f1_macro: 0.23217631447283488
[2m[36m(func pid=132175)[0m f1_weighted: 0.25597023283609455
[2m[36m(func pid=132175)[0m f1_per_class: [0.31, 0.309, 0.063, 0.316, 0.066, 0.281, 0.152, 0.331, 0.227, 0.267]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.4590 | Steps: 4 | Val loss: 8.8840 | Batch size: 32 | lr: 0.1 | Duration: 3.27s
== Status ==
Current time: 2024-01-07 12:45:45 (running for 00:47:51.50)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.481 |      0.232 |                   58 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.2896455223880597
[2m[36m(func pid=132175)[0m top5: 0.7952425373134329
[2m[36m(func pid=132175)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=132175)[0m f1_macro: 0.2740638417692967
[2m[36m(func pid=132175)[0m f1_weighted: 0.3184604602405486
[2m[36m(func pid=132175)[0m f1_per_class: [0.34, 0.402, 0.068, 0.324, 0.09, 0.298, 0.283, 0.333, 0.286, 0.317]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 0.4828 | Steps: 4 | Val loss: 7.7071 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 12:45:51 (running for 00:47:57.29)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.459 |      0.274 |                   59 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.3400186567164179
[2m[36m(func pid=132175)[0m top5: 0.8348880597014925
[2m[36m(func pid=132175)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=132175)[0m f1_macro: 0.312842109874013
[2m[36m(func pid=132175)[0m f1_weighted: 0.36516170589502944
[2m[36m(func pid=132175)[0m f1_per_class: [0.368, 0.377, 0.236, 0.418, 0.084, 0.254, 0.372, 0.344, 0.303, 0.372]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 0.9920 | Steps: 4 | Val loss: 7.2644 | Batch size: 32 | lr: 0.1 | Duration: 3.18s
== Status ==
Current time: 2024-01-07 12:45:57 (running for 00:48:03.06)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.483 |      0.313 |                   60 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.35261194029850745
[2m[36m(func pid=132175)[0m top5: 0.8722014925373134
[2m[36m(func pid=132175)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=132175)[0m f1_macro: 0.3425849066585319
[2m[36m(func pid=132175)[0m f1_weighted: 0.37640603249697885
[2m[36m(func pid=132175)[0m f1_per_class: [0.395, 0.34, 0.45, 0.445, 0.092, 0.26, 0.396, 0.327, 0.325, 0.395]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 0.7274 | Steps: 4 | Val loss: 7.1435 | Batch size: 32 | lr: 0.1 | Duration: 3.26s
== Status ==
Current time: 2024-01-07 12:46:02 (running for 00:48:08.76)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.992 |      0.343 |                   61 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.3726679104477612
[2m[36m(func pid=132175)[0m top5: 0.9011194029850746
[2m[36m(func pid=132175)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=132175)[0m f1_macro: 0.3464869312120151
[2m[36m(func pid=132175)[0m f1_weighted: 0.39505498872968164
[2m[36m(func pid=132175)[0m f1_per_class: [0.403, 0.351, 0.486, 0.417, 0.113, 0.257, 0.49, 0.289, 0.301, 0.357]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.0236 | Steps: 4 | Val loss: 9.1826 | Batch size: 32 | lr: 0.1 | Duration: 3.15s
== Status ==
Current time: 2024-01-07 12:46:08 (running for 00:48:14.58)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.727 |      0.346 |                   62 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.2980410447761194
[2m[36m(func pid=132175)[0m top5: 0.8208955223880597
[2m[36m(func pid=132175)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=132175)[0m f1_macro: 0.2996134616511654
[2m[36m(func pid=132175)[0m f1_weighted: 0.29939364362716797
[2m[36m(func pid=132175)[0m f1_per_class: [0.393, 0.357, 0.488, 0.155, 0.107, 0.282, 0.413, 0.316, 0.181, 0.306]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 0.7501 | Steps: 4 | Val loss: 10.8804 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 12:46:14 (running for 00:48:20.24)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.024 |      0.3   |                   63 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.2453358208955224
[2m[36m(func pid=132175)[0m top5: 0.7467350746268657
[2m[36m(func pid=132175)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=132175)[0m f1_macro: 0.25904138756419826
[2m[36m(func pid=132175)[0m f1_weighted: 0.22438900047192384
[2m[36m(func pid=132175)[0m f1_per_class: [0.417, 0.358, 0.409, 0.136, 0.115, 0.299, 0.178, 0.303, 0.176, 0.198]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.4707 | Steps: 4 | Val loss: 10.8577 | Batch size: 32 | lr: 0.1 | Duration: 3.32s
== Status ==
Current time: 2024-01-07 12:46:19 (running for 00:48:25.93)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.75  |      0.259 |                   64 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.2560634328358209
[2m[36m(func pid=132175)[0m top5: 0.7444029850746269
[2m[36m(func pid=132175)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=132175)[0m f1_macro: 0.2586626389393577
[2m[36m(func pid=132175)[0m f1_weighted: 0.2518529317318259
[2m[36m(func pid=132175)[0m f1_per_class: [0.403, 0.266, 0.37, 0.377, 0.103, 0.279, 0.104, 0.295, 0.232, 0.158]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 0.2779 | Steps: 4 | Val loss: 11.3934 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 12:46:26 (running for 00:48:32.05)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.471 |      0.259 |                   65 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.27845149253731344
[2m[36m(func pid=132175)[0m top5: 0.7238805970149254
[2m[36m(func pid=132175)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=132175)[0m f1_macro: 0.25409475877164933
[2m[36m(func pid=132175)[0m f1_weighted: 0.24869684782556595
[2m[36m(func pid=132175)[0m f1_per_class: [0.278, 0.141, 0.29, 0.486, 0.09, 0.278, 0.048, 0.387, 0.274, 0.269]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 5.0163 | Steps: 4 | Val loss: 10.0687 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 12:46:31 (running for 00:48:37.69)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.278 |      0.254 |                   66 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.27472014925373134
[2m[36m(func pid=132175)[0m top5: 0.7742537313432836
[2m[36m(func pid=132175)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=132175)[0m f1_macro: 0.26629801964001526
[2m[36m(func pid=132175)[0m f1_weighted: 0.2752352989273481
[2m[36m(func pid=132175)[0m f1_per_class: [0.154, 0.231, 0.4, 0.437, 0.117, 0.314, 0.146, 0.271, 0.243, 0.35]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.8367 | Steps: 4 | Val loss: 9.9281 | Batch size: 32 | lr: 0.1 | Duration: 3.41s
== Status ==
Current time: 2024-01-07 12:46:37 (running for 00:48:43.25)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  5.016 |      0.266 |                   67 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.3306902985074627
[2m[36m(func pid=132175)[0m top5: 0.7994402985074627
[2m[36m(func pid=132175)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=132175)[0m f1_macro: 0.2865596105097787
[2m[36m(func pid=132175)[0m f1_weighted: 0.30822146391350475
[2m[36m(func pid=132175)[0m f1_per_class: [0.242, 0.426, 0.357, 0.181, 0.043, 0.196, 0.427, 0.206, 0.287, 0.5]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.6952 | Steps: 4 | Val loss: 8.5730 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
== Status ==
Current time: 2024-01-07 12:46:43 (running for 00:48:49.33)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.837 |      0.287 |                   68 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.35447761194029853
[2m[36m(func pid=132175)[0m top5: 0.816231343283582
[2m[36m(func pid=132175)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=132175)[0m f1_macro: 0.2991480472424546
[2m[36m(func pid=132175)[0m f1_weighted: 0.380412873702361
[2m[36m(func pid=132175)[0m f1_per_class: [0.467, 0.448, 0.162, 0.405, 0.066, 0.095, 0.475, 0.267, 0.211, 0.395]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.4614 | Steps: 4 | Val loss: 8.2179 | Batch size: 32 | lr: 0.1 | Duration: 3.36s
== Status ==
Current time: 2024-01-07 12:46:49 (running for 00:48:55.11)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.695 |      0.299 |                   69 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.3414179104477612
[2m[36m(func pid=132175)[0m top5: 0.8418843283582089
[2m[36m(func pid=132175)[0m f1_micro: 0.3414179104477612
[2m[36m(func pid=132175)[0m f1_macro: 0.288059827827447
[2m[36m(func pid=132175)[0m f1_weighted: 0.3431452966123501
[2m[36m(func pid=132175)[0m f1_per_class: [0.476, 0.272, 0.131, 0.513, 0.129, 0.204, 0.298, 0.335, 0.199, 0.324]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.3045 | Steps: 4 | Val loss: 9.0019 | Batch size: 32 | lr: 0.1 | Duration: 3.19s
== Status ==
Current time: 2024-01-07 12:46:55 (running for 00:49:01.01)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.461 |      0.288 |                   70 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.2896455223880597
[2m[36m(func pid=132175)[0m top5: 0.7957089552238806
[2m[36m(func pid=132175)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=132175)[0m f1_macro: 0.26967984353583796
[2m[36m(func pid=132175)[0m f1_weighted: 0.3035488779350987
[2m[36m(func pid=132175)[0m f1_per_class: [0.444, 0.242, 0.109, 0.436, 0.192, 0.289, 0.238, 0.264, 0.198, 0.286]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.2217 | Steps: 4 | Val loss: 10.2003 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 12:47:00 (running for 00:49:06.67)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.305 |      0.27  |                   71 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.2737873134328358
[2m[36m(func pid=132175)[0m top5: 0.7425373134328358
[2m[36m(func pid=132175)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=132175)[0m f1_macro: 0.24818968071952266
[2m[36m(func pid=132175)[0m f1_weighted: 0.28732928387403983
[2m[36m(func pid=132175)[0m f1_per_class: [0.195, 0.22, 0.115, 0.292, 0.206, 0.332, 0.322, 0.305, 0.209, 0.286]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 0.3819 | Steps: 4 | Val loss: 11.3658 | Batch size: 32 | lr: 0.1 | Duration: 3.29s
== Status ==
Current time: 2024-01-07 12:47:06 (running for 00:49:12.47)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  1.222 |      0.248 |                   72 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.2332089552238806
[2m[36m(func pid=132175)[0m top5: 0.7131529850746269
[2m[36m(func pid=132175)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=132175)[0m f1_macro: 0.22353541878154132
[2m[36m(func pid=132175)[0m f1_weighted: 0.26787017899485394
[2m[36m(func pid=132175)[0m f1_per_class: [0.133, 0.18, 0.137, 0.261, 0.08, 0.218, 0.353, 0.328, 0.21, 0.336]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 0.9381 | Steps: 4 | Val loss: 10.0293 | Batch size: 32 | lr: 0.1 | Duration: 3.39s
== Status ==
Current time: 2024-01-07 12:47:12 (running for 00:49:18.37)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.382 |      0.224 |                   73 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=132175)[0m top1: 0.27052238805970147
[2m[36m(func pid=132175)[0m top5: 0.7672574626865671
[2m[36m(func pid=132175)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=132175)[0m f1_macro: 0.2441303146918626
[2m[36m(func pid=132175)[0m f1_weighted: 0.29838713735691735
[2m[36m(func pid=132175)[0m f1_per_class: [0.182, 0.218, 0.169, 0.439, 0.071, 0.195, 0.27, 0.326, 0.226, 0.346]
[2m[36m(func pid=132175)[0m 
[2m[36m(func pid=132175)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.1483 | Steps: 4 | Val loss: 8.9138 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 12:47:18 (running for 00:49:24.42)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.3205
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00023 | RUNNING    | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  0.938 |      0.244 |                   74 |
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 12:47:18 (running for 00:49:24.87)
Memory usage on this node: 16.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: 0.32025000000000003
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/120.02 GiB heap, 0.0/55.43 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/BarlowTwins
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_9b9e8_00000 | TERMINATED | 192.168.7.53:21163  | 0.0001 |       0.99 |         0      |  1.328 |      0.297 |                  100 |
| train_9b9e8_00001 | TERMINATED | 192.168.7.53:21543  | 0.001  |       0.99 |         0      |  0.422 |      0.293 |                  100 |
| train_9b9e8_00002 | TERMINATED | 192.168.7.53:21969  | 0.01   |       0.99 |         0      |  0.92  |      0.313 |                   75 |
| train_9b9e8_00003 | TERMINATED | 192.168.7.53:22392  | 0.1    |       0.99 |         0      | 11.958 |      0.295 |                  100 |
| train_9b9e8_00004 | TERMINATED | 192.168.7.53:39840  | 0.0001 |       0.9  |         0      |  2.576 |      0.201 |                   75 |
| train_9b9e8_00005 | TERMINATED | 192.168.7.53:45540  | 0.001  |       0.9  |         0      |  1.114 |      0.329 |                  100 |
| train_9b9e8_00006 | TERMINATED | 192.168.7.53:46161  | 0.01   |       0.9  |         0      |  0.379 |      0.263 |                   75 |
| train_9b9e8_00007 | TERMINATED | 192.168.7.53:47180  | 0.1    |       0.9  |         0      |  1.804 |      0.294 |                   75 |
| train_9b9e8_00008 | TERMINATED | 192.168.7.53:58891  | 0.0001 |       0.99 |         0.0001 |  1.432 |      0.307 |                   75 |
| train_9b9e8_00009 | TERMINATED | 192.168.7.53:64137  | 0.001  |       0.99 |         0.0001 |  0.545 |      0.313 |                   75 |
| train_9b9e8_00010 | TERMINATED | 192.168.7.53:65548  | 0.01   |       0.99 |         0.0001 |  3.502 |      0.299 |                   75 |
| train_9b9e8_00011 | TERMINATED | 192.168.7.53:70356  | 0.1    |       0.99 |         0.0001 |  6.593 |      0.321 |                   75 |
| train_9b9e8_00012 | TERMINATED | 192.168.7.53:77753  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.205 |                   75 |
| train_9b9e8_00013 | TERMINATED | 192.168.7.53:82573  | 0.001  |       0.9  |         0.0001 |  1.312 |      0.302 |                   75 |
| train_9b9e8_00014 | TERMINATED | 192.168.7.53:84655  | 0.01   |       0.9  |         0.0001 |  0.485 |      0.315 |                   75 |
| train_9b9e8_00015 | TERMINATED | 192.168.7.53:89025  | 0.1    |       0.9  |         0.0001 |  1.55  |      0.269 |                   75 |
| train_9b9e8_00016 | TERMINATED | 192.168.7.53:96422  | 0.0001 |       0.99 |         1e-05  |  1.571 |      0.286 |                   75 |
| train_9b9e8_00017 | TERMINATED | 192.168.7.53:101169 | 0.001  |       0.99 |         1e-05  |  0.62  |      0.29  |                   75 |
| train_9b9e8_00018 | TERMINATED | 192.168.7.53:103447 | 0.01   |       0.99 |         1e-05  |  1.034 |      0.224 |                   75 |
| train_9b9e8_00019 | TERMINATED | 192.168.7.53:107845 | 0.1    |       0.99 |         1e-05  |  4.057 |      0.253 |                  100 |
| train_9b9e8_00020 | TERMINATED | 192.168.7.53:115877 | 0.0001 |       0.9  |         1e-05  |  2.596 |      0.208 |                   75 |
| train_9b9e8_00021 | TERMINATED | 192.168.7.53:119092 | 0.001  |       0.9  |         1e-05  |  1.28  |      0.321 |                  100 |
| train_9b9e8_00022 | TERMINATED | 192.168.7.53:121943 | 0.01   |       0.9  |         1e-05  |  0.417 |      0.31  |                   75 |
| train_9b9e8_00023 | TERMINATED | 192.168.7.53:132175 | 0.1    |       0.9  |         1e-05  |  2.148 |      0.287 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+


2024-01-07 12:47:18,951	INFO tune.py:798 -- Total run time: 2966.01 seconds (2964.85 seconds for the tuning loop).
[2m[36m(func pid=132175)[0m top1: 0.33675373134328357
[2m[36m(func pid=132175)[0m top5: 0.7980410447761194
[2m[36m(func pid=132175)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=132175)[0m f1_macro: 0.2873315927985965
[2m[36m(func pid=132175)[0m f1_weighted: 0.3383918733300791
[2m[36m(func pid=132175)[0m f1_per_class: [0.264, 0.376, 0.176, 0.486, 0.111, 0.313, 0.22, 0.29, 0.242, 0.395]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341347.1 ON aap04 CANCELLED AT 2024-01-07T12:47:26 ***
srun: error: aap04: task 0: Exited with exit code 1

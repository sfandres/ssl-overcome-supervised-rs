IP Head: 192.168.7.53:6379
STARTING HEAD at aap04
2024-01-07 10:16:44,686	INFO usage_lib.py:461 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-01-07 10:16:44,686	INFO scripts.py:710 -- Local node IP: 192.168.7.53
2024-01-07 10:16:47,214	SUCC scripts.py:747 -- --------------------
2024-01-07 10:16:47,214	SUCC scripts.py:748 -- Ray runtime started.
2024-01-07 10:16:47,214	SUCC scripts.py:749 -- --------------------
2024-01-07 10:16:47,214	INFO scripts.py:751 -- Next steps
2024-01-07 10:16:47,214	INFO scripts.py:752 -- To connect to this Ray runtime from another node, run
2024-01-07 10:16:47,214	INFO scripts.py:755 --   ray start --address='192.168.7.53:6379'
2024-01-07 10:16:47,214	INFO scripts.py:771 -- Alternatively, use the following Python code:
2024-01-07 10:16:47,214	INFO scripts.py:773 -- import ray
2024-01-07 10:16:47,214	INFO scripts.py:777 -- ray.init(address='auto', _node_ip_address='192.168.7.53')
2024-01-07 10:16:47,214	INFO scripts.py:790 -- To see the status of the cluster, use
2024-01-07 10:16:47,215	INFO scripts.py:791 --   ray status
2024-01-07 10:16:47,215	INFO scripts.py:801 -- If connection fails, check your firewall settings and network configuration.
2024-01-07 10:16:47,215	INFO scripts.py:809 -- To terminate the Ray runtime, run
2024-01-07 10:16:47,215	INFO scripts.py:810 --   ray stop
2024-01-07 10:16:47,215	INFO scripts.py:891 -- --block
2024-01-07 10:16:47,215	INFO scripts.py:892 -- This command will now block forever until terminated by a signal.
2024-01-07 10:16:47,215	INFO scripts.py:895 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.

torch initial seed:              15478069989963603725
torch current seed:              42
torch.cuda.is_available():       True
torch.cuda.device_count():       4
torch.cuda.current_device():     0
torch.cuda.device(0):            <torch.cuda.device object at 0x7fecc5d8f0d0>
torch.cuda.get_device_name(0):   Tesla V100-PCIE-32GB
torch.backends.cudnn.benchmark:  False
os.sched_getaffinity:            72
os.cpu_count():                  72

model_name:          Supervised
task_name:           multiclass
backbone_name:       resnet18
input_data:          None
dataset_name:        Sentinel2AndaluciaLULC
dataset_level:       Level_N2
train_rate:          10
epochs:              100
learning_rate:       0.01
save_every:          5
batch_size:          32
num_workers:         4
ini_weights:         random
seed:                42
dropout:             None
transfer_learning:   LP
show:                False
verbose:             False
balanced_dataset:    False
torch_compile:       False
distributed:         False
ray_tune:            gridsearch
load_best_hyperparameters: False
grace_period:        75
num_samples_trials:  1
gpus_per_trial:      1

Initial imbalanced dataset:
Diff. classes --> [ 1 21 22 23 31 35 41 42 47 51]
Samples/class --> [10 10 10 10 10 10 10 10 10 10]

Creating the sample distribution plot...
Sample distribution computation in train dataset (s): 2.04
Resulting balanced dataloader:
Diff. classes     --> [0 1 2 3 4 5 6 7 8 9]
New samples/class --> [10 10 10 10 10 10 10 10 10 10]
Done!

Supervised model resnet18 with random weights
Old final fully-connected layer: Linear(in_features=512, out_features=1000, bias=True)
No dropout layer
New final fully-connected layer: Linear(in_features=512, out_features=10, bias=True)
Linear probing adjusted
Device: 0

Setting a new configuration using tune.grid_search

2024-01-07 10:17:29,266	INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 192.168.7.53:6379...
2024-01-07 10:17:29,279	INFO worker.py:1553 -- Connected to Ray cluster.
2024-01-07 10:17:47,323	WARNING worker.py:1866 -- Warning: The actor ImplicitFunc is very large (44 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.
== Status ==
Current time: 2024-01-07 10:17:47 (running for 00:00:17.16)
Memory usage on this node: 13.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (23 PENDING, 1 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |
| train_952df_00001 | PENDING  |                     | 0.001  |       0.99 |         0      |
| train_952df_00002 | PENDING  |                     | 0.01   |       0.99 |         0      |
| train_952df_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=151704)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=151704)[0m Configuration completed!
[2m[36m(func pid=151704)[0m New optimizer parameters:
[2m[36m(func pid=151704)[0m SGD (
[2m[36m(func pid=151704)[0m Parameter Group 0
[2m[36m(func pid=151704)[0m     dampening: 0
[2m[36m(func pid=151704)[0m     differentiable: False
[2m[36m(func pid=151704)[0m     foreach: None
[2m[36m(func pid=151704)[0m     lr: 0.0001
[2m[36m(func pid=151704)[0m     maximize: False
[2m[36m(func pid=151704)[0m     momentum: 0.99
[2m[36m(func pid=151704)[0m     nesterov: False
[2m[36m(func pid=151704)[0m     weight_decay: 0
[2m[36m(func pid=151704)[0m )
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0493 | Steps: 4 | Val loss: 2.3270 | Batch size: 32 | lr: 0.0001 | Duration: 4.71s
[2m[36m(func pid=151704)[0m top1: 0.14738805970149255
[2m[36m(func pid=151704)[0m top5: 0.5447761194029851
[2m[36m(func pid=151704)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=151704)[0m f1_macro: 0.04355369893622189
[2m[36m(func pid=151704)[0m f1_weighted: 0.0824108860177666
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.0, 0.0, 0.259, 0.0, 0.0, 0.0, 0.177, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 10:17:55 (running for 00:00:25.24)
Memory usage on this node: 15.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (22 PENDING, 2 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |
| train_952df_00002 | PENDING  |                     | 0.01   |       0.99 |         0      |
| train_952df_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152082)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152082)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=152082)[0m Configuration completed!
[2m[36m(func pid=152082)[0m New optimizer parameters:
[2m[36m(func pid=152082)[0m SGD (
[2m[36m(func pid=152082)[0m Parameter Group 0
[2m[36m(func pid=152082)[0m     dampening: 0
[2m[36m(func pid=152082)[0m     differentiable: False
[2m[36m(func pid=152082)[0m     foreach: None
[2m[36m(func pid=152082)[0m     lr: 0.001
[2m[36m(func pid=152082)[0m     maximize: False
[2m[36m(func pid=152082)[0m     momentum: 0.99
[2m[36m(func pid=152082)[0m     nesterov: False
[2m[36m(func pid=152082)[0m     weight_decay: 0
[2m[36m(func pid=152082)[0m )
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0781 | Steps: 4 | Val loss: 2.3302 | Batch size: 32 | lr: 0.001 | Duration: 4.69s
[2m[36m(func pid=152082)[0m top1: 0.12779850746268656
[2m[36m(func pid=152082)[0m top5: 0.49347014925373134
[2m[36m(func pid=152082)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=152082)[0m f1_macro: 0.05303652686827263
[2m[36m(func pid=152082)[0m f1_weighted: 0.08563590144617961
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.005, 0.004, 0.25, 0.0, 0.0, 0.0, 0.255, 0.0, 0.016]
== Status ==
Current time: 2024-01-07 10:18:03 (running for 00:00:33.38)
Memory usage on this node: 18.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (21 PENDING, 3 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |
| train_952df_00003 | PENDING  |                     | 0.1    |       0.99 |         0      |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152502)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152502)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=152502)[0m Configuration completed!
[2m[36m(func pid=152502)[0m New optimizer parameters:
[2m[36m(func pid=152502)[0m SGD (
[2m[36m(func pid=152502)[0m Parameter Group 0
[2m[36m(func pid=152502)[0m     dampening: 0
[2m[36m(func pid=152502)[0m     differentiable: False
[2m[36m(func pid=152502)[0m     foreach: None
[2m[36m(func pid=152502)[0m     lr: 0.01
[2m[36m(func pid=152502)[0m     maximize: False
[2m[36m(func pid=152502)[0m     momentum: 0.99
[2m[36m(func pid=152502)[0m     nesterov: False
[2m[36m(func pid=152502)[0m     weight_decay: 0
[2m[36m(func pid=152502)[0m )
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9119 | Steps: 4 | Val loss: 2.3506 | Batch size: 32 | lr: 0.01 | Duration: 4.71s
[2m[36m(func pid=152502)[0m top1: 0.009794776119402986
[2m[36m(func pid=152502)[0m top5: 0.5489738805970149
[2m[36m(func pid=152502)[0m f1_micro: 0.009794776119402986
[2m[36m(func pid=152502)[0m f1_macro: 0.0038402206316374256
[2m[36m(func pid=152502)[0m f1_weighted: 0.00046134893011616366
[2m[36m(func pid=152502)[0m f1_per_class: [0.016, 0.0, 0.023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 10:18:11 (running for 00:00:41.48)
Memory usage on this node: 20.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |
|-------------------+----------+---------------------+--------+------------+----------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |
+-------------------+----------+---------------------+--------+------------+----------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 10:18:19 (running for 00:00:49.68)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |        |            |                      |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |        |            |                      |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.912 |      0.004 |                    1 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |        |            |                      |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152923)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=152923)[0m Configuration completed!
[2m[36m(func pid=152923)[0m New optimizer parameters:
[2m[36m(func pid=152923)[0m SGD (
[2m[36m(func pid=152923)[0m Parameter Group 0
[2m[36m(func pid=152923)[0m     dampening: 0
[2m[36m(func pid=152923)[0m     differentiable: False
[2m[36m(func pid=152923)[0m     foreach: None
[2m[36m(func pid=152923)[0m     lr: 0.1
[2m[36m(func pid=152923)[0m     maximize: False
[2m[36m(func pid=152923)[0m     momentum: 0.99
[2m[36m(func pid=152923)[0m     nesterov: False
[2m[36m(func pid=152923)[0m     weight_decay: 0
[2m[36m(func pid=152923)[0m )
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9861 | Steps: 4 | Val loss: 2.2240 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0523 | Steps: 4 | Val loss: 2.3193 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8840 | Steps: 4 | Val loss: 2.3353 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 6.4041 | Steps: 4 | Val loss: 3.4455 | Batch size: 32 | lr: 0.1 | Duration: 4.36s
== Status ==
Current time: 2024-01-07 10:18:25 (running for 00:00:54.71)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  3.049 |      0.044 |                    1 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  3.078 |      0.053 |                    1 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.912 |      0.004 |                    1 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |        |            |                      |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152082)[0m top1: 0.00792910447761194
[2m[36m(func pid=152082)[0m top5: 0.5307835820895522
[2m[36m(func pid=152082)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=152082)[0m f1_macro: 0.009734167709637045
[2m[36m(func pid=152082)[0m f1_weighted: 0.002892542917452786
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.085, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m top1: 0.03404850746268657
[2m[36m(func pid=152502)[0m top5: 0.8810634328358209
[2m[36m(func pid=152502)[0m f1_micro: 0.03404850746268657
[2m[36m(func pid=152502)[0m f1_macro: 0.03844832396082635
[2m[36m(func pid=152502)[0m f1_weighted: 0.004005892067552177
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.0, 0.323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062, 0.0]
[2m[36m(func pid=151704)[0m top1: 0.19169776119402984
[2m[36m(func pid=151704)[0m top5: 0.5769589552238806
[2m[36m(func pid=151704)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=151704)[0m f1_macro: 0.040682329869426444
[2m[36m(func pid=151704)[0m f1_weighted: 0.09941111647837307
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.0, 0.0, 0.347, 0.0, 0.0, 0.0, 0.043, 0.0, 0.017]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m top1: 0.2980410447761194
[2m[36m(func pid=152923)[0m top5: 0.7117537313432836
[2m[36m(func pid=152923)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=152923)[0m f1_macro: 0.04592166726554078
[2m[36m(func pid=152923)[0m f1_weighted: 0.136865416896831
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0603 | Steps: 4 | Val loss: 2.3702 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 4.1417 | Steps: 4 | Val loss: 2.2578 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0314 | Steps: 4 | Val loss: 2.3294 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 37.7601 | Steps: 4 | Val loss: 13.8007 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=152082)[0m top1: 0.009794776119402986
[2m[36m(func pid=152082)[0m top5: 0.5018656716417911
[2m[36m(func pid=152082)[0m f1_micro: 0.009794776119402986
[2m[36m(func pid=152082)[0m f1_macro: 0.009249881544657664
[2m[36m(func pid=152082)[0m f1_weighted: 0.00271427239910964
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0]
[2m[36m(func pid=152082)[0m 
== Status ==
Current time: 2024-01-07 10:18:30 (running for 00:01:00.35)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  3.052 |      0.041 |                    2 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  3.06  |      0.009 |                    3 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.986 |      0.038 |                    2 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  6.404 |      0.046 |                    1 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.21361940298507462
[2m[36m(func pid=151704)[0m top5: 0.5471082089552238
[2m[36m(func pid=151704)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=151704)[0m f1_macro: 0.04084586026120445
[2m[36m(func pid=151704)[0m f1_weighted: 0.1056233316154493
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.0, 0.0, 0.373, 0.0, 0.0, 0.0, 0.027, 0.0, 0.009]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m top1: 0.12686567164179105
[2m[36m(func pid=152502)[0m top5: 0.5438432835820896
[2m[36m(func pid=152502)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=152502)[0m f1_macro: 0.08046237063413744
[2m[36m(func pid=152502)[0m f1_weighted: 0.06139812352296306
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.105, 0.294, 0.016, 0.0, 0.296, 0.0, 0.046, 0.0, 0.048]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m top1: 0.11520522388059702
[2m[36m(func pid=152923)[0m top5: 0.22901119402985073
[2m[36m(func pid=152923)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=152923)[0m f1_macro: 0.025878406708595387
[2m[36m(func pid=152923)[0m f1_weighted: 0.025243515128758724
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.205, 0.0, 0.0, 0.053, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9816 | Steps: 4 | Val loss: 2.3342 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9475 | Steps: 4 | Val loss: 2.3027 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.7482 | Steps: 4 | Val loss: 2.6560 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 36.9283 | Steps: 4 | Val loss: 7.7386 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 10:18:35 (running for 00:01:05.53)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  3.031 |      0.041 |                    3 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  3.06  |      0.009 |                    3 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  3.748 |      0.024 |                    4 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 37.76  |      0.026 |                    2 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152502)[0m top1: 0.11194029850746269
[2m[36m(func pid=152502)[0m top5: 0.5004664179104478
[2m[36m(func pid=152502)[0m f1_micro: 0.11194029850746269
[2m[36m(func pid=152502)[0m f1_macro: 0.0235300051816085
[2m[36m(func pid=152502)[0m f1_weighted: 0.024804807658021778
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.215, 0.0, 0.0, 0.0, 0.02]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=151704)[0m top1: 0.21361940298507462
[2m[36m(func pid=151704)[0m top5: 0.5317164179104478
[2m[36m(func pid=151704)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=151704)[0m f1_macro: 0.04233105159354848
[2m[36m(func pid=151704)[0m f1_weighted: 0.10693084530831624
[2m[36m(func pid=151704)[0m f1_per_class: [0.018, 0.0, 0.0, 0.381, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.039645522388059705
[2m[36m(func pid=152082)[0m top5: 0.5634328358208955
[2m[36m(func pid=152082)[0m f1_micro: 0.039645522388059705
[2m[36m(func pid=152082)[0m f1_macro: 0.0247379544351093
[2m[36m(func pid=152082)[0m f1_weighted: 0.024879063887737397
[2m[36m(func pid=152082)[0m f1_per_class: [0.069, 0.0, 0.022, 0.074, 0.0, 0.0, 0.0, 0.0, 0.082, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.300839552238806
[2m[36m(func pid=152923)[0m top5: 0.5223880597014925
[2m[36m(func pid=152923)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=152923)[0m f1_macro: 0.07131337064422565
[2m[36m(func pid=152923)[0m f1_weighted: 0.14195512083765982
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.0, 0.242, 0.0, 0.0, 0.0, 0.471, 0.0, 0.0, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9798 | Steps: 4 | Val loss: 2.3502 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 5.0360 | Steps: 4 | Val loss: 2.7808 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.1092 | Steps: 4 | Val loss: 2.1828 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 32.6996 | Steps: 4 | Val loss: 15.4453 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 10:18:41 (running for 00:01:10.88)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.982 |      0.042 |                    4 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.947 |      0.025 |                    4 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.036 |      0.012 |                    5 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 36.928 |      0.071 |                    3 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152502)[0m top1: 0.02845149253731343
[2m[36m(func pid=152502)[0m top5: 0.6124067164179104
[2m[36m(func pid=152502)[0m f1_micro: 0.02845149253731343
[2m[36m(func pid=152502)[0m f1_macro: 0.012362996896618437
[2m[36m(func pid=152502)[0m f1_weighted: 0.029838022573365527
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.016, 0.0, 0.0, 0.017, 0.0, 0.09, 0.0, 0.0, 0.0]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=151704)[0m top1: 0.16184701492537312
[2m[36m(func pid=151704)[0m top5: 0.5144589552238806
[2m[36m(func pid=151704)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=151704)[0m f1_macro: 0.05024273173022015
[2m[36m(func pid=151704)[0m f1_weighted: 0.09831149673494799
[2m[36m(func pid=151704)[0m f1_per_class: [0.133, 0.0, 0.028, 0.342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.19449626865671643
[2m[36m(func pid=152082)[0m top5: 0.7430037313432836
[2m[36m(func pid=152082)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=152082)[0m f1_macro: 0.0460878298514173
[2m[36m(func pid=152082)[0m f1_weighted: 0.09611262759211779
[2m[36m(func pid=152082)[0m f1_per_class: [0.126, 0.0, 0.0, 0.335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.09328358208955224
[2m[36m(func pid=152923)[0m top5: 0.6683768656716418
[2m[36m(func pid=152923)[0m f1_micro: 0.09328358208955224
[2m[36m(func pid=152923)[0m f1_macro: 0.05169361061273581
[2m[36m(func pid=152923)[0m f1_weighted: 0.05389139448466333
[2m[36m(func pid=152923)[0m f1_per_class: [0.201, 0.287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 4.8172 | Steps: 4 | Val loss: 2.8473 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8853 | Steps: 4 | Val loss: 2.3495 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6955 | Steps: 4 | Val loss: 2.0785 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 32.9604 | Steps: 4 | Val loss: 18.0045 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 10:18:46 (running for 00:01:15.93)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.98  |      0.05  |                    5 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  3.109 |      0.046 |                    5 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.817 |      0.063 |                    6 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 32.7   |      0.052 |                    4 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152502)[0m top1: 0.06949626865671642
[2m[36m(func pid=152502)[0m top5: 0.5946828358208955
[2m[36m(func pid=152502)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=152502)[0m f1_macro: 0.0630330799961025
[2m[36m(func pid=152502)[0m f1_weighted: 0.08384193609197507
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.0, 0.32, 0.293, 0.017, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=151704)[0m top1: 0.07369402985074627
[2m[36m(func pid=151704)[0m top5: 0.507929104477612
[2m[36m(func pid=151704)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=151704)[0m f1_macro: 0.033427878359879024
[2m[36m(func pid=151704)[0m f1_weighted: 0.06439577144152087
[2m[36m(func pid=151704)[0m f1_per_class: [0.095, 0.0, 0.016, 0.224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.30597014925373134
[2m[36m(func pid=152082)[0m top5: 0.9090485074626866
[2m[36m(func pid=152082)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=152082)[0m f1_macro: 0.10119337634958861
[2m[36m(func pid=152082)[0m f1_weighted: 0.1867130349101976
[2m[36m(func pid=152082)[0m f1_per_class: [0.078, 0.0, 0.125, 0.471, 0.033, 0.207, 0.097, 0.0, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.14738805970149255
[2m[36m(func pid=152923)[0m top5: 0.7985074626865671
[2m[36m(func pid=152923)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=152923)[0m f1_macro: 0.04178876543756363
[2m[36m(func pid=152923)[0m f1_weighted: 0.08114710561992393
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.0, 0.0, 0.203, 0.0, 0.215, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.9414 | Steps: 4 | Val loss: 3.7418 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8360 | Steps: 4 | Val loss: 2.3470 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5884 | Steps: 4 | Val loss: 2.0698 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 34.2008 | Steps: 4 | Val loss: 31.2938 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 10:18:51 (running for 00:01:21.20)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.885 |      0.033 |                    6 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.695 |      0.101 |                    6 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  3.941 |      0.04  |                    7 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 32.96  |      0.042 |                    5 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152502)[0m top1: 0.033582089552238806
[2m[36m(func pid=152502)[0m top5: 0.5886194029850746
[2m[36m(func pid=152502)[0m f1_micro: 0.033582089552238806
[2m[36m(func pid=152502)[0m f1_macro: 0.039503932244404114
[2m[36m(func pid=152502)[0m f1_weighted: 0.004064579552329099
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.0, 0.333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062, 0.0]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=151704)[0m top1: 0.018656716417910446
[2m[36m(func pid=151704)[0m top5: 0.5041977611940298
[2m[36m(func pid=151704)[0m f1_micro: 0.018656716417910446
[2m[36m(func pid=151704)[0m f1_macro: 0.01249893350863358
[2m[36m(func pid=151704)[0m f1_weighted: 0.01875147956182506
[2m[36m(func pid=151704)[0m f1_per_class: [0.049, 0.0, 0.013, 0.063, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.19962686567164178
[2m[36m(func pid=152082)[0m top5: 0.8078358208955224
[2m[36m(func pid=152082)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=152082)[0m f1_macro: 0.11557082542919095
[2m[36m(func pid=152082)[0m f1_weighted: 0.12872095153765709
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.309, 0.32, 0.032, 0.051, 0.291, 0.093, 0.061, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.020522388059701493
[2m[36m(func pid=152923)[0m top5: 0.5788246268656716
[2m[36m(func pid=152923)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=152923)[0m f1_macro: 0.01168031490269654
[2m[36m(func pid=152923)[0m f1_weighted: 0.018975560408420328
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.088, 0.0, 0.0, 0.016, 0.0, 0.012, 0.0, 0.0, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 5.2215 | Steps: 4 | Val loss: 3.8523 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7627 | Steps: 4 | Val loss: 2.3451 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 32.9168 | Steps: 4 | Val loss: 34.3497 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6920 | Steps: 4 | Val loss: 2.1680 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=152502)[0m top1: 0.0708955223880597
[2m[36m(func pid=152502)[0m top5: 0.44076492537313433
[2m[36m(func pid=152502)[0m f1_micro: 0.0708955223880597
[2m[36m(func pid=152502)[0m f1_macro: 0.06361442027484573
[2m[36m(func pid=152502)[0m f1_weighted: 0.012413491074953216
[2m[36m(func pid=152502)[0m f1_per_class: [0.149, 0.0, 0.364, 0.0, 0.0, 0.0, 0.0, 0.124, 0.0, 0.0]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:18:56 (running for 00:01:26.56)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.763 |      0.001 |                    8 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.588 |      0.116 |                    7 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.222 |      0.064 |                    8 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 34.201 |      0.012 |                    6 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.006063432835820896
[2m[36m(func pid=151704)[0m top5: 0.4976679104477612
[2m[36m(func pid=151704)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=151704)[0m f1_macro: 0.0012578616352201257
[2m[36m(func pid=151704)[0m f1_weighted: 7.626959541913076e-05
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m top1: 0.01585820895522388
[2m[36m(func pid=152923)[0m top5: 0.6529850746268657
[2m[36m(func pid=152923)[0m f1_micro: 0.01585820895522388
[2m[36m(func pid=152923)[0m f1_macro: 0.025605190532711864
[2m[36m(func pid=152923)[0m f1_weighted: 0.0045817761928163584
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.182, 0.0, 0.0, 0.051, 0.0, 0.023]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m top1: 0.21735074626865672
[2m[36m(func pid=152082)[0m top5: 0.4510261194029851
[2m[36m(func pid=152082)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=152082)[0m f1_macro: 0.1357927374474979
[2m[36m(func pid=152082)[0m f1_weighted: 0.10905068234123574
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.335, 0.357, 0.0, 0.0, 0.191, 0.0, 0.475, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 4.1594 | Steps: 4 | Val loss: 3.0982 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7826 | Steps: 4 | Val loss: 2.3505 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 45.3178 | Steps: 4 | Val loss: 24.5222 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6967 | Steps: 4 | Val loss: 2.3066 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=152502)[0m top1: 0.25326492537313433
[2m[36m(func pid=152502)[0m top5: 0.6459888059701493
[2m[36m(func pid=152502)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=152502)[0m f1_macro: 0.17360896395778086
[2m[36m(func pid=152502)[0m f1_weighted: 0.16645610817706694
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.377, 0.37, 0.136, 0.0, 0.259, 0.0, 0.537, 0.0, 0.056]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:19:02 (running for 00:01:31.98)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.783 |      0.001 |                    9 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.692 |      0.136 |                    8 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.159 |      0.174 |                    9 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 32.917 |      0.026 |                    7 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.006063432835820896
[2m[36m(func pid=151704)[0m top5: 0.5200559701492538
[2m[36m(func pid=151704)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=151704)[0m f1_macro: 0.0012287334593572778
[2m[36m(func pid=151704)[0m f1_weighted: 7.450342803938718e-05
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m top1: 0.22108208955223882
[2m[36m(func pid=152923)[0m top5: 0.6240671641791045
[2m[36m(func pid=152923)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=152923)[0m f1_macro: 0.07809068536175835
[2m[36m(func pid=152923)[0m f1_weighted: 0.15200982643789082
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.0, 0.0, 0.443, 0.095, 0.243, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m top1: 0.12733208955223882
[2m[36m(func pid=152082)[0m top5: 0.40158582089552236
[2m[36m(func pid=152082)[0m f1_micro: 0.12733208955223882
[2m[36m(func pid=152082)[0m f1_macro: 0.11071115837969855
[2m[36m(func pid=152082)[0m f1_weighted: 0.07546626353161709
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.26, 0.361, 0.0, 0.046, 0.127, 0.0, 0.215, 0.0, 0.098]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.8831 | Steps: 4 | Val loss: 2.5364 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.0069 | Steps: 4 | Val loss: 2.3770 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 42.9966 | Steps: 4 | Val loss: 27.1805 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6465 | Steps: 4 | Val loss: 2.4014 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=152502)[0m top1: 0.17630597014925373
[2m[36m(func pid=152502)[0m top5: 0.8922574626865671
[2m[36m(func pid=152502)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=152502)[0m f1_macro: 0.07651678277735276
[2m[36m(func pid=152502)[0m f1_weighted: 0.12656295435565162
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.234, 0.0, 0.153, 0.0, 0.252, 0.041, 0.0, 0.067, 0.018]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:19:07 (running for 00:01:37.21)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.783 |      0.001 |                    9 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.697 |      0.111 |                    9 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  3.883 |      0.077 |                   10 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 42.997 |      0.157 |                    9 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.006063432835820896
[2m[36m(func pid=151704)[0m top5: 0.5634328358208955
[2m[36m(func pid=151704)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=151704)[0m f1_macro: 0.0012380952380952382
[2m[36m(func pid=151704)[0m f1_weighted: 7.507107320540157e-05
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m top1: 0.2439365671641791
[2m[36m(func pid=152923)[0m top5: 0.6828358208955224
[2m[36m(func pid=152923)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=152923)[0m f1_macro: 0.15654738094657228
[2m[36m(func pid=152923)[0m f1_weighted: 0.19661574805486318
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.35, 0.552, 0.0, 0.068, 0.21, 0.361, 0.0, 0.024, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m top1: 0.08069029850746269
[2m[36m(func pid=152082)[0m top5: 0.4505597014925373
[2m[36m(func pid=152082)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=152082)[0m f1_macro: 0.06246594634472848
[2m[36m(func pid=152082)[0m f1_weighted: 0.03251345032360881
[2m[36m(func pid=152082)[0m f1_per_class: [0.034, 0.0, 0.169, 0.0, 0.049, 0.176, 0.0, 0.174, 0.0, 0.023]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 4.2431 | Steps: 4 | Val loss: 3.0462 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 44.7001 | Steps: 4 | Val loss: 59.9435 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 3.0311 | Steps: 4 | Val loss: 2.3969 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.7196 | Steps: 4 | Val loss: 2.5166 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=152502)[0m top1: 0.25699626865671643
[2m[36m(func pid=152502)[0m top5: 0.8078358208955224
[2m[36m(func pid=152502)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=152502)[0m f1_macro: 0.08091048018496516
[2m[36m(func pid=152502)[0m f1_weighted: 0.1596593997595508
[2m[36m(func pid=152502)[0m f1_per_class: [0.252, 0.0, 0.0, 0.0, 0.04, 0.0, 0.517, 0.0, 0.0, 0.0]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:19:12 (running for 00:01:42.29)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  3.007 |      0.001 |                   10 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.647 |      0.062 |                   10 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.243 |      0.081 |                   11 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 44.7   |      0.107 |                   10 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152923)[0m top1: 0.029850746268656716
[2m[36m(func pid=152923)[0m top5: 0.570429104477612
[2m[36m(func pid=152923)[0m f1_micro: 0.029850746268656716
[2m[36m(func pid=152923)[0m f1_macro: 0.10661949688049646
[2m[36m(func pid=152923)[0m f1_weighted: 0.011148846899048189
[2m[36m(func pid=152923)[0m f1_per_class: [0.226, 0.0, 0.545, 0.0, 0.268, 0.0, 0.003, 0.0, 0.0, 0.024]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=151704)[0m top1: 0.006996268656716418
[2m[36m(func pid=151704)[0m top5: 0.6343283582089553
[2m[36m(func pid=151704)[0m f1_micro: 0.006996268656716418
[2m[36m(func pid=151704)[0m f1_macro: 0.0034211919127765432
[2m[36m(func pid=151704)[0m f1_weighted: 0.0005150790190046907
[2m[36m(func pid=151704)[0m f1_per_class: [0.021, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.10587686567164178
[2m[36m(func pid=152082)[0m top5: 0.2826492537313433
[2m[36m(func pid=152082)[0m f1_micro: 0.10587686567164178
[2m[36m(func pid=152082)[0m f1_macro: 0.14137977511165506
[2m[36m(func pid=152082)[0m f1_weighted: 0.05792318007113845
[2m[36m(func pid=152082)[0m f1_per_class: [0.176, 0.0, 0.483, 0.0, 0.056, 0.271, 0.0, 0.278, 0.099, 0.05]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 5.9881 | Steps: 4 | Val loss: 4.8179 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 70.5746 | Steps: 4 | Val loss: 62.7790 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.7130 | Steps: 4 | Val loss: 2.3898 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.4249 | Steps: 4 | Val loss: 2.6106 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 10:19:17 (running for 00:01:47.63)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  3.031 |      0.003 |                   11 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.72  |      0.141 |                   11 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.988 |      0.051 |                   12 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 44.7   |      0.107 |                   10 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152502)[0m top1: 0.0708955223880597
[2m[36m(func pid=152502)[0m top5: 0.8675373134328358
[2m[36m(func pid=152502)[0m f1_micro: 0.0708955223880597
[2m[36m(func pid=152502)[0m f1_macro: 0.050994725241589677
[2m[36m(func pid=152502)[0m f1_weighted: 0.08394841738403669
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.0, 0.173, 0.27, 0.019, 0.0, 0.024, 0.0, 0.0, 0.024]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m top1: 0.06389925373134328
[2m[36m(func pid=152923)[0m top5: 0.5652985074626866
[2m[36m(func pid=152923)[0m f1_micro: 0.06389925373134328
[2m[36m(func pid=152923)[0m f1_macro: 0.096948482100119
[2m[36m(func pid=152923)[0m f1_weighted: 0.026913486110230325
[2m[36m(func pid=152923)[0m f1_per_class: [0.246, 0.0, 0.4, 0.0, 0.035, 0.128, 0.0, 0.0, 0.123, 0.038]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=151704)[0m top1: 0.02005597014925373
[2m[36m(func pid=151704)[0m top5: 0.5979477611940298
[2m[36m(func pid=151704)[0m f1_micro: 0.02005597014925373
[2m[36m(func pid=151704)[0m f1_macro: 0.013707711681714372
[2m[36m(func pid=151704)[0m f1_weighted: 0.01773342940278322
[2m[36m(func pid=151704)[0m f1_per_class: [0.03, 0.0, 0.017, 0.0, 0.0, 0.0, 0.053, 0.0, 0.036, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.06343283582089553
[2m[36m(func pid=152082)[0m top5: 0.31902985074626866
[2m[36m(func pid=152082)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=152082)[0m f1_macro: 0.0877251118892389
[2m[36m(func pid=152082)[0m f1_weighted: 0.03521566255995913
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.0, 0.143, 0.0, 0.094, 0.008, 0.0, 0.502, 0.101, 0.029]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 4.8602 | Steps: 4 | Val loss: 4.1695 | Batch size: 32 | lr: 0.01 | Duration: 2.67s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 53.0093 | Steps: 4 | Val loss: 49.6512 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.7511 | Steps: 4 | Val loss: 2.3768 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.5405 | Steps: 4 | Val loss: 2.5441 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=152502)[0m top1: 0.1875
[2m[36m(func pid=152502)[0m top5: 0.6777052238805971
[2m[36m(func pid=152502)[0m f1_micro: 0.1875
[2m[36m(func pid=152502)[0m f1_macro: 0.1630028111424838
[2m[36m(func pid=152502)[0m f1_weighted: 0.13082331213935142
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.418, 0.386, 0.0, 0.156, 0.348, 0.0, 0.196, 0.126, 0.0]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:19:23 (running for 00:01:52.90)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.713 |      0.014 |                   12 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.425 |      0.088 |                   12 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.86  |      0.163 |                   13 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 53.009 |      0.045 |                   12 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152923)[0m top1: 0.03871268656716418
[2m[36m(func pid=152923)[0m top5: 0.5671641791044776
[2m[36m(func pid=152923)[0m f1_micro: 0.03871268656716418
[2m[36m(func pid=152923)[0m f1_macro: 0.04537664430392392
[2m[36m(func pid=152923)[0m f1_weighted: 0.01796323747780087
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.0, 0.122, 0.0, 0.028, 0.032, 0.0, 0.176, 0.094, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=151704)[0m top1: 0.06529850746268656
[2m[36m(func pid=151704)[0m top5: 0.5368470149253731
[2m[36m(func pid=151704)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=151704)[0m f1_macro: 0.035207190663970225
[2m[36m(func pid=151704)[0m f1_weighted: 0.07756819364708042
[2m[36m(func pid=151704)[0m f1_per_class: [0.027, 0.0, 0.043, 0.0, 0.0, 0.0, 0.254, 0.0, 0.027, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.05503731343283582
[2m[36m(func pid=152082)[0m top5: 0.4085820895522388
[2m[36m(func pid=152082)[0m f1_micro: 0.05503731343283582
[2m[36m(func pid=152082)[0m f1_macro: 0.13317649153035493
[2m[36m(func pid=152082)[0m f1_weighted: 0.03607734214275973
[2m[36m(func pid=152082)[0m f1_per_class: [0.185, 0.0, 0.526, 0.0, 0.061, 0.0, 0.0, 0.432, 0.1, 0.027]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 4.6769 | Steps: 4 | Val loss: 4.0536 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 55.8382 | Steps: 4 | Val loss: 27.2008 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7128 | Steps: 4 | Val loss: 2.3554 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5387 | Steps: 4 | Val loss: 2.2855 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=152502)[0m top1: 0.28171641791044777
[2m[36m(func pid=152502)[0m top5: 0.7131529850746269
[2m[36m(func pid=152502)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=152502)[0m f1_macro: 0.2031968407631309
[2m[36m(func pid=152502)[0m f1_weighted: 0.15679358914126218
[2m[36m(func pid=152502)[0m f1_per_class: [0.333, 0.43, 0.471, 0.0, 0.0, 0.478, 0.0, 0.321, 0.0, 0.0]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:19:28 (running for 00:01:58.09)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.751 |      0.035 |                   13 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.54  |      0.133 |                   13 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.677 |      0.203 |                   14 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 55.838 |      0.164 |                   13 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152923)[0m top1: 0.3031716417910448
[2m[36m(func pid=152923)[0m top5: 0.8218283582089553
[2m[36m(func pid=152923)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=152923)[0m f1_macro: 0.16429855756092868
[2m[36m(func pid=152923)[0m f1_weighted: 0.24518181237299091
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.469, 0.198, 0.516, 0.0, 0.0, 0.0, 0.252, 0.112, 0.097]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=151704)[0m top1: 0.12360074626865672
[2m[36m(func pid=151704)[0m top5: 0.5186567164179104
[2m[36m(func pid=151704)[0m f1_micro: 0.12360074626865672
[2m[36m(func pid=151704)[0m f1_macro: 0.06028617258892281
[2m[36m(func pid=151704)[0m f1_weighted: 0.12107996185168478
[2m[36m(func pid=151704)[0m f1_per_class: [0.044, 0.0, 0.127, 0.0, 0.0, 0.0, 0.397, 0.0, 0.035, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.15811567164179105
[2m[36m(func pid=152082)[0m top5: 0.6333955223880597
[2m[36m(func pid=152082)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=152082)[0m f1_macro: 0.16774695774567067
[2m[36m(func pid=152082)[0m f1_weighted: 0.15481737000096107
[2m[36m(func pid=152082)[0m f1_per_class: [0.234, 0.0, 0.444, 0.42, 0.03, 0.0, 0.0, 0.51, 0.0, 0.04]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 4.7728 | Steps: 4 | Val loss: 2.7759 | Batch size: 32 | lr: 0.01 | Duration: 2.66s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 57.8966 | Steps: 4 | Val loss: 36.9157 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.9422 | Steps: 4 | Val loss: 2.3512 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.4885 | Steps: 4 | Val loss: 2.1109 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=152502)[0m top1: 0.29617537313432835
[2m[36m(func pid=152502)[0m top5: 0.8805970149253731
[2m[36m(func pid=152502)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=152502)[0m f1_macro: 0.20963601946584748
[2m[36m(func pid=152502)[0m f1_weighted: 0.24441948312850886
[2m[36m(func pid=152502)[0m f1_per_class: [0.245, 0.021, 0.471, 0.534, 0.0, 0.28, 0.096, 0.39, 0.0, 0.059]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:19:33 (running for 00:02:03.25)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.713 |      0.06  |                   14 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.539 |      0.168 |                   14 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.773 |      0.21  |                   15 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 57.897 |      0.163 |                   14 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152923)[0m top1: 0.28638059701492535
[2m[36m(func pid=152923)[0m top5: 0.8610074626865671
[2m[36m(func pid=152923)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=152923)[0m f1_macro: 0.1628465739590425
[2m[36m(func pid=152923)[0m f1_weighted: 0.2663046115713252
[2m[36m(func pid=152923)[0m f1_per_class: [0.157, 0.433, 0.0, 0.0, 0.0, 0.0, 0.542, 0.458, 0.0, 0.038]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=151704)[0m top1: 0.15858208955223882
[2m[36m(func pid=151704)[0m top5: 0.5499067164179104
[2m[36m(func pid=151704)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=151704)[0m f1_macro: 0.08634239379261008
[2m[36m(func pid=151704)[0m f1_weighted: 0.13441254640193345
[2m[36m(func pid=151704)[0m f1_per_class: [0.066, 0.0, 0.333, 0.0, 0.0, 0.016, 0.432, 0.0, 0.017, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.20149253731343283
[2m[36m(func pid=152082)[0m top5: 0.8050373134328358
[2m[36m(func pid=152082)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=152082)[0m f1_macro: 0.12955984607606644
[2m[36m(func pid=152082)[0m f1_weighted: 0.16786693926003984
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.0, 0.227, 0.492, 0.025, 0.0, 0.003, 0.472, 0.0, 0.077]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 3.2374 | Steps: 4 | Val loss: 4.2982 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 83.1170 | Steps: 4 | Val loss: 34.8167 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.9072 | Steps: 4 | Val loss: 2.3322 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.5190 | Steps: 4 | Val loss: 2.0272 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=152502)[0m top1: 0.16138059701492538
[2m[36m(func pid=152502)[0m top5: 0.8292910447761194
[2m[36m(func pid=152502)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=152502)[0m f1_macro: 0.0873558176948014
[2m[36m(func pid=152502)[0m f1_weighted: 0.18723550295068928
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.0, 0.119, 0.174, 0.0, 0.0, 0.451, 0.0, 0.099, 0.03]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m top1: 0.29757462686567165
[2m[36m(func pid=152923)[0m top5: 0.8815298507462687
[2m[36m(func pid=152923)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=152923)[0m f1_macro: 0.18102163031145996
[2m[36m(func pid=152923)[0m f1_weighted: 0.22642967815365483
[2m[36m(func pid=152923)[0m f1_per_class: [0.194, 0.402, 0.0, 0.0, 0.0, 0.378, 0.268, 0.512, 0.0, 0.057]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=151704)[0m top1: 0.19962686567164178
[2m[36m(func pid=151704)[0m top5: 0.5457089552238806
[2m[36m(func pid=151704)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=151704)[0m f1_macro: 0.11122703218349739
[2m[36m(func pid=151704)[0m f1_weighted: 0.1613015734535944
[2m[36m(func pid=151704)[0m f1_per_class: [0.077, 0.0, 0.4, 0.0, 0.0, 0.145, 0.47, 0.0, 0.021, 0.0]
== Status ==
Current time: 2024-01-07 10:19:39 (running for 00:02:08.78)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.907 |      0.111 |                   16 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.489 |      0.13  |                   15 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  3.237 |      0.087 |                   16 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 83.117 |      0.181 |                   15 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.21548507462686567
[2m[36m(func pid=152082)[0m top5: 0.9104477611940298
[2m[36m(func pid=152082)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=152082)[0m f1_macro: 0.12071937345583961
[2m[36m(func pid=152082)[0m f1_weighted: 0.2022577149532161
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.0, 0.152, 0.484, 0.027, 0.0, 0.144, 0.4, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 6.2245 | Steps: 4 | Val loss: 3.9783 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 101.8540 | Steps: 4 | Val loss: 33.7810 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8043 | Steps: 4 | Val loss: 2.3025 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.2069 | Steps: 4 | Val loss: 2.0054 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=152923)[0m top1: 0.345615671641791
[2m[36m(func pid=152923)[0m top5: 0.9328358208955224
[2m[36m(func pid=152923)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=152923)[0m f1_macro: 0.2226570367568602
[2m[36m(func pid=152923)[0m f1_weighted: 0.2270724102988045
[2m[36m(func pid=152923)[0m f1_per_class: [0.326, 0.0, 0.632, 0.535, 0.0, 0.375, 0.015, 0.344, 0.0, 0.0]
[2m[36m(func pid=152502)[0m top1: 0.26725746268656714
[2m[36m(func pid=152502)[0m top5: 0.808768656716418
[2m[36m(func pid=152502)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=152502)[0m f1_macro: 0.11584858516198779
[2m[36m(func pid=152502)[0m f1_weighted: 0.25781826717834244
[2m[36m(func pid=152502)[0m f1_per_class: [0.043, 0.0, 0.097, 0.292, 0.0, 0.0, 0.575, 0.0, 0.091, 0.062]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:19:44 (running for 00:02:14.12)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.804 |      0.117 |                   17 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   2.519 |      0.121 |                   16 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   6.225 |      0.116 |                   17 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 101.854 |      0.223 |                   16 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |         |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |         |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |         |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |         |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |         |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |         |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |         |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |         |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.2150186567164179
[2m[36m(func pid=151704)[0m top5: 0.5382462686567164
[2m[36m(func pid=151704)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=151704)[0m f1_macro: 0.11728278452139149
[2m[36m(func pid=151704)[0m f1_weighted: 0.17128087896663105
[2m[36m(func pid=151704)[0m f1_per_class: [0.091, 0.0, 0.316, 0.0, 0.0, 0.248, 0.461, 0.0, 0.057, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.2560634328358209
[2m[36m(func pid=152082)[0m top5: 0.9015858208955224
[2m[36m(func pid=152082)[0m f1_micro: 0.2560634328358209
[2m[36m(func pid=152082)[0m f1_macro: 0.1552454444108166
[2m[36m(func pid=152082)[0m f1_weighted: 0.2729559062877143
[2m[36m(func pid=152082)[0m f1_per_class: [0.109, 0.316, 0.169, 0.199, 0.047, 0.0, 0.492, 0.221, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 78.0305 | Steps: 4 | Val loss: 36.1054 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 4.4242 | Steps: 4 | Val loss: 3.6057 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8114 | Steps: 4 | Val loss: 2.2511 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4508 | Steps: 4 | Val loss: 2.0075 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=152923)[0m top1: 0.3987873134328358
[2m[36m(func pid=152923)[0m top5: 0.8969216417910447
[2m[36m(func pid=152923)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=152923)[0m f1_macro: 0.18771435152713006
[2m[36m(func pid=152923)[0m f1_weighted: 0.38311742886765304
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.102, 0.07, 0.508, 0.0, 0.394, 0.549, 0.254, 0.0, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.2905783582089552
[2m[36m(func pid=152502)[0m top5: 0.8348880597014925
[2m[36m(func pid=152502)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=152502)[0m f1_macro: 0.23018202467305024
[2m[36m(func pid=152502)[0m f1_weighted: 0.23529397622009898
[2m[36m(func pid=152502)[0m f1_per_class: [0.178, 0.0, 0.571, 0.526, 0.193, 0.409, 0.079, 0.078, 0.095, 0.172]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:19:49 (running for 00:02:19.24)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.811 |      0.12  |                   18 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.207 |      0.155 |                   17 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.424 |      0.23  |                   18 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 78.03  |      0.188 |                   17 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.21548507462686567
[2m[36m(func pid=151704)[0m top5: 0.5513059701492538
[2m[36m(func pid=151704)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=151704)[0m f1_macro: 0.12033733621160252
[2m[36m(func pid=151704)[0m f1_weighted: 0.16663519041094405
[2m[36m(func pid=151704)[0m f1_per_class: [0.111, 0.0, 0.353, 0.0, 0.0, 0.262, 0.44, 0.0, 0.038, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.30223880597014924
[2m[36m(func pid=152082)[0m top5: 0.8782649253731343
[2m[36m(func pid=152082)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=152082)[0m f1_macro: 0.15264293328174056
[2m[36m(func pid=152082)[0m f1_weighted: 0.22597764220842353
[2m[36m(func pid=152082)[0m f1_per_class: [0.185, 0.395, 0.297, 0.0, 0.053, 0.0, 0.489, 0.106, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 100.0654 | Steps: 4 | Val loss: 73.8383 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 4.3304 | Steps: 4 | Val loss: 4.9414 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.6897 | Steps: 4 | Val loss: 2.2066 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.6028 | Steps: 4 | Val loss: 1.9441 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=152923)[0m top1: 0.14412313432835822
[2m[36m(func pid=152923)[0m top5: 0.6902985074626866
[2m[36m(func pid=152923)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=152923)[0m f1_macro: 0.10255870974104975
[2m[36m(func pid=152923)[0m f1_weighted: 0.08327695662678375
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.319, 0.133, 0.0, 0.042, 0.0, 0.0, 0.392, 0.141, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.17630597014925373
[2m[36m(func pid=152502)[0m top5: 0.675839552238806
[2m[36m(func pid=152502)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=152502)[0m f1_macro: 0.19550589133840168
[2m[36m(func pid=152502)[0m f1_weighted: 0.11616252198236295
[2m[36m(func pid=152502)[0m f1_per_class: [0.293, 0.293, 0.489, 0.0, 0.049, 0.238, 0.0, 0.453, 0.062, 0.077]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:19:54 (running for 00:02:24.34)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.69  |      0.127 |                   19 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   2.451 |      0.153 |                   18 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   4.33  |      0.196 |                   19 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 100.065 |      0.103 |                   18 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |         |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |         |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |         |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |         |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |         |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |         |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |         |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |         |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.19682835820895522
[2m[36m(func pid=151704)[0m top5: 0.5788246268656716
[2m[36m(func pid=151704)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=151704)[0m f1_macro: 0.12683464300413866
[2m[36m(func pid=151704)[0m f1_weighted: 0.14627355868238603
[2m[36m(func pid=151704)[0m f1_per_class: [0.142, 0.0, 0.444, 0.0, 0.0, 0.263, 0.365, 0.0, 0.054, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.333955223880597
[2m[36m(func pid=152082)[0m top5: 0.8740671641791045
[2m[36m(func pid=152082)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=152082)[0m f1_macro: 0.14859936928842166
[2m[36m(func pid=152082)[0m f1_weighted: 0.24311524899695058
[2m[36m(func pid=152082)[0m f1_per_class: [0.199, 0.414, 0.265, 0.0, 0.0, 0.0, 0.546, 0.062, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 73.9389 | Steps: 4 | Val loss: 92.2353 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 3.8804 | Steps: 4 | Val loss: 6.7094 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.8365 | Steps: 4 | Val loss: 2.1711 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=152923)[0m top1: 0.09841417910447761
[2m[36m(func pid=152923)[0m top5: 0.5830223880597015
[2m[36m(func pid=152923)[0m f1_micro: 0.0984141791044776
[2m[36m(func pid=152923)[0m f1_macro: 0.1422410765651848
[2m[36m(func pid=152923)[0m f1_weighted: 0.07007768093660675
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.196, 0.579, 0.0, 0.027, 0.0, 0.0, 0.486, 0.134, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.2754 | Steps: 4 | Val loss: 1.7983 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=152502)[0m top1: 0.1021455223880597
[2m[36m(func pid=152502)[0m top5: 0.5960820895522388
[2m[36m(func pid=152502)[0m f1_micro: 0.10214552238805971
[2m[36m(func pid=152502)[0m f1_macro: 0.08937678516453393
[2m[36m(func pid=152502)[0m f1_weighted: 0.08897041155431881
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.187, 0.133, 0.113, 0.031, 0.015, 0.0, 0.38, 0.0, 0.035]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:20:00 (running for 00:02:29.75)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.836 |      0.124 |                   20 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.603 |      0.149 |                   19 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  3.88  |      0.089 |                   20 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 73.939 |      0.142 |                   19 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.16557835820895522
[2m[36m(func pid=151704)[0m top5: 0.6198694029850746
[2m[36m(func pid=151704)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=151704)[0m f1_macro: 0.12394471665753588
[2m[36m(func pid=151704)[0m f1_weighted: 0.10407417250404172
[2m[36m(func pid=151704)[0m f1_per_class: [0.168, 0.0, 0.5, 0.0, 0.0, 0.255, 0.224, 0.0, 0.031, 0.062]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.39505597014925375
[2m[36m(func pid=152082)[0m top5: 0.8824626865671642
[2m[36m(func pid=152082)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=152082)[0m f1_macro: 0.2561374764040658
[2m[36m(func pid=152082)[0m f1_weighted: 0.3678208416004593
[2m[36m(func pid=152082)[0m f1_per_class: [0.155, 0.453, 0.205, 0.19, 0.163, 0.344, 0.546, 0.506, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 74.9332 | Steps: 4 | Val loss: 76.6374 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 3.7561 | Steps: 4 | Val loss: 6.8206 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.6759 | Steps: 4 | Val loss: 2.1290 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=152923)[0m top1: 0.12080223880597014
[2m[36m(func pid=152923)[0m top5: 0.6175373134328358
[2m[36m(func pid=152923)[0m f1_micro: 0.12080223880597014
[2m[36m(func pid=152923)[0m f1_macro: 0.130366373820946
[2m[36m(func pid=152923)[0m f1_weighted: 0.11857334739780037
[2m[36m(func pid=152923)[0m f1_per_class: [0.104, 0.201, 0.18, 0.0, 0.11, 0.0, 0.17, 0.496, 0.0, 0.042]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.19402985074626866
[2m[36m(func pid=152502)[0m top5: 0.5111940298507462
[2m[36m(func pid=152502)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=152502)[0m f1_macro: 0.15210784923820103
[2m[36m(func pid=152502)[0m f1_weighted: 0.17817316193829238
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.0, 0.4, 0.503, 0.035, 0.074, 0.0, 0.452, 0.0, 0.058]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.1752 | Steps: 4 | Val loss: 1.8186 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 10:20:05 (running for 00:02:34.92)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.676 |      0.107 |                   21 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.275 |      0.256 |                   20 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  3.756 |      0.152 |                   21 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 74.933 |      0.13  |                   20 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.15298507462686567
[2m[36m(func pid=151704)[0m top5: 0.7346082089552238
[2m[36m(func pid=151704)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=151704)[0m f1_macro: 0.10670905369271173
[2m[36m(func pid=151704)[0m f1_weighted: 0.08732876290003225
[2m[36m(func pid=151704)[0m f1_per_class: [0.211, 0.0, 0.37, 0.0, 0.0, 0.248, 0.171, 0.0, 0.034, 0.033]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 72.5460 | Steps: 4 | Val loss: 77.3001 | Batch size: 32 | lr: 0.1 | Duration: 2.59s
[2m[36m(func pid=152082)[0m top1: 0.34095149253731344
[2m[36m(func pid=152082)[0m top5: 0.894589552238806
[2m[36m(func pid=152082)[0m f1_micro: 0.34095149253731344
[2m[36m(func pid=152082)[0m f1_macro: 0.2164575967753533
[2m[36m(func pid=152082)[0m f1_weighted: 0.3008723987439003
[2m[36m(func pid=152082)[0m f1_per_class: [0.179, 0.28, 0.218, 0.512, 0.111, 0.38, 0.136, 0.348, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 4.6951 | Steps: 4 | Val loss: 5.6813 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.6625 | Steps: 4 | Val loss: 2.0999 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=152923)[0m top1: 0.15111940298507462
[2m[36m(func pid=152923)[0m top5: 0.7602611940298507
[2m[36m(func pid=152923)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=152923)[0m f1_macro: 0.10238042047964686
[2m[36m(func pid=152923)[0m f1_weighted: 0.14822153029882093
[2m[36m(func pid=152923)[0m f1_per_class: [0.117, 0.005, 0.301, 0.0, 0.091, 0.0, 0.476, 0.0, 0.0, 0.033]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.19496268656716417
[2m[36m(func pid=152502)[0m top5: 0.6450559701492538
[2m[36m(func pid=152502)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=152502)[0m f1_macro: 0.21140827312778637
[2m[36m(func pid=152502)[0m f1_weighted: 0.1883963694710209
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.0, 0.6, 0.396, 0.064, 0.251, 0.033, 0.522, 0.102, 0.146]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.2439 | Steps: 4 | Val loss: 1.9708 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 10:20:10 (running for 00:02:40.13)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.663 |      0.095 |                   22 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.175 |      0.216 |                   21 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.695 |      0.211 |                   22 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 72.546 |      0.102 |                   21 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.14319029850746268
[2m[36m(func pid=151704)[0m top5: 0.8833955223880597
[2m[36m(func pid=151704)[0m f1_micro: 0.14319029850746268
[2m[36m(func pid=151704)[0m f1_macro: 0.09531432206153707
[2m[36m(func pid=151704)[0m f1_weighted: 0.08543405280800592
[2m[36m(func pid=151704)[0m f1_per_class: [0.165, 0.0, 0.282, 0.105, 0.0, 0.26, 0.066, 0.0, 0.04, 0.035]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 63.0328 | Steps: 4 | Val loss: 30.7444 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=152082)[0m top1: 0.25093283582089554
[2m[36m(func pid=152082)[0m top5: 0.8428171641791045
[2m[36m(func pid=152082)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=152082)[0m f1_macro: 0.1648362311633071
[2m[36m(func pid=152082)[0m f1_weighted: 0.18635949111672706
[2m[36m(func pid=152082)[0m f1_per_class: [0.044, 0.0, 0.297, 0.41, 0.0, 0.318, 0.012, 0.43, 0.137, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 6.8856 | Steps: 4 | Val loss: 4.5934 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.6728 | Steps: 4 | Val loss: 2.0717 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=152923)[0m top1: 0.4085820895522388
[2m[36m(func pid=152923)[0m top5: 0.8726679104477612
[2m[36m(func pid=152923)[0m f1_micro: 0.40858208955223885
[2m[36m(func pid=152923)[0m f1_macro: 0.20607643849431198
[2m[36m(func pid=152923)[0m f1_weighted: 0.3625909423081008
[2m[36m(func pid=152923)[0m f1_per_class: [0.143, 0.148, 0.131, 0.536, 0.197, 0.024, 0.572, 0.121, 0.0, 0.189]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.20942164179104478
[2m[36m(func pid=152502)[0m top5: 0.7621268656716418
[2m[36m(func pid=152502)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=152502)[0m f1_macro: 0.2191182972540234
[2m[36m(func pid=152502)[0m f1_weighted: 0.20280166011996714
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.0, 0.6, 0.275, 0.127, 0.29, 0.189, 0.477, 0.091, 0.143]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4266 | Steps: 4 | Val loss: 2.0571 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 10:20:15 (running for 00:02:45.32)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.673 |      0.108 |                   23 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.244 |      0.165 |                   22 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  6.886 |      0.219 |                   23 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 63.033 |      0.206 |                   22 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.19029850746268656
[2m[36m(func pid=151704)[0m top5: 0.9071828358208955
[2m[36m(func pid=151704)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=151704)[0m f1_macro: 0.10847515837753059
[2m[36m(func pid=151704)[0m f1_weighted: 0.15841071356500597
[2m[36m(func pid=151704)[0m f1_per_class: [0.061, 0.134, 0.21, 0.346, 0.0, 0.291, 0.009, 0.0, 0.0, 0.035]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 73.7157 | Steps: 4 | Val loss: 42.0254 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=152082)[0m top1: 0.2719216417910448
[2m[36m(func pid=152082)[0m top5: 0.8059701492537313
[2m[36m(func pid=152082)[0m f1_micro: 0.2719216417910448
[2m[36m(func pid=152082)[0m f1_macro: 0.19053243985308332
[2m[36m(func pid=152082)[0m f1_weighted: 0.2059542538638487
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.0, 0.449, 0.485, 0.0, 0.305, 0.0, 0.489, 0.127, 0.05]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 4.1728 | Steps: 4 | Val loss: 4.4633 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.6014 | Steps: 4 | Val loss: 2.0593 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=152502)[0m top1: 0.15578358208955223
[2m[36m(func pid=152502)[0m top5: 0.7299440298507462
[2m[36m(func pid=152502)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=152502)[0m f1_macro: 0.18850664867193753
[2m[36m(func pid=152502)[0m f1_weighted: 0.14104854118256963
[2m[36m(func pid=152502)[0m f1_per_class: [0.125, 0.097, 0.462, 0.199, 0.073, 0.297, 0.0, 0.431, 0.085, 0.117]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m top1: 0.33722014925373134
[2m[36m(func pid=152923)[0m top5: 0.8451492537313433
[2m[36m(func pid=152923)[0m f1_micro: 0.33722014925373134
[2m[36m(func pid=152923)[0m f1_macro: 0.2599655537802799
[2m[36m(func pid=152923)[0m f1_weighted: 0.25838768356778963
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.215, 0.667, 0.484, 0.244, 0.441, 0.0, 0.477, 0.071, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.4030 | Steps: 4 | Val loss: 2.1584 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 10:20:20 (running for 00:02:50.53)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.601 |      0.171 |                   24 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.427 |      0.191 |                   23 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.173 |      0.189 |                   24 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 73.716 |      0.26  |                   23 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.26492537313432835
[2m[36m(func pid=151704)[0m top5: 0.8055037313432836
[2m[36m(func pid=151704)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=151704)[0m f1_macro: 0.17052454416140383
[2m[36m(func pid=151704)[0m f1_weighted: 0.24838747727907104
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.41, 0.191, 0.425, 0.0, 0.345, 0.003, 0.3, 0.0, 0.031]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.6578 | Steps: 4 | Val loss: 5.4852 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 71.5285 | Steps: 4 | Val loss: 46.2448 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=152082)[0m top1: 0.27005597014925375
[2m[36m(func pid=152082)[0m top5: 0.8157649253731343
[2m[36m(func pid=152082)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=152082)[0m f1_macro: 0.21410984072232003
[2m[36m(func pid=152082)[0m f1_weighted: 0.2123582078419849
[2m[36m(func pid=152082)[0m f1_per_class: [0.175, 0.0, 0.465, 0.491, 0.13, 0.341, 0.0, 0.48, 0.026, 0.033]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.6318 | Steps: 4 | Val loss: 2.0632 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=152923)[0m top1: 0.24953358208955223
[2m[36m(func pid=152923)[0m top5: 0.8283582089552238
[2m[36m(func pid=152923)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=152923)[0m f1_macro: 0.13922148887216795
[2m[36m(func pid=152923)[0m f1_weighted: 0.20071938785628474
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.037, 0.143, 0.528, 0.064, 0.225, 0.0, 0.285, 0.111, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.22294776119402984
[2m[36m(func pid=152502)[0m top5: 0.7042910447761194
[2m[36m(func pid=152502)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=152502)[0m f1_macro: 0.138144673000863
[2m[36m(func pid=152502)[0m f1_weighted: 0.10221194053739198
[2m[36m(func pid=152502)[0m f1_per_class: [0.224, 0.378, 0.176, 0.0, 0.0, 0.0, 0.006, 0.462, 0.06, 0.074]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.8394 | Steps: 4 | Val loss: 2.1962 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 10:20:26 (running for 00:02:55.96)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.632 |      0.176 |                   25 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.403 |      0.214 |                   24 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  3.658 |      0.138 |                   25 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 71.529 |      0.139 |                   24 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.29384328358208955
[2m[36m(func pid=151704)[0m top5: 0.715018656716418
[2m[36m(func pid=151704)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=151704)[0m f1_macro: 0.17640922653194988
[2m[36m(func pid=151704)[0m f1_weighted: 0.25128588761967957
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.461, 0.2, 0.416, 0.0, 0.286, 0.0, 0.376, 0.0, 0.025]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 47.9519 | Steps: 4 | Val loss: 34.6943 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 5.9157 | Steps: 4 | Val loss: 5.2192 | Batch size: 32 | lr: 0.01 | Duration: 2.68s
[2m[36m(func pid=152082)[0m top1: 0.19076492537313433
[2m[36m(func pid=152082)[0m top5: 0.8143656716417911
[2m[36m(func pid=152082)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=152082)[0m f1_macro: 0.21252584008199027
[2m[36m(func pid=152082)[0m f1_weighted: 0.15951705972145863
[2m[36m(func pid=152082)[0m f1_per_class: [0.326, 0.0, 0.5, 0.26, 0.087, 0.4, 0.0, 0.529, 0.0, 0.023]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6517 | Steps: 4 | Val loss: 2.0695 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=152923)[0m top1: 0.322294776119403
[2m[36m(func pid=152923)[0m top5: 0.8675373134328358
[2m[36m(func pid=152923)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=152923)[0m f1_macro: 0.20999436726020107
[2m[36m(func pid=152923)[0m f1_weighted: 0.32670321397606183
[2m[36m(func pid=152923)[0m f1_per_class: [0.201, 0.021, 0.328, 0.491, 0.044, 0.0, 0.518, 0.411, 0.019, 0.065]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.22434701492537312
[2m[36m(func pid=152502)[0m top5: 0.746268656716418
[2m[36m(func pid=152502)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=152502)[0m f1_macro: 0.10988893938456334
[2m[36m(func pid=152502)[0m f1_weighted: 0.12412069611930979
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.409, 0.079, 0.0, 0.0, 0.0, 0.106, 0.319, 0.048, 0.138]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.0271 | Steps: 4 | Val loss: 2.2164 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 10:20:31 (running for 00:03:01.07)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.652 |      0.139 |                   26 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.839 |      0.213 |                   25 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.916 |      0.11  |                   26 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 47.952 |      0.21  |                   25 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.279384328358209
[2m[36m(func pid=151704)[0m top5: 0.683768656716418
[2m[36m(func pid=151704)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=151704)[0m f1_macro: 0.13949127500184647
[2m[36m(func pid=151704)[0m f1_weighted: 0.2101007086602151
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.469, 0.227, 0.396, 0.0, 0.031, 0.0, 0.236, 0.0, 0.036]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 36.3173 | Steps: 4 | Val loss: 44.4978 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 5.1365 | Steps: 4 | Val loss: 3.7653 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=152082)[0m top1: 0.13666044776119404
[2m[36m(func pid=152082)[0m top5: 0.8027052238805971
[2m[36m(func pid=152082)[0m f1_micro: 0.13666044776119404
[2m[36m(func pid=152082)[0m f1_macro: 0.1691918607932784
[2m[36m(func pid=152082)[0m f1_weighted: 0.11484300461261207
[2m[36m(func pid=152082)[0m f1_per_class: [0.23, 0.005, 0.465, 0.151, 0.042, 0.319, 0.003, 0.457, 0.0, 0.02]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.5342 | Steps: 4 | Val loss: 2.0876 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=152923)[0m top1: 0.29151119402985076
[2m[36m(func pid=152923)[0m top5: 0.8218283582089553
[2m[36m(func pid=152923)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=152923)[0m f1_macro: 0.16632144293381762
[2m[36m(func pid=152923)[0m f1_weighted: 0.2817766683387642
[2m[36m(func pid=152923)[0m f1_per_class: [0.164, 0.0, 0.272, 0.487, 0.071, 0.097, 0.429, 0.0, 0.0, 0.144]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.4319029850746269
[2m[36m(func pid=152502)[0m top5: 0.7961753731343284
[2m[36m(func pid=152502)[0m f1_micro: 0.4319029850746269
[2m[36m(func pid=152502)[0m f1_macro: 0.2792530481433695
[2m[36m(func pid=152502)[0m f1_weighted: 0.37574578596027014
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.212, 0.611, 0.597, 0.187, 0.0, 0.457, 0.435, 0.125, 0.168]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.3326 | Steps: 4 | Val loss: 2.2974 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 10:20:36 (running for 00:03:06.29)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.534 |      0.12  |                   27 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.027 |      0.169 |                   26 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.137 |      0.279 |                   27 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 36.317 |      0.166 |                   26 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.22667910447761194
[2m[36m(func pid=151704)[0m top5: 0.6660447761194029
[2m[36m(func pid=151704)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=151704)[0m f1_macro: 0.12028144680585158
[2m[36m(func pid=151704)[0m f1_weighted: 0.16950542804790752
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.458, 0.282, 0.281, 0.0, 0.0, 0.0, 0.181, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 77.4391 | Steps: 4 | Val loss: 53.5493 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 3.1536 | Steps: 4 | Val loss: 4.9710 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=152082)[0m top1: 0.13059701492537312
[2m[36m(func pid=152082)[0m top5: 0.7905783582089553
[2m[36m(func pid=152082)[0m f1_micro: 0.13059701492537312
[2m[36m(func pid=152082)[0m f1_macro: 0.12525845250701334
[2m[36m(func pid=152082)[0m f1_weighted: 0.11055426882507033
[2m[36m(func pid=152082)[0m f1_per_class: [0.157, 0.216, 0.164, 0.0, 0.03, 0.093, 0.11, 0.434, 0.0, 0.049]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.7181 | Steps: 4 | Val loss: 2.1014 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=152923)[0m top1: 0.22294776119402984
[2m[36m(func pid=152923)[0m top5: 0.7714552238805971
[2m[36m(func pid=152923)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=152923)[0m f1_macro: 0.1989577464830118
[2m[36m(func pid=152923)[0m f1_weighted: 0.20478267684260265
[2m[36m(func pid=152923)[0m f1_per_class: [0.119, 0.192, 0.367, 0.414, 0.289, 0.305, 0.006, 0.203, 0.0, 0.095]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.24580223880597016
[2m[36m(func pid=152502)[0m top5: 0.7555970149253731
[2m[36m(func pid=152502)[0m f1_micro: 0.24580223880597016
[2m[36m(func pid=152502)[0m f1_macro: 0.19671438404932806
[2m[36m(func pid=152502)[0m f1_weighted: 0.24671185121707273
[2m[36m(func pid=152502)[0m f1_per_class: [0.14, 0.0, 0.444, 0.196, 0.056, 0.0, 0.543, 0.339, 0.086, 0.164]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.2761 | Steps: 4 | Val loss: 2.4705 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 10:20:41 (running for 00:03:11.37)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.718 |      0.11  |                   28 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.333 |      0.125 |                   27 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  3.154 |      0.197 |                   28 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 77.439 |      0.199 |                   27 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.1865671641791045
[2m[36m(func pid=151704)[0m top5: 0.6604477611940298
[2m[36m(func pid=151704)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=151704)[0m f1_macro: 0.10964687399383433
[2m[36m(func pid=151704)[0m f1_weighted: 0.1439626922993646
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.453, 0.293, 0.199, 0.0, 0.0, 0.0, 0.151, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 44.4979 | Steps: 4 | Val loss: 48.5902 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 6.5983 | Steps: 4 | Val loss: 7.0073 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=152082)[0m top1: 0.12360074626865672
[2m[36m(func pid=152082)[0m top5: 0.7980410447761194
[2m[36m(func pid=152082)[0m f1_micro: 0.12360074626865672
[2m[36m(func pid=152082)[0m f1_macro: 0.09431459907267187
[2m[36m(func pid=152082)[0m f1_weighted: 0.08559955625068608
[2m[36m(func pid=152082)[0m f1_per_class: [0.071, 0.244, 0.107, 0.0, 0.03, 0.0, 0.054, 0.438, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.5087 | Steps: 4 | Val loss: 2.1206 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=152923)[0m top1: 0.34421641791044777
[2m[36m(func pid=152923)[0m top5: 0.7378731343283582
[2m[36m(func pid=152923)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=152923)[0m f1_macro: 0.223191799405935
[2m[36m(func pid=152923)[0m f1_weighted: 0.26851013099369037
[2m[36m(func pid=152923)[0m f1_per_class: [0.277, 0.482, 0.222, 0.435, 0.0, 0.31, 0.003, 0.327, 0.0, 0.176]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.13619402985074627
[2m[36m(func pid=152502)[0m top5: 0.6497201492537313
[2m[36m(func pid=152502)[0m f1_micro: 0.13619402985074627
[2m[36m(func pid=152502)[0m f1_macro: 0.17596499410340458
[2m[36m(func pid=152502)[0m f1_weighted: 0.12057893948020204
[2m[36m(func pid=152502)[0m f1_per_class: [0.196, 0.0, 0.471, 0.0, 0.042, 0.258, 0.183, 0.443, 0.088, 0.08]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:20:46 (running for 00:03:16.60)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.509 |      0.084 |                   29 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.276 |      0.094 |                   28 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  6.598 |      0.176 |                   29 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 44.498 |      0.223 |                   28 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.12639925373134328
[2m[36m(func pid=151704)[0m top5: 0.6487873134328358
[2m[36m(func pid=151704)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=151704)[0m f1_macro: 0.08398161652137455
[2m[36m(func pid=151704)[0m f1_weighted: 0.09977345855304828
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.309, 0.268, 0.135, 0.0, 0.0, 0.0, 0.128, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.4405 | Steps: 4 | Val loss: 2.6158 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 54.9409 | Steps: 4 | Val loss: 48.5932 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 4.3350 | Steps: 4 | Val loss: 7.8933 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=152082)[0m top1: 0.13526119402985073
[2m[36m(func pid=152082)[0m top5: 0.789179104477612
[2m[36m(func pid=152082)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=152082)[0m f1_macro: 0.10693616251370668
[2m[36m(func pid=152082)[0m f1_weighted: 0.09988116753826477
[2m[36m(func pid=152082)[0m f1_per_class: [0.078, 0.253, 0.107, 0.0, 0.032, 0.0, 0.08, 0.519, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.5030 | Steps: 4 | Val loss: 2.1414 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=152923)[0m top1: 0.22574626865671643
[2m[36m(func pid=152923)[0m top5: 0.8246268656716418
[2m[36m(func pid=152923)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=152923)[0m f1_macro: 0.1539494896433011
[2m[36m(func pid=152923)[0m f1_weighted: 0.16674265860312407
[2m[36m(func pid=152923)[0m f1_per_class: [0.177, 0.446, 0.161, 0.191, 0.0, 0.0, 0.003, 0.51, 0.051, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.10074626865671642
[2m[36m(func pid=152502)[0m top5: 0.6096082089552238
[2m[36m(func pid=152502)[0m f1_micro: 0.10074626865671642
[2m[36m(func pid=152502)[0m f1_macro: 0.09116480626709639
[2m[36m(func pid=152502)[0m f1_weighted: 0.053757839099562776
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.011, 0.055, 0.0, 0.062, 0.213, 0.0, 0.35, 0.184, 0.037]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:20:52 (running for 00:03:21.94)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.503 |      0.059 |                   30 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.44  |      0.107 |                   29 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.335 |      0.091 |                   30 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 54.941 |      0.154 |                   29 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.0914179104477612
[2m[36m(func pid=151704)[0m top5: 0.6478544776119403
[2m[36m(func pid=151704)[0m f1_micro: 0.0914179104477612
[2m[36m(func pid=151704)[0m f1_macro: 0.059294062365094505
[2m[36m(func pid=151704)[0m f1_weighted: 0.059768671316718834
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.16, 0.229, 0.086, 0.0, 0.0, 0.0, 0.117, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.5424 | Steps: 4 | Val loss: 2.5917 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 58.8230 | Steps: 4 | Val loss: 46.5209 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 8.0822 | Steps: 4 | Val loss: 6.6512 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=152082)[0m top1: 0.17957089552238806
[2m[36m(func pid=152082)[0m top5: 0.7910447761194029
[2m[36m(func pid=152082)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=152082)[0m f1_macro: 0.1592626687633219
[2m[36m(func pid=152082)[0m f1_weighted: 0.14941837875820654
[2m[36m(func pid=152082)[0m f1_per_class: [0.188, 0.298, 0.314, 0.0, 0.038, 0.0, 0.202, 0.553, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.6223 | Steps: 4 | Val loss: 2.1586 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=152923)[0m top1: 0.19309701492537312
[2m[36m(func pid=152923)[0m top5: 0.8694029850746269
[2m[36m(func pid=152923)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=152923)[0m f1_macro: 0.14479438771614428
[2m[36m(func pid=152923)[0m f1_weighted: 0.23296909329153173
[2m[36m(func pid=152923)[0m f1_per_class: [0.163, 0.115, 0.142, 0.235, 0.0, 0.0, 0.419, 0.288, 0.044, 0.043]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.22014925373134328
[2m[36m(func pid=152502)[0m top5: 0.6972947761194029
[2m[36m(func pid=152502)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=152502)[0m f1_macro: 0.16606848589017695
[2m[36m(func pid=152502)[0m f1_weighted: 0.19585984965773243
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.511, 0.039, 0.0, 0.097, 0.297, 0.156, 0.416, 0.028, 0.115]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:20:57 (running for 00:03:27.02)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.622 |      0.048 |                   31 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.542 |      0.159 |                   30 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  8.082 |      0.166 |                   31 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 58.823 |      0.145 |                   30 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.08162313432835822
[2m[36m(func pid=151704)[0m top5: 0.6417910447761194
[2m[36m(func pid=151704)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=151704)[0m f1_macro: 0.04763206968028481
[2m[36m(func pid=151704)[0m f1_weighted: 0.04449999510337758
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.126, 0.18, 0.054, 0.0, 0.0, 0.0, 0.116, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.9384 | Steps: 4 | Val loss: 2.4808 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 74.4991 | Steps: 4 | Val loss: 41.9027 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 6.8049 | Steps: 4 | Val loss: 4.3440 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.7967 | Steps: 4 | Val loss: 2.1730 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=152923)[0m top1: 0.24720149253731344
[2m[36m(func pid=152923)[0m top5: 0.78125
[2m[36m(func pid=152923)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=152923)[0m f1_macro: 0.20531091515810954
[2m[36m(func pid=152923)[0m f1_weighted: 0.24581833452044677
[2m[36m(func pid=152923)[0m f1_per_class: [0.044, 0.011, 0.556, 0.346, 0.277, 0.0, 0.4, 0.355, 0.021, 0.043]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m top1: 0.2196828358208955
[2m[36m(func pid=152082)[0m top5: 0.8344216417910447
[2m[36m(func pid=152082)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=152082)[0m f1_macro: 0.233705337782229
[2m[36m(func pid=152082)[0m f1_weighted: 0.22925524181783943
[2m[36m(func pid=152082)[0m f1_per_class: [0.157, 0.338, 0.571, 0.0, 0.057, 0.294, 0.331, 0.495, 0.093, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m top1: 0.3362873134328358
[2m[36m(func pid=152502)[0m top5: 0.851679104477612
[2m[36m(func pid=152502)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=152502)[0m f1_macro: 0.23878662920792668
[2m[36m(func pid=152502)[0m f1_weighted: 0.27234371513223715
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.426, 0.64, 0.09, 0.197, 0.0, 0.488, 0.308, 0.12, 0.119]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:21:02 (running for 00:03:32.61)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.797 |      0.046 |                   32 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.938 |      0.234 |                   31 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  6.805 |      0.239 |                   32 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 74.499 |      0.205 |                   31 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.08162313432835822
[2m[36m(func pid=151704)[0m top5: 0.628731343283582
[2m[36m(func pid=151704)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=151704)[0m f1_macro: 0.0455367253166847
[2m[36m(func pid=151704)[0m f1_weighted: 0.0448215294608119
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.114, 0.162, 0.063, 0.0, 0.0, 0.0, 0.117, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 97.8266 | Steps: 4 | Val loss: 71.3255 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6339 | Steps: 4 | Val loss: 2.4141 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 3.6434 | Steps: 4 | Val loss: 4.9257 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=152923)[0m top1: 0.19542910447761194
[2m[36m(func pid=152923)[0m top5: 0.6739738805970149
[2m[36m(func pid=152923)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=152923)[0m f1_macro: 0.12285913160420721
[2m[36m(func pid=152923)[0m f1_weighted: 0.16186450481514603
[2m[36m(func pid=152923)[0m f1_per_class: [0.319, 0.0, 0.0, 0.473, 0.052, 0.081, 0.0, 0.221, 0.0, 0.083]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m top1: 0.16651119402985073
[2m[36m(func pid=152082)[0m top5: 0.8740671641791045
[2m[36m(func pid=152082)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=152082)[0m f1_macro: 0.21516529683867902
[2m[36m(func pid=152082)[0m f1_weighted: 0.17009704425430988
[2m[36m(func pid=152082)[0m f1_per_class: [0.328, 0.149, 0.545, 0.0, 0.097, 0.422, 0.218, 0.301, 0.09, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6277 | Steps: 4 | Val loss: 2.1842 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=152502)[0m top1: 0.396455223880597
[2m[36m(func pid=152502)[0m top5: 0.8502798507462687
[2m[36m(func pid=152502)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=152502)[0m f1_macro: 0.2586598802583786
[2m[36m(func pid=152502)[0m f1_weighted: 0.3553474218665785
[2m[36m(func pid=152502)[0m f1_per_class: [0.25, 0.232, 0.375, 0.543, 0.154, 0.0, 0.436, 0.382, 0.041, 0.174]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:21:08 (running for 00:03:37.81)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.628 |      0.042 |                   33 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.634 |      0.215 |                   32 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  3.643 |      0.259 |                   33 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 97.827 |      0.123 |                   32 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.08115671641791045
[2m[36m(func pid=151704)[0m top5: 0.6291977611940298
[2m[36m(func pid=151704)[0m f1_micro: 0.08115671641791045
[2m[36m(func pid=151704)[0m f1_macro: 0.04192356923699044
[2m[36m(func pid=151704)[0m f1_weighted: 0.04462209748358091
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.071, 0.142, 0.089, 0.0, 0.0, 0.0, 0.117, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 83.1981 | Steps: 4 | Val loss: 113.6567 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.0925 | Steps: 4 | Val loss: 1.9624 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 6.6879 | Steps: 4 | Val loss: 6.9417 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=152923)[0m top1: 0.10074626865671642
[2m[36m(func pid=152923)[0m top5: 0.4295708955223881
[2m[36m(func pid=152923)[0m f1_micro: 0.10074626865671642
[2m[36m(func pid=152923)[0m f1_macro: 0.15896897243634878
[2m[36m(func pid=152923)[0m f1_weighted: 0.06289056411825075
[2m[36m(func pid=152923)[0m f1_per_class: [0.27, 0.0, 0.571, 0.023, 0.037, 0.132, 0.0, 0.556, 0.0, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.4519 | Steps: 4 | Val loss: 2.2100 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=152082)[0m top1: 0.24720149253731344
[2m[36m(func pid=152082)[0m top5: 0.8913246268656716
[2m[36m(func pid=152082)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=152082)[0m f1_macro: 0.21642911380277777
[2m[36m(func pid=152082)[0m f1_weighted: 0.24510215655368786
[2m[36m(func pid=152082)[0m f1_per_class: [0.189, 0.048, 0.579, 0.132, 0.141, 0.414, 0.452, 0.107, 0.103, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m top1: 0.26026119402985076
[2m[36m(func pid=152502)[0m top5: 0.7374067164179104
[2m[36m(func pid=152502)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=152502)[0m f1_macro: 0.20608266240915624
[2m[36m(func pid=152502)[0m f1_weighted: 0.2586255055688058
[2m[36m(func pid=152502)[0m f1_per_class: [0.122, 0.032, 0.143, 0.44, 0.14, 0.307, 0.208, 0.425, 0.067, 0.176]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 80.5574 | Steps: 4 | Val loss: 112.0343 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=151704)[0m top1: 0.09001865671641791
[2m[36m(func pid=151704)[0m top5: 0.6319962686567164
[2m[36m(func pid=151704)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=151704)[0m f1_macro: 0.056839153840223436
[2m[36m(func pid=151704)[0m f1_weighted: 0.05931445472546373
[2m[36m(func pid=151704)[0m f1_per_class: [0.0, 0.084, 0.11, 0.127, 0.097, 0.0, 0.0, 0.123, 0.027, 0.0]
== Status ==
Current time: 2024-01-07 10:21:13 (running for 00:03:43.09)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.452 |      0.057 |                   34 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.092 |      0.216 |                   33 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  6.688 |      0.206 |                   34 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 83.198 |      0.159 |                   33 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.8929 | Steps: 4 | Val loss: 1.7847 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 9.6460 | Steps: 4 | Val loss: 7.7170 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=152923)[0m top1: 0.06623134328358209
[2m[36m(func pid=152923)[0m top5: 0.42117537313432835
[2m[36m(func pid=152923)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=152923)[0m f1_macro: 0.11361718481578789
[2m[36m(func pid=152923)[0m f1_weighted: 0.07100617899466262
[2m[36m(func pid=152923)[0m f1_per_class: [0.157, 0.171, 0.073, 0.0, 0.025, 0.04, 0.0, 0.495, 0.112, 0.062]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.4192 | Steps: 4 | Val loss: 2.2213 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=152082)[0m top1: 0.43796641791044777
[2m[36m(func pid=152082)[0m top5: 0.8666044776119403
[2m[36m(func pid=152082)[0m f1_micro: 0.43796641791044777
[2m[36m(func pid=152082)[0m f1_macro: 0.24026357458620798
[2m[36m(func pid=152082)[0m f1_weighted: 0.3652854277560523
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.0, 0.513, 0.57, 0.182, 0.451, 0.497, 0.0, 0.0, 0.19]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m top1: 0.19776119402985073
[2m[36m(func pid=152502)[0m top5: 0.695429104477612
[2m[36m(func pid=152502)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=152502)[0m f1_macro: 0.21182371241257686
[2m[36m(func pid=152502)[0m f1_weighted: 0.15489177416243005
[2m[36m(func pid=152502)[0m f1_per_class: [0.273, 0.0, 0.5, 0.299, 0.141, 0.368, 0.003, 0.221, 0.111, 0.203]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 38.8381 | Steps: 4 | Val loss: 88.9611 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 10:21:18 (running for 00:03:48.45)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.419 |      0.069 |                   35 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.893 |      0.24  |                   34 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  9.646 |      0.212 |                   35 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 80.557 |      0.114 |                   34 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.09934701492537314
[2m[36m(func pid=151704)[0m top5: 0.6478544776119403
[2m[36m(func pid=151704)[0m f1_micro: 0.09934701492537314
[2m[36m(func pid=151704)[0m f1_macro: 0.06876711528265153
[2m[36m(func pid=151704)[0m f1_weighted: 0.0702889584599482
[2m[36m(func pid=151704)[0m f1_per_class: [0.061, 0.066, 0.114, 0.169, 0.104, 0.0, 0.0, 0.133, 0.041, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.3858 | Steps: 4 | Val loss: 1.8739 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 6.0793 | Steps: 4 | Val loss: 7.5970 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=152923)[0m top1: 0.1571828358208955
[2m[36m(func pid=152923)[0m top5: 0.5447761194029851
[2m[36m(func pid=152923)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=152923)[0m f1_macro: 0.17004831646973181
[2m[36m(func pid=152923)[0m f1_weighted: 0.09658947923833876
[2m[36m(func pid=152923)[0m f1_per_class: [0.16, 0.381, 0.478, 0.0, 0.098, 0.0, 0.003, 0.32, 0.075, 0.185]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.5014 | Steps: 4 | Val loss: 2.2328 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=152082)[0m top1: 0.44263059701492535
[2m[36m(func pid=152082)[0m top5: 0.8418843283582089
[2m[36m(func pid=152082)[0m f1_micro: 0.44263059701492535
[2m[36m(func pid=152082)[0m f1_macro: 0.24714373183459717
[2m[36m(func pid=152082)[0m f1_weighted: 0.3895478301496681
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.0, 0.5, 0.586, 0.258, 0.453, 0.562, 0.016, 0.0, 0.097]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m top1: 0.17444029850746268
[2m[36m(func pid=152502)[0m top5: 0.7154850746268657
[2m[36m(func pid=152502)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=152502)[0m f1_macro: 0.14785904374641323
[2m[36m(func pid=152502)[0m f1_weighted: 0.16523649146633193
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.0, 0.04, 0.232, 0.087, 0.282, 0.137, 0.344, 0.114, 0.242]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 76.2881 | Steps: 4 | Val loss: 81.0011 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 10:21:23 (running for 00:03:53.68)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.501 |      0.079 |                   36 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.386 |      0.247 |                   35 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  6.079 |      0.148 |                   36 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 38.838 |      0.17  |                   35 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.1021455223880597
[2m[36m(func pid=151704)[0m top5: 0.6730410447761194
[2m[36m(func pid=151704)[0m f1_micro: 0.10214552238805971
[2m[36m(func pid=151704)[0m f1_macro: 0.07913781269848698
[2m[36m(func pid=151704)[0m f1_weighted: 0.06810287749374429
[2m[36m(func pid=151704)[0m f1_per_class: [0.125, 0.037, 0.129, 0.165, 0.095, 0.0, 0.0, 0.146, 0.095, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.4593 | Steps: 4 | Val loss: 2.1214 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=152923)[0m top1: 0.24067164179104478
[2m[36m(func pid=152923)[0m top5: 0.6343283582089553
[2m[36m(func pid=152923)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=152923)[0m f1_macro: 0.21467967674113209
[2m[36m(func pid=152923)[0m f1_weighted: 0.2052988200981189
[2m[36m(func pid=152923)[0m f1_per_class: [0.338, 0.37, 0.632, 0.0, 0.261, 0.0, 0.423, 0.0, 0.07, 0.053]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 6.0505 | Steps: 4 | Val loss: 7.3831 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.6730 | Steps: 4 | Val loss: 2.2555 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=152082)[0m top1: 0.2416044776119403
[2m[36m(func pid=152082)[0m top5: 0.8484141791044776
[2m[36m(func pid=152082)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=152082)[0m f1_macro: 0.1614804784129526
[2m[36m(func pid=152082)[0m f1_weighted: 0.2649759945413811
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.0, 0.15, 0.166, 0.2, 0.26, 0.58, 0.225, 0.0, 0.032]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m top1: 0.240205223880597
[2m[36m(func pid=152502)[0m top5: 0.8050373134328358
[2m[36m(func pid=152502)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=152502)[0m f1_macro: 0.12395362649318101
[2m[36m(func pid=152502)[0m f1_weighted: 0.2640177329481355
[2m[36m(func pid=152502)[0m f1_per_class: [0.0, 0.0, 0.068, 0.406, 0.035, 0.008, 0.486, 0.0, 0.069, 0.167]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 54.5331 | Steps: 4 | Val loss: 48.4658 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 10:21:29 (running for 00:03:59.11)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.673 |      0.093 |                   37 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.459 |      0.161 |                   36 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  6.051 |      0.124 |                   37 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 76.288 |      0.215 |                   36 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.11473880597014925
[2m[36m(func pid=151704)[0m top5: 0.6623134328358209
[2m[36m(func pid=151704)[0m f1_micro: 0.11473880597014925
[2m[36m(func pid=151704)[0m f1_macro: 0.09250141441196555
[2m[36m(func pid=151704)[0m f1_weighted: 0.08317746009294881
[2m[36m(func pid=151704)[0m f1_per_class: [0.099, 0.021, 0.176, 0.222, 0.063, 0.0, 0.0, 0.179, 0.079, 0.085]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.9111 | Steps: 4 | Val loss: 2.4948 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=152923)[0m top1: 0.24813432835820895
[2m[36m(func pid=152923)[0m top5: 0.7658582089552238
[2m[36m(func pid=152923)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=152923)[0m f1_macro: 0.16556628891364428
[2m[36m(func pid=152923)[0m f1_weighted: 0.2403693805688123
[2m[36m(func pid=152923)[0m f1_per_class: [0.219, 0.227, 0.333, 0.066, 0.118, 0.038, 0.567, 0.0, 0.044, 0.043]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 4.9287 | Steps: 4 | Val loss: 6.8482 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.5184 | Steps: 4 | Val loss: 2.2772 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=152082)[0m top1: 0.19916044776119404
[2m[36m(func pid=152082)[0m top5: 0.8288246268656716
[2m[36m(func pid=152082)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=152082)[0m f1_macro: 0.18124959935550872
[2m[36m(func pid=152082)[0m f1_weighted: 0.22312634637266743
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.0, 0.088, 0.0, 0.364, 0.316, 0.519, 0.495, 0.0, 0.03]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m top1: 0.25326492537313433
[2m[36m(func pid=152502)[0m top5: 0.8297574626865671
[2m[36m(func pid=152502)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=152502)[0m f1_macro: 0.2104619612030166
[2m[36m(func pid=152502)[0m f1_weighted: 0.29701633267131305
[2m[36m(func pid=152502)[0m f1_per_class: [0.087, 0.189, 0.667, 0.373, 0.029, 0.0, 0.496, 0.0, 0.163, 0.101]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 43.2632 | Steps: 4 | Val loss: 43.1564 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 10:21:34 (running for 00:04:04.38)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.518 |      0.11  |                   38 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.911 |      0.181 |                   37 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.929 |      0.21  |                   38 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 54.533 |      0.166 |                   37 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.1333955223880597
[2m[36m(func pid=151704)[0m top5: 0.6357276119402985
[2m[36m(func pid=151704)[0m f1_micro: 0.1333955223880597
[2m[36m(func pid=151704)[0m f1_macro: 0.109534586162878
[2m[36m(func pid=151704)[0m f1_weighted: 0.10200791729865015
[2m[36m(func pid=151704)[0m f1_per_class: [0.125, 0.005, 0.232, 0.28, 0.055, 0.0, 0.0, 0.267, 0.07, 0.061]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.2741 | Steps: 4 | Val loss: 2.5205 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=152923)[0m top1: 0.3824626865671642
[2m[36m(func pid=152923)[0m top5: 0.8041044776119403
[2m[36m(func pid=152923)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=152923)[0m f1_macro: 0.25915532955444803
[2m[36m(func pid=152923)[0m f1_weighted: 0.3636156481890172
[2m[36m(func pid=152923)[0m f1_per_class: [0.192, 0.011, 0.393, 0.525, 0.0, 0.267, 0.503, 0.426, 0.027, 0.247]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 5.3307 | Steps: 4 | Val loss: 7.1639 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.5618 | Steps: 4 | Val loss: 2.2641 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=152082)[0m top1: 0.17257462686567165
[2m[36m(func pid=152082)[0m top5: 0.6646455223880597
[2m[36m(func pid=152082)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=152082)[0m f1_macro: 0.15221922308934882
[2m[36m(func pid=152082)[0m f1_weighted: 0.13457294767012032
[2m[36m(func pid=152082)[0m f1_per_class: [0.214, 0.016, 0.08, 0.0, 0.159, 0.457, 0.172, 0.377, 0.0, 0.046]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 42.7533 | Steps: 4 | Val loss: 55.1732 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=152502)[0m top1: 0.21175373134328357
[2m[36m(func pid=152502)[0m top5: 0.784981343283582
[2m[36m(func pid=152502)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=152502)[0m f1_macro: 0.1325784908438627
[2m[36m(func pid=152502)[0m f1_weighted: 0.19734791354985828
[2m[36m(func pid=152502)[0m f1_per_class: [0.25, 0.306, 0.0, 0.0, 0.072, 0.23, 0.374, 0.0, 0.0, 0.093]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:21:39 (running for 00:04:09.54)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.562 |      0.137 |                   39 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.274 |      0.152 |                   38 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.331 |      0.133 |                   39 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 43.263 |      0.259 |                   38 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.13059701492537312
[2m[36m(func pid=151704)[0m top5: 0.6231343283582089
[2m[36m(func pid=151704)[0m f1_micro: 0.13059701492537312
[2m[36m(func pid=151704)[0m f1_macro: 0.1369059122251222
[2m[36m(func pid=151704)[0m f1_weighted: 0.10434400880127774
[2m[36m(func pid=151704)[0m f1_per_class: [0.142, 0.0, 0.407, 0.262, 0.041, 0.0, 0.0, 0.377, 0.097, 0.042]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.4090 | Steps: 4 | Val loss: 2.8725 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=152923)[0m top1: 0.21175373134328357
[2m[36m(func pid=152923)[0m top5: 0.7131529850746269
[2m[36m(func pid=152923)[0m f1_micro: 0.21175373134328357
[2m[36m(func pid=152923)[0m f1_macro: 0.148629212549942
[2m[36m(func pid=152923)[0m f1_weighted: 0.18368588799656094
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.005, 0.069, 0.34, 0.0, 0.365, 0.082, 0.241, 0.131, 0.253]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 6.2715 | Steps: 4 | Val loss: 6.4620 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.4409 | Steps: 4 | Val loss: 2.2681 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=152082)[0m top1: 0.18889925373134328
[2m[36m(func pid=152082)[0m top5: 0.6324626865671642
[2m[36m(func pid=152082)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=152082)[0m f1_macro: 0.18934740188124272
[2m[36m(func pid=152082)[0m f1_weighted: 0.12361364780833442
[2m[36m(func pid=152082)[0m f1_per_class: [0.126, 0.298, 0.421, 0.0, 0.111, 0.41, 0.0, 0.291, 0.0, 0.237]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m top1: 0.1958955223880597
[2m[36m(func pid=152502)[0m top5: 0.753731343283582
[2m[36m(func pid=152502)[0m f1_micro: 0.19589552238805974
[2m[36m(func pid=152502)[0m f1_macro: 0.1785219884552394
[2m[36m(func pid=152502)[0m f1_weighted: 0.16050480368727743
[2m[36m(func pid=152502)[0m f1_per_class: [0.327, 0.353, 0.143, 0.0, 0.28, 0.242, 0.189, 0.0, 0.144, 0.107]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 80.4027 | Steps: 4 | Val loss: 59.2453 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 10:21:45 (running for 00:04:14.89)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.441 |      0.143 |                   40 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.409 |      0.189 |                   39 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  6.271 |      0.179 |                   40 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 42.753 |      0.149 |                   39 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.11100746268656717
[2m[36m(func pid=151704)[0m top5: 0.6422574626865671
[2m[36m(func pid=151704)[0m f1_micro: 0.11100746268656717
[2m[36m(func pid=151704)[0m f1_macro: 0.1430562557892237
[2m[36m(func pid=151704)[0m f1_weighted: 0.09102909689764434
[2m[36m(func pid=151704)[0m f1_per_class: [0.143, 0.0, 0.465, 0.198, 0.037, 0.0, 0.0, 0.46, 0.083, 0.044]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.8358 | Steps: 4 | Val loss: 3.1092 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=152923)[0m top1: 0.15391791044776118
[2m[36m(func pid=152923)[0m top5: 0.7574626865671642
[2m[36m(func pid=152923)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=152923)[0m f1_macro: 0.09851184065421012
[2m[36m(func pid=152923)[0m f1_weighted: 0.1278522312905388
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.032, 0.038, 0.042, 0.0, 0.0, 0.305, 0.231, 0.108, 0.23]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 5.5094 | Steps: 4 | Val loss: 4.6842 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.7803 | Steps: 4 | Val loss: 2.2955 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=152082)[0m top1: 0.17630597014925373
[2m[36m(func pid=152082)[0m top5: 0.6431902985074627
[2m[36m(func pid=152082)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=152082)[0m f1_macro: 0.1509532762260094
[2m[36m(func pid=152082)[0m f1_weighted: 0.11959720826891743
[2m[36m(func pid=152082)[0m f1_per_class: [0.148, 0.323, 0.235, 0.0, 0.065, 0.354, 0.0, 0.308, 0.0, 0.077]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m top1: 0.259794776119403
[2m[36m(func pid=152502)[0m top5: 0.8344216417910447
[2m[36m(func pid=152502)[0m f1_micro: 0.259794776119403
[2m[36m(func pid=152502)[0m f1_macro: 0.25991848390851746
[2m[36m(func pid=152502)[0m f1_weighted: 0.2948889819561761
[2m[36m(func pid=152502)[0m f1_per_class: [0.229, 0.217, 0.471, 0.346, 0.271, 0.249, 0.343, 0.282, 0.109, 0.082]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 62.2059 | Steps: 4 | Val loss: 43.4150 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 10:21:50 (running for 00:04:20.29)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.78  |      0.136 |                   41 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.836 |      0.151 |                   40 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.509 |      0.26  |                   41 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 80.403 |      0.099 |                   40 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.08768656716417911
[2m[36m(func pid=151704)[0m top5: 0.6786380597014925
[2m[36m(func pid=151704)[0m f1_micro: 0.08768656716417911
[2m[36m(func pid=151704)[0m f1_macro: 0.136008616023972
[2m[36m(func pid=151704)[0m f1_weighted: 0.07066867140345029
[2m[36m(func pid=151704)[0m f1_per_class: [0.13, 0.0, 0.474, 0.119, 0.037, 0.0, 0.003, 0.491, 0.057, 0.049]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m top1: 0.3125
[2m[36m(func pid=152923)[0m top5: 0.8372201492537313
[2m[36m(func pid=152923)[0m f1_micro: 0.3125
[2m[36m(func pid=152923)[0m f1_macro: 0.21009124575702262
[2m[36m(func pid=152923)[0m f1_weighted: 0.29318279010050574
[2m[36m(func pid=152923)[0m f1_per_class: [0.269, 0.308, 0.038, 0.199, 0.1, 0.0, 0.511, 0.381, 0.026, 0.27]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 3.5349 | Steps: 4 | Val loss: 3.1917 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 5.6857 | Steps: 4 | Val loss: 5.0630 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.4819 | Steps: 4 | Val loss: 2.3010 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=152082)[0m top1: 0.16324626865671643
[2m[36m(func pid=152082)[0m top5: 0.6352611940298507
[2m[36m(func pid=152082)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=152082)[0m f1_macro: 0.15577144248043673
[2m[36m(func pid=152082)[0m f1_weighted: 0.10541579885749386
[2m[36m(func pid=152082)[0m f1_per_class: [0.235, 0.304, 0.414, 0.0, 0.043, 0.229, 0.0, 0.332, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 49.9262 | Steps: 4 | Val loss: 33.7732 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=152502)[0m top1: 0.2775186567164179
[2m[36m(func pid=152502)[0m top5: 0.8479477611940298
[2m[36m(func pid=152502)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=152502)[0m f1_macro: 0.2428945777573678
[2m[36m(func pid=152502)[0m f1_weighted: 0.2516823869356622
[2m[36m(func pid=152502)[0m f1_per_class: [0.22, 0.198, 0.56, 0.538, 0.29, 0.239, 0.047, 0.265, 0.0, 0.073]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:21:55 (running for 00:04:25.46)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.482 |      0.132 |                   42 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  3.535 |      0.156 |                   41 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.686 |      0.243 |                   42 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 62.206 |      0.21  |                   41 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.07555970149253731
[2m[36m(func pid=151704)[0m top5: 0.7453358208955224
[2m[36m(func pid=151704)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=151704)[0m f1_macro: 0.13218906075866754
[2m[36m(func pid=151704)[0m f1_weighted: 0.07178864854801648
[2m[36m(func pid=151704)[0m f1_per_class: [0.128, 0.0, 0.514, 0.089, 0.035, 0.0, 0.054, 0.389, 0.054, 0.059]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m top1: 0.45009328358208955
[2m[36m(func pid=152923)[0m top5: 0.894589552238806
[2m[36m(func pid=152923)[0m f1_micro: 0.45009328358208955
[2m[36m(func pid=152923)[0m f1_macro: 0.3057735352063754
[2m[36m(func pid=152923)[0m f1_weighted: 0.42438964096443665
[2m[36m(func pid=152923)[0m f1_per_class: [0.271, 0.535, 0.35, 0.426, 0.296, 0.07, 0.577, 0.37, 0.028, 0.135]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.3220 | Steps: 4 | Val loss: 3.3110 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 5.4337 | Steps: 4 | Val loss: 5.6099 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.4945 | Steps: 4 | Val loss: 2.2700 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=152082)[0m top1: 0.11194029850746269
[2m[36m(func pid=152082)[0m top5: 0.6198694029850746
[2m[36m(func pid=152082)[0m f1_micro: 0.11194029850746269
[2m[36m(func pid=152082)[0m f1_macro: 0.14416562851140957
[2m[36m(func pid=152082)[0m f1_weighted: 0.08219158869861874
[2m[36m(func pid=152082)[0m f1_per_class: [0.044, 0.246, 0.48, 0.0, 0.03, 0.039, 0.0, 0.461, 0.142, 0.0]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m top1: 0.2453358208955224
[2m[36m(func pid=152502)[0m top5: 0.8805970149253731
[2m[36m(func pid=152502)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=152502)[0m f1_macro: 0.20134979190162036
[2m[36m(func pid=152502)[0m f1_weighted: 0.23716183568850824
[2m[36m(func pid=152502)[0m f1_per_class: [0.33, 0.398, 0.174, 0.439, 0.253, 0.0, 0.08, 0.189, 0.0, 0.152]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 51.7958 | Steps: 4 | Val loss: 63.0862 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 10:22:01 (running for 00:04:30.74)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.494 |      0.11  |                   43 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.322 |      0.144 |                   42 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.434 |      0.201 |                   43 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 49.926 |      0.306 |                   42 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.08815298507462686
[2m[36m(func pid=151704)[0m top5: 0.7971082089552238
[2m[36m(func pid=151704)[0m f1_micro: 0.08815298507462686
[2m[36m(func pid=151704)[0m f1_macro: 0.11031407856731099
[2m[36m(func pid=151704)[0m f1_weighted: 0.09266468699852652
[2m[36m(func pid=151704)[0m f1_per_class: [0.143, 0.0, 0.429, 0.078, 0.036, 0.0, 0.189, 0.104, 0.062, 0.062]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m top1: 0.2775186567164179
[2m[36m(func pid=152923)[0m top5: 0.7541977611940298
[2m[36m(func pid=152923)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=152923)[0m f1_macro: 0.2208829184100692
[2m[36m(func pid=152923)[0m f1_weighted: 0.1958587309125939
[2m[36m(func pid=152923)[0m f1_per_class: [0.185, 0.47, 0.379, 0.179, 0.235, 0.274, 0.003, 0.421, 0.0, 0.062]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 5.4799 | Steps: 4 | Val loss: 5.2380 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.9521 | Steps: 4 | Val loss: 3.3567 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.5430 | Steps: 4 | Val loss: 2.1952 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=152502)[0m top1: 0.4006529850746269
[2m[36m(func pid=152502)[0m top5: 0.8773320895522388
[2m[36m(func pid=152502)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=152502)[0m f1_macro: 0.23221160798204749
[2m[36m(func pid=152502)[0m f1_weighted: 0.3933323345891231
[2m[36m(func pid=152502)[0m f1_per_class: [0.112, 0.533, 0.073, 0.374, 0.189, 0.0, 0.577, 0.339, 0.0, 0.125]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 44.9852 | Steps: 4 | Val loss: 84.6542 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=152082)[0m top1: 0.07835820895522388
[2m[36m(func pid=152082)[0m top5: 0.7765858208955224
[2m[36m(func pid=152082)[0m f1_micro: 0.07835820895522388
[2m[36m(func pid=152082)[0m f1_macro: 0.15354189817030978
[2m[36m(func pid=152082)[0m f1_weighted: 0.06751338013583104
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.161, 0.72, 0.0, 0.026, 0.007, 0.006, 0.493, 0.122, 0.0]
[2m[36m(func pid=152082)[0m 
== Status ==
Current time: 2024-01-07 10:22:06 (running for 00:04:36.14)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.543 |      0.125 |                   44 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.952 |      0.154 |                   43 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.48  |      0.232 |                   44 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 51.796 |      0.221 |                   43 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.13712686567164178
[2m[36m(func pid=151704)[0m top5: 0.8194962686567164
[2m[36m(func pid=151704)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=151704)[0m f1_macro: 0.124594885509309
[2m[36m(func pid=151704)[0m f1_weighted: 0.1531863129921234
[2m[36m(func pid=151704)[0m f1_per_class: [0.165, 0.0, 0.426, 0.142, 0.037, 0.0, 0.35, 0.0, 0.073, 0.053]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m top1: 0.20242537313432835
[2m[36m(func pid=152923)[0m top5: 0.6343283582089553
[2m[36m(func pid=152923)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=152923)[0m f1_macro: 0.1543289382705016
[2m[36m(func pid=152923)[0m f1_weighted: 0.18817362985270963
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.457, 0.192, 0.276, 0.05, 0.153, 0.0, 0.188, 0.0, 0.227]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 5.1910 | Steps: 4 | Val loss: 6.1099 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 3.2851 | Steps: 4 | Val loss: 2.8444 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.4585 | Steps: 4 | Val loss: 2.1018 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=152502)[0m top1: 0.24440298507462688
[2m[36m(func pid=152502)[0m top5: 0.8861940298507462
[2m[36m(func pid=152502)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=152502)[0m f1_macro: 0.19940617397332153
[2m[36m(func pid=152502)[0m f1_weighted: 0.2598346836627987
[2m[36m(func pid=152502)[0m f1_per_class: [0.124, 0.546, 0.065, 0.168, 0.298, 0.0, 0.305, 0.335, 0.076, 0.077]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 61.6365 | Steps: 4 | Val loss: 106.1494 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=152082)[0m top1: 0.11147388059701492
[2m[36m(func pid=152082)[0m top5: 0.855410447761194
[2m[36m(func pid=152082)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=152082)[0m f1_macro: 0.13530862554546724
[2m[36m(func pid=152082)[0m f1_weighted: 0.12694586239184877
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.209, 0.489, 0.0, 0.026, 0.029, 0.223, 0.225, 0.152, 0.0]
[2m[36m(func pid=152082)[0m 
== Status ==
Current time: 2024-01-07 10:22:11 (running for 00:04:41.47)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.458 |      0.155 |                   45 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  3.285 |      0.135 |                   44 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.191 |      0.199 |                   45 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 44.985 |      0.154 |                   44 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.20289179104477612
[2m[36m(func pid=151704)[0m top5: 0.8432835820895522
[2m[36m(func pid=151704)[0m f1_micro: 0.20289179104477612
[2m[36m(func pid=151704)[0m f1_macro: 0.15497121962281662
[2m[36m(func pid=151704)[0m f1_weighted: 0.22272308748782077
[2m[36m(func pid=151704)[0m f1_per_class: [0.226, 0.0, 0.408, 0.249, 0.038, 0.0, 0.475, 0.0, 0.112, 0.042]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m top1: 0.1296641791044776
[2m[36m(func pid=152923)[0m top5: 0.5597014925373134
[2m[36m(func pid=152923)[0m f1_micro: 0.1296641791044776
[2m[36m(func pid=152923)[0m f1_macro: 0.1308005077642026
[2m[36m(func pid=152923)[0m f1_weighted: 0.15727522901516744
[2m[36m(func pid=152923)[0m f1_per_class: [0.128, 0.203, 0.144, 0.355, 0.025, 0.033, 0.0, 0.227, 0.028, 0.167]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 3.3873 | Steps: 4 | Val loss: 6.2320 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 3.1036 | Steps: 4 | Val loss: 2.1973 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.4597 | Steps: 4 | Val loss: 2.0436 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 52.2519 | Steps: 4 | Val loss: 94.3844 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
[2m[36m(func pid=152502)[0m top1: 0.24766791044776118
[2m[36m(func pid=152502)[0m top5: 0.8334888059701493
[2m[36m(func pid=152502)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=152502)[0m f1_macro: 0.24270903612240974
[2m[36m(func pid=152502)[0m f1_weighted: 0.20194997791200459
[2m[36m(func pid=152502)[0m f1_per_class: [0.25, 0.53, 0.137, 0.099, 0.333, 0.36, 0.006, 0.453, 0.11, 0.148]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m top1: 0.240205223880597
[2m[36m(func pid=152082)[0m top5: 0.9333022388059702
[2m[36m(func pid=152082)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=152082)[0m f1_macro: 0.1358361549941638
[2m[36m(func pid=152082)[0m f1_weighted: 0.2841537517896793
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.251, 0.132, 0.359, 0.035, 0.183, 0.399, 0.0, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
== Status ==
Current time: 2024-01-07 10:22:17 (running for 00:04:46.90)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.46  |      0.168 |                   46 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  3.104 |      0.136 |                   45 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  3.387 |      0.243 |                   46 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 61.637 |      0.131 |                   45 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.2677238805970149
[2m[36m(func pid=151704)[0m top5: 0.8740671641791045
[2m[36m(func pid=151704)[0m f1_micro: 0.2677238805970149
[2m[36m(func pid=151704)[0m f1_macro: 0.1679166526079928
[2m[36m(func pid=151704)[0m f1_weighted: 0.2672387928452925
[2m[36m(func pid=151704)[0m f1_per_class: [0.252, 0.0, 0.344, 0.308, 0.044, 0.0, 0.569, 0.0, 0.117, 0.047]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m top1: 0.15858208955223882
[2m[36m(func pid=152923)[0m top5: 0.5928171641791045
[2m[36m(func pid=152923)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=152923)[0m f1_macro: 0.1861101185356499
[2m[36m(func pid=152923)[0m f1_weighted: 0.17762580214281487
[2m[36m(func pid=152923)[0m f1_per_class: [0.333, 0.114, 0.324, 0.438, 0.027, 0.042, 0.003, 0.272, 0.082, 0.225]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 5.0639 | Steps: 4 | Val loss: 7.7311 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.3545 | Steps: 4 | Val loss: 1.8840 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.3441 | Steps: 4 | Val loss: 2.0187 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 40.1544 | Steps: 4 | Val loss: 58.9143 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=152502)[0m top1: 0.20335820895522388
[2m[36m(func pid=152502)[0m top5: 0.7975746268656716
[2m[36m(func pid=152502)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=152502)[0m f1_macro: 0.20758457491628407
[2m[36m(func pid=152502)[0m f1_weighted: 0.17410550455752996
[2m[36m(func pid=152502)[0m f1_per_class: [0.207, 0.422, 0.4, 0.165, 0.187, 0.217, 0.006, 0.33, 0.0, 0.143]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m top1: 0.35447761194029853
[2m[36m(func pid=152082)[0m top5: 0.9398320895522388
[2m[36m(func pid=152082)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=152082)[0m f1_macro: 0.1554432409272169
[2m[36m(func pid=152082)[0m f1_weighted: 0.3043052268472985
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.011, 0.198, 0.562, 0.077, 0.362, 0.344, 0.0, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
== Status ==
Current time: 2024-01-07 10:22:22 (running for 00:04:52.15)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.344 |      0.176 |                   47 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.355 |      0.155 |                   46 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.064 |      0.208 |                   47 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 52.252 |      0.186 |                   46 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.31949626865671643
[2m[36m(func pid=151704)[0m top5: 0.8857276119402985
[2m[36m(func pid=151704)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=151704)[0m f1_macro: 0.1757745493835906
[2m[36m(func pid=151704)[0m f1_weighted: 0.3057609048358776
[2m[36m(func pid=151704)[0m f1_per_class: [0.244, 0.0, 0.25, 0.43, 0.053, 0.0, 0.583, 0.0, 0.128, 0.069]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152923)[0m top1: 0.261660447761194
[2m[36m(func pid=152923)[0m top5: 0.7901119402985075
[2m[36m(func pid=152923)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=152923)[0m f1_macro: 0.24452519470503836
[2m[36m(func pid=152923)[0m f1_weighted: 0.2655746165094266
[2m[36m(func pid=152923)[0m f1_per_class: [0.216, 0.372, 0.267, 0.456, 0.059, 0.233, 0.019, 0.507, 0.125, 0.191]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 4.8939 | Steps: 4 | Val loss: 6.7031 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.3333 | Steps: 4 | Val loss: 2.1310 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.4150 | Steps: 4 | Val loss: 1.9970 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 55.6237 | Steps: 4 | Val loss: 46.8901 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=152502)[0m top1: 0.19916044776119404
[2m[36m(func pid=152502)[0m top5: 0.7215485074626866
[2m[36m(func pid=152502)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=152502)[0m f1_macro: 0.2697566947783203
[2m[36m(func pid=152502)[0m f1_weighted: 0.2273810698068286
[2m[36m(func pid=152502)[0m f1_per_class: [0.35, 0.31, 0.632, 0.385, 0.031, 0.057, 0.049, 0.454, 0.141, 0.289]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m top1: 0.34421641791044777
[2m[36m(func pid=152082)[0m top5: 0.9263059701492538
[2m[36m(func pid=152082)[0m f1_micro: 0.34421641791044777
[2m[36m(func pid=152082)[0m f1_macro: 0.20539323976921436
[2m[36m(func pid=152082)[0m f1_weighted: 0.22975673439612174
[2m[36m(func pid=152082)[0m f1_per_class: [0.25, 0.0, 0.645, 0.519, 0.14, 0.383, 0.102, 0.016, 0.0, 0.0]
[2m[36m(func pid=152082)[0m 
== Status ==
Current time: 2024-01-07 10:22:27 (running for 00:04:57.27)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.344 |      0.176 |                   47 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.333 |      0.205 |                   47 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.894 |      0.27  |                   48 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 55.624 |      0.258 |                   48 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152923)[0m top1: 0.34468283582089554
[2m[36m(func pid=152923)[0m top5: 0.84375
[2m[36m(func pid=152923)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=152923)[0m f1_macro: 0.25836201704116396
[2m[36m(func pid=152923)[0m f1_weighted: 0.34202593970164624
[2m[36m(func pid=152923)[0m f1_per_class: [0.118, 0.513, 0.375, 0.175, 0.175, 0.142, 0.529, 0.333, 0.143, 0.08]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=151704)[0m top1: 0.34048507462686567
[2m[36m(func pid=151704)[0m top5: 0.8847947761194029
[2m[36m(func pid=151704)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=151704)[0m f1_macro: 0.1593262978126178
[2m[36m(func pid=151704)[0m f1_weighted: 0.3149420483616184
[2m[36m(func pid=151704)[0m f1_per_class: [0.075, 0.0, 0.189, 0.469, 0.058, 0.0, 0.59, 0.0, 0.13, 0.083]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 4.4706 | Steps: 4 | Val loss: 9.2605 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 3.0775 | Steps: 4 | Val loss: 2.1392 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 34.5277 | Steps: 4 | Val loss: 54.7689 | Batch size: 32 | lr: 0.1 | Duration: 2.61s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.2738 | Steps: 4 | Val loss: 1.9720 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=152502)[0m top1: 0.11054104477611941
[2m[36m(func pid=152502)[0m top5: 0.7896455223880597
[2m[36m(func pid=152502)[0m f1_micro: 0.11054104477611941
[2m[36m(func pid=152502)[0m f1_macro: 0.1754935966184904
[2m[36m(func pid=152502)[0m f1_weighted: 0.1368733436548634
[2m[36m(func pid=152502)[0m f1_per_class: [0.226, 0.281, 0.556, 0.007, 0.047, 0.0, 0.183, 0.386, 0.026, 0.043]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m top1: 0.35261194029850745
[2m[36m(func pid=152082)[0m top5: 0.9006529850746269
[2m[36m(func pid=152082)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=152082)[0m f1_macro: 0.23926152741014844
[2m[36m(func pid=152082)[0m f1_weighted: 0.23682841005248026
[2m[36m(func pid=152082)[0m f1_per_class: [0.322, 0.0, 0.471, 0.516, 0.241, 0.409, 0.053, 0.314, 0.0, 0.067]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.3208955223880597
[2m[36m(func pid=152923)[0m top5: 0.7989738805970149
[2m[36m(func pid=152923)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=152923)[0m f1_macro: 0.20225029799494995
[2m[36m(func pid=152923)[0m f1_weighted: 0.27763585586310724
[2m[36m(func pid=152923)[0m f1_per_class: [0.105, 0.507, 0.357, 0.023, 0.0, 0.0, 0.518, 0.413, 0.023, 0.077]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:22:33 (running for 00:05:02.82)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.274 |      0.154 |                   49 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  3.077 |      0.239 |                   48 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.471 |      0.175 |                   49 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 34.528 |      0.202 |                   49 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.3521455223880597
[2m[36m(func pid=151704)[0m top5: 0.8805970149253731
[2m[36m(func pid=151704)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=151704)[0m f1_macro: 0.15375922647337323
[2m[36m(func pid=151704)[0m f1_weighted: 0.31592071665765453
[2m[36m(func pid=151704)[0m f1_per_class: [0.059, 0.016, 0.175, 0.475, 0.061, 0.0, 0.586, 0.0, 0.063, 0.103]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 5.6152 | Steps: 4 | Val loss: 8.2419 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.5480 | Steps: 4 | Val loss: 2.0101 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 47.2850 | Steps: 4 | Val loss: 59.6779 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.5587 | Steps: 4 | Val loss: 1.9630 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=152502)[0m top1: 0.14225746268656717
[2m[36m(func pid=152502)[0m top5: 0.8199626865671642
[2m[36m(func pid=152502)[0m f1_micro: 0.14225746268656717
[2m[36m(func pid=152502)[0m f1_macro: 0.21175490576115624
[2m[36m(func pid=152502)[0m f1_weighted: 0.15516490774536992
[2m[36m(func pid=152502)[0m f1_per_class: [0.278, 0.362, 0.632, 0.038, 0.036, 0.0, 0.165, 0.295, 0.109, 0.203]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m top1: 0.3516791044776119
[2m[36m(func pid=152082)[0m top5: 0.8773320895522388
[2m[36m(func pid=152082)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=152082)[0m f1_macro: 0.24158877681205754
[2m[36m(func pid=152082)[0m f1_weighted: 0.26330569724120334
[2m[36m(func pid=152082)[0m f1_per_class: [0.248, 0.005, 0.25, 0.545, 0.2, 0.422, 0.074, 0.521, 0.0, 0.15]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.24486940298507462
[2m[36m(func pid=152923)[0m top5: 0.769589552238806
[2m[36m(func pid=152923)[0m f1_micro: 0.24486940298507462
[2m[36m(func pid=152923)[0m f1_macro: 0.16970946390042746
[2m[36m(func pid=152923)[0m f1_weighted: 0.2571318632403655
[2m[36m(func pid=152923)[0m f1_per_class: [0.099, 0.506, 0.068, 0.214, 0.0, 0.082, 0.251, 0.393, 0.0, 0.084]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=151704)[0m top1: 0.36007462686567165
[2m[36m(func pid=151704)[0m top5: 0.875
[2m[36m(func pid=151704)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=151704)[0m f1_macro: 0.15549146245197049
[2m[36m(func pid=151704)[0m f1_weighted: 0.32112670505178953
[2m[36m(func pid=151704)[0m f1_per_class: [0.066, 0.026, 0.161, 0.481, 0.067, 0.008, 0.59, 0.0, 0.049, 0.107]
== Status ==
Current time: 2024-01-07 10:22:38 (running for 00:05:08.17)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.559 |      0.155 |                   50 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.548 |      0.242 |                   49 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.615 |      0.212 |                   50 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 47.285 |      0.17  |                   50 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 5.7276 | Steps: 4 | Val loss: 8.2351 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.3876 | Steps: 4 | Val loss: 2.3643 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 34.1166 | Steps: 4 | Val loss: 64.8628 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.4356 | Steps: 4 | Val loss: 1.9548 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=152502)[0m top1: 0.15485074626865672
[2m[36m(func pid=152502)[0m top5: 0.7975746268656716
[2m[36m(func pid=152502)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=152502)[0m f1_macro: 0.23475851040290427
[2m[36m(func pid=152502)[0m f1_weighted: 0.1762220797943303
[2m[36m(func pid=152502)[0m f1_per_class: [0.33, 0.425, 0.632, 0.182, 0.033, 0.0, 0.045, 0.383, 0.112, 0.207]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m top1: 0.18703358208955223
[2m[36m(func pid=152082)[0m top5: 0.8083022388059702
[2m[36m(func pid=152082)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=152082)[0m f1_macro: 0.2135321767579624
[2m[36m(func pid=152082)[0m f1_weighted: 0.1310708498059994
[2m[36m(func pid=152082)[0m f1_per_class: [0.315, 0.259, 0.444, 0.013, 0.255, 0.425, 0.003, 0.377, 0.0, 0.044]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.22527985074626866
[2m[36m(func pid=152923)[0m top5: 0.7663246268656716
[2m[36m(func pid=152923)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=152923)[0m f1_macro: 0.15278739983722878
[2m[36m(func pid=152923)[0m f1_weighted: 0.22781643982773173
[2m[36m(func pid=152923)[0m f1_per_class: [0.043, 0.352, 0.062, 0.395, 0.0, 0.25, 0.023, 0.339, 0.0, 0.064]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:22:43 (running for 00:05:13.24)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.436 |      0.171 |                   51 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.388 |      0.214 |                   50 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.728 |      0.235 |                   51 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 34.117 |      0.153 |                   51 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.3666044776119403
[2m[36m(func pid=151704)[0m top5: 0.8763992537313433
[2m[36m(func pid=151704)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=151704)[0m f1_macro: 0.17134280958595513
[2m[36m(func pid=151704)[0m f1_weighted: 0.33972345163061635
[2m[36m(func pid=151704)[0m f1_per_class: [0.065, 0.1, 0.145, 0.478, 0.068, 0.105, 0.576, 0.0, 0.037, 0.141]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 5.8345 | Steps: 4 | Val loss: 5.9793 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.1253 | Steps: 4 | Val loss: 3.0647 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 47.4687 | Steps: 4 | Val loss: 60.2684 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.3844 | Steps: 4 | Val loss: 1.9227 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=152502)[0m top1: 0.29384328358208955
[2m[36m(func pid=152502)[0m top5: 0.7877798507462687
[2m[36m(func pid=152502)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=152502)[0m f1_macro: 0.2711946636514232
[2m[36m(func pid=152502)[0m f1_weighted: 0.2876215591760365
[2m[36m(func pid=152502)[0m f1_per_class: [0.281, 0.544, 0.667, 0.401, 0.036, 0.0, 0.136, 0.47, 0.101, 0.077]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m top1: 0.19542910447761194
[2m[36m(func pid=152082)[0m top5: 0.582089552238806
[2m[36m(func pid=152082)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=152082)[0m f1_macro: 0.18579178328825133
[2m[36m(func pid=152082)[0m f1_weighted: 0.1446060681389311
[2m[36m(func pid=152082)[0m f1_per_class: [0.298, 0.39, 0.07, 0.0, 0.273, 0.428, 0.0, 0.334, 0.0, 0.066]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.29384328358208955
[2m[36m(func pid=152923)[0m top5: 0.7784514925373134
[2m[36m(func pid=152923)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=152923)[0m f1_macro: 0.20186847108623543
[2m[36m(func pid=152923)[0m f1_weighted: 0.2660237702071669
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.333, 0.082, 0.454, 0.19, 0.389, 0.041, 0.376, 0.0, 0.154]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:22:49 (running for 00:05:18.73)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.384 |      0.198 |                   52 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.125 |      0.186 |                   51 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.835 |      0.271 |                   52 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 47.469 |      0.202 |                   52 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.3829291044776119
[2m[36m(func pid=151704)[0m top5: 0.8773320895522388
[2m[36m(func pid=151704)[0m f1_micro: 0.3829291044776119
[2m[36m(func pid=151704)[0m f1_macro: 0.19807365426768184
[2m[36m(func pid=151704)[0m f1_weighted: 0.3681994558185937
[2m[36m(func pid=151704)[0m f1_per_class: [0.033, 0.217, 0.179, 0.457, 0.08, 0.258, 0.567, 0.0, 0.021, 0.169]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 5.6743 | Steps: 4 | Val loss: 4.2521 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.9299 | Steps: 4 | Val loss: 3.5553 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 31.3234 | Steps: 4 | Val loss: 56.5208 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.4047 | Steps: 4 | Val loss: 1.9220 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=152502)[0m top1: 0.31902985074626866
[2m[36m(func pid=152502)[0m top5: 0.9053171641791045
[2m[36m(func pid=152502)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=152502)[0m f1_macro: 0.23708107834865183
[2m[36m(func pid=152502)[0m f1_weighted: 0.2972578125946755
[2m[36m(func pid=152502)[0m f1_per_class: [0.305, 0.557, 0.4, 0.335, 0.097, 0.182, 0.204, 0.29, 0.0, 0.0]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m top1: 0.20569029850746268
[2m[36m(func pid=152082)[0m top5: 0.5825559701492538
[2m[36m(func pid=152082)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=152082)[0m f1_macro: 0.19657375112896566
[2m[36m(func pid=152082)[0m f1_weighted: 0.16457560359453538
[2m[36m(func pid=152082)[0m f1_per_class: [0.111, 0.483, 0.041, 0.0, 0.261, 0.474, 0.0, 0.291, 0.115, 0.19]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.25513059701492535
[2m[36m(func pid=152923)[0m top5: 0.777518656716418
[2m[36m(func pid=152923)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=152923)[0m f1_macro: 0.22287164031367554
[2m[36m(func pid=152923)[0m f1_weighted: 0.22792101387590352
[2m[36m(func pid=152923)[0m f1_per_class: [0.179, 0.42, 0.142, 0.249, 0.291, 0.299, 0.065, 0.394, 0.04, 0.15]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:22:54 (running for 00:05:23.84)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.405 |      0.222 |                   53 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.93  |      0.197 |                   52 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.674 |      0.237 |                   53 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.323 |      0.223 |                   53 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.3931902985074627
[2m[36m(func pid=151704)[0m top5: 0.871268656716418
[2m[36m(func pid=151704)[0m f1_micro: 0.39319029850746273
[2m[36m(func pid=151704)[0m f1_macro: 0.22239437014508998
[2m[36m(func pid=151704)[0m f1_weighted: 0.39187787213779157
[2m[36m(func pid=151704)[0m f1_per_class: [0.099, 0.296, 0.185, 0.435, 0.095, 0.379, 0.572, 0.0, 0.0, 0.164]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 4.4864 | Steps: 4 | Val loss: 4.0984 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.3393 | Steps: 4 | Val loss: 3.7091 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 21.7129 | Steps: 4 | Val loss: 86.3572 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.2879 | Steps: 4 | Val loss: 1.9173 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=152502)[0m top1: 0.46222014925373134
[2m[36m(func pid=152502)[0m top5: 0.9244402985074627
[2m[36m(func pid=152502)[0m f1_micro: 0.46222014925373134
[2m[36m(func pid=152502)[0m f1_macro: 0.3221641768575327
[2m[36m(func pid=152502)[0m f1_weighted: 0.4589217377659669
[2m[36m(func pid=152502)[0m f1_per_class: [0.202, 0.503, 0.15, 0.536, 0.368, 0.494, 0.442, 0.449, 0.0, 0.077]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m top1: 0.18190298507462688
[2m[36m(func pid=152082)[0m top5: 0.6077425373134329
[2m[36m(func pid=152082)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=152082)[0m f1_macro: 0.18244534809915083
[2m[36m(func pid=152082)[0m f1_weighted: 0.14466953621503192
[2m[36m(func pid=152082)[0m f1_per_class: [0.083, 0.501, 0.058, 0.0, 0.286, 0.208, 0.0, 0.442, 0.096, 0.151]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.1837686567164179
[2m[36m(func pid=152923)[0m top5: 0.6697761194029851
[2m[36m(func pid=152923)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=152923)[0m f1_macro: 0.24968532166465024
[2m[36m(func pid=152923)[0m f1_weighted: 0.16673098637294995
[2m[36m(func pid=152923)[0m f1_per_class: [0.276, 0.459, 0.556, 0.02, 0.122, 0.366, 0.003, 0.419, 0.094, 0.182]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=151704)[0m top1: 0.37406716417910446
[2m[36m(func pid=151704)[0m top5: 0.8684701492537313
[2m[36m(func pid=151704)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=151704)[0m f1_macro: 0.23382185406894201
[2m[36m(func pid=151704)[0m f1_weighted: 0.3745551551193428
[2m[36m(func pid=151704)[0m f1_per_class: [0.2, 0.345, 0.242, 0.368, 0.087, 0.391, 0.535, 0.0, 0.021, 0.15]
== Status ==
Current time: 2024-01-07 10:22:59 (running for 00:05:29.02)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.288 |      0.234 |                   54 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.339 |      0.182 |                   53 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.486 |      0.322 |                   54 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 21.713 |      0.25  |                   54 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 6.6823 | Steps: 4 | Val loss: 5.7355 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.4415 | Steps: 4 | Val loss: 3.7683 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 31.7139 | Steps: 4 | Val loss: 105.8854 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.4537 | Steps: 4 | Val loss: 1.9103 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=152502)[0m top1: 0.39972014925373134
[2m[36m(func pid=152502)[0m top5: 0.9174440298507462
[2m[36m(func pid=152502)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=152502)[0m f1_macro: 0.22643639719446457
[2m[36m(func pid=152502)[0m f1_weighted: 0.38047164162792885
[2m[36m(func pid=152502)[0m f1_per_class: [0.098, 0.286, 0.121, 0.551, 0.118, 0.396, 0.383, 0.234, 0.0, 0.077]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m top1: 0.17490671641791045
[2m[36m(func pid=152082)[0m top5: 0.6716417910447762
[2m[36m(func pid=152082)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=152082)[0m f1_macro: 0.1987073369629287
[2m[36m(func pid=152082)[0m f1_weighted: 0.1441332151159388
[2m[36m(func pid=152082)[0m f1_per_class: [0.289, 0.492, 0.171, 0.0, 0.25, 0.008, 0.055, 0.501, 0.087, 0.133]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.1553171641791045
[2m[36m(func pid=152923)[0m top5: 0.5951492537313433
[2m[36m(func pid=152923)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=152923)[0m f1_macro: 0.1910774089652679
[2m[36m(func pid=152923)[0m f1_weighted: 0.1276021403209322
[2m[36m(func pid=152923)[0m f1_per_class: [0.186, 0.462, 0.632, 0.0, 0.057, 0.159, 0.0, 0.329, 0.087, 0.0]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:23:04 (running for 00:05:34.40)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.454 |      0.222 |                   55 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.441 |      0.199 |                   54 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  6.682 |      0.226 |                   55 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.714 |      0.191 |                   55 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.3460820895522388
[2m[36m(func pid=151704)[0m top5: 0.871268656716418
[2m[36m(func pid=151704)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=151704)[0m f1_macro: 0.22246298658023372
[2m[36m(func pid=151704)[0m f1_weighted: 0.3353459242112459
[2m[36m(func pid=151704)[0m f1_per_class: [0.23, 0.331, 0.355, 0.272, 0.091, 0.388, 0.503, 0.0, 0.0, 0.054]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 6.7761 | Steps: 4 | Val loss: 5.5740 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.2537 | Steps: 4 | Val loss: 3.7453 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 80.2577 | Steps: 4 | Val loss: 109.4846 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.3989 | Steps: 4 | Val loss: 1.9295 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=152502)[0m top1: 0.34048507462686567
[2m[36m(func pid=152502)[0m top5: 0.9039179104477612
[2m[36m(func pid=152502)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=152502)[0m f1_macro: 0.22342389921631636
[2m[36m(func pid=152502)[0m f1_weighted: 0.34459913047985896
[2m[36m(func pid=152502)[0m f1_per_class: [0.176, 0.251, 0.158, 0.476, 0.0, 0.358, 0.357, 0.175, 0.154, 0.129]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m top1: 0.2868470149253731
[2m[36m(func pid=152082)[0m top5: 0.6847014925373134
[2m[36m(func pid=152082)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=152082)[0m f1_macro: 0.2646472663152236
[2m[36m(func pid=152082)[0m f1_weighted: 0.2782796728503374
[2m[36m(func pid=152082)[0m f1_per_class: [0.327, 0.508, 0.381, 0.0, 0.29, 0.008, 0.508, 0.385, 0.101, 0.138]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.18050373134328357
[2m[36m(func pid=152923)[0m top5: 0.5629664179104478
[2m[36m(func pid=152923)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=152923)[0m f1_macro: 0.18891427826532517
[2m[36m(func pid=152923)[0m f1_weighted: 0.13296073482102097
[2m[36m(func pid=152923)[0m f1_per_class: [0.218, 0.431, 0.471, 0.032, 0.042, 0.119, 0.0, 0.451, 0.049, 0.077]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:23:09 (running for 00:05:39.53)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.399 |      0.209 |                   56 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.254 |      0.265 |                   55 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  6.776 |      0.223 |                   56 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 80.258 |      0.189 |                   56 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.30597014925373134
[2m[36m(func pid=151704)[0m top5: 0.871268656716418
[2m[36m(func pid=151704)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=151704)[0m f1_macro: 0.20931212086216405
[2m[36m(func pid=151704)[0m f1_weighted: 0.2829083461940973
[2m[36m(func pid=151704)[0m f1_per_class: [0.224, 0.348, 0.349, 0.197, 0.104, 0.374, 0.391, 0.0, 0.0, 0.105]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 11.5030 | Steps: 4 | Val loss: 8.3646 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.8791 | Steps: 4 | Val loss: 3.7634 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 39.3894 | Steps: 4 | Val loss: 81.0272 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.4066 | Steps: 4 | Val loss: 1.9361 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=152502)[0m top1: 0.1707089552238806
[2m[36m(func pid=152502)[0m top5: 0.8880597014925373
[2m[36m(func pid=152502)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=152502)[0m f1_macro: 0.21543654351360755
[2m[36m(func pid=152502)[0m f1_weighted: 0.2060497777530831
[2m[36m(func pid=152502)[0m f1_per_class: [0.043, 0.325, 0.415, 0.146, 0.222, 0.386, 0.151, 0.175, 0.084, 0.207]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m top1: 0.3670708955223881
[2m[36m(func pid=152082)[0m top5: 0.6842350746268657
[2m[36m(func pid=152082)[0m f1_micro: 0.3670708955223881
[2m[36m(func pid=152082)[0m f1_macro: 0.25566576748205205
[2m[36m(func pid=152082)[0m f1_weighted: 0.2897640993885873
[2m[36m(func pid=152082)[0m f1_per_class: [0.259, 0.505, 0.571, 0.0, 0.159, 0.04, 0.585, 0.163, 0.061, 0.214]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.23460820895522388
[2m[36m(func pid=152923)[0m top5: 0.7360074626865671
[2m[36m(func pid=152923)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=152923)[0m f1_macro: 0.20996475029818265
[2m[36m(func pid=152923)[0m f1_weighted: 0.20516486056853073
[2m[36m(func pid=152923)[0m f1_per_class: [0.275, 0.409, 0.667, 0.358, 0.047, 0.047, 0.018, 0.236, 0.0, 0.044]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:23:15 (running for 00:05:44.79)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.407 |      0.19  |                   57 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.879 |      0.256 |                   56 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      | 11.503 |      0.215 |                   57 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 39.389 |      0.21  |                   57 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.2635261194029851
[2m[36m(func pid=151704)[0m top5: 0.8722014925373134
[2m[36m(func pid=151704)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=151704)[0m f1_macro: 0.18990694316588394
[2m[36m(func pid=151704)[0m f1_weighted: 0.22842004342209635
[2m[36m(func pid=151704)[0m f1_per_class: [0.24, 0.336, 0.417, 0.177, 0.081, 0.332, 0.247, 0.016, 0.0, 0.053]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 4.3183 | Steps: 4 | Val loss: 5.0951 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.8528 | Steps: 4 | Val loss: 3.6442 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 72.2057 | Steps: 4 | Val loss: 73.2127 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.3491 | Steps: 4 | Val loss: 1.9400 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=152502)[0m top1: 0.33255597014925375
[2m[36m(func pid=152502)[0m top5: 0.8782649253731343
[2m[36m(func pid=152502)[0m f1_micro: 0.33255597014925375
[2m[36m(func pid=152502)[0m f1_macro: 0.24364577615299887
[2m[36m(func pid=152502)[0m f1_weighted: 0.36947098701220704
[2m[36m(func pid=152502)[0m f1_per_class: [0.119, 0.371, 0.233, 0.336, 0.174, 0.168, 0.552, 0.36, 0.047, 0.078]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m top1: 0.3763992537313433
[2m[36m(func pid=152082)[0m top5: 0.6763059701492538
[2m[36m(func pid=152082)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=152082)[0m f1_macro: 0.24080060435900616
[2m[36m(func pid=152082)[0m f1_weighted: 0.28566626560147956
[2m[36m(func pid=152082)[0m f1_per_class: [0.286, 0.499, 0.571, 0.0, 0.103, 0.047, 0.583, 0.135, 0.025, 0.158]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.28078358208955223
[2m[36m(func pid=152923)[0m top5: 0.7761194029850746
[2m[36m(func pid=152923)[0m f1_micro: 0.28078358208955223
[2m[36m(func pid=152923)[0m f1_macro: 0.15183466385110536
[2m[36m(func pid=152923)[0m f1_weighted: 0.276288984298121
[2m[36m(func pid=152923)[0m f1_per_class: [0.085, 0.276, 0.144, 0.459, 0.052, 0.172, 0.259, 0.0, 0.0, 0.071]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:23:20 (running for 00:05:49.87)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.349 |      0.204 |                   58 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.853 |      0.241 |                   57 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.318 |      0.244 |                   58 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 72.206 |      0.152 |                   58 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.2355410447761194
[2m[36m(func pid=151704)[0m top5: 0.8624067164179104
[2m[36m(func pid=151704)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=151704)[0m f1_macro: 0.20419874557307457
[2m[36m(func pid=151704)[0m f1_weighted: 0.1911868908844009
[2m[36m(func pid=151704)[0m f1_per_class: [0.28, 0.316, 0.533, 0.193, 0.079, 0.296, 0.094, 0.188, 0.0, 0.062]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 4.2862 | Steps: 4 | Val loss: 7.0144 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.2025 | Steps: 4 | Val loss: 3.4365 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 39.8236 | Steps: 4 | Val loss: 77.6125 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.4478 | Steps: 4 | Val loss: 1.9517 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=152502)[0m top1: 0.17723880597014927
[2m[36m(func pid=152502)[0m top5: 0.851679104477612
[2m[36m(func pid=152502)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=152502)[0m f1_macro: 0.15280586875425606
[2m[36m(func pid=152502)[0m f1_weighted: 0.1668150241996558
[2m[36m(func pid=152502)[0m f1_per_class: [0.176, 0.335, 0.289, 0.165, 0.08, 0.0, 0.131, 0.303, 0.0, 0.048]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152082)[0m top1: 0.31902985074626866
[2m[36m(func pid=152082)[0m top5: 0.6716417910447762
[2m[36m(func pid=152082)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=152082)[0m f1_macro: 0.22738593452158806
[2m[36m(func pid=152082)[0m f1_weighted: 0.2660739714124164
[2m[36m(func pid=152082)[0m f1_per_class: [0.282, 0.366, 0.419, 0.0, 0.055, 0.062, 0.559, 0.305, 0.027, 0.2]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m top1: 0.26632462686567165
[2m[36m(func pid=152923)[0m top5: 0.7737873134328358
[2m[36m(func pid=152923)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=152923)[0m f1_macro: 0.16056185753930763
[2m[36m(func pid=152923)[0m f1_weighted: 0.26966178337401525
[2m[36m(func pid=152923)[0m f1_per_class: [0.074, 0.381, 0.046, 0.033, 0.068, 0.268, 0.536, 0.0, 0.0, 0.2]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:23:25 (running for 00:05:55.20)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.448 |      0.222 |                   59 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.203 |      0.227 |                   58 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.286 |      0.153 |                   59 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 39.824 |      0.161 |                   59 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.23833955223880596
[2m[36m(func pid=151704)[0m top5: 0.8582089552238806
[2m[36m(func pid=151704)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=151704)[0m f1_macro: 0.2224954932309573
[2m[36m(func pid=151704)[0m f1_weighted: 0.19123655733433367
[2m[36m(func pid=151704)[0m f1_per_class: [0.265, 0.306, 0.5, 0.209, 0.107, 0.292, 0.037, 0.45, 0.0, 0.059]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 7.2757 | Steps: 4 | Val loss: 8.3726 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 63.1806 | Steps: 4 | Val loss: 98.9825 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.7139 | Steps: 4 | Val loss: 3.5082 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.4133 | Steps: 4 | Val loss: 1.9731 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=152502)[0m top1: 0.18470149253731344
[2m[36m(func pid=152502)[0m top5: 0.8442164179104478
[2m[36m(func pid=152502)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=152502)[0m f1_macro: 0.20696513533519534
[2m[36m(func pid=152502)[0m f1_weighted: 0.14812002383800063
[2m[36m(func pid=152502)[0m f1_per_class: [0.263, 0.364, 0.611, 0.056, 0.214, 0.0, 0.119, 0.402, 0.0, 0.04]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m top1: 0.2756529850746269
[2m[36m(func pid=152923)[0m top5: 0.6198694029850746
[2m[36m(func pid=152923)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=152923)[0m f1_macro: 0.19993674007272288
[2m[36m(func pid=152923)[0m f1_weighted: 0.2647569623134496
[2m[36m(func pid=152923)[0m f1_per_class: [0.104, 0.425, 0.063, 0.0, 0.268, 0.317, 0.459, 0.215, 0.0, 0.148]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m top1: 0.17863805970149255
[2m[36m(func pid=152082)[0m top5: 0.6543843283582089
[2m[36m(func pid=152082)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=152082)[0m f1_macro: 0.16604786137922628
[2m[36m(func pid=152082)[0m f1_weighted: 0.2021493222684638
[2m[36m(func pid=152082)[0m f1_per_class: [0.043, 0.217, 0.263, 0.0, 0.04, 0.199, 0.373, 0.477, 0.0, 0.048]
[2m[36m(func pid=152082)[0m 
== Status ==
Current time: 2024-01-07 10:23:30 (running for 00:06:00.37)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.413 |      0.219 |                   60 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.714 |      0.166 |                   59 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  7.276 |      0.207 |                   60 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 63.181 |      0.2   |                   60 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.24300373134328357
[2m[36m(func pid=151704)[0m top5: 0.8157649253731343
[2m[36m(func pid=151704)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=151704)[0m f1_macro: 0.21895881250939234
[2m[36m(func pid=151704)[0m f1_weighted: 0.19281912005108104
[2m[36m(func pid=151704)[0m f1_per_class: [0.23, 0.287, 0.5, 0.259, 0.105, 0.297, 0.006, 0.454, 0.0, 0.051]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 6.8034 | Steps: 4 | Val loss: 6.6948 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 95.9014 | Steps: 4 | Val loss: 105.5251 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.4073 | Steps: 4 | Val loss: 3.3365 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3114 | Steps: 4 | Val loss: 1.9844 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=152502)[0m top1: 0.24953358208955223
[2m[36m(func pid=152502)[0m top5: 0.8563432835820896
[2m[36m(func pid=152502)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=152502)[0m f1_macro: 0.2564945481771597
[2m[36m(func pid=152502)[0m f1_weighted: 0.2514443681572256
[2m[36m(func pid=152502)[0m f1_per_class: [0.273, 0.391, 0.692, 0.262, 0.169, 0.037, 0.231, 0.455, 0.0, 0.055]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m top1: 0.2868470149253731
[2m[36m(func pid=152923)[0m top5: 0.6343283582089553
[2m[36m(func pid=152923)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=152923)[0m f1_macro: 0.23812319705856302
[2m[36m(func pid=152923)[0m f1_weighted: 0.2836701981782906
[2m[36m(func pid=152923)[0m f1_per_class: [0.152, 0.524, 0.226, 0.0, 0.292, 0.413, 0.419, 0.192, 0.087, 0.077]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:23:35 (running for 00:06:05.56)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.311 |      0.221 |                   61 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.714 |      0.166 |                   59 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  6.803 |      0.256 |                   61 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 95.901 |      0.238 |                   61 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.25886194029850745
[2m[36m(func pid=151704)[0m top5: 0.7877798507462687
[2m[36m(func pid=151704)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=151704)[0m f1_macro: 0.2212631009090889
[2m[36m(func pid=151704)[0m f1_weighted: 0.21069798379591323
[2m[36m(func pid=151704)[0m f1_per_class: [0.24, 0.232, 0.5, 0.369, 0.091, 0.298, 0.0, 0.424, 0.0, 0.059]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.11007462686567164
[2m[36m(func pid=152082)[0m top5: 0.6124067164179104
[2m[36m(func pid=152082)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=152082)[0m f1_macro: 0.11289083315556883
[2m[36m(func pid=152082)[0m f1_weighted: 0.08306345515643888
[2m[36m(func pid=152082)[0m f1_per_class: [0.044, 0.144, 0.2, 0.0, 0.038, 0.276, 0.006, 0.38, 0.0, 0.04]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 6.1035 | Steps: 4 | Val loss: 7.3301 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 72.0695 | Steps: 4 | Val loss: 131.8124 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.4127 | Steps: 4 | Val loss: 2.0058 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.8291 | Steps: 4 | Val loss: 3.0311 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=152502)[0m top1: 0.24113805970149255
[2m[36m(func pid=152502)[0m top5: 0.8577425373134329
[2m[36m(func pid=152502)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=152502)[0m f1_macro: 0.2557017069306787
[2m[36m(func pid=152502)[0m f1_weighted: 0.25504932837344974
[2m[36m(func pid=152502)[0m f1_per_class: [0.32, 0.22, 0.625, 0.457, 0.042, 0.225, 0.095, 0.402, 0.0, 0.171]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m top1: 0.14412313432835822
[2m[36m(func pid=152923)[0m top5: 0.6501865671641791
[2m[36m(func pid=152923)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=152923)[0m f1_macro: 0.20434005314760956
[2m[36m(func pid=152923)[0m f1_weighted: 0.14885795399938984
[2m[36m(func pid=152923)[0m f1_per_class: [0.267, 0.453, 0.485, 0.0, 0.286, 0.0, 0.138, 0.28, 0.068, 0.067]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:23:41 (running for 00:06:11.07)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.311 |      0.221 |                   61 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.829 |      0.129 |                   61 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  6.104 |      0.256 |                   62 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 72.069 |      0.204 |                   62 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.2756529850746269
[2m[36m(func pid=151704)[0m top5: 0.7751865671641791
[2m[36m(func pid=151704)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=151704)[0m f1_macro: 0.2095378538817248
[2m[36m(func pid=151704)[0m f1_weighted: 0.21989342902506484
[2m[36m(func pid=151704)[0m f1_per_class: [0.235, 0.189, 0.444, 0.43, 0.085, 0.322, 0.0, 0.39, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.1310634328358209
[2m[36m(func pid=152082)[0m top5: 0.6907649253731343
[2m[36m(func pid=152082)[0m f1_micro: 0.1310634328358209
[2m[36m(func pid=152082)[0m f1_macro: 0.1293685564624478
[2m[36m(func pid=152082)[0m f1_weighted: 0.11681840646049531
[2m[36m(func pid=152082)[0m f1_per_class: [0.185, 0.102, 0.086, 0.147, 0.04, 0.278, 0.0, 0.337, 0.051, 0.068]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 7.9066 | Steps: 4 | Val loss: 10.7330 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 87.3763 | Steps: 4 | Val loss: 102.0668 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.2553 | Steps: 4 | Val loss: 2.0344 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.9890 | Steps: 4 | Val loss: 2.8426 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=152502)[0m top1: 0.14925373134328357
[2m[36m(func pid=152502)[0m top5: 0.8423507462686567
[2m[36m(func pid=152502)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=152502)[0m f1_macro: 0.19316160977935376
[2m[36m(func pid=152502)[0m f1_weighted: 0.15747871790000748
[2m[36m(func pid=152502)[0m f1_per_class: [0.311, 0.016, 0.476, 0.351, 0.029, 0.193, 0.022, 0.26, 0.027, 0.246]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m top1: 0.25326492537313433
[2m[36m(func pid=152923)[0m top5: 0.6576492537313433
[2m[36m(func pid=152923)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=152923)[0m f1_macro: 0.21897971026147553
[2m[36m(func pid=152923)[0m f1_weighted: 0.22219763530403547
[2m[36m(func pid=152923)[0m f1_per_class: [0.297, 0.502, 0.667, 0.0, 0.118, 0.0, 0.404, 0.0, 0.085, 0.116]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:23:46 (running for 00:06:16.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.255 |      0.2   |                   63 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.829 |      0.129 |                   61 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  7.907 |      0.193 |                   63 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 87.376 |      0.219 |                   63 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.27238805970149255
[2m[36m(func pid=151704)[0m top5: 0.7630597014925373
[2m[36m(func pid=151704)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=151704)[0m f1_macro: 0.19954551984367994
[2m[36m(func pid=151704)[0m f1_weighted: 0.21858037388085064
[2m[36m(func pid=151704)[0m f1_per_class: [0.25, 0.153, 0.377, 0.449, 0.07, 0.333, 0.0, 0.363, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.26865671641791045
[2m[36m(func pid=152082)[0m top5: 0.6842350746268657
[2m[36m(func pid=152082)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=152082)[0m f1_macro: 0.21284163985885315
[2m[36m(func pid=152082)[0m f1_weighted: 0.22081336156095788
[2m[36m(func pid=152082)[0m f1_per_class: [0.25, 0.005, 0.253, 0.533, 0.05, 0.281, 0.0, 0.429, 0.161, 0.167]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 5.4692 | Steps: 4 | Val loss: 9.5772 | Batch size: 32 | lr: 0.01 | Duration: 3.09s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 74.2422 | Steps: 4 | Val loss: 67.0016 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.2069 | Steps: 4 | Val loss: 2.0331 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.9994 | Steps: 4 | Val loss: 3.0072 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=152502)[0m top1: 0.15111940298507462
[2m[36m(func pid=152502)[0m top5: 0.8530783582089553
[2m[36m(func pid=152502)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=152502)[0m f1_macro: 0.23192299709375347
[2m[36m(func pid=152502)[0m f1_weighted: 0.16807690927922164
[2m[36m(func pid=152502)[0m f1_per_class: [0.405, 0.0, 0.611, 0.262, 0.029, 0.252, 0.089, 0.366, 0.12, 0.184]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m top1: 0.37033582089552236
[2m[36m(func pid=152923)[0m top5: 0.7397388059701493
[2m[36m(func pid=152923)[0m f1_micro: 0.37033582089552236
[2m[36m(func pid=152923)[0m f1_macro: 0.24053397590109732
[2m[36m(func pid=152923)[0m f1_weighted: 0.2860110975095178
[2m[36m(func pid=152923)[0m f1_per_class: [0.235, 0.47, 0.6, 0.0, 0.211, 0.159, 0.577, 0.058, 0.027, 0.069]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:23:52 (running for 00:06:21.74)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.207 |      0.2   |                   64 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.989 |      0.213 |                   62 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.469 |      0.232 |                   64 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 74.242 |      0.241 |                   64 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.271455223880597
[2m[36m(func pid=151704)[0m top5: 0.7705223880597015
[2m[36m(func pid=151704)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=151704)[0m f1_macro: 0.2001493962987329
[2m[36m(func pid=151704)[0m f1_weighted: 0.21558626571818723
[2m[36m(func pid=151704)[0m f1_per_class: [0.261, 0.099, 0.426, 0.468, 0.071, 0.361, 0.0, 0.316, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.33115671641791045
[2m[36m(func pid=152082)[0m top5: 0.6814365671641791
[2m[36m(func pid=152082)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=152082)[0m f1_macro: 0.2805392172988532
[2m[36m(func pid=152082)[0m f1_weighted: 0.23604139924159268
[2m[36m(func pid=152082)[0m f1_per_class: [0.231, 0.0, 0.833, 0.553, 0.089, 0.327, 0.0, 0.5, 0.091, 0.182]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 4.2496 | Steps: 4 | Val loss: 8.0172 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 72.5530 | Steps: 4 | Val loss: 75.8154 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.3741 | Steps: 4 | Val loss: 2.0534 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.5489 | Steps: 4 | Val loss: 2.9826 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=152502)[0m top1: 0.228544776119403
[2m[36m(func pid=152502)[0m top5: 0.8703358208955224
[2m[36m(func pid=152502)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=152502)[0m f1_macro: 0.25114972378539
[2m[36m(func pid=152502)[0m f1_weighted: 0.2674413375398095
[2m[36m(func pid=152502)[0m f1_per_class: [0.111, 0.128, 0.579, 0.307, 0.046, 0.314, 0.304, 0.339, 0.137, 0.246]
[2m[36m(func pid=152502)[0m 
[2m[36m(func pid=152923)[0m top1: 0.4025186567164179
[2m[36m(func pid=152923)[0m top5: 0.8264925373134329
[2m[36m(func pid=152923)[0m f1_micro: 0.4025186567164179
[2m[36m(func pid=152923)[0m f1_macro: 0.31508445188415335
[2m[36m(func pid=152923)[0m f1_weighted: 0.32058934939651024
[2m[36m(func pid=152923)[0m f1_per_class: [0.281, 0.384, 0.526, 0.58, 0.349, 0.402, 0.018, 0.49, 0.0, 0.12]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:23:57 (running for 00:06:26.89)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.374 |      0.193 |                   65 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.999 |      0.281 |                   63 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.25  |      0.251 |                   65 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 72.553 |      0.315 |                   65 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.2667910447761194
[2m[36m(func pid=151704)[0m top5: 0.7677238805970149
[2m[36m(func pid=151704)[0m f1_micro: 0.2667910447761194
[2m[36m(func pid=151704)[0m f1_macro: 0.19317075440866266
[2m[36m(func pid=151704)[0m f1_weighted: 0.21739160245725953
[2m[36m(func pid=151704)[0m f1_per_class: [0.254, 0.077, 0.367, 0.483, 0.058, 0.381, 0.0, 0.312, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.34281716417910446
[2m[36m(func pid=152082)[0m top5: 0.6907649253731343
[2m[36m(func pid=152082)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=152082)[0m f1_macro: 0.2748114638739255
[2m[36m(func pid=152082)[0m f1_weighted: 0.23534734967596013
[2m[36m(func pid=152082)[0m f1_per_class: [0.376, 0.0, 0.667, 0.54, 0.137, 0.356, 0.0, 0.462, 0.093, 0.118]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.8097 | Steps: 4 | Val loss: 5.9645 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 51.1596 | Steps: 4 | Val loss: 118.9200 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.3624 | Steps: 4 | Val loss: 2.0767 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.3434 | Steps: 4 | Val loss: 2.7788 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=152923)[0m top1: 0.3619402985074627
[2m[36m(func pid=152923)[0m top5: 0.6749067164179104
[2m[36m(func pid=152923)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=152923)[0m f1_macro: 0.2442759758864795
[2m[36m(func pid=152923)[0m f1_weighted: 0.2616779287589588
[2m[36m(func pid=152923)[0m f1_per_class: [0.083, 0.127, 0.488, 0.584, 0.153, 0.397, 0.0, 0.404, 0.0, 0.207]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.36007462686567165
[2m[36m(func pid=152502)[0m top5: 0.8917910447761194
[2m[36m(func pid=152502)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=152502)[0m f1_macro: 0.2681692929740009
[2m[36m(func pid=152502)[0m f1_weighted: 0.38818370052237966
[2m[36m(func pid=152502)[0m f1_per_class: [0.167, 0.464, 0.195, 0.277, 0.102, 0.354, 0.553, 0.236, 0.14, 0.194]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:24:02 (running for 00:06:32.23)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.362 |      0.181 |                   66 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.549 |      0.275 |                   64 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  1.81  |      0.268 |                   66 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 51.16  |      0.244 |                   66 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.24113805970149255
[2m[36m(func pid=151704)[0m top5: 0.7541977611940298
[2m[36m(func pid=151704)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=151704)[0m f1_macro: 0.1806214054615773
[2m[36m(func pid=151704)[0m f1_weighted: 0.21073917297412811
[2m[36m(func pid=151704)[0m f1_per_class: [0.254, 0.067, 0.306, 0.479, 0.044, 0.36, 0.0, 0.298, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.3087686567164179
[2m[36m(func pid=152082)[0m top5: 0.6861007462686567
[2m[36m(func pid=152082)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=152082)[0m f1_macro: 0.250751297563086
[2m[36m(func pid=152082)[0m f1_weighted: 0.23171108269019075
[2m[36m(func pid=152082)[0m f1_per_class: [0.211, 0.037, 0.6, 0.529, 0.109, 0.324, 0.0, 0.492, 0.052, 0.154]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 73.7475 | Steps: 4 | Val loss: 153.4372 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.5048 | Steps: 4 | Val loss: 5.6382 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.3980 | Steps: 4 | Val loss: 2.0837 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.0821 | Steps: 4 | Val loss: 2.6669 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=152923)[0m top1: 0.20569029850746268
[2m[36m(func pid=152923)[0m top5: 0.5513059701492538
[2m[36m(func pid=152923)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=152923)[0m f1_macro: 0.17567621681026976
[2m[36m(func pid=152923)[0m f1_weighted: 0.18122715855805688
[2m[36m(func pid=152923)[0m f1_per_class: [0.184, 0.0, 0.125, 0.416, 0.044, 0.344, 0.0, 0.296, 0.0, 0.348]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.3903917910447761
[2m[36m(func pid=152502)[0m top5: 0.9015858208955224
[2m[36m(func pid=152502)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=152502)[0m f1_macro: 0.2475399318630783
[2m[36m(func pid=152502)[0m f1_weighted: 0.38922659413554905
[2m[36m(func pid=152502)[0m f1_per_class: [0.161, 0.538, 0.085, 0.29, 0.235, 0.264, 0.551, 0.258, 0.0, 0.094]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:24:07 (running for 00:06:37.38)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.398 |      0.166 |                   67 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.343 |      0.251 |                   65 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  1.505 |      0.248 |                   67 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 73.747 |      0.176 |                   67 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.21361940298507462
[2m[36m(func pid=151704)[0m top5: 0.7597947761194029
[2m[36m(func pid=151704)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=151704)[0m f1_macro: 0.1663122274766397
[2m[36m(func pid=151704)[0m f1_weighted: 0.19076375976424584
[2m[36m(func pid=151704)[0m f1_per_class: [0.259, 0.042, 0.333, 0.475, 0.037, 0.24, 0.0, 0.277, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.23694029850746268
[2m[36m(func pid=152082)[0m top5: 0.7308768656716418
[2m[36m(func pid=152082)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=152082)[0m f1_macro: 0.2680798140991506
[2m[36m(func pid=152082)[0m f1_weighted: 0.25124464039430583
[2m[36m(func pid=152082)[0m f1_per_class: [0.094, 0.42, 0.571, 0.37, 0.11, 0.317, 0.0, 0.495, 0.048, 0.255]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 87.5027 | Steps: 4 | Val loss: 200.8593 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 5.2461 | Steps: 4 | Val loss: 7.1956 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.2850 | Steps: 4 | Val loss: 2.1163 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.1817 | Steps: 4 | Val loss: 2.5388 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=152923)[0m top1: 0.07789179104477612
[2m[36m(func pid=152923)[0m top5: 0.41091417910447764
[2m[36m(func pid=152923)[0m f1_micro: 0.07789179104477612
[2m[36m(func pid=152923)[0m f1_macro: 0.09458348822300747
[2m[36m(func pid=152923)[0m f1_weighted: 0.04882593603947052
[2m[36m(func pid=152923)[0m f1_per_class: [0.146, 0.0, 0.078, 0.042, 0.026, 0.057, 0.0, 0.435, 0.0, 0.162]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.3148320895522388
[2m[36m(func pid=152502)[0m top5: 0.8484141791044776
[2m[36m(func pid=152502)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=152502)[0m f1_macro: 0.20576404825495903
[2m[36m(func pid=152502)[0m f1_weighted: 0.3091863392724114
[2m[36m(func pid=152502)[0m f1_per_class: [0.145, 0.537, 0.074, 0.329, 0.0, 0.229, 0.233, 0.435, 0.0, 0.076]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:24:13 (running for 00:06:42.72)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.285 |      0.135 |                   68 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.082 |      0.268 |                   66 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  5.246 |      0.206 |                   68 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 87.503 |      0.095 |                   68 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.19263059701492538
[2m[36m(func pid=151704)[0m top5: 0.753731343283582
[2m[36m(func pid=151704)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=151704)[0m f1_macro: 0.135104165574539
[2m[36m(func pid=151704)[0m f1_weighted: 0.16487771728240389
[2m[36m(func pid=151704)[0m f1_per_class: [0.231, 0.037, 0.255, 0.473, 0.031, 0.023, 0.0, 0.3, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.228544776119403
[2m[36m(func pid=152082)[0m top5: 0.7653917910447762
[2m[36m(func pid=152082)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=152082)[0m f1_macro: 0.23406471122325642
[2m[36m(func pid=152082)[0m f1_weighted: 0.19382807669471402
[2m[36m(func pid=152082)[0m f1_per_class: [0.101, 0.506, 0.444, 0.016, 0.147, 0.292, 0.096, 0.509, 0.112, 0.118]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 114.7391 | Steps: 4 | Val loss: 204.8748 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 4.8738 | Steps: 4 | Val loss: 6.6179 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.2925 | Steps: 4 | Val loss: 2.1318 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.7575 | Steps: 4 | Val loss: 2.3199 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=152923)[0m top1: 0.06203358208955224
[2m[36m(func pid=152923)[0m top5: 0.34794776119402987
[2m[36m(func pid=152923)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=152923)[0m f1_macro: 0.101350797028009
[2m[36m(func pid=152923)[0m f1_weighted: 0.04781907611730464
[2m[36m(func pid=152923)[0m f1_per_class: [0.149, 0.021, 0.074, 0.0, 0.024, 0.083, 0.0, 0.505, 0.0, 0.158]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.3185634328358209
[2m[36m(func pid=152502)[0m top5: 0.8381529850746269
[2m[36m(func pid=152502)[0m f1_micro: 0.3185634328358209
[2m[36m(func pid=152502)[0m f1_macro: 0.19850382421473714
[2m[36m(func pid=152502)[0m f1_weighted: 0.26386101028015363
[2m[36m(func pid=152502)[0m f1_per_class: [0.181, 0.55, 0.168, 0.409, 0.0, 0.179, 0.022, 0.377, 0.027, 0.073]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:24:18 (running for 00:06:48.10)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.292 |      0.131 |                   69 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   2.182 |      0.234 |                   67 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   4.874 |      0.199 |                   69 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 114.739 |      0.101 |                   69 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |         |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |         |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |         |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |         |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |         |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |         |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |         |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |         |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.16930970149253732
[2m[36m(func pid=151704)[0m top5: 0.7569962686567164
[2m[36m(func pid=151704)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=151704)[0m f1_macro: 0.13114862316628606
[2m[36m(func pid=151704)[0m f1_weighted: 0.1471674424202396
[2m[36m(func pid=151704)[0m f1_per_class: [0.268, 0.026, 0.265, 0.423, 0.029, 0.0, 0.0, 0.3, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.283115671641791
[2m[36m(func pid=152082)[0m top5: 0.7905783582089553
[2m[36m(func pid=152082)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=152082)[0m f1_macro: 0.22741655680591655
[2m[36m(func pid=152082)[0m f1_weighted: 0.26349817561068256
[2m[36m(func pid=152082)[0m f1_per_class: [0.138, 0.422, 0.176, 0.0, 0.27, 0.091, 0.475, 0.447, 0.194, 0.06]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 92.0837 | Steps: 4 | Val loss: 146.4664 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.5823 | Steps: 4 | Val loss: 5.2087 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.3963 | Steps: 4 | Val loss: 2.1482 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.3161 | Steps: 4 | Val loss: 2.3068 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=152923)[0m top1: 0.10914179104477612
[2m[36m(func pid=152923)[0m top5: 0.46781716417910446
[2m[36m(func pid=152923)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=152923)[0m f1_macro: 0.12172420182872006
[2m[36m(func pid=152923)[0m f1_weighted: 0.08217425418203257
[2m[36m(func pid=152923)[0m f1_per_class: [0.164, 0.225, 0.081, 0.0, 0.046, 0.19, 0.0, 0.25, 0.0, 0.261]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152502)[0m top1: 0.3628731343283582
[2m[36m(func pid=152502)[0m top5: 0.8544776119402985
[2m[36m(func pid=152502)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=152502)[0m f1_macro: 0.2612748293641884
[2m[36m(func pid=152502)[0m f1_weighted: 0.3305168814252179
[2m[36m(func pid=152502)[0m f1_per_class: [0.213, 0.549, 0.327, 0.441, 0.0, 0.455, 0.096, 0.41, 0.043, 0.078]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:24:23 (running for 00:06:53.31)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.396 |      0.114 |                   70 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.757 |      0.227 |                   68 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.582 |      0.261 |                   70 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 92.084 |      0.122 |                   70 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.1553171641791045
[2m[36m(func pid=151704)[0m top5: 0.7565298507462687
[2m[36m(func pid=151704)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=151704)[0m f1_macro: 0.11355103390320016
[2m[36m(func pid=151704)[0m f1_weighted: 0.13875253716179545
[2m[36m(func pid=151704)[0m f1_per_class: [0.178, 0.041, 0.2, 0.393, 0.028, 0.0, 0.0, 0.296, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.2653917910447761
[2m[36m(func pid=152082)[0m top5: 0.840018656716418
[2m[36m(func pid=152082)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=152082)[0m f1_macro: 0.18017016606543773
[2m[36m(func pid=152082)[0m f1_weighted: 0.2503345791718134
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.418, 0.084, 0.023, 0.36, 0.0, 0.507, 0.216, 0.138, 0.056]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 34.3182 | Steps: 4 | Val loss: 108.5404 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.7629 | Steps: 4 | Val loss: 5.5352 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.2480 | Steps: 4 | Val loss: 2.1515 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=152923)[0m top1: 0.17957089552238806
[2m[36m(func pid=152923)[0m top5: 0.6399253731343284
[2m[36m(func pid=152923)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=152923)[0m f1_macro: 0.15170671161106006
[2m[36m(func pid=152923)[0m f1_weighted: 0.13412177218589613
[2m[36m(func pid=152923)[0m f1_per_class: [0.085, 0.428, 0.393, 0.003, 0.137, 0.155, 0.104, 0.0, 0.146, 0.065]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.2235 | Steps: 4 | Val loss: 2.2887 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=152502)[0m top1: 0.3521455223880597
[2m[36m(func pid=152502)[0m top5: 0.8871268656716418
[2m[36m(func pid=152502)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=152502)[0m f1_macro: 0.307849862945141
[2m[36m(func pid=152502)[0m f1_weighted: 0.3628315926529563
[2m[36m(func pid=152502)[0m f1_per_class: [0.194, 0.539, 0.424, 0.255, 0.24, 0.431, 0.379, 0.391, 0.135, 0.089]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:24:29 (running for 00:06:58.73)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.248 |      0.11  |                   71 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.316 |      0.18  |                   69 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.763 |      0.308 |                   71 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 34.318 |      0.152 |                   71 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.1478544776119403
[2m[36m(func pid=151704)[0m top5: 0.7551305970149254
[2m[36m(func pid=151704)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=151704)[0m f1_macro: 0.10979784724166888
[2m[36m(func pid=151704)[0m f1_weighted: 0.13872195747377525
[2m[36m(func pid=151704)[0m f1_per_class: [0.159, 0.108, 0.157, 0.355, 0.029, 0.0, 0.0, 0.291, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.2868470149253731
[2m[36m(func pid=152082)[0m top5: 0.8572761194029851
[2m[36m(func pid=152082)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=152082)[0m f1_macro: 0.17621508355380575
[2m[36m(func pid=152082)[0m f1_weighted: 0.3004419310019046
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.459, 0.064, 0.206, 0.34, 0.0, 0.517, 0.092, 0.026, 0.058]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 84.2284 | Steps: 4 | Val loss: 119.4600 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 3.3078 | Steps: 4 | Val loss: 6.1007 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.1791 | Steps: 4 | Val loss: 2.1358 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=152923)[0m top1: 0.2234141791044776
[2m[36m(func pid=152923)[0m top5: 0.6567164179104478
[2m[36m(func pid=152923)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=152923)[0m f1_macro: 0.19124447846408194
[2m[36m(func pid=152923)[0m f1_weighted: 0.19993927676510856
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.458, 0.556, 0.0, 0.343, 0.0, 0.37, 0.0, 0.127, 0.059]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.1048 | Steps: 4 | Val loss: 2.1702 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=152502)[0m top1: 0.3614738805970149
[2m[36m(func pid=152502)[0m top5: 0.8987873134328358
[2m[36m(func pid=152502)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=152502)[0m f1_macro: 0.29723870044305356
[2m[36m(func pid=152502)[0m f1_weighted: 0.34416375902848023
[2m[36m(func pid=152502)[0m f1_per_class: [0.282, 0.508, 0.571, 0.09, 0.308, 0.461, 0.544, 0.0, 0.129, 0.079]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:24:34 (running for 00:07:03.92)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.179 |      0.104 |                   72 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.224 |      0.176 |                   70 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  3.308 |      0.297 |                   72 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 84.228 |      0.191 |                   72 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.12826492537313433
[2m[36m(func pid=151704)[0m top5: 0.75
[2m[36m(func pid=151704)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=151704)[0m f1_macro: 0.10432680972937103
[2m[36m(func pid=151704)[0m f1_weighted: 0.11340837611009927
[2m[36m(func pid=151704)[0m f1_per_class: [0.147, 0.191, 0.174, 0.214, 0.029, 0.0, 0.0, 0.288, 0.0, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.3292910447761194
[2m[36m(func pid=152082)[0m top5: 0.8773320895522388
[2m[36m(func pid=152082)[0m f1_micro: 0.3292910447761194
[2m[36m(func pid=152082)[0m f1_macro: 0.21800435285076566
[2m[36m(func pid=152082)[0m f1_weighted: 0.3649768086213633
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.327, 0.07, 0.467, 0.361, 0.0, 0.514, 0.356, 0.025, 0.06]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 75.1812 | Steps: 4 | Val loss: 84.6925 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.6161 | Steps: 4 | Val loss: 4.8444 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.4853 | Steps: 4 | Val loss: 2.1526 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=152923)[0m top1: 0.3255597014925373
[2m[36m(func pid=152923)[0m top5: 0.8232276119402985
[2m[36m(func pid=152923)[0m f1_micro: 0.3255597014925373
[2m[36m(func pid=152923)[0m f1_macro: 0.21617539544192507
[2m[36m(func pid=152923)[0m f1_weighted: 0.29748109537763656
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.482, 0.556, 0.14, 0.2, 0.0, 0.554, 0.0, 0.136, 0.094]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.3834 | Steps: 4 | Val loss: 2.0262 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=152502)[0m top1: 0.4099813432835821
[2m[36m(func pid=152502)[0m top5: 0.9211753731343284
[2m[36m(func pid=152502)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=152502)[0m f1_macro: 0.26784269951448053
[2m[36m(func pid=152502)[0m f1_weighted: 0.40250010936623937
[2m[36m(func pid=152502)[0m f1_per_class: [0.292, 0.468, 0.377, 0.351, 0.125, 0.382, 0.565, 0.0, 0.062, 0.056]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:24:39 (running for 00:07:09.32)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.485 |      0.099 |                   73 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.105 |      0.218 |                   71 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.616 |      0.268 |                   73 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 75.181 |      0.216 |                   73 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.11707089552238806
[2m[36m(func pid=151704)[0m top5: 0.7551305970149254
[2m[36m(func pid=151704)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=151704)[0m f1_macro: 0.09896070921865364
[2m[36m(func pid=151704)[0m f1_weighted: 0.08663725957275083
[2m[36m(func pid=151704)[0m f1_per_class: [0.146, 0.241, 0.136, 0.077, 0.031, 0.0, 0.0, 0.313, 0.047, 0.0]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.36007462686567165
[2m[36m(func pid=152082)[0m top5: 0.8726679104477612
[2m[36m(func pid=152082)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=152082)[0m f1_macro: 0.22625950431293557
[2m[36m(func pid=152082)[0m f1_weighted: 0.37363789883388376
[2m[36m(func pid=152082)[0m f1_per_class: [0.0, 0.355, 0.106, 0.539, 0.217, 0.0, 0.444, 0.384, 0.132, 0.085]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 85.2148 | Steps: 4 | Val loss: 67.8449 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 4.2483 | Steps: 4 | Val loss: 5.5728 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.4065 | Steps: 4 | Val loss: 2.1443 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=152923)[0m top1: 0.4626865671641791
[2m[36m(func pid=152923)[0m top5: 0.8796641791044776
[2m[36m(func pid=152923)[0m f1_micro: 0.4626865671641791
[2m[36m(func pid=152923)[0m f1_macro: 0.26468006805344996
[2m[36m(func pid=152923)[0m f1_weighted: 0.4186308734228385
[2m[36m(func pid=152923)[0m f1_per_class: [0.0, 0.498, 0.632, 0.546, 0.222, 0.0, 0.578, 0.0, 0.046, 0.126]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.6779 | Steps: 4 | Val loss: 2.0720 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=152502)[0m top1: 0.376865671641791
[2m[36m(func pid=152502)[0m top5: 0.9048507462686567
[2m[36m(func pid=152502)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=152502)[0m f1_macro: 0.3066223619641202
[2m[36m(func pid=152502)[0m f1_weighted: 0.38674948350234417
[2m[36m(func pid=152502)[0m f1_per_class: [0.377, 0.333, 0.606, 0.547, 0.06, 0.328, 0.353, 0.366, 0.0, 0.097]
[2m[36m(func pid=152502)[0m 
== Status ==
Current time: 2024-01-07 10:24:45 (running for 00:07:14.79)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=0
Bracket: Iter 75.000: None
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (20 PENDING, 4 RUNNING)
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status   | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00000 | RUNNING  | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.407 |      0.112 |                   74 |
| train_952df_00001 | RUNNING  | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  2.383 |      0.226 |                   72 |
| train_952df_00002 | RUNNING  | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  4.248 |      0.307 |                   74 |
| train_952df_00003 | RUNNING  | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 85.215 |      0.265 |                   74 |
| train_952df_00004 | PENDING  |                     | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING  |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING  |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING  |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING  |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING  |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING  |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING  |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING  |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING  |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING  |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING  |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING  |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING  |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING  |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING  |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
+-------------------+----------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=151704)[0m top1: 0.10960820895522388
[2m[36m(func pid=151704)[0m top5: 0.7472014925373134
[2m[36m(func pid=151704)[0m f1_micro: 0.10960820895522388
[2m[36m(func pid=151704)[0m f1_macro: 0.1116717287103394
[2m[36m(func pid=151704)[0m f1_weighted: 0.06746927869383078
[2m[36m(func pid=151704)[0m f1_per_class: [0.192, 0.231, 0.186, 0.0, 0.029, 0.0, 0.0, 0.333, 0.072, 0.074]
[2m[36m(func pid=151704)[0m 
[2m[36m(func pid=152082)[0m top1: 0.28544776119402987
[2m[36m(func pid=152082)[0m top5: 0.8530783582089553
[2m[36m(func pid=152082)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=152082)[0m f1_macro: 0.19770167022380808
[2m[36m(func pid=152082)[0m f1_weighted: 0.2737930565286937
[2m[36m(func pid=152082)[0m f1_per_class: [0.175, 0.198, 0.203, 0.535, 0.069, 0.096, 0.173, 0.321, 0.081, 0.126]
[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 92.0991 | Steps: 4 | Val loss: 45.6556 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=152502)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.2017 | Steps: 4 | Val loss: 7.9809 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=151704)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.4801 | Steps: 4 | Val loss: 2.1530 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=152923)[0m top1: 0.45848880597014924
[2m[36m(func pid=152923)[0m top5: 0.8805970149253731
[2m[36m(func pid=152923)[0m f1_micro: 0.45848880597014924
[2m[36m(func pid=152923)[0m f1_macro: 0.30036740954282054
[2m[36m(func pid=152923)[0m f1_weighted: 0.4350491037419563
[2m[36m(func pid=152923)[0m f1_per_class: [0.32, 0.519, 0.478, 0.502, 0.19, 0.077, 0.581, 0.173, 0.046, 0.117]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.5302 | Steps: 4 | Val loss: 2.3360 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=152502)[0m top1: 0.28638059701492535
[2m[36m(func pid=152502)[0m top5: 0.8115671641791045
[2m[36m(func pid=152502)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=152502)[0m f1_macro: 0.251429271263217
[2m[36m(func pid=152502)[0m f1_weighted: 0.2583322477038413
[2m[36m(func pid=152502)[0m f1_per_class: [0.289, 0.249, 0.485, 0.51, 0.044, 0.335, 0.003, 0.371, 0.027, 0.2]
[2m[36m(func pid=151704)[0m top1: 0.12033582089552239
[2m[36m(func pid=151704)[0m top5: 0.753731343283582
[2m[36m(func pid=151704)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=151704)[0m f1_macro: 0.11568157310120859
[2m[36m(func pid=151704)[0m f1_weighted: 0.07431205396178298
[2m[36m(func pid=151704)[0m f1_per_class: [0.173, 0.262, 0.18, 0.0, 0.031, 0.0, 0.0, 0.364, 0.072, 0.074]
[2m[36m(func pid=152082)[0m top1: 0.2234141791044776
[2m[36m(func pid=152082)[0m top5: 0.8283582089552238
[2m[36m(func pid=152082)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=152082)[0m f1_macro: 0.2026280065219957
[2m[36m(func pid=152082)[0m f1_weighted: 0.19688022342099212
[2m[36m(func pid=152082)[0m f1_per_class: [0.238, 0.078, 0.49, 0.485, 0.039, 0.134, 0.003, 0.337, 0.084, 0.138]
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 59.8655 | Steps: 4 | Val loss: 112.9385 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=152923)[0m top1: 0.2178171641791045
[2m[36m(func pid=152923)[0m top5: 0.6716417910447762
[2m[36m(func pid=152923)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=152923)[0m f1_macro: 0.20194837662831816
[2m[36m(func pid=152923)[0m f1_weighted: 0.15529037030704793
[2m[36m(func pid=152923)[0m f1_per_class: [0.175, 0.483, 0.177, 0.0, 0.276, 0.435, 0.0, 0.207, 0.027, 0.239]
== Status ==
Current time: 2024-01-07 10:24:50 (running for 00:07:20.19)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.27549999999999997
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 3 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00001 | RUNNING    | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.678 |      0.198 |                   73 |
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 92.099 |      0.3   |                   75 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 10:24:57 (running for 00:07:27.55)
Memory usage on this node: 20.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.27549999999999997
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (19 PENDING, 3 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00001 | RUNNING    | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.678 |      0.198 |                   73 |
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 59.865 |      0.202 |                   76 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | PENDING    |                     | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152082)[0m 
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=170188)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170188)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=170188)[0m Configuration completed!
[2m[36m(func pid=170188)[0m New optimizer parameters:
[2m[36m(func pid=170188)[0m SGD (
[2m[36m(func pid=170188)[0m Parameter Group 0
[2m[36m(func pid=170188)[0m     dampening: 0
[2m[36m(func pid=170188)[0m     differentiable: False
[2m[36m(func pid=170188)[0m     foreach: None
[2m[36m(func pid=170188)[0m     lr: 0.0001
[2m[36m(func pid=170188)[0m     maximize: False
[2m[36m(func pid=170188)[0m     momentum: 0.9
[2m[36m(func pid=170188)[0m     nesterov: False
[2m[36m(func pid=170188)[0m     weight_decay: 0
[2m[36m(func pid=170188)[0m )
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 82.4837 | Steps: 4 | Val loss: 182.4759 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=152082)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.8416 | Steps: 4 | Val loss: 2.8119 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0460 | Steps: 4 | Val loss: 2.3244 | Batch size: 32 | lr: 0.0001 | Duration: 4.53s
[2m[36m(func pid=152923)[0m top1: 0.2126865671641791
[2m[36m(func pid=152923)[0m top5: 0.6096082089552238
[2m[36m(func pid=152923)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=152923)[0m f1_macro: 0.20483295814019686
[2m[36m(func pid=152923)[0m f1_weighted: 0.15305520783493567
[2m[36m(func pid=152923)[0m f1_per_class: [0.184, 0.504, 0.198, 0.0, 0.25, 0.386, 0.0, 0.198, 0.0, 0.328]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=152082)[0m top1: 0.14458955223880596
[2m[36m(func pid=152082)[0m top5: 0.7807835820895522
[2m[36m(func pid=152082)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=152082)[0m f1_macro: 0.18304555080859303
[2m[36m(func pid=152082)[0m f1_weighted: 0.13900571930860633
[2m[36m(func pid=152082)[0m f1_per_class: [0.12, 0.058, 0.56, 0.294, 0.034, 0.117, 0.0, 0.41, 0.05, 0.188]
[2m[36m(func pid=170188)[0m top1: 0.14692164179104478
[2m[36m(func pid=170188)[0m top5: 0.5541044776119403
[2m[36m(func pid=170188)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=170188)[0m f1_macro: 0.043564745899843924
[2m[36m(func pid=170188)[0m f1_weighted: 0.08273602897793536
[2m[36m(func pid=170188)[0m f1_per_class: [0.0, 0.0, 0.0, 0.26, 0.0, 0.0, 0.0, 0.175, 0.0, 0.0]
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 87.4339 | Steps: 4 | Val loss: 183.7450 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
== Status ==
Current time: 2024-01-07 10:25:03 (running for 00:07:32.98)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=2
Bracket: Iter 75.000: 0.27549999999999997
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (18 PENDING, 4 RUNNING, 2 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00001 | RUNNING    | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.53  |      0.203 |                   74 |
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 82.484 |      0.205 |                   77 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |        |            |                      |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | PENDING    |                     | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170672)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=170672)[0m Configuration completed!
[2m[36m(func pid=170672)[0m New optimizer parameters:
[2m[36m(func pid=170672)[0m SGD (
[2m[36m(func pid=170672)[0m Parameter Group 0
[2m[36m(func pid=170672)[0m     dampening: 0
[2m[36m(func pid=170672)[0m     differentiable: False
[2m[36m(func pid=170672)[0m     foreach: None
[2m[36m(func pid=170672)[0m     lr: 0.001
[2m[36m(func pid=170672)[0m     maximize: False
[2m[36m(func pid=170672)[0m     momentum: 0.9
[2m[36m(func pid=170672)[0m     nesterov: False
[2m[36m(func pid=170672)[0m     weight_decay: 0
[2m[36m(func pid=170672)[0m )
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=152923)[0m top1: 0.24860074626865672
[2m[36m(func pid=152923)[0m top5: 0.5680970149253731
[2m[36m(func pid=152923)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=152923)[0m f1_macro: 0.20023290308652414
[2m[36m(func pid=152923)[0m f1_weighted: 0.15349755315434419
[2m[36m(func pid=152923)[0m f1_per_class: [0.292, 0.471, 0.109, 0.0, 0.18, 0.373, 0.0, 0.327, 0.0, 0.25]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1326 | Steps: 4 | Val loss: 2.3151 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 81.4197 | Steps: 4 | Val loss: 192.8871 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0909 | Steps: 4 | Val loss: 2.3185 | Batch size: 32 | lr: 0.001 | Duration: 4.56s
[2m[36m(func pid=170188)[0m top1: 0.19449626865671643
[2m[36m(func pid=170188)[0m top5: 0.585820895522388
[2m[36m(func pid=170188)[0m f1_micro: 0.19449626865671643
[2m[36m(func pid=170188)[0m f1_macro: 0.04157973783843575
[2m[36m(func pid=170188)[0m f1_weighted: 0.10020360686579431
[2m[36m(func pid=170188)[0m f1_per_class: [0.0, 0.0, 0.0, 0.349, 0.0, 0.0, 0.0, 0.047, 0.0, 0.02]
[2m[36m(func pid=152923)[0m top1: 0.2042910447761194
[2m[36m(func pid=152923)[0m top5: 0.4976679104477612
[2m[36m(func pid=152923)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=152923)[0m f1_macro: 0.19469822159554004
[2m[36m(func pid=152923)[0m f1_weighted: 0.14265485815238307
[2m[36m(func pid=152923)[0m f1_per_class: [0.226, 0.395, 0.082, 0.0, 0.084, 0.289, 0.0, 0.514, 0.09, 0.267]
[2m[36m(func pid=170672)[0m top1: 0.17537313432835822
[2m[36m(func pid=170672)[0m top5: 0.5256529850746269
[2m[36m(func pid=170672)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=170672)[0m f1_macro: 0.06250071074482791
[2m[36m(func pid=170672)[0m f1_weighted: 0.10581443102443973
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.0, 0.024, 0.321, 0.0, 0.0, 0.0, 0.281, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 10:25:08 (running for 00:07:38.02)
Memory usage on this node: 21.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 87.434 |      0.2   |                   78 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  3.046 |      0.044 |                    1 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=171228)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=171228)[0m Configuration completed!
[2m[36m(func pid=171228)[0m New optimizer parameters:
[2m[36m(func pid=171228)[0m SGD (
[2m[36m(func pid=171228)[0m Parameter Group 0
[2m[36m(func pid=171228)[0m     dampening: 0
[2m[36m(func pid=171228)[0m     differentiable: False
[2m[36m(func pid=171228)[0m     foreach: None
[2m[36m(func pid=171228)[0m     lr: 0.01
[2m[36m(func pid=171228)[0m     maximize: False
[2m[36m(func pid=171228)[0m     momentum: 0.9
[2m[36m(func pid=171228)[0m     nesterov: False
[2m[36m(func pid=171228)[0m     weight_decay: 0
[2m[36m(func pid=171228)[0m )
[2m[36m(func pid=171228)[0m 
== Status ==
Current time: 2024-01-07 10:25:16 (running for 00:07:45.98)
Memory usage on this node: 23.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 81.42  |      0.195 |                   79 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  3.046 |      0.044 |                    1 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |        |            |                      |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 52.8151 | Steps: 4 | Val loss: 198.8381 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0512 | Steps: 4 | Val loss: 2.3158 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9646 | Steps: 4 | Val loss: 2.2916 | Batch size: 32 | lr: 0.001 | Duration: 3.28s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9904 | Steps: 4 | Val loss: 2.3289 | Batch size: 32 | lr: 0.01 | Duration: 4.61s
== Status ==
Current time: 2024-01-07 10:25:21 (running for 00:07:51.00)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 81.42  |      0.195 |                   79 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  3.133 |      0.042 |                    2 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  3.091 |      0.063 |                    1 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |        |            |                      |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152923)[0m top1: 0.12406716417910447
[2m[36m(func pid=152923)[0m top5: 0.4430970149253731
[2m[36m(func pid=152923)[0m f1_micro: 0.12406716417910447
[2m[36m(func pid=152923)[0m f1_macro: 0.16878482654995008
[2m[36m(func pid=152923)[0m f1_weighted: 0.10215762187799525
[2m[36m(func pid=152923)[0m f1_per_class: [0.357, 0.259, 0.239, 0.0, 0.047, 0.199, 0.0, 0.346, 0.13, 0.11]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=170188)[0m top1: 0.22621268656716417
[2m[36m(func pid=170188)[0m top5: 0.5862873134328358
[2m[36m(func pid=170188)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=170188)[0m f1_macro: 0.04318357371728414
[2m[36m(func pid=170188)[0m f1_weighted: 0.10950963674259598
[2m[36m(func pid=170188)[0m f1_per_class: [0.0, 0.0, 0.0, 0.386, 0.0, 0.0, 0.0, 0.027, 0.0, 0.018]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m top1: 0.014458955223880597
[2m[36m(func pid=170672)[0m top5: 0.5601679104477612
[2m[36m(func pid=170672)[0m f1_micro: 0.014458955223880597
[2m[36m(func pid=170672)[0m f1_macro: 0.006561540001959941
[2m[36m(func pid=170672)[0m f1_weighted: 0.014885702939789953
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.0, 0.013, 0.053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m top1: 0.006063432835820896
[2m[36m(func pid=171228)[0m top5: 0.6707089552238806
[2m[36m(func pid=171228)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=171228)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=171228)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 75.6947 | Steps: 4 | Val loss: 220.5038 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9183 | Steps: 4 | Val loss: 2.3250 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.7977 | Steps: 4 | Val loss: 2.2730 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.6772 | Steps: 4 | Val loss: 2.1390 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 10:25:27 (running for 00:07:56.74)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 75.695 |      0.128 |                   81 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  3.051 |      0.043 |                    3 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.965 |      0.007 |                    2 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.99  |      0.001 |                    1 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152923)[0m top1: 0.05503731343283582
[2m[36m(func pid=152923)[0m top5: 0.4001865671641791
[2m[36m(func pid=152923)[0m f1_micro: 0.05503731343283582
[2m[36m(func pid=152923)[0m f1_macro: 0.12814656170227107
[2m[36m(func pid=152923)[0m f1_weighted: 0.027839572392185218
[2m[36m(func pid=152923)[0m f1_per_class: [0.389, 0.016, 0.56, 0.0, 0.035, 0.072, 0.0, 0.016, 0.096, 0.097]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=170188)[0m top1: 0.23041044776119404
[2m[36m(func pid=170188)[0m top5: 0.5569029850746269
[2m[36m(func pid=170188)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=170188)[0m f1_macro: 0.042147337057295745
[2m[36m(func pid=170188)[0m f1_weighted: 0.11134064786034177
[2m[36m(func pid=170188)[0m f1_per_class: [0.0, 0.005, 0.0, 0.391, 0.0, 0.0, 0.0, 0.026, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m top1: 0.08395522388059702
[2m[36m(func pid=170672)[0m top5: 0.7122201492537313
[2m[36m(func pid=170672)[0m f1_micro: 0.08395522388059702
[2m[36m(func pid=170672)[0m f1_macro: 0.03305022080723015
[2m[36m(func pid=170672)[0m f1_weighted: 0.05414514128290456
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.314, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m top1: 0.11520522388059702
[2m[36m(func pid=171228)[0m top5: 0.7896455223880597
[2m[36m(func pid=171228)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=171228)[0m f1_macro: 0.044270341138194245
[2m[36m(func pid=171228)[0m f1_weighted: 0.06633816211820791
[2m[36m(func pid=171228)[0m f1_per_class: [0.101, 0.0, 0.0, 0.201, 0.0, 0.0, 0.0, 0.14, 0.0, 0.0]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 108.9378 | Steps: 4 | Val loss: 192.4264 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8909 | Steps: 4 | Val loss: 2.3313 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8166 | Steps: 4 | Val loss: 2.2801 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9997 | Steps: 4 | Val loss: 2.1188 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:25:32 (running for 00:08:01.99)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 108.938 |      0.124 |                   82 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.918 |      0.042 |                    4 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.798 |      0.033 |                    3 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   2.677 |      0.044 |                    2 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |         |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |         |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |         |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |         |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |         |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=152923)[0m top1: 0.051305970149253734
[2m[36m(func pid=152923)[0m top5: 0.4361007462686567
[2m[36m(func pid=152923)[0m f1_micro: 0.051305970149253734
[2m[36m(func pid=152923)[0m f1_macro: 0.1243842356300396
[2m[36m(func pid=152923)[0m f1_weighted: 0.030709084747339934
[2m[36m(func pid=152923)[0m f1_per_class: [0.336, 0.0, 0.632, 0.0, 0.039, 0.054, 0.032, 0.0, 0.099, 0.052]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=170188)[0m top1: 0.22201492537313433
[2m[36m(func pid=170188)[0m top5: 0.542910447761194
[2m[36m(func pid=170188)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=170188)[0m f1_macro: 0.043019218893558066
[2m[36m(func pid=170188)[0m f1_weighted: 0.10906203500818624
[2m[36m(func pid=170188)[0m f1_per_class: [0.019, 0.0, 0.0, 0.384, 0.0, 0.0, 0.0, 0.028, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m top1: 0.15205223880597016
[2m[36m(func pid=170672)[0m top5: 0.6721082089552238
[2m[36m(func pid=170672)[0m f1_micro: 0.15205223880597016
[2m[36m(func pid=170672)[0m f1_macro: 0.03660643821391485
[2m[36m(func pid=170672)[0m f1_weighted: 0.05636089412749337
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.326, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m top1: 0.2933768656716418
[2m[36m(func pid=171228)[0m top5: 0.6487873134328358
[2m[36m(func pid=171228)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=171228)[0m f1_macro: 0.08748439990015935
[2m[36m(func pid=171228)[0m f1_weighted: 0.14552843735214827
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.0, 0.296, 0.0, 0.099, 0.0, 0.48, 0.0, 0.0, 0.0]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 104.5876 | Steps: 4 | Val loss: 122.3682 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.0484 | Steps: 4 | Val loss: 2.3370 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8603 | Steps: 4 | Val loss: 2.2534 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7083 | Steps: 4 | Val loss: 2.3966 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=152923)[0m top1: 0.17957089552238806
[2m[36m(func pid=152923)[0m top5: 0.6277985074626866
[2m[36m(func pid=152923)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=152923)[0m f1_macro: 0.19826534759870554
[2m[36m(func pid=152923)[0m f1_weighted: 0.20651187733516913
[2m[36m(func pid=152923)[0m f1_per_class: [0.338, 0.005, 0.609, 0.343, 0.044, 0.203, 0.238, 0.0, 0.124, 0.08]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:25:37 (running for 00:08:07.40)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 104.588 |      0.198 |                   83 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   3.048 |      0.05  |                    6 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.817 |      0.037 |                    4 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   3     |      0.087 |                    3 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |         |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |         |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |         |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |         |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |         |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m top1: 0.20569029850746268
[2m[36m(func pid=170188)[0m top5: 0.5214552238805971
[2m[36m(func pid=170188)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=170188)[0m f1_macro: 0.05030004527829137
[2m[36m(func pid=170188)[0m f1_weighted: 0.10536419458650241
[2m[36m(func pid=170188)[0m f1_per_class: [0.05, 0.01, 0.069, 0.364, 0.0, 0.0, 0.0, 0.009, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m top1: 0.1515858208955224
[2m[36m(func pid=170672)[0m top5: 0.6651119402985075
[2m[36m(func pid=170672)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=170672)[0m f1_macro: 0.04847490906616427
[2m[36m(func pid=170672)[0m f1_weighted: 0.05356903864806974
[2m[36m(func pid=170672)[0m f1_per_class: [0.078, 0.282, 0.115, 0.003, 0.0, 0.0, 0.006, 0.0, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m top1: 0.22761194029850745
[2m[36m(func pid=171228)[0m top5: 0.5998134328358209
[2m[36m(func pid=171228)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=171228)[0m f1_macro: 0.07744680036253622
[2m[36m(func pid=171228)[0m f1_weighted: 0.10487412402480746
[2m[36m(func pid=171228)[0m f1_per_class: [0.071, 0.399, 0.0, 0.0, 0.0, 0.304, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 91.6727 | Steps: 4 | Val loss: 100.3493 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8961 | Steps: 4 | Val loss: 2.3252 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7157 | Steps: 4 | Val loss: 2.2273 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8279 | Steps: 4 | Val loss: 2.0851 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=152923)[0m top1: 0.3726679104477612
[2m[36m(func pid=152923)[0m top5: 0.7234141791044776
[2m[36m(func pid=152923)[0m f1_micro: 0.3726679104477612
[2m[36m(func pid=152923)[0m f1_macro: 0.2386685122780458
[2m[36m(func pid=152923)[0m f1_weighted: 0.31120963241219657
[2m[36m(func pid=152923)[0m f1_per_class: [0.329, 0.005, 0.436, 0.557, 0.117, 0.39, 0.325, 0.0, 0.027, 0.2]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=170188)[0m top1: 0.19962686567164178
[2m[36m(func pid=170188)[0m top5: 0.5270522388059702
[2m[36m(func pid=170188)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=170188)[0m f1_macro: 0.06044445416752098
[2m[36m(func pid=170188)[0m f1_weighted: 0.10508707042948778
[2m[36m(func pid=170188)[0m f1_per_class: [0.095, 0.02, 0.117, 0.35, 0.0, 0.0, 0.0, 0.023, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 10:25:43 (running for 00:08:12.72)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 91.673 |      0.239 |                   84 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.896 |      0.06  |                    7 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.86  |      0.048 |                    5 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.708 |      0.077 |                    4 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m top1: 0.1166044776119403
[2m[36m(func pid=170672)[0m top5: 0.648320895522388
[2m[36m(func pid=170672)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=170672)[0m f1_macro: 0.08439758978033354
[2m[36m(func pid=170672)[0m f1_weighted: 0.0670364919837167
[2m[36m(func pid=170672)[0m f1_per_class: [0.126, 0.24, 0.32, 0.047, 0.012, 0.047, 0.003, 0.0, 0.049, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m top1: 0.15485074626865672
[2m[36m(func pid=171228)[0m top5: 0.6002798507462687
[2m[36m(func pid=171228)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=171228)[0m f1_macro: 0.09814965680688634
[2m[36m(func pid=171228)[0m f1_weighted: 0.15410633788382308
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.171, 0.348, 0.439, 0.024, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 119.8600 | Steps: 4 | Val loss: 96.7152 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9048 | Steps: 4 | Val loss: 2.3175 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.6760 | Steps: 4 | Val loss: 2.1969 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.6217 | Steps: 4 | Val loss: 2.2049 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=152923)[0m top1: 0.396455223880597
[2m[36m(func pid=152923)[0m top5: 0.7448694029850746
[2m[36m(func pid=152923)[0m f1_micro: 0.39645522388059706
[2m[36m(func pid=152923)[0m f1_macro: 0.21942268360187253
[2m[36m(func pid=152923)[0m f1_weighted: 0.3284502855536011
[2m[36m(func pid=152923)[0m f1_per_class: [0.189, 0.037, 0.2, 0.545, 0.231, 0.454, 0.367, 0.0, 0.0, 0.171]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:25:48 (running for 00:08:18.04)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 119.86  |      0.219 |                   85 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.905 |      0.06  |                    8 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.716 |      0.084 |                    6 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   2.828 |      0.098 |                    5 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |         |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |         |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |         |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |         |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |         |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m top1: 0.19869402985074627
[2m[36m(func pid=170188)[0m top5: 0.5242537313432836
[2m[36m(func pid=170188)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=170188)[0m f1_macro: 0.06024702988543257
[2m[36m(func pid=170188)[0m f1_weighted: 0.10602483954612725
[2m[36m(func pid=170188)[0m f1_per_class: [0.093, 0.025, 0.107, 0.35, 0.0, 0.0, 0.0, 0.028, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m top1: 0.15298507462686567
[2m[36m(func pid=170672)[0m top5: 0.6870335820895522
[2m[36m(func pid=170672)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=170672)[0m f1_macro: 0.10800768147255184
[2m[36m(func pid=170672)[0m f1_weighted: 0.10255122392694167
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.31, 0.4, 0.074, 0.0, 0.201, 0.0, 0.0, 0.095, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m top1: 0.06623134328358209
[2m[36m(func pid=171228)[0m top5: 0.71875
[2m[36m(func pid=171228)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=171228)[0m f1_macro: 0.08264405641366426
[2m[36m(func pid=171228)[0m f1_weighted: 0.09224617629702524
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.0, 0.314, 0.193, 0.019, 0.0, 0.082, 0.201, 0.0, 0.017]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 75.3464 | Steps: 4 | Val loss: 73.9128 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8784 | Steps: 4 | Val loss: 2.3105 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7314 | Steps: 4 | Val loss: 2.1611 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.5625 | Steps: 4 | Val loss: 2.1118 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=152923)[0m top1: 0.41138059701492535
[2m[36m(func pid=152923)[0m top5: 0.8106343283582089
[2m[36m(func pid=152923)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=152923)[0m f1_macro: 0.25442481212597035
[2m[36m(func pid=152923)[0m f1_weighted: 0.40569324911895616
[2m[36m(func pid=152923)[0m f1_per_class: [0.163, 0.389, 0.096, 0.516, 0.244, 0.44, 0.454, 0.0, 0.027, 0.214]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:25:53 (running for 00:08:23.32)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 75.346 |      0.254 |                   86 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.878 |      0.06  |                    9 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.676 |      0.108 |                    7 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.622 |      0.083 |                    6 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m top1: 0.18470149253731344
[2m[36m(func pid=170188)[0m top5: 0.5405783582089553
[2m[36m(func pid=170188)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=170188)[0m f1_macro: 0.05997705291560762
[2m[36m(func pid=170188)[0m f1_weighted: 0.1059786865348819
[2m[36m(func pid=170188)[0m f1_per_class: [0.119, 0.026, 0.04, 0.339, 0.0, 0.0, 0.0, 0.076, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m top1: 0.21222014925373134
[2m[36m(func pid=170672)[0m top5: 0.7765858208955224
[2m[36m(func pid=170672)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=170672)[0m f1_macro: 0.11240987394298152
[2m[36m(func pid=170672)[0m f1_weighted: 0.16573699913121337
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.263, 0.2, 0.311, 0.0, 0.259, 0.0, 0.0, 0.092, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m top1: 0.20708955223880596
[2m[36m(func pid=171228)[0m top5: 0.5834888059701493
[2m[36m(func pid=171228)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=171228)[0m f1_macro: 0.07356973348288612
[2m[36m(func pid=171228)[0m f1_weighted: 0.13181735086490648
[2m[36m(func pid=171228)[0m f1_per_class: [0.141, 0.0, 0.0, 0.0, 0.0, 0.263, 0.332, 0.0, 0.0, 0.0]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 71.9551 | Steps: 4 | Val loss: 95.6839 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8293 | Steps: 4 | Val loss: 2.3124 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.0607 | Steps: 4 | Val loss: 2.2519 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6688 | Steps: 4 | Val loss: 2.1190 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=152923)[0m top1: 0.355410447761194
[2m[36m(func pid=152923)[0m top5: 0.7262126865671642
[2m[36m(func pid=152923)[0m f1_micro: 0.355410447761194
[2m[36m(func pid=152923)[0m f1_macro: 0.21528908200033667
[2m[36m(func pid=152923)[0m f1_weighted: 0.2817912869582333
[2m[36m(func pid=152923)[0m f1_per_class: [0.043, 0.433, 0.319, 0.01, 0.258, 0.442, 0.488, 0.0, 0.082, 0.077]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:25:58 (running for 00:08:28.62)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 71.955 |      0.215 |                   87 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.829 |      0.049 |                   10 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.731 |      0.112 |                    8 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.562 |      0.074 |                    7 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m top1: 0.13759328358208955
[2m[36m(func pid=170188)[0m top5: 0.5307835820895522
[2m[36m(func pid=170188)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=170188)[0m f1_macro: 0.04939096126660144
[2m[36m(func pid=170188)[0m f1_weighted: 0.08917703381223291
[2m[36m(func pid=170188)[0m f1_per_class: [0.101, 0.026, 0.034, 0.286, 0.0, 0.0, 0.0, 0.047, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m top1: 0.09328358208955224
[2m[36m(func pid=171228)[0m top5: 0.613339552238806
[2m[36m(func pid=171228)[0m f1_micro: 0.09328358208955224
[2m[36m(func pid=171228)[0m f1_macro: 0.1458521532218066
[2m[36m(func pid=171228)[0m f1_weighted: 0.08219176644651538
[2m[36m(func pid=171228)[0m f1_per_class: [0.158, 0.225, 0.429, 0.0, 0.028, 0.099, 0.0, 0.433, 0.0, 0.086]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m top1: 0.2537313432835821
[2m[36m(func pid=170672)[0m top5: 0.7807835820895522
[2m[36m(func pid=170672)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=170672)[0m f1_macro: 0.1119395249298506
[2m[36m(func pid=170672)[0m f1_weighted: 0.17129510856794344
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.071, 0.31, 0.448, 0.0, 0.275, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 51.1914 | Steps: 4 | Val loss: 111.8768 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8309 | Steps: 4 | Val loss: 2.3238 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.5097 | Steps: 4 | Val loss: 2.5426 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6089 | Steps: 4 | Val loss: 2.1187 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=152923)[0m top1: 0.26119402985074625
[2m[36m(func pid=152923)[0m top5: 0.6492537313432836
[2m[36m(func pid=152923)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=152923)[0m f1_macro: 0.24152304915864717
[2m[36m(func pid=152923)[0m f1_weighted: 0.17206006969262763
[2m[36m(func pid=152923)[0m f1_per_class: [0.35, 0.469, 0.364, 0.0, 0.198, 0.364, 0.028, 0.449, 0.119, 0.077]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:26:04 (running for 00:08:33.95)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 51.191 |      0.242 |                   88 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.831 |      0.036 |                   11 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.669 |      0.112 |                    9 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  3.061 |      0.146 |                    8 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m top1: 0.0853544776119403
[2m[36m(func pid=170188)[0m top5: 0.5200559701492538
[2m[36m(func pid=170188)[0m f1_micro: 0.0853544776119403
[2m[36m(func pid=170188)[0m f1_macro: 0.03554346083239303
[2m[36m(func pid=170188)[0m f1_weighted: 0.06475825284624968
[2m[36m(func pid=170188)[0m f1_per_class: [0.082, 0.026, 0.026, 0.206, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m top1: 0.05083955223880597
[2m[36m(func pid=171228)[0m top5: 0.6035447761194029
[2m[36m(func pid=171228)[0m f1_micro: 0.05083955223880597
[2m[36m(func pid=171228)[0m f1_macro: 0.10346986800859817
[2m[36m(func pid=171228)[0m f1_weighted: 0.03379417481877087
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.0, 0.31, 0.0, 0.119, 0.0, 0.0, 0.466, 0.113, 0.027]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m top1: 0.2140858208955224
[2m[36m(func pid=170672)[0m top5: 0.7229477611940298
[2m[36m(func pid=170672)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=170672)[0m f1_macro: 0.15303137254268226
[2m[36m(func pid=170672)[0m f1_weighted: 0.1588557367043347
[2m[36m(func pid=170672)[0m f1_per_class: [0.21, 0.058, 0.421, 0.357, 0.0, 0.256, 0.0, 0.229, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 42.1545 | Steps: 4 | Val loss: 146.1342 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8944 | Steps: 4 | Val loss: 2.3365 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7361 | Steps: 4 | Val loss: 2.1534 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.6310 | Steps: 4 | Val loss: 2.1395 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=152923)[0m top1: 0.21128731343283583
[2m[36m(func pid=152923)[0m top5: 0.5932835820895522
[2m[36m(func pid=152923)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=152923)[0m f1_macro: 0.16832467103957888
[2m[36m(func pid=152923)[0m f1_weighted: 0.13192389487863396
[2m[36m(func pid=152923)[0m f1_per_class: [0.225, 0.551, 0.157, 0.0, 0.151, 0.076, 0.0, 0.279, 0.123, 0.12]
[2m[36m(func pid=152923)[0m 
== Status ==
Current time: 2024-01-07 10:26:09 (running for 00:08:39.14)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 42.154 |      0.168 |                   89 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.894 |      0.024 |                   12 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.609 |      0.153 |                   10 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.51  |      0.103 |                    9 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m top1: 0.051305970149253734
[2m[36m(func pid=170188)[0m top5: 0.5004664179104478
[2m[36m(func pid=170188)[0m f1_micro: 0.051305970149253734
[2m[36m(func pid=170188)[0m f1_macro: 0.023994175467868056
[2m[36m(func pid=170188)[0m f1_weighted: 0.04250992571872064
[2m[36m(func pid=170188)[0m f1_per_class: [0.068, 0.011, 0.021, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m top1: 0.17164179104477612
[2m[36m(func pid=171228)[0m top5: 0.7350746268656716
[2m[36m(func pid=171228)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=171228)[0m f1_macro: 0.13313621463101377
[2m[36m(func pid=171228)[0m f1_weighted: 0.12058827625096791
[2m[36m(func pid=171228)[0m f1_per_class: [0.132, 0.253, 0.0, 0.042, 0.088, 0.397, 0.0, 0.255, 0.0, 0.165]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m top1: 0.1875
[2m[36m(func pid=170672)[0m top5: 0.7439365671641791
[2m[36m(func pid=170672)[0m f1_micro: 0.1875
[2m[36m(func pid=170672)[0m f1_macro: 0.10273740788278414
[2m[36m(func pid=170672)[0m f1_weighted: 0.12785807459021242
[2m[36m(func pid=170672)[0m f1_per_class: [0.149, 0.155, 0.0, 0.167, 0.0, 0.342, 0.0, 0.214, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 66.4291 | Steps: 4 | Val loss: 183.4180 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8806 | Steps: 4 | Val loss: 2.3473 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=152923)[0m top1: 0.12686567164179105
[2m[36m(func pid=152923)[0m top5: 0.5760261194029851
[2m[36m(func pid=152923)[0m f1_micro: 0.12686567164179105
[2m[36m(func pid=152923)[0m f1_macro: 0.11899575432904526
[2m[36m(func pid=152923)[0m f1_weighted: 0.07976237259733411
[2m[36m(func pid=152923)[0m f1_per_class: [0.204, 0.312, 0.222, 0.017, 0.11, 0.016, 0.0, 0.199, 0.021, 0.089]
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 3.2491 | Steps: 4 | Val loss: 2.0208 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.5801 | Steps: 4 | Val loss: 2.1797 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 10:26:14 (running for 00:08:44.52)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 66.429 |      0.119 |                   90 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.881 |      0.021 |                   13 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.631 |      0.103 |                   11 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.736 |      0.133 |                   10 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m top1: 0.045242537313432835
[2m[36m(func pid=170188)[0m top5: 0.4846082089552239
[2m[36m(func pid=170188)[0m f1_micro: 0.045242537313432835
[2m[36m(func pid=170188)[0m f1_macro: 0.02148049505957868
[2m[36m(func pid=170188)[0m f1_weighted: 0.04027086996855784
[2m[36m(func pid=170188)[0m f1_per_class: [0.052, 0.011, 0.019, 0.134, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m top1: 0.1791044776119403
[2m[36m(func pid=171228)[0m top5: 0.9090485074626866
[2m[36m(func pid=171228)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=171228)[0m f1_macro: 0.09237005105331605
[2m[36m(func pid=171228)[0m f1_weighted: 0.1382001013922486
[2m[36m(func pid=171228)[0m f1_per_class: [0.111, 0.174, 0.119, 0.162, 0.0, 0.253, 0.105, 0.0, 0.0, 0.0]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m top1: 0.12453358208955224
[2m[36m(func pid=170672)[0m top5: 0.777518656716418
[2m[36m(func pid=170672)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=170672)[0m f1_macro: 0.14552411987466274
[2m[36m(func pid=170672)[0m f1_weighted: 0.1019043290198553
[2m[36m(func pid=170672)[0m f1_per_class: [0.167, 0.199, 0.353, 0.0, 0.0, 0.368, 0.0, 0.342, 0.0, 0.027]
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 66.8040 | Steps: 4 | Val loss: 179.6322 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.9037 | Steps: 4 | Val loss: 2.3473 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9375 | Steps: 4 | Val loss: 1.9744 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=152923)[0m top1: 0.10261194029850747
[2m[36m(func pid=152923)[0m top5: 0.5625
[2m[36m(func pid=152923)[0m f1_micro: 0.10261194029850747
[2m[36m(func pid=152923)[0m f1_macro: 0.10464178583428638
[2m[36m(func pid=152923)[0m f1_weighted: 0.05330923320935616
[2m[36m(func pid=152923)[0m f1_per_class: [0.317, 0.021, 0.222, 0.095, 0.086, 0.008, 0.0, 0.211, 0.027, 0.058]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.7347 | Steps: 4 | Val loss: 2.1397 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=170188)[0m top1: 0.0457089552238806
[2m[36m(func pid=170188)[0m top5: 0.4701492537313433
[2m[36m(func pid=170188)[0m f1_micro: 0.0457089552238806
[2m[36m(func pid=170188)[0m f1_macro: 0.02053293360304511
[2m[36m(func pid=170188)[0m f1_weighted: 0.039864315583725385
[2m[36m(func pid=170188)[0m f1_per_class: [0.045, 0.005, 0.019, 0.136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 10:26:20 (running for 00:08:50.02)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 66.804 |      0.105 |                   91 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.904 |      0.021 |                   14 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.58  |      0.146 |                   12 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  3.249 |      0.092 |                   11 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m top1: 0.21641791044776118
[2m[36m(func pid=171228)[0m top5: 0.8423507462686567
[2m[36m(func pid=171228)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=171228)[0m f1_macro: 0.17406536812069523
[2m[36m(func pid=171228)[0m f1_weighted: 0.20101443896136692
[2m[36m(func pid=171228)[0m f1_per_class: [0.317, 0.354, 0.483, 0.038, 0.104, 0.0, 0.398, 0.0, 0.0, 0.047]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 71.4973 | Steps: 4 | Val loss: 123.4417 | Batch size: 32 | lr: 0.1 | Duration: 2.62s
[2m[36m(func pid=170672)[0m top1: 0.15904850746268656
[2m[36m(func pid=170672)[0m top5: 0.8255597014925373
[2m[36m(func pid=170672)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=170672)[0m f1_macro: 0.16653003904647776
[2m[36m(func pid=170672)[0m f1_weighted: 0.17546121675094542
[2m[36m(func pid=170672)[0m f1_per_class: [0.204, 0.263, 0.355, 0.01, 0.0, 0.211, 0.258, 0.341, 0.0, 0.023]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8503 | Steps: 4 | Val loss: 2.3346 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=152923)[0m top1: 0.20475746268656717
[2m[36m(func pid=152923)[0m top5: 0.6357276119402985
[2m[36m(func pid=152923)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=152923)[0m f1_macro: 0.15711907976619577
[2m[36m(func pid=152923)[0m f1_weighted: 0.17265296761626553
[2m[36m(func pid=152923)[0m f1_per_class: [0.252, 0.011, 0.101, 0.398, 0.083, 0.295, 0.003, 0.276, 0.048, 0.106]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8267 | Steps: 4 | Val loss: 2.3609 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5616 | Steps: 4 | Val loss: 2.0929 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 10:26:25 (running for 00:08:55.47)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 71.497 |      0.157 |                   92 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.904 |      0.021 |                   14 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.735 |      0.167 |                   13 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.827 |      0.125 |                   13 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.16138059701492538
[2m[36m(func pid=171228)[0m top5: 0.6707089552238806
[2m[36m(func pid=171228)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=171228)[0m f1_macro: 0.12535894058274125
[2m[36m(func pid=171228)[0m f1_weighted: 0.16034542394424653
[2m[36m(func pid=171228)[0m f1_per_class: [0.211, 0.0, 0.293, 0.0, 0.029, 0.0, 0.499, 0.016, 0.091, 0.114]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.06110074626865672
[2m[36m(func pid=170188)[0m top5: 0.4762126865671642
[2m[36m(func pid=170188)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=170188)[0m f1_macro: 0.024231222747663938
[2m[36m(func pid=170188)[0m f1_weighted: 0.04922510208803805
[2m[36m(func pid=170188)[0m f1_per_class: [0.046, 0.005, 0.022, 0.169, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 35.8152 | Steps: 4 | Val loss: 103.8973 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=170672)[0m top1: 0.3138992537313433
[2m[36m(func pid=170672)[0m top5: 0.777518656716418
[2m[36m(func pid=170672)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=170672)[0m f1_macro: 0.1043607383872986
[2m[36m(func pid=170672)[0m f1_weighted: 0.1883018787685157
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.101, 0.147, 0.0, 0.038, 0.176, 0.496, 0.0, 0.038, 0.048]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=152923)[0m top1: 0.3628731343283582
[2m[36m(func pid=152923)[0m top5: 0.722481343283582
[2m[36m(func pid=152923)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=152923)[0m f1_macro: 0.240656477852784
[2m[36m(func pid=152923)[0m f1_weighted: 0.3361128203592402
[2m[36m(func pid=152923)[0m f1_per_class: [0.323, 0.0, 0.148, 0.504, 0.114, 0.367, 0.4, 0.406, 0.049, 0.097]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7925 | Steps: 4 | Val loss: 2.3158 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.4352 | Steps: 4 | Val loss: 2.1988 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.6440 | Steps: 4 | Val loss: 2.1082 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 10:26:30 (running for 00:09:00.65)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 35.815 |      0.241 |                   93 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.85  |      0.024 |                   15 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.562 |      0.104 |                   14 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.435 |      0.186 |                   14 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.23274253731343283
[2m[36m(func pid=171228)[0m top5: 0.7803171641791045
[2m[36m(func pid=171228)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=171228)[0m f1_macro: 0.1859819712021789
[2m[36m(func pid=171228)[0m f1_weighted: 0.226569993945504
[2m[36m(func pid=171228)[0m f1_per_class: [0.19, 0.0, 0.468, 0.444, 0.043, 0.29, 0.173, 0.174, 0.0, 0.077]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.07462686567164178
[2m[36m(func pid=170188)[0m top5: 0.498134328358209
[2m[36m(func pid=170188)[0m f1_micro: 0.07462686567164178
[2m[36m(func pid=170188)[0m f1_macro: 0.027793611464997876
[2m[36m(func pid=170188)[0m f1_weighted: 0.058211809398891486
[2m[36m(func pid=170188)[0m f1_per_class: [0.042, 0.015, 0.025, 0.196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 96.7504 | Steps: 4 | Val loss: 117.9558 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=170672)[0m top1: 0.2905783582089552
[2m[36m(func pid=170672)[0m top5: 0.8283582089552238
[2m[36m(func pid=170672)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=170672)[0m f1_macro: 0.10243199605793482
[2m[36m(func pid=170672)[0m f1_weighted: 0.19204549653094796
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.032, 0.103, 0.003, 0.042, 0.258, 0.511, 0.016, 0.059, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=152923)[0m top1: 0.36427238805970147
[2m[36m(func pid=152923)[0m top5: 0.7262126865671642
[2m[36m(func pid=152923)[0m f1_micro: 0.3642723880597015
[2m[36m(func pid=152923)[0m f1_macro: 0.18916071949432034
[2m[36m(func pid=152923)[0m f1_weighted: 0.31722443326072924
[2m[36m(func pid=152923)[0m f1_per_class: [0.211, 0.0, 0.179, 0.526, 0.105, 0.365, 0.406, 0.0, 0.027, 0.074]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.4959 | Steps: 4 | Val loss: 1.9789 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.7945 | Steps: 4 | Val loss: 2.3020 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.6676 | Steps: 4 | Val loss: 2.1424 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 10:26:36 (running for 00:09:05.87)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 96.75  |      0.189 |                   94 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.793 |      0.028 |                   16 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.644 |      0.102 |                   15 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.496 |      0.215 |                   15 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.2332089552238806
[2m[36m(func pid=171228)[0m top5: 0.8166977611940298
[2m[36m(func pid=171228)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=171228)[0m f1_macro: 0.2149395079772089
[2m[36m(func pid=171228)[0m f1_weighted: 0.1705568330560522
[2m[36m(func pid=171228)[0m f1_per_class: [0.269, 0.389, 0.5, 0.0, 0.0, 0.308, 0.113, 0.36, 0.133, 0.077]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.07742537313432836
[2m[36m(func pid=170188)[0m top5: 0.5279850746268657
[2m[36m(func pid=170188)[0m f1_micro: 0.07742537313432836
[2m[36m(func pid=170188)[0m f1_macro: 0.03245536917323658
[2m[36m(func pid=170188)[0m f1_weighted: 0.06496224139619455
[2m[36m(func pid=170188)[0m f1_per_class: [0.048, 0.061, 0.024, 0.191, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 104.3353 | Steps: 4 | Val loss: 119.2295 | Batch size: 32 | lr: 0.1 | Duration: 2.70s
[2m[36m(func pid=170672)[0m top1: 0.22527985074626866
[2m[36m(func pid=170672)[0m top5: 0.8782649253731343
[2m[36m(func pid=170672)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=170672)[0m f1_macro: 0.17949058303208423
[2m[36m(func pid=170672)[0m f1_weighted: 0.2538423823934581
[2m[36m(func pid=170672)[0m f1_per_class: [0.105, 0.149, 0.237, 0.209, 0.055, 0.315, 0.372, 0.297, 0.057, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=152923)[0m top1: 0.333955223880597
[2m[36m(func pid=152923)[0m top5: 0.6940298507462687
[2m[36m(func pid=152923)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=152923)[0m f1_macro: 0.20223365780609912
[2m[36m(func pid=152923)[0m f1_weighted: 0.3054905083287194
[2m[36m(func pid=152923)[0m f1_per_class: [0.128, 0.0, 0.511, 0.528, 0.084, 0.331, 0.376, 0.0, 0.065, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.2453 | Steps: 4 | Val loss: 2.4380 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7514 | Steps: 4 | Val loss: 2.2914 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.4972 | Steps: 4 | Val loss: 2.1684 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 10:26:41 (running for 00:09:11.24)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 104.335 |      0.202 |                   95 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.794 |      0.032 |                   17 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.668 |      0.179 |                   16 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   2.245 |      0.201 |                   16 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |         |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |         |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |         |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |         |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |         |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.27658582089552236
[2m[36m(func pid=171228)[0m top5: 0.6198694029850746
[2m[36m(func pid=171228)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=171228)[0m f1_macro: 0.20059061324180644
[2m[36m(func pid=171228)[0m f1_weighted: 0.20870594571516246
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.453, 0.526, 0.0, 0.0, 0.0, 0.315, 0.481, 0.143, 0.087]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.09001865671641791
[2m[36m(func pid=170188)[0m top5: 0.5489738805970149
[2m[36m(func pid=170188)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=170188)[0m f1_macro: 0.03745044809877337
[2m[36m(func pid=170188)[0m f1_weighted: 0.07356025284941455
[2m[36m(func pid=170188)[0m f1_per_class: [0.047, 0.097, 0.031, 0.199, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 84.4188 | Steps: 4 | Val loss: 129.6460 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=170672)[0m top1: 0.17117537313432835
[2m[36m(func pid=170672)[0m top5: 0.7672574626865671
[2m[36m(func pid=170672)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=170672)[0m f1_macro: 0.1449389818315158
[2m[36m(func pid=170672)[0m f1_weighted: 0.0938223734633145
[2m[36m(func pid=170672)[0m f1_per_class: [0.141, 0.358, 0.476, 0.0, 0.045, 0.022, 0.0, 0.407, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=152923)[0m top1: 0.20102611940298507
[2m[36m(func pid=152923)[0m top5: 0.6259328358208955
[2m[36m(func pid=152923)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=152923)[0m f1_macro: 0.1783496590092833
[2m[36m(func pid=152923)[0m f1_weighted: 0.1991685478967971
[2m[36m(func pid=152923)[0m f1_per_class: [0.246, 0.0, 0.564, 0.375, 0.057, 0.227, 0.186, 0.0, 0.129, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6591 | Steps: 4 | Val loss: 2.0332 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7409 | Steps: 4 | Val loss: 2.2940 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.5491 | Steps: 4 | Val loss: 2.2218 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 10:26:46 (running for 00:09:16.49)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 84.419 |      0.178 |                   96 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.751 |      0.037 |                   18 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.497 |      0.145 |                   17 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.659 |      0.182 |                   17 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.17164179104477612
[2m[36m(func pid=171228)[0m top5: 0.8404850746268657
[2m[36m(func pid=171228)[0m f1_micro: 0.17164179104477612
[2m[36m(func pid=171228)[0m f1_macro: 0.18208498638222329
[2m[36m(func pid=171228)[0m f1_weighted: 0.20595889771067666
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.322, 0.114, 0.194, 0.32, 0.176, 0.147, 0.489, 0.026, 0.033]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 70.6378 | Steps: 4 | Val loss: 139.0699 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
[2m[36m(func pid=170188)[0m top1: 0.06949626865671642
[2m[36m(func pid=170188)[0m top5: 0.5419776119402985
[2m[36m(func pid=170188)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=170188)[0m f1_macro: 0.03384429595287915
[2m[36m(func pid=170188)[0m f1_weighted: 0.0639792883859623
[2m[36m(func pid=170188)[0m f1_per_class: [0.042, 0.118, 0.026, 0.15, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m top1: 0.14598880597014927
[2m[36m(func pid=170672)[0m top5: 0.6534514925373134
[2m[36m(func pid=170672)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=170672)[0m f1_macro: 0.14814924333913942
[2m[36m(func pid=170672)[0m f1_weighted: 0.08626762579856757
[2m[36m(func pid=170672)[0m f1_per_class: [0.17, 0.294, 0.48, 0.0, 0.036, 0.0, 0.0, 0.502, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=152923)[0m top1: 0.125
[2m[36m(func pid=152923)[0m top5: 0.5592350746268657
[2m[36m(func pid=152923)[0m f1_micro: 0.125
[2m[36m(func pid=152923)[0m f1_macro: 0.17112898968734871
[2m[36m(func pid=152923)[0m f1_weighted: 0.12549445660562172
[2m[36m(func pid=152923)[0m f1_per_class: [0.386, 0.016, 0.667, 0.249, 0.039, 0.184, 0.054, 0.0, 0.116, 0.0]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7128 | Steps: 4 | Val loss: 2.0405 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7429 | Steps: 4 | Val loss: 2.2954 | Batch size: 32 | lr: 0.0001 | Duration: 3.26s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.4409 | Steps: 4 | Val loss: 2.2412 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 10:26:52 (running for 00:09:21.94)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 70.638 |      0.171 |                   97 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.741 |      0.034 |                   19 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.549 |      0.148 |                   18 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.713 |      0.191 |                   18 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.29384328358208955
[2m[36m(func pid=171228)[0m top5: 0.8689365671641791
[2m[36m(func pid=171228)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=171228)[0m f1_macro: 0.1914107206285024
[2m[36m(func pid=171228)[0m f1_weighted: 0.22726256120883803
[2m[36m(func pid=171228)[0m f1_per_class: [0.236, 0.0, 0.212, 0.533, 0.076, 0.326, 0.045, 0.338, 0.0, 0.148]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 90.5568 | Steps: 4 | Val loss: 135.3332 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=170188)[0m top1: 0.06343283582089553
[2m[36m(func pid=170188)[0m top5: 0.5415111940298507
[2m[36m(func pid=170188)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=170188)[0m f1_macro: 0.035023955597550775
[2m[36m(func pid=170188)[0m f1_weighted: 0.06233154204696214
[2m[36m(func pid=170188)[0m f1_per_class: [0.036, 0.14, 0.023, 0.128, 0.0, 0.0, 0.003, 0.0, 0.02, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=152923)[0m top1: 0.11940298507462686
[2m[36m(func pid=152923)[0m top5: 0.6501865671641791
[2m[36m(func pid=152923)[0m f1_micro: 0.11940298507462686
[2m[36m(func pid=152923)[0m f1_macro: 0.1676827121922933
[2m[36m(func pid=152923)[0m f1_weighted: 0.14095775336912628
[2m[36m(func pid=152923)[0m f1_per_class: [0.327, 0.245, 0.583, 0.216, 0.031, 0.058, 0.057, 0.0, 0.112, 0.047]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=170672)[0m top1: 0.1296641791044776
[2m[36m(func pid=170672)[0m top5: 0.6385261194029851
[2m[36m(func pid=170672)[0m f1_micro: 0.1296641791044776
[2m[36m(func pid=170672)[0m f1_macro: 0.12252387320199225
[2m[36m(func pid=170672)[0m f1_weighted: 0.09590479409742596
[2m[36m(func pid=170672)[0m f1_per_class: [0.124, 0.271, 0.256, 0.062, 0.033, 0.0, 0.0, 0.48, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.6873 | Steps: 4 | Val loss: 2.0351 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7773 | Steps: 4 | Val loss: 2.3040 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 43.6773 | Steps: 4 | Val loss: 120.5673 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=171228)[0m top1: 0.373134328358209
[2m[36m(func pid=171228)[0m top5: 0.8488805970149254
[2m[36m(func pid=171228)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=171228)[0m f1_macro: 0.185864955442771
[2m[36m(func pid=171228)[0m f1_weighted: 0.2514196215778855
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.494, 0.571, 0.0, 0.133, 0.0, 0.538, 0.0, 0.0, 0.121]
== Status ==
Current time: 2024-01-07 10:26:57 (running for 00:09:27.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 90.557 |      0.168 |                   98 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.743 |      0.035 |                   20 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.441 |      0.123 |                   19 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.687 |      0.186 |                   19 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=171228)[0m 

[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.6521 | Steps: 4 | Val loss: 2.2331 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=170188)[0m top1: 0.055970149253731345
[2m[36m(func pid=170188)[0m top5: 0.5405783582089553
[2m[36m(func pid=170188)[0m f1_micro: 0.055970149253731345
[2m[36m(func pid=170188)[0m f1_macro: 0.036194441500641
[2m[36m(func pid=170188)[0m f1_weighted: 0.0592908977079067
[2m[36m(func pid=170188)[0m f1_per_class: [0.052, 0.166, 0.019, 0.103, 0.0, 0.0, 0.0, 0.0, 0.022, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=152923)[0m top1: 0.2019589552238806
[2m[36m(func pid=152923)[0m top5: 0.6347947761194029
[2m[36m(func pid=152923)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=152923)[0m f1_macro: 0.18953515427085993
[2m[36m(func pid=152923)[0m f1_weighted: 0.17677524055237429
[2m[36m(func pid=152923)[0m f1_per_class: [0.225, 0.469, 0.471, 0.062, 0.039, 0.081, 0.141, 0.291, 0.066, 0.05]
[2m[36m(func pid=152923)[0m 
[2m[36m(func pid=170672)[0m top1: 0.20522388059701493
[2m[36m(func pid=170672)[0m top5: 0.625
[2m[36m(func pid=170672)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=170672)[0m f1_macro: 0.12226256857597555
[2m[36m(func pid=170672)[0m f1_weighted: 0.18684121686379543
[2m[36m(func pid=170672)[0m f1_per_class: [0.035, 0.336, 0.088, 0.389, 0.046, 0.0, 0.0, 0.329, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 3.6097 | Steps: 4 | Val loss: 2.3274 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7896 | Steps: 4 | Val loss: 2.3060 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=152923)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 31.7654 | Steps: 4 | Val loss: 124.5821 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=171228)[0m top1: 0.12639925373134328
[2m[36m(func pid=171228)[0m top5: 0.7318097014925373
[2m[36m(func pid=171228)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=171228)[0m f1_macro: 0.13241915384015007
[2m[36m(func pid=171228)[0m f1_weighted: 0.08599089030377587
[2m[36m(func pid=171228)[0m f1_per_class: [0.269, 0.288, 0.0, 0.023, 0.316, 0.0, 0.0, 0.351, 0.043, 0.034]
== Status ==
Current time: 2024-01-07 10:27:02 (running for 00:09:32.57)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=3
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (17 PENDING, 4 RUNNING, 3 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00003 | RUNNING    | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 43.677 |      0.19  |                   99 |
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.777 |      0.036 |                   21 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.652 |      0.122 |                   20 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  3.61  |      0.132 |                   20 |
| train_952df_00007 | PENDING    |                     | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.5787 | Steps: 4 | Val loss: 2.1463 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=170188)[0m top1: 0.043843283582089554
[2m[36m(func pid=170188)[0m top5: 0.5569029850746269
[2m[36m(func pid=170188)[0m f1_micro: 0.043843283582089554
[2m[36m(func pid=170188)[0m f1_macro: 0.02932461746535554
[2m[36m(func pid=170188)[0m f1_weighted: 0.04859930099272701
[2m[36m(func pid=170188)[0m f1_per_class: [0.023, 0.168, 0.017, 0.063, 0.0, 0.0, 0.003, 0.0, 0.019, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=152923)[0m top1: 0.2513992537313433
[2m[36m(func pid=152923)[0m top5: 0.566231343283582
[2m[36m(func pid=152923)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=152923)[0m f1_macro: 0.21238681714201296
[2m[36m(func pid=152923)[0m f1_weighted: 0.16920245207427984
[2m[36m(func pid=152923)[0m f1_per_class: [0.164, 0.428, 0.471, 0.0, 0.118, 0.276, 0.085, 0.538, 0.0, 0.044]
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 3.6829 | Steps: 4 | Val loss: 3.2186 | Batch size: 32 | lr: 0.01 | Duration: 2.74s
[2m[36m(func pid=170672)[0m top1: 0.2579291044776119
[2m[36m(func pid=170672)[0m top5: 0.6403917910447762
[2m[36m(func pid=170672)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=170672)[0m f1_macro: 0.11806521337037938
[2m[36m(func pid=170672)[0m f1_weighted: 0.17749688473528155
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.021, 0.161, 0.53, 0.042, 0.0, 0.0, 0.427, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7962 | Steps: 4 | Val loss: 2.3005 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=171228)[0m top1: 0.24207089552238806
[2m[36m(func pid=171228)[0m top5: 0.6254664179104478
[2m[36m(func pid=171228)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=171228)[0m f1_macro: 0.15355020581452178
[2m[36m(func pid=171228)[0m f1_weighted: 0.19056935300082622
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.0, 0.045, 0.437, 0.116, 0.349, 0.0, 0.45, 0.0, 0.138]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5672 | Steps: 4 | Val loss: 2.0866 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=170188)[0m top1: 0.05363805970149254
[2m[36m(func pid=170188)[0m top5: 0.5746268656716418
[2m[36m(func pid=170188)[0m f1_micro: 0.05363805970149254
[2m[36m(func pid=170188)[0m f1_macro: 0.030955028541128617
[2m[36m(func pid=170188)[0m f1_weighted: 0.0591803851036782
[2m[36m(func pid=170188)[0m f1_per_class: [0.021, 0.159, 0.018, 0.102, 0.0, 0.0, 0.009, 0.0, 0.0, 0.0]
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.4334 | Steps: 4 | Val loss: 2.3707 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=170672)[0m top1: 0.20848880597014927
[2m[36m(func pid=170672)[0m top5: 0.6949626865671642
[2m[36m(func pid=170672)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=170672)[0m f1_macro: 0.14887763212160068
[2m[36m(func pid=170672)[0m f1_weighted: 0.19159685742799104
[2m[36m(func pid=170672)[0m f1_per_class: [0.113, 0.158, 0.237, 0.476, 0.026, 0.0, 0.0, 0.479, 0.0, 0.0]
[2m[36m(func pid=171228)[0m top1: 0.1646455223880597
[2m[36m(func pid=171228)[0m top5: 0.7486007462686567
[2m[36m(func pid=171228)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=171228)[0m f1_macro: 0.14881876673873096
[2m[36m(func pid=171228)[0m f1_weighted: 0.1272225368179962
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.319, 0.0, 0.0, 0.062, 0.353, 0.0, 0.435, 0.119, 0.2]
== Status ==
Current time: 2024-01-07 10:27:08 (running for 00:09:37.70)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.79  |      0.029 |                   22 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.579 |      0.118 |                   21 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  3.683 |      0.154 |                   21 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 10:27:14 (running for 00:09:43.92)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.796 |      0.031 |                   23 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.579 |      0.118 |                   21 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  3.683 |      0.154 |                   21 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=176697)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=176697)[0m Configuration completed!
[2m[36m(func pid=176697)[0m New optimizer parameters:
[2m[36m(func pid=176697)[0m SGD (
[2m[36m(func pid=176697)[0m Parameter Group 0
[2m[36m(func pid=176697)[0m     dampening: 0
[2m[36m(func pid=176697)[0m     differentiable: False
[2m[36m(func pid=176697)[0m     foreach: None
[2m[36m(func pid=176697)[0m     lr: 0.1
[2m[36m(func pid=176697)[0m     maximize: False
[2m[36m(func pid=176697)[0m     momentum: 0.9
[2m[36m(func pid=176697)[0m     nesterov: False
[2m[36m(func pid=176697)[0m     weight_decay: 0
[2m[36m(func pid=176697)[0m )
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.2079 | Steps: 4 | Val loss: 2.2934 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7399 | Steps: 4 | Val loss: 2.2890 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.3887 | Steps: 4 | Val loss: 2.0873 | Batch size: 32 | lr: 0.001 | Duration: 3.30s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 5.3652 | Steps: 4 | Val loss: 2.4221 | Batch size: 32 | lr: 0.1 | Duration: 4.50s
== Status ==
Current time: 2024-01-07 10:27:19 (running for 00:09:48.94)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.796 |      0.031 |                   23 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.567 |      0.149 |                   22 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.433 |      0.149 |                   22 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |        |            |                      |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.26026119402985076
[2m[36m(func pid=171228)[0m top5: 0.7346082089552238
[2m[36m(func pid=171228)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=171228)[0m f1_macro: 0.14440253228297895
[2m[36m(func pid=171228)[0m f1_weighted: 0.23996409622520484
[2m[36m(func pid=171228)[0m f1_per_class: [0.171, 0.0, 0.267, 0.195, 0.036, 0.089, 0.566, 0.0, 0.0, 0.121]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.07649253731343283
[2m[36m(func pid=170188)[0m top5: 0.5904850746268657
[2m[36m(func pid=170188)[0m f1_micro: 0.07649253731343283
[2m[36m(func pid=170188)[0m f1_macro: 0.04310969744795215
[2m[36m(func pid=170188)[0m f1_weighted: 0.08161637061727857
[2m[36m(func pid=170188)[0m f1_per_class: [0.036, 0.159, 0.023, 0.147, 0.0, 0.0, 0.037, 0.016, 0.012, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m top1: 0.11333955223880597
[2m[36m(func pid=170672)[0m top5: 0.7583955223880597
[2m[36m(func pid=170672)[0m f1_micro: 0.11333955223880597
[2m[36m(func pid=170672)[0m f1_macro: 0.10958019100635645
[2m[36m(func pid=170672)[0m f1_weighted: 0.08056096520110918
[2m[36m(func pid=170672)[0m f1_per_class: [0.107, 0.235, 0.239, 0.035, 0.027, 0.008, 0.0, 0.445, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m top1: 0.2980410447761194
[2m[36m(func pid=176697)[0m top5: 0.8745335820895522
[2m[36m(func pid=176697)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=176697)[0m f1_macro: 0.04592166726554078
[2m[36m(func pid=176697)[0m f1_weighted: 0.136865416896831
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.459, 0.0, 0.0, 0.0]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.5712 | Steps: 4 | Val loss: 2.3424 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7326 | Steps: 4 | Val loss: 2.2798 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.5085 | Steps: 4 | Val loss: 2.1230 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 20.8417 | Steps: 4 | Val loss: 14.1958 | Batch size: 32 | lr: 0.1 | Duration: 2.65s
== Status ==
Current time: 2024-01-07 10:27:24 (running for 00:09:54.41)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.74  |      0.043 |                   24 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.389 |      0.11  |                   23 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.571 |      0.151 |                   24 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  5.365 |      0.046 |                    1 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.27052238805970147
[2m[36m(func pid=171228)[0m top5: 0.808768656716418
[2m[36m(func pid=171228)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=171228)[0m f1_macro: 0.1513175493798612
[2m[36m(func pid=171228)[0m f1_weighted: 0.25840818591634324
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.0, 0.083, 0.492, 0.067, 0.322, 0.254, 0.091, 0.0, 0.203]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.09421641791044776
[2m[36m(func pid=170188)[0m top5: 0.6077425373134329
[2m[36m(func pid=170188)[0m f1_micro: 0.09421641791044776
[2m[36m(func pid=170188)[0m f1_macro: 0.04316980069786265
[2m[36m(func pid=170188)[0m f1_weighted: 0.09241864120085398
[2m[36m(func pid=170188)[0m f1_per_class: [0.027, 0.128, 0.031, 0.186, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m top1: 0.11613805970149253
[2m[36m(func pid=170672)[0m top5: 0.789179104477612
[2m[36m(func pid=170672)[0m f1_micro: 0.11613805970149253
[2m[36m(func pid=170672)[0m f1_macro: 0.10081319437028972
[2m[36m(func pid=170672)[0m f1_weighted: 0.08710446515931966
[2m[36m(func pid=170672)[0m f1_per_class: [0.042, 0.231, 0.125, 0.01, 0.032, 0.149, 0.006, 0.414, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m top1: 0.1142723880597015
[2m[36m(func pid=176697)[0m top5: 0.363339552238806
[2m[36m(func pid=176697)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=176697)[0m f1_macro: 0.020510673922143157
[2m[36m(func pid=176697)[0m f1_weighted: 0.023438036897971425
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.205, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.3895 | Steps: 4 | Val loss: 1.9270 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.7749 | Steps: 4 | Val loss: 2.2723 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.5622 | Steps: 4 | Val loss: 2.1584 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 45.2798 | Steps: 4 | Val loss: 13.5493 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 10:27:30 (running for 00:09:59.70)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.733 |      0.043 |                   25 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.509 |      0.101 |                   24 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.389 |      0.293 |                   25 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 20.842 |      0.021 |                    2 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m top1: 0.11240671641791045
[2m[36m(func pid=170188)[0m top5: 0.6291977611940298
[2m[36m(func pid=170188)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=170188)[0m f1_macro: 0.05083352762204955
[2m[36m(func pid=170188)[0m f1_weighted: 0.10471269566432953
[2m[36m(func pid=170188)[0m f1_per_class: [0.03, 0.126, 0.037, 0.219, 0.0, 0.0, 0.065, 0.031, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m top1: 0.3069029850746269
[2m[36m(func pid=171228)[0m top5: 0.8101679104477612
[2m[36m(func pid=171228)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=171228)[0m f1_macro: 0.29348592958477127
[2m[36m(func pid=171228)[0m f1_weighted: 0.2672439036450132
[2m[36m(func pid=171228)[0m f1_per_class: [0.253, 0.465, 0.526, 0.003, 0.18, 0.427, 0.325, 0.423, 0.121, 0.211]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m top1: 0.1501865671641791
[2m[36m(func pid=170672)[0m top5: 0.7863805970149254
[2m[36m(func pid=170672)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=170672)[0m f1_macro: 0.09841960933329474
[2m[36m(func pid=170672)[0m f1_weighted: 0.09006105330028057
[2m[36m(func pid=170672)[0m f1_per_class: [0.035, 0.225, 0.112, 0.003, 0.058, 0.295, 0.0, 0.255, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m top1: 0.03031716417910448
[2m[36m(func pid=176697)[0m top5: 0.5265858208955224
[2m[36m(func pid=176697)[0m f1_micro: 0.03031716417910448
[2m[36m(func pid=176697)[0m f1_macro: 0.04574292824292825
[2m[36m(func pid=176697)[0m f1_weighted: 0.007645864980286622
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.0, 0.273, 0.0, 0.067, 0.0, 0.0, 0.089, 0.0, 0.029]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.7402 | Steps: 4 | Val loss: 2.2680 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.1592 | Steps: 4 | Val loss: 2.3070 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.3656 | Steps: 4 | Val loss: 2.1978 | Batch size: 32 | lr: 0.001 | Duration: 3.27s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 22.5517 | Steps: 4 | Val loss: 10.2647 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=170188)[0m top1: 0.12033582089552239
[2m[36m(func pid=170188)[0m top5: 0.6333955223880597
[2m[36m(func pid=170188)[0m f1_micro: 0.12033582089552239
[2m[36m(func pid=170188)[0m f1_macro: 0.04850787712201966
[2m[36m(func pid=170188)[0m f1_weighted: 0.09966173083171131
[2m[36m(func pid=170188)[0m f1_per_class: [0.036, 0.083, 0.054, 0.241, 0.0, 0.0, 0.054, 0.016, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
== Status ==
Current time: 2024-01-07 10:27:35 (running for 00:10:05.09)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.74  |      0.049 |                   27 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.562 |      0.098 |                   25 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.389 |      0.293 |                   25 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 45.28  |      0.046 |                    3 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.21128731343283583
[2m[36m(func pid=171228)[0m top5: 0.7901119402985075
[2m[36m(func pid=171228)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=171228)[0m f1_macro: 0.18092704550536615
[2m[36m(func pid=171228)[0m f1_weighted: 0.168307440032832
[2m[36m(func pid=171228)[0m f1_per_class: [0.229, 0.536, 0.0, 0.0, 0.0, 0.349, 0.009, 0.409, 0.083, 0.194]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m top1: 0.29990671641791045
[2m[36m(func pid=176697)[0m top5: 0.7392723880597015
[2m[36m(func pid=176697)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=176697)[0m f1_macro: 0.055045280452446575
[2m[36m(func pid=176697)[0m f1_weighted: 0.14076738626728902
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.463, 0.0, 0.088, 0.0]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.14458955223880596
[2m[36m(func pid=170672)[0m top5: 0.7094216417910447
[2m[36m(func pid=170672)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=170672)[0m f1_macro: 0.08489169045885372
[2m[36m(func pid=170672)[0m f1_weighted: 0.059563383651836624
[2m[36m(func pid=170672)[0m f1_per_class: [0.12, 0.046, 0.105, 0.0, 0.049, 0.312, 0.0, 0.216, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.2799 | Steps: 4 | Val loss: 1.9807 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.8236 | Steps: 4 | Val loss: 2.2736 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 24.0406 | Steps: 4 | Val loss: 12.5679 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.3468 | Steps: 4 | Val loss: 2.2186 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 10:27:40 (running for 00:10:10.47)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.74  |      0.049 |                   27 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.366 |      0.085 |                   26 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.28  |      0.178 |                   27 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 22.552 |      0.055 |                    4 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.28171641791044777
[2m[36m(func pid=171228)[0m top5: 0.8526119402985075
[2m[36m(func pid=171228)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=171228)[0m f1_macro: 0.1781208247570547
[2m[36m(func pid=171228)[0m f1_weighted: 0.23313214736724994
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.078, 0.127, 0.516, 0.093, 0.334, 0.031, 0.432, 0.0, 0.171]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.11753731343283583
[2m[36m(func pid=170188)[0m top5: 0.6263992537313433
[2m[36m(func pid=170188)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=170188)[0m f1_macro: 0.04963846146854588
[2m[36m(func pid=170188)[0m f1_weighted: 0.09501442856287484
[2m[36m(func pid=170188)[0m f1_per_class: [0.039, 0.063, 0.052, 0.238, 0.0, 0.0, 0.046, 0.047, 0.011, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=176697)[0m top1: 0.01585820895522388
[2m[36m(func pid=176697)[0m top5: 0.5984141791044776
[2m[36m(func pid=176697)[0m f1_micro: 0.01585820895522388
[2m[36m(func pid=176697)[0m f1_macro: 0.03425901175958397
[2m[36m(func pid=176697)[0m f1_weighted: 0.011115056379585474
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.054, 0.273, 0.0, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.15578358208955223
[2m[36m(func pid=170672)[0m top5: 0.6972947761194029
[2m[36m(func pid=170672)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=170672)[0m f1_macro: 0.11835682576566568
[2m[36m(func pid=170672)[0m f1_weighted: 0.057411480322441
[2m[36m(func pid=170672)[0m f1_per_class: [0.127, 0.0, 0.462, 0.0, 0.0, 0.312, 0.0, 0.283, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.7663 | Steps: 4 | Val loss: 2.2804 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 3.1143 | Steps: 4 | Val loss: 2.7845 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 23.4374 | Steps: 4 | Val loss: 8.0637 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.5358 | Steps: 4 | Val loss: 2.1979 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=170188)[0m top1: 0.11707089552238806
[2m[36m(func pid=170188)[0m top5: 0.6100746268656716
[2m[36m(func pid=170188)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=170188)[0m f1_macro: 0.052035662423965234
[2m[36m(func pid=170188)[0m f1_weighted: 0.08728574745402169
[2m[36m(func pid=170188)[0m f1_per_class: [0.049, 0.044, 0.051, 0.246, 0.0, 0.0, 0.012, 0.089, 0.029, 0.0]
[2m[36m(func pid=170188)[0m 
== Status ==
Current time: 2024-01-07 10:27:46 (running for 00:10:15.82)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.766 |      0.052 |                   29 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.347 |      0.118 |                   27 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.28  |      0.178 |                   27 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 24.041 |      0.034 |                    5 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.15065298507462688
[2m[36m(func pid=171228)[0m top5: 0.6478544776119403
[2m[36m(func pid=171228)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=171228)[0m f1_macro: 0.09464486199106512
[2m[36m(func pid=171228)[0m f1_weighted: 0.15101044439175218
[2m[36m(func pid=171228)[0m f1_per_class: [0.136, 0.031, 0.169, 0.0, 0.028, 0.0, 0.468, 0.016, 0.0, 0.098]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m top1: 0.05923507462686567
[2m[36m(func pid=176697)[0m top5: 0.7136194029850746
[2m[36m(func pid=176697)[0m f1_micro: 0.05923507462686567
[2m[36m(func pid=176697)[0m f1_macro: 0.06719467398031342
[2m[36m(func pid=176697)[0m f1_weighted: 0.08371556497564604
[2m[36m(func pid=176697)[0m f1_per_class: [0.139, 0.0, 0.222, 0.126, 0.018, 0.0, 0.147, 0.0, 0.02, 0.0]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.14738805970149255
[2m[36m(func pid=170672)[0m top5: 0.6930970149253731
[2m[36m(func pid=170672)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=170672)[0m f1_macro: 0.10793570786394187
[2m[36m(func pid=170672)[0m f1_weighted: 0.05970405319356547
[2m[36m(func pid=170672)[0m f1_per_class: [0.116, 0.0, 0.267, 0.0, 0.0, 0.273, 0.0, 0.424, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.7482 | Steps: 4 | Val loss: 2.2770 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.0626 | Steps: 4 | Val loss: 2.2159 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 13.5133 | Steps: 4 | Val loss: 6.1012 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6179 | Steps: 4 | Val loss: 2.1080 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 10:27:51 (running for 00:10:21.11)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.766 |      0.052 |                   29 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.536 |      0.108 |                   28 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.063 |      0.193 |                   29 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 23.437 |      0.067 |                    6 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.22901119402985073
[2m[36m(func pid=171228)[0m top5: 0.7038246268656716
[2m[36m(func pid=171228)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=171228)[0m f1_macro: 0.19300452372287347
[2m[36m(func pid=171228)[0m f1_weighted: 0.20411137445010555
[2m[36m(func pid=171228)[0m f1_per_class: [0.328, 0.224, 0.375, 0.0, 0.053, 0.303, 0.394, 0.0, 0.048, 0.205]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.12453358208955224
[2m[36m(func pid=170188)[0m top5: 0.6343283582089553
[2m[36m(func pid=170188)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=170188)[0m f1_macro: 0.06359016142996915
[2m[36m(func pid=170188)[0m f1_weighted: 0.10142951917205585
[2m[36m(func pid=170188)[0m f1_per_class: [0.049, 0.067, 0.064, 0.253, 0.0, 0.0, 0.03, 0.126, 0.047, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=176697)[0m top1: 0.06763059701492537
[2m[36m(func pid=176697)[0m top5: 0.7472014925373134
[2m[36m(func pid=176697)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=176697)[0m f1_macro: 0.028571959822411692
[2m[36m(func pid=176697)[0m f1_weighted: 0.021750808287204642
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.067, 0.0, 0.0, 0.037, 0.0, 0.0, 0.157, 0.025, 0.0]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.17490671641791045
[2m[36m(func pid=170672)[0m top5: 0.7793843283582089
[2m[36m(func pid=170672)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=170672)[0m f1_macro: 0.15793451033493994
[2m[36m(func pid=170672)[0m f1_weighted: 0.10992825503250603
[2m[36m(func pid=170672)[0m f1_per_class: [0.148, 0.18, 0.444, 0.003, 0.0, 0.278, 0.047, 0.428, 0.051, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 3.0017 | Steps: 4 | Val loss: 1.6286 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.7689 | Steps: 4 | Val loss: 2.2750 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 9.7493 | Steps: 4 | Val loss: 13.7311 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.4687 | Steps: 4 | Val loss: 2.0279 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=171228)[0m top1: 0.4006529850746269
[2m[36m(func pid=171228)[0m top5: 0.8801305970149254
[2m[36m(func pid=171228)[0m f1_micro: 0.4006529850746269
[2m[36m(func pid=171228)[0m f1_macro: 0.3346699422019689
[2m[36m(func pid=171228)[0m f1_weighted: 0.36717743335480885
[2m[36m(func pid=171228)[0m f1_per_class: [0.286, 0.442, 0.593, 0.564, 0.235, 0.36, 0.169, 0.472, 0.063, 0.164]
[2m[36m(func pid=171228)[0m 
== Status ==
Current time: 2024-01-07 10:27:56 (running for 00:10:26.49)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.748 |      0.064 |                   30 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.618 |      0.158 |                   29 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  3.002 |      0.335 |                   30 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 13.513 |      0.029 |                    7 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m top1: 0.12779850746268656
[2m[36m(func pid=170188)[0m top5: 0.6469216417910447
[2m[36m(func pid=170188)[0m f1_micro: 0.12779850746268656
[2m[36m(func pid=170188)[0m f1_macro: 0.07524659986519876
[2m[36m(func pid=170188)[0m f1_weighted: 0.11614553064250938
[2m[36m(func pid=170188)[0m f1_per_class: [0.047, 0.08, 0.071, 0.246, 0.0, 0.0, 0.065, 0.199, 0.044, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=176697)[0m top1: 0.1828358208955224
[2m[36m(func pid=176697)[0m top5: 0.3689365671641791
[2m[36m(func pid=176697)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=176697)[0m f1_macro: 0.06113380434829665
[2m[36m(func pid=176697)[0m f1_weighted: 0.07906364806411845
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.382, 0.0, 0.0, 0.072, 0.093, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.261660447761194
[2m[36m(func pid=170672)[0m top5: 0.8642723880597015
[2m[36m(func pid=170672)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=170672)[0m f1_macro: 0.17735176269179653
[2m[36m(func pid=170672)[0m f1_weighted: 0.18450537280717785
[2m[36m(func pid=170672)[0m f1_per_class: [0.041, 0.392, 0.182, 0.086, 0.104, 0.389, 0.065, 0.438, 0.023, 0.055]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.5426 | Steps: 4 | Val loss: 2.5126 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6993 | Steps: 4 | Val loss: 2.2538 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 14.6616 | Steps: 4 | Val loss: 8.8238 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.3424 | Steps: 4 | Val loss: 2.0447 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 10:28:02 (running for 00:10:31.73)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.769 |      0.075 |                   31 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.469 |      0.177 |                   30 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.543 |      0.223 |                   31 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  9.749 |      0.061 |                    8 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.2947761194029851
[2m[36m(func pid=171228)[0m top5: 0.746268656716418
[2m[36m(func pid=171228)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=171228)[0m f1_macro: 0.2225072583514572
[2m[36m(func pid=171228)[0m f1_weighted: 0.2802938751603107
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.473, 0.063, 0.455, 0.167, 0.353, 0.0, 0.386, 0.183, 0.146]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.15065298507462688
[2m[36m(func pid=170188)[0m top5: 0.6814365671641791
[2m[36m(func pid=170188)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=170188)[0m f1_macro: 0.08483587678633989
[2m[36m(func pid=170188)[0m f1_weighted: 0.1260433005793725
[2m[36m(func pid=170188)[0m f1_per_class: [0.074, 0.067, 0.098, 0.284, 0.0, 0.0, 0.065, 0.209, 0.052, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=176697)[0m top1: 0.14319029850746268
[2m[36m(func pid=176697)[0m top5: 0.8148320895522388
[2m[36m(func pid=176697)[0m f1_micro: 0.14319029850746268
[2m[36m(func pid=176697)[0m f1_macro: 0.08911190985535726
[2m[36m(func pid=176697)[0m f1_weighted: 0.14354507577803735
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.0, 0.367, 0.445, 0.022, 0.0, 0.057, 0.0, 0.0, 0.0]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.24253731343283583
[2m[36m(func pid=170672)[0m top5: 0.7798507462686567
[2m[36m(func pid=170672)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=170672)[0m f1_macro: 0.18502726664418717
[2m[36m(func pid=170672)[0m f1_weighted: 0.23074110703628684
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.385, 0.195, 0.367, 0.074, 0.251, 0.003, 0.503, 0.042, 0.03]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.0450 | Steps: 4 | Val loss: 2.7078 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.8390 | Steps: 4 | Val loss: 2.2572 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 11.0100 | Steps: 4 | Val loss: 6.6248 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.4765 | Steps: 4 | Val loss: 2.0677 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 10:28:07 (running for 00:10:37.12)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.699 |      0.085 |                   32 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.342 |      0.185 |                   31 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.045 |      0.232 |                   32 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 14.662 |      0.089 |                    9 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.20335820895522388
[2m[36m(func pid=171228)[0m top5: 0.5946828358208955
[2m[36m(func pid=171228)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=171228)[0m f1_macro: 0.23216553908589838
[2m[36m(func pid=171228)[0m f1_weighted: 0.15120609070185712
[2m[36m(func pid=171228)[0m f1_per_class: [0.131, 0.45, 0.6, 0.0, 0.13, 0.332, 0.0, 0.405, 0.089, 0.184]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.13246268656716417
[2m[36m(func pid=170188)[0m top5: 0.6884328358208955
[2m[36m(func pid=170188)[0m f1_micro: 0.13246268656716417
[2m[36m(func pid=170188)[0m f1_macro: 0.07492522787628587
[2m[36m(func pid=170188)[0m f1_weighted: 0.11110616143012385
[2m[36m(func pid=170188)[0m f1_per_class: [0.074, 0.049, 0.103, 0.265, 0.0, 0.0, 0.056, 0.127, 0.074, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=176697)[0m top1: 0.15438432835820895
[2m[36m(func pid=176697)[0m top5: 0.9384328358208955
[2m[36m(func pid=176697)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=176697)[0m f1_macro: 0.10125626475752506
[2m[36m(func pid=176697)[0m f1_weighted: 0.08052474193613711
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.19, 0.4, 0.013, 0.0, 0.242, 0.018, 0.149, 0.0, 0.0]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.19542910447761194
[2m[36m(func pid=170672)[0m top5: 0.7448694029850746
[2m[36m(func pid=170672)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=170672)[0m f1_macro: 0.14743713934379926
[2m[36m(func pid=170672)[0m f1_weighted: 0.19817540307964823
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.187, 0.205, 0.481, 0.059, 0.016, 0.0, 0.468, 0.027, 0.032]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.3112 | Steps: 4 | Val loss: 1.5624 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.7050 | Steps: 4 | Val loss: 2.2478 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 11.3717 | Steps: 4 | Val loss: 10.2799 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 10:28:12 (running for 00:10:42.47)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.839 |      0.075 |                   33 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.477 |      0.147 |                   32 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.311 |      0.252 |                   33 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.01  |      0.101 |                   10 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.4098 | Steps: 4 | Val loss: 2.1125 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=171228)[0m top1: 0.43050373134328357
[2m[36m(func pid=171228)[0m top5: 0.914179104477612
[2m[36m(func pid=171228)[0m f1_micro: 0.43050373134328357
[2m[36m(func pid=171228)[0m f1_macro: 0.2516247254232035
[2m[36m(func pid=171228)[0m f1_weighted: 0.3996708435935066
[2m[36m(func pid=171228)[0m f1_per_class: [0.262, 0.335, 0.486, 0.568, 0.084, 0.0, 0.576, 0.016, 0.0, 0.19]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.1525186567164179
[2m[36m(func pid=170188)[0m top5: 0.7103544776119403
[2m[36m(func pid=170188)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=170188)[0m f1_macro: 0.09298351891289852
[2m[36m(func pid=170188)[0m f1_weighted: 0.13113825567024334
[2m[36m(func pid=170188)[0m f1_per_class: [0.083, 0.039, 0.121, 0.284, 0.0, 0.0, 0.089, 0.231, 0.083, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=176697)[0m top1: 0.25886194029850745
[2m[36m(func pid=176697)[0m top5: 0.71875
[2m[36m(func pid=176697)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=176697)[0m f1_macro: 0.1448609377562402
[2m[36m(func pid=176697)[0m f1_weighted: 0.18323474568044001
[2m[36m(func pid=176697)[0m f1_per_class: [0.253, 0.0, 0.462, 0.0, 0.038, 0.0, 0.561, 0.135, 0.0, 0.0]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.03824626865671642
[2m[36m(func pid=170672)[0m top5: 0.8125
[2m[36m(func pid=170672)[0m f1_micro: 0.03824626865671642
[2m[36m(func pid=170672)[0m f1_macro: 0.07010898836437689
[2m[36m(func pid=170672)[0m f1_weighted: 0.034700025578939336
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.0, 0.222, 0.07, 0.173, 0.0, 0.0, 0.214, 0.0, 0.022]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.7382 | Steps: 4 | Val loss: 2.2478 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.2905 | Steps: 4 | Val loss: 2.3486 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 15.1446 | Steps: 4 | Val loss: 8.7536 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.5337 | Steps: 4 | Val loss: 2.0771 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 10:28:18 (running for 00:10:47.94)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.705 |      0.093 |                   34 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.41  |      0.07  |                   33 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.291 |      0.13  |                   34 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.372 |      0.145 |                   11 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.18003731343283583
[2m[36m(func pid=171228)[0m top5: 0.7919776119402985
[2m[36m(func pid=171228)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=171228)[0m f1_macro: 0.13011181880771994
[2m[36m(func pid=171228)[0m f1_weighted: 0.17004086233582927
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.377, 0.093, 0.105, 0.041, 0.0, 0.175, 0.368, 0.0, 0.142]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.14878731343283583
[2m[36m(func pid=170188)[0m top5: 0.7192164179104478
[2m[36m(func pid=170188)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=170188)[0m f1_macro: 0.08724502423625433
[2m[36m(func pid=170188)[0m f1_weighted: 0.13342383585078124
[2m[36m(func pid=170188)[0m f1_per_class: [0.063, 0.033, 0.097, 0.279, 0.0, 0.0, 0.112, 0.208, 0.08, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=176697)[0m top1: 0.24347014925373134
[2m[36m(func pid=176697)[0m top5: 0.7924440298507462
[2m[36m(func pid=176697)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=176697)[0m f1_macro: 0.2279259773240671
[2m[36m(func pid=176697)[0m f1_weighted: 0.2564848664115185
[2m[36m(func pid=176697)[0m f1_per_class: [0.218, 0.444, 0.562, 0.0, 0.08, 0.0, 0.488, 0.405, 0.082, 0.0]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.20242537313432835
[2m[36m(func pid=170672)[0m top5: 0.7709888059701493
[2m[36m(func pid=170672)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=170672)[0m f1_macro: 0.17640624766714824
[2m[36m(func pid=170672)[0m f1_weighted: 0.18874011358799456
[2m[36m(func pid=170672)[0m f1_per_class: [0.264, 0.0, 0.272, 0.499, 0.091, 0.111, 0.0, 0.498, 0.0, 0.029]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.7572 | Steps: 4 | Val loss: 2.2391 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.3816 | Steps: 4 | Val loss: 3.1546 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 11.8428 | Steps: 4 | Val loss: 10.4830 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.3878 | Steps: 4 | Val loss: 2.1313 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=171228)[0m top1: 0.10634328358208955
[2m[36m(func pid=171228)[0m top5: 0.46455223880597013
[2m[36m(func pid=171228)[0m f1_micro: 0.10634328358208955
[2m[36m(func pid=171228)[0m f1_macro: 0.16451434834643336
[2m[36m(func pid=171228)[0m f1_weighted: 0.05766814338210165
[2m[36m(func pid=171228)[0m f1_per_class: [0.268, 0.12, 0.632, 0.0, 0.05, 0.008, 0.0, 0.383, 0.096, 0.088]
== Status ==
Current time: 2024-01-07 10:28:23 (running for 00:10:53.36)
Memory usage on this node: 24.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.738 |      0.087 |                   35 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.534 |      0.176 |                   34 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.382 |      0.165 |                   35 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 15.145 |      0.228 |                   12 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m top1: 0.17117537313432835
[2m[36m(func pid=170188)[0m top5: 0.7523320895522388
[2m[36m(func pid=170188)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=170188)[0m f1_macro: 0.09786752622453482
[2m[36m(func pid=170188)[0m f1_weighted: 0.17333652790610796
[2m[36m(func pid=170188)[0m f1_per_class: [0.074, 0.038, 0.085, 0.282, 0.0, 0.0, 0.246, 0.174, 0.079, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m top1: 0.21222014925373134
[2m[36m(func pid=176697)[0m top5: 0.7513992537313433
[2m[36m(func pid=176697)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=176697)[0m f1_macro: 0.13998413043394745
[2m[36m(func pid=176697)[0m f1_weighted: 0.09209390281524088
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.317, 0.375, 0.0, 0.0, 0.0, 0.0, 0.542, 0.092, 0.074]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.16324626865671643
[2m[36m(func pid=170672)[0m top5: 0.7164179104477612
[2m[36m(func pid=170672)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=170672)[0m f1_macro: 0.14785312967869008
[2m[36m(func pid=170672)[0m f1_weighted: 0.14022867188194818
[2m[36m(func pid=170672)[0m f1_per_class: [0.179, 0.0, 0.417, 0.346, 0.062, 0.222, 0.0, 0.188, 0.0, 0.065]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.5699 | Steps: 4 | Val loss: 1.6795 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 18.2334 | Steps: 4 | Val loss: 4.8887 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.7147 | Steps: 4 | Val loss: 2.2142 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 10:28:29 (running for 00:10:58.80)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.757 |      0.098 |                   36 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.388 |      0.148 |                   35 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.382 |      0.165 |                   35 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 18.233 |      0.169 |                   14 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.5711 | Steps: 4 | Val loss: 2.2054 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=171228)[0m top1: 0.42863805970149255
[2m[36m(func pid=171228)[0m top5: 0.9067164179104478
[2m[36m(func pid=171228)[0m f1_micro: 0.42863805970149255
[2m[36m(func pid=171228)[0m f1_macro: 0.2592664662997238
[2m[36m(func pid=171228)[0m f1_weighted: 0.4066438259400116
[2m[36m(func pid=171228)[0m f1_per_class: [0.23, 0.34, 0.345, 0.558, 0.279, 0.098, 0.568, 0.015, 0.018, 0.142]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m top1: 0.2439365671641791
[2m[36m(func pid=176697)[0m top5: 0.8852611940298507
[2m[36m(func pid=176697)[0m f1_micro: 0.2439365671641791
[2m[36m(func pid=176697)[0m f1_macro: 0.1691805153264798
[2m[36m(func pid=176697)[0m f1_weighted: 0.2508171436470952
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.233, 0.0, 0.138, 0.044, 0.354, 0.353, 0.427, 0.0, 0.143]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m top1: 0.23180970149253732
[2m[36m(func pid=170188)[0m top5: 0.7938432835820896
[2m[36m(func pid=170188)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=170188)[0m f1_macro: 0.12321877461336694
[2m[36m(func pid=170188)[0m f1_weighted: 0.22828850188137198
[2m[36m(func pid=170188)[0m f1_per_class: [0.107, 0.035, 0.123, 0.345, 0.0, 0.0, 0.372, 0.16, 0.091, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m top1: 0.09794776119402986
[2m[36m(func pid=170672)[0m top5: 0.7215485074626866
[2m[36m(func pid=170672)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=170672)[0m f1_macro: 0.1069504050155103
[2m[36m(func pid=170672)[0m f1_weighted: 0.04915644515723123
[2m[36m(func pid=170672)[0m f1_per_class: [0.136, 0.0, 0.488, 0.055, 0.058, 0.15, 0.0, 0.182, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.8864 | Steps: 4 | Val loss: 1.7421 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.6820 | Steps: 4 | Val loss: 2.2178 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 14.3897 | Steps: 4 | Val loss: 17.7221 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=171228)[0m top1: 0.42723880597014924
[2m[36m(func pid=171228)[0m top5: 0.902518656716418
[2m[36m(func pid=171228)[0m f1_micro: 0.4272388059701493
[2m[36m(func pid=171228)[0m f1_macro: 0.26957920667262336
[2m[36m(func pid=171228)[0m f1_weighted: 0.3893004647278296
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.38, 0.212, 0.583, 0.27, 0.414, 0.29, 0.389, 0.0, 0.158]
== Status ==
Current time: 2024-01-07 10:28:34 (running for 00:11:04.12)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.715 |      0.123 |                   37 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.571 |      0.107 |                   36 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.886 |      0.27  |                   37 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 18.233 |      0.169 |                   14 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m top1: 0.22388059701492538
[2m[36m(func pid=170188)[0m top5: 0.7859141791044776
[2m[36m(func pid=170188)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=170188)[0m f1_macro: 0.11757567797190585
[2m[36m(func pid=170188)[0m f1_weighted: 0.2197300101204578
[2m[36m(func pid=170188)[0m f1_per_class: [0.078, 0.035, 0.131, 0.335, 0.0, 0.0, 0.356, 0.147, 0.093, 0.0]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m top1: 0.2019589552238806
[2m[36m(func pid=176697)[0m top5: 0.5167910447761194
[2m[36m(func pid=176697)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=176697)[0m f1_macro: 0.12179172704049371
[2m[36m(func pid=176697)[0m f1_weighted: 0.19207379792034734
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.0, 0.058, 0.0, 0.065, 0.0, 0.543, 0.498, 0.0, 0.054]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.4655 | Steps: 4 | Val loss: 2.1532 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=170672)[0m top1: 0.134794776119403
[2m[36m(func pid=170672)[0m top5: 0.7700559701492538
[2m[36m(func pid=170672)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=170672)[0m f1_macro: 0.11025153543430463
[2m[36m(func pid=170672)[0m f1_weighted: 0.11606034916803983
[2m[36m(func pid=170672)[0m f1_per_class: [0.144, 0.115, 0.229, 0.242, 0.037, 0.084, 0.0, 0.252, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.6656 | Steps: 4 | Val loss: 2.2186 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.5927 | Steps: 4 | Val loss: 2.4416 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 15.3002 | Steps: 4 | Val loss: 9.8681 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 10:28:39 (running for 00:11:09.38)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.682 |      0.118 |                   38 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.465 |      0.11  |                   37 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.886 |      0.27  |                   37 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 15.3   |      0.206 |                   16 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.20708955223880596
[2m[36m(func pid=171228)[0m top5: 0.7355410447761194
[2m[36m(func pid=171228)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=171228)[0m f1_macro: 0.13887094857071536
[2m[36m(func pid=171228)[0m f1_weighted: 0.20428776378253827
[2m[36m(func pid=171228)[0m f1_per_class: [0.043, 0.343, 0.104, 0.0, 0.037, 0.0, 0.412, 0.335, 0.0, 0.115]
[2m[36m(func pid=176697)[0m top1: 0.23973880597014927
[2m[36m(func pid=176697)[0m top5: 0.8777985074626866
[2m[36m(func pid=176697)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=176697)[0m f1_macro: 0.20594837962152934
[2m[36m(func pid=176697)[0m f1_weighted: 0.15337557068038313
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.387, 0.696, 0.0, 0.085, 0.23, 0.087, 0.498, 0.0, 0.077]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.23647388059701493
[2m[36m(func pid=170188)[0m top5: 0.7863805970149254
[2m[36m(func pid=170188)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=170188)[0m f1_macro: 0.12255383622539232
[2m[36m(func pid=170188)[0m f1_weighted: 0.23775928648407296
[2m[36m(func pid=170188)[0m f1_per_class: [0.071, 0.035, 0.105, 0.331, 0.0, 0.0, 0.416, 0.18, 0.087, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.3835 | Steps: 4 | Val loss: 2.1303 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 12.0099 | Steps: 4 | Val loss: 12.6756 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.6753 | Steps: 4 | Val loss: 2.2172 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.8227 | Steps: 4 | Val loss: 3.1959 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=170672)[0m top1: 0.12546641791044777
[2m[36m(func pid=170672)[0m top5: 0.7686567164179104
[2m[36m(func pid=170672)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=170672)[0m f1_macro: 0.14052943219682815
[2m[36m(func pid=170672)[0m f1_weighted: 0.09554723870055572
[2m[36m(func pid=170672)[0m f1_per_class: [0.176, 0.25, 0.308, 0.057, 0.03, 0.04, 0.0, 0.406, 0.065, 0.074]
[2m[36m(func pid=170672)[0m 
== Status ==
Current time: 2024-01-07 10:28:44 (running for 00:11:14.54)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.666 |      0.123 |                   39 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.384 |      0.141 |                   38 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.593 |      0.139 |                   38 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 12.01  |      0.148 |                   17 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.23227611940298507
[2m[36m(func pid=176697)[0m top5: 0.7028917910447762
[2m[36m(func pid=176697)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=176697)[0m f1_macro: 0.147517247733113
[2m[36m(func pid=176697)[0m f1_weighted: 0.22274734836131413
[2m[36m(func pid=176697)[0m f1_per_class: [0.154, 0.0, 0.0, 0.0, 0.107, 0.384, 0.538, 0.241, 0.0, 0.052]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m top1: 0.24347014925373134
[2m[36m(func pid=170188)[0m top5: 0.7943097014925373
[2m[36m(func pid=170188)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=170188)[0m f1_macro: 0.12580404294358621
[2m[36m(func pid=170188)[0m f1_weighted: 0.24374584769366534
[2m[36m(func pid=170188)[0m f1_per_class: [0.063, 0.041, 0.074, 0.343, 0.0, 0.0, 0.409, 0.262, 0.067, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m top1: 0.08348880597014925
[2m[36m(func pid=171228)[0m top5: 0.6651119402985075
[2m[36m(func pid=171228)[0m f1_micro: 0.08348880597014925
[2m[36m(func pid=171228)[0m f1_macro: 0.13376110143543746
[2m[36m(func pid=171228)[0m f1_weighted: 0.07048331639320801
[2m[36m(func pid=171228)[0m f1_per_class: [0.359, 0.011, 0.348, 0.0, 0.037, 0.0, 0.162, 0.078, 0.092, 0.25]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.4973 | Steps: 4 | Val loss: 2.1308 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 14.3992 | Steps: 4 | Val loss: 16.8459 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 1.9129 | Steps: 4 | Val loss: 1.9442 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.6930 | Steps: 4 | Val loss: 2.2176 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=170672)[0m top1: 0.1357276119402985
[2m[36m(func pid=170672)[0m top5: 0.7173507462686567
[2m[36m(func pid=170672)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=170672)[0m f1_macro: 0.17379672509004224
[2m[36m(func pid=170672)[0m f1_weighted: 0.10985511313703258
[2m[36m(func pid=170672)[0m f1_per_class: [0.172, 0.308, 0.393, 0.0, 0.041, 0.157, 0.003, 0.464, 0.122, 0.077]
[2m[36m(func pid=170672)[0m 
== Status ==
Current time: 2024-01-07 10:28:50 (running for 00:11:19.74)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.675 |      0.126 |                   40 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.497 |      0.174 |                   39 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.823 |      0.134 |                   39 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 14.399 |      0.112 |                   18 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.15858208955223882
[2m[36m(func pid=176697)[0m top5: 0.6105410447761194
[2m[36m(func pid=176697)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=176697)[0m f1_macro: 0.11192876046669889
[2m[36m(func pid=176697)[0m f1_weighted: 0.09871714571629242
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.0, 0.038, 0.0, 0.0, 0.289, 0.115, 0.505, 0.0, 0.171]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=171228)[0m top1: 0.33955223880597013
[2m[36m(func pid=171228)[0m top5: 0.8311567164179104
[2m[36m(func pid=171228)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=171228)[0m f1_macro: 0.23050328297551842
[2m[36m(func pid=171228)[0m f1_weighted: 0.31418223569698
[2m[36m(func pid=171228)[0m f1_per_class: [0.242, 0.533, 0.174, 0.352, 0.226, 0.347, 0.245, 0.016, 0.043, 0.126]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.24113805970149255
[2m[36m(func pid=170188)[0m top5: 0.7863805970149254
[2m[36m(func pid=170188)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=170188)[0m f1_macro: 0.1236267499670898
[2m[36m(func pid=170188)[0m f1_weighted: 0.2299928620291487
[2m[36m(func pid=170188)[0m f1_per_class: [0.062, 0.021, 0.07, 0.359, 0.0, 0.0, 0.351, 0.307, 0.067, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.6220 | Steps: 4 | Val loss: 2.0478 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 12.0914 | Steps: 4 | Val loss: 7.8701 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.9066 | Steps: 4 | Val loss: 1.5921 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.6417 | Steps: 4 | Val loss: 2.2193 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=170672)[0m top1: 0.16511194029850745
[2m[36m(func pid=170672)[0m top5: 0.8507462686567164
[2m[36m(func pid=170672)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=170672)[0m f1_macro: 0.1652268346113408
[2m[36m(func pid=170672)[0m f1_weighted: 0.16184633543038043
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.309, 0.293, 0.016, 0.043, 0.186, 0.17, 0.437, 0.124, 0.074]
[2m[36m(func pid=170672)[0m 
== Status ==
Current time: 2024-01-07 10:28:55 (running for 00:11:24.96)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.693 |      0.124 |                   41 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.622 |      0.165 |                   40 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.913 |      0.231 |                   40 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 12.091 |      0.188 |                   19 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.21595149253731344
[2m[36m(func pid=176697)[0m top5: 0.8222947761194029
[2m[36m(func pid=176697)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=176697)[0m f1_macro: 0.18805841998066974
[2m[36m(func pid=176697)[0m f1_weighted: 0.2469227076408637
[2m[36m(func pid=176697)[0m f1_per_class: [0.259, 0.037, 0.262, 0.454, 0.03, 0.0, 0.256, 0.518, 0.0, 0.066]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=171228)[0m top1: 0.44402985074626866
[2m[36m(func pid=171228)[0m top5: 0.8936567164179104
[2m[36m(func pid=171228)[0m f1_micro: 0.44402985074626866
[2m[36m(func pid=171228)[0m f1_macro: 0.3138905580281971
[2m[36m(func pid=171228)[0m f1_weighted: 0.41835419820992903
[2m[36m(func pid=171228)[0m f1_per_class: [0.263, 0.532, 0.324, 0.551, 0.323, 0.409, 0.335, 0.259, 0.0, 0.143]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.22994402985074627
[2m[36m(func pid=170188)[0m top5: 0.7789179104477612
[2m[36m(func pid=170188)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=170188)[0m f1_macro: 0.11715195109407543
[2m[36m(func pid=170188)[0m f1_weighted: 0.20958027709165866
[2m[36m(func pid=170188)[0m f1_per_class: [0.063, 0.016, 0.071, 0.359, 0.0, 0.008, 0.282, 0.306, 0.066, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.6159 | Steps: 4 | Val loss: 2.0349 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.9637 | Steps: 4 | Val loss: 2.0847 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 14.4670 | Steps: 4 | Val loss: 11.4274 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.6273 | Steps: 4 | Val loss: 2.2180 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=170672)[0m top1: 0.18563432835820895
[2m[36m(func pid=170672)[0m top5: 0.8731343283582089
[2m[36m(func pid=170672)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=170672)[0m f1_macro: 0.1651684825851062
[2m[36m(func pid=170672)[0m f1_weighted: 0.22862846192080882
[2m[36m(func pid=170672)[0m f1_per_class: [0.1, 0.172, 0.147, 0.315, 0.027, 0.0, 0.263, 0.421, 0.129, 0.077]
[2m[36m(func pid=170672)[0m 
== Status ==
Current time: 2024-01-07 10:29:00 (running for 00:11:30.26)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.642 |      0.117 |                   42 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.616 |      0.165 |                   41 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.964 |      0.305 |                   42 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 12.091 |      0.188 |                   19 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.27611940298507465
[2m[36m(func pid=171228)[0m top5: 0.7915111940298507
[2m[36m(func pid=171228)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=171228)[0m f1_macro: 0.3045237846846175
[2m[36m(func pid=171228)[0m f1_weighted: 0.2819508812598896
[2m[36m(func pid=171228)[0m f1_per_class: [0.281, 0.43, 0.571, 0.016, 0.134, 0.355, 0.403, 0.433, 0.119, 0.303]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m top1: 0.15485074626865672
[2m[36m(func pid=176697)[0m top5: 0.6152052238805971
[2m[36m(func pid=176697)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=176697)[0m f1_macro: 0.17396739211633888
[2m[36m(func pid=176697)[0m f1_weighted: 0.15371457168820385
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.0, 0.581, 0.412, 0.049, 0.0, 0.0, 0.549, 0.073, 0.077]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m top1: 0.23460820895522388
[2m[36m(func pid=170188)[0m top5: 0.7873134328358209
[2m[36m(func pid=170188)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=170188)[0m f1_macro: 0.11974117890013689
[2m[36m(func pid=170188)[0m f1_weighted: 0.21667752082979747
[2m[36m(func pid=170188)[0m f1_per_class: [0.072, 0.021, 0.071, 0.367, 0.0, 0.024, 0.291, 0.304, 0.046, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.1725 | Steps: 4 | Val loss: 2.1406 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.9027 | Steps: 4 | Val loss: 2.1668 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 11.8442 | Steps: 4 | Val loss: 17.2000 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.6481 | Steps: 4 | Val loss: 2.2221 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=170672)[0m top1: 0.09235074626865672
[2m[36m(func pid=170672)[0m top5: 0.840018656716418
[2m[36m(func pid=170672)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=170672)[0m f1_macro: 0.09502934529051589
[2m[36m(func pid=170672)[0m f1_weighted: 0.09465999076261238
[2m[36m(func pid=170672)[0m f1_per_class: [0.197, 0.0, 0.143, 0.228, 0.022, 0.0, 0.021, 0.339, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
== Status ==
Current time: 2024-01-07 10:29:05 (running for 00:11:35.31)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.627 |      0.12  |                   43 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.173 |      0.095 |                   42 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.903 |      0.259 |                   43 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 14.467 |      0.174 |                   20 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.35027985074626866
[2m[36m(func pid=171228)[0m top5: 0.7472014925373134
[2m[36m(func pid=171228)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=171228)[0m f1_macro: 0.25932343914056155
[2m[36m(func pid=171228)[0m f1_weighted: 0.28297269476518416
[2m[36m(func pid=171228)[0m f1_per_class: [0.318, 0.437, 0.556, 0.0, 0.073, 0.0, 0.57, 0.419, 0.027, 0.194]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m top1: 0.15811567164179105
[2m[36m(func pid=176697)[0m top5: 0.5620335820895522
[2m[36m(func pid=176697)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=176697)[0m f1_macro: 0.1861060125363526
[2m[36m(func pid=176697)[0m f1_weighted: 0.10813787674648513
[2m[36m(func pid=176697)[0m f1_per_class: [0.093, 0.0, 0.6, 0.146, 0.165, 0.302, 0.0, 0.425, 0.0, 0.129]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m top1: 0.22574626865671643
[2m[36m(func pid=170188)[0m top5: 0.800839552238806
[2m[36m(func pid=170188)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=170188)[0m f1_macro: 0.11658599858836735
[2m[36m(func pid=170188)[0m f1_weighted: 0.219562289882173
[2m[36m(func pid=170188)[0m f1_per_class: [0.057, 0.04, 0.064, 0.348, 0.0, 0.053, 0.307, 0.264, 0.032, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.3344 | Steps: 4 | Val loss: 2.3059 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.6756 | Steps: 4 | Val loss: 2.4675 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 17.0472 | Steps: 4 | Val loss: 15.0016 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.7052 | Steps: 4 | Val loss: 2.2108 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=170672)[0m top1: 0.05550373134328358
[2m[36m(func pid=170672)[0m top5: 0.7980410447761194
[2m[36m(func pid=170672)[0m f1_micro: 0.05550373134328358
[2m[36m(func pid=170672)[0m f1_macro: 0.09328641328236147
[2m[36m(func pid=170672)[0m f1_weighted: 0.027058849253952147
[2m[36m(func pid=170672)[0m f1_per_class: [0.187, 0.0, 0.379, 0.003, 0.02, 0.0, 0.0, 0.343, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m top1: 0.12313432835820895
[2m[36m(func pid=176697)[0m top5: 0.7019589552238806
[2m[36m(func pid=176697)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=176697)[0m f1_macro: 0.1664382807387647
[2m[36m(func pid=176697)[0m f1_weighted: 0.12803522452580648
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.0, 0.6, 0.303, 0.127, 0.0, 0.045, 0.361, 0.084, 0.143]
[2m[36m(func pid=176697)[0m 
== Status ==
Current time: 2024-01-07 10:29:10 (running for 00:11:40.57)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.648 |      0.117 |                   44 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.334 |      0.093 |                   43 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.903 |      0.259 |                   43 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 17.047 |      0.166 |                   22 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.12173507462686567
[2m[36m(func pid=171228)[0m top5: 0.820429104477612
[2m[36m(func pid=171228)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=171228)[0m f1_macro: 0.1081706822751504
[2m[36m(func pid=171228)[0m f1_weighted: 0.11616193961851974
[2m[36m(func pid=171228)[0m f1_per_class: [0.074, 0.047, 0.044, 0.151, 0.038, 0.0, 0.138, 0.342, 0.0, 0.248]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.2579291044776119
[2m[36m(func pid=170188)[0m top5: 0.8278917910447762
[2m[36m(func pid=170188)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=170188)[0m f1_macro: 0.12546656126159672
[2m[36m(func pid=170188)[0m f1_weighted: 0.25691885705293704
[2m[36m(func pid=170188)[0m f1_per_class: [0.039, 0.062, 0.071, 0.366, 0.0, 0.061, 0.413, 0.205, 0.04, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.5571 | Steps: 4 | Val loss: 2.3235 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 15.1207 | Steps: 4 | Val loss: 17.9144 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.1576 | Steps: 4 | Val loss: 2.1661 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.6428 | Steps: 4 | Val loss: 2.2027 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=170672)[0m top1: 0.06763059701492537
[2m[36m(func pid=170672)[0m top5: 0.7513992537313433
[2m[36m(func pid=170672)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=170672)[0m f1_macro: 0.0865402268329909
[2m[36m(func pid=170672)[0m f1_weighted: 0.020806394459198577
[2m[36m(func pid=170672)[0m f1_per_class: [0.196, 0.0, 0.4, 0.0, 0.024, 0.0, 0.0, 0.245, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
== Status ==
Current time: 2024-01-07 10:29:16 (running for 00:11:45.75)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.705 |      0.125 |                   45 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.557 |      0.087 |                   44 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.158 |      0.261 |                   45 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 17.047 |      0.166 |                   22 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.31156716417910446
[2m[36m(func pid=171228)[0m top5: 0.7910447761194029
[2m[36m(func pid=171228)[0m f1_micro: 0.31156716417910446
[2m[36m(func pid=171228)[0m f1_macro: 0.261210358683868
[2m[36m(func pid=171228)[0m f1_weighted: 0.23178776766340933
[2m[36m(func pid=171228)[0m f1_per_class: [0.355, 0.005, 0.609, 0.546, 0.058, 0.221, 0.028, 0.53, 0.0, 0.261]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m top1: 0.06623134328358209
[2m[36m(func pid=176697)[0m top5: 0.5890858208955224
[2m[36m(func pid=176697)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=176697)[0m f1_macro: 0.07775700559785678
[2m[36m(func pid=176697)[0m f1_weighted: 0.03344989886601508
[2m[36m(func pid=176697)[0m f1_per_class: [0.103, 0.032, 0.061, 0.0, 0.025, 0.0, 0.0, 0.407, 0.0, 0.15]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m top1: 0.2849813432835821
[2m[36m(func pid=170188)[0m top5: 0.8540111940298507
[2m[36m(func pid=170188)[0m f1_micro: 0.2849813432835821
[2m[36m(func pid=170188)[0m f1_macro: 0.13562344855368533
[2m[36m(func pid=170188)[0m f1_weighted: 0.28652465567888563
[2m[36m(func pid=170188)[0m f1_per_class: [0.024, 0.106, 0.068, 0.371, 0.0, 0.061, 0.482, 0.219, 0.027, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.3153 | Steps: 4 | Val loss: 2.2017 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.8652 | Steps: 4 | Val loss: 1.9263 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 15.3773 | Steps: 4 | Val loss: 9.5571 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7897 | Steps: 4 | Val loss: 2.2095 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 10:29:21 (running for 00:11:51.03)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.643 |      0.136 |                   46 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.557 |      0.087 |                   44 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.865 |      0.247 |                   46 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 15.121 |      0.078 |                   23 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170672)[0m top1: 0.10727611940298508
[2m[36m(func pid=170672)[0m top5: 0.6763059701492538
[2m[36m(func pid=170672)[0m f1_micro: 0.10727611940298508
[2m[36m(func pid=170672)[0m f1_macro: 0.10476998379926912
[2m[36m(func pid=170672)[0m f1_weighted: 0.06436217860486869
[2m[36m(func pid=170672)[0m f1_per_class: [0.252, 0.246, 0.262, 0.0, 0.028, 0.0, 0.0, 0.259, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m top1: 0.2490671641791045
[2m[36m(func pid=171228)[0m top5: 0.8512126865671642
[2m[36m(func pid=171228)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=171228)[0m f1_macro: 0.24729535979352
[2m[36m(func pid=171228)[0m f1_weighted: 0.21988362039502898
[2m[36m(func pid=171228)[0m f1_per_class: [0.358, 0.407, 0.0, 0.079, 0.226, 0.39, 0.132, 0.485, 0.11, 0.286]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m top1: 0.2224813432835821
[2m[36m(func pid=176697)[0m top5: 0.8717350746268657
[2m[36m(func pid=176697)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=176697)[0m f1_macro: 0.135099711534637
[2m[36m(func pid=176697)[0m f1_weighted: 0.23987968978204863
[2m[36m(func pid=176697)[0m f1_per_class: [0.145, 0.26, 0.0, 0.087, 0.034, 0.0, 0.499, 0.325, 0.0, 0.0]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m top1: 0.27052238805970147
[2m[36m(func pid=170188)[0m top5: 0.8526119402985075
[2m[36m(func pid=170188)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=170188)[0m f1_macro: 0.1263831070566286
[2m[36m(func pid=170188)[0m f1_weighted: 0.2757268069006612
[2m[36m(func pid=170188)[0m f1_per_class: [0.036, 0.14, 0.057, 0.353, 0.0, 0.045, 0.461, 0.153, 0.019, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.0711 | Steps: 4 | Val loss: 2.0856 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.3676 | Steps: 4 | Val loss: 2.1161 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 13.1266 | Steps: 4 | Val loss: 12.1633 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.7245 | Steps: 4 | Val loss: 2.2074 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 10:29:26 (running for 00:11:56.23)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.79  |      0.126 |                   47 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.315 |      0.105 |                   45 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.071 |      0.244 |                   47 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 15.377 |      0.135 |                   24 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.32276119402985076
[2m[36m(func pid=171228)[0m top5: 0.7611940298507462
[2m[36m(func pid=171228)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=171228)[0m f1_macro: 0.2435149755148644
[2m[36m(func pid=171228)[0m f1_weighted: 0.2824956113333365
[2m[36m(func pid=171228)[0m f1_per_class: [0.282, 0.36, 0.293, 0.0, 0.23, 0.373, 0.522, 0.162, 0.061, 0.154]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m top1: 0.1837686567164179
[2m[36m(func pid=176697)[0m top5: 0.7364738805970149
[2m[36m(func pid=176697)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=176697)[0m f1_macro: 0.19005550980761582
[2m[36m(func pid=176697)[0m f1_weighted: 0.1684804229573976
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.0, 0.696, 0.036, 0.186, 0.0, 0.431, 0.346, 0.097, 0.11]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.1515858208955224
[2m[36m(func pid=170672)[0m top5: 0.6949626865671642
[2m[36m(func pid=170672)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=170672)[0m f1_macro: 0.08379419664249789
[2m[36m(func pid=170672)[0m f1_weighted: 0.07622957289123203
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.325, 0.16, 0.0, 0.048, 0.023, 0.0, 0.281, 0.0, 0.0]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m top1: 0.2635261194029851
[2m[36m(func pid=170188)[0m top5: 0.8577425373134329
[2m[36m(func pid=170188)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=170188)[0m f1_macro: 0.1307966627278173
[2m[36m(func pid=170188)[0m f1_weighted: 0.2756103224349489
[2m[36m(func pid=170188)[0m f1_per_class: [0.043, 0.201, 0.059, 0.343, 0.0, 0.045, 0.432, 0.165, 0.02, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.9217 | Steps: 4 | Val loss: 2.6735 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 14.3240 | Steps: 4 | Val loss: 25.0090 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.3819 | Steps: 4 | Val loss: 2.0315 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.6304 | Steps: 4 | Val loss: 2.2105 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 10:29:31 (running for 00:12:01.60)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.724 |      0.131 |                   48 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.368 |      0.084 |                   46 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.922 |      0.157 |                   48 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 13.127 |      0.19  |                   25 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.1707089552238806
[2m[36m(func pid=171228)[0m top5: 0.8278917910447762
[2m[36m(func pid=171228)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=171228)[0m f1_macro: 0.15685075073516536
[2m[36m(func pid=171228)[0m f1_weighted: 0.18787434673022255
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.317, 0.038, 0.069, 0.073, 0.04, 0.256, 0.454, 0.114, 0.207]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m top1: 0.06902985074626866
[2m[36m(func pid=176697)[0m top5: 0.6082089552238806
[2m[36m(func pid=176697)[0m f1_micro: 0.06902985074626866
[2m[36m(func pid=176697)[0m f1_macro: 0.055048241889840185
[2m[36m(func pid=176697)[0m f1_weighted: 0.019820796667694898
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.0, 0.134, 0.0, 0.028, 0.0, 0.006, 0.271, 0.0, 0.112]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.17723880597014927
[2m[36m(func pid=170672)[0m top5: 0.7388059701492538
[2m[36m(func pid=170672)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=170672)[0m f1_macro: 0.11874857569160795
[2m[36m(func pid=170672)[0m f1_weighted: 0.11439774725723754
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.328, 0.176, 0.08, 0.047, 0.099, 0.0, 0.38, 0.0, 0.077]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m top1: 0.24860074626865672
[2m[36m(func pid=170188)[0m top5: 0.8521455223880597
[2m[36m(func pid=170188)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=170188)[0m f1_macro: 0.13448886519442854
[2m[36m(func pid=170188)[0m f1_weighted: 0.2708488830868074
[2m[36m(func pid=170188)[0m f1_per_class: [0.046, 0.253, 0.066, 0.3, 0.0, 0.068, 0.418, 0.146, 0.048, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 18.1119 | Steps: 4 | Val loss: 11.6643 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.4698 | Steps: 4 | Val loss: 2.4338 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.2737 | Steps: 4 | Val loss: 2.0287 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.7174 | Steps: 4 | Val loss: 2.2048 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 10:29:37 (running for 00:12:06.86)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.63  |      0.134 |                   49 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.382 |      0.119 |                   47 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.922 |      0.157 |                   48 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 18.112 |      0.211 |                   27 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.228544776119403
[2m[36m(func pid=176697)[0m top5: 0.7728544776119403
[2m[36m(func pid=176697)[0m f1_micro: 0.228544776119403
[2m[36m(func pid=176697)[0m f1_macro: 0.21124793340261636
[2m[36m(func pid=176697)[0m f1_weighted: 0.2570712023107527
[2m[36m(func pid=176697)[0m f1_per_class: [0.135, 0.444, 0.457, 0.35, 0.045, 0.0, 0.181, 0.378, 0.0, 0.122]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=171228)[0m top1: 0.2957089552238806
[2m[36m(func pid=171228)[0m top5: 0.7112873134328358
[2m[36m(func pid=171228)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=171228)[0m f1_macro: 0.24802215052945337
[2m[36m(func pid=171228)[0m f1_weighted: 0.25009014845756133
[2m[36m(func pid=171228)[0m f1_per_class: [0.329, 0.337, 0.609, 0.547, 0.047, 0.0, 0.0, 0.471, 0.0, 0.142]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m top1: 0.24067164179104478
[2m[36m(func pid=170672)[0m top5: 0.7868470149253731
[2m[36m(func pid=170672)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=170672)[0m f1_macro: 0.19392648490153472
[2m[36m(func pid=170672)[0m f1_weighted: 0.22662956773575785
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.287, 0.333, 0.462, 0.047, 0.137, 0.0, 0.426, 0.129, 0.118]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m top1: 0.2728544776119403
[2m[36m(func pid=170188)[0m top5: 0.8577425373134329
[2m[36m(func pid=170188)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=170188)[0m f1_macro: 0.14972283222491253
[2m[36m(func pid=170188)[0m f1_weighted: 0.2950814031123515
[2m[36m(func pid=170188)[0m f1_per_class: [0.061, 0.32, 0.068, 0.299, 0.0, 0.108, 0.447, 0.115, 0.079, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 17.7065 | Steps: 4 | Val loss: 17.9362 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.0032 | Steps: 4 | Val loss: 2.3845 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.3419 | Steps: 4 | Val loss: 2.0308 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.6438 | Steps: 4 | Val loss: 2.2132 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=176697)[0m top1: 0.332089552238806
[2m[36m(func pid=176697)[0m top5: 0.6310634328358209
[2m[36m(func pid=176697)[0m f1_micro: 0.332089552238806
[2m[36m(func pid=176697)[0m f1_macro: 0.21087901325004
[2m[36m(func pid=176697)[0m f1_weighted: 0.2468353769966609
[2m[36m(func pid=176697)[0m f1_per_class: [0.211, 0.555, 0.478, 0.439, 0.149, 0.17, 0.0, 0.0, 0.0, 0.108]
[2m[36m(func pid=176697)[0m 
== Status ==
Current time: 2024-01-07 10:29:42 (running for 00:12:12.11)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.717 |      0.15  |                   50 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.274 |      0.194 |                   48 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.47  |      0.248 |                   49 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 17.706 |      0.211 |                   28 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.28591417910447764
[2m[36m(func pid=171228)[0m top5: 0.7518656716417911
[2m[36m(func pid=171228)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=171228)[0m f1_macro: 0.21149443453081682
[2m[36m(func pid=171228)[0m f1_weighted: 0.2696499694290578
[2m[36m(func pid=171228)[0m f1_per_class: [0.241, 0.399, 0.0, 0.555, 0.035, 0.0, 0.012, 0.521, 0.145, 0.207]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m top1: 0.21735074626865672
[2m[36m(func pid=170672)[0m top5: 0.8521455223880597
[2m[36m(func pid=170672)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=170672)[0m f1_macro: 0.2466777989034779
[2m[36m(func pid=170672)[0m f1_weighted: 0.2591937511054549
[2m[36m(func pid=170672)[0m f1_per_class: [0.163, 0.231, 0.588, 0.347, 0.061, 0.206, 0.203, 0.462, 0.115, 0.093]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m top1: 0.23973880597014927
[2m[36m(func pid=170188)[0m top5: 0.8367537313432836
[2m[36m(func pid=170188)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=170188)[0m f1_macro: 0.12960774621303545
[2m[36m(func pid=170188)[0m f1_weighted: 0.261847209718551
[2m[36m(func pid=170188)[0m f1_per_class: [0.043, 0.312, 0.065, 0.273, 0.0, 0.036, 0.398, 0.097, 0.071, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 14.9362 | Steps: 4 | Val loss: 16.8310 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.1002 | Steps: 4 | Val loss: 1.9584 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.2868 | Steps: 4 | Val loss: 2.0082 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.8344 | Steps: 4 | Val loss: 2.2206 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=176697)[0m top1: 0.19076492537313433
[2m[36m(func pid=176697)[0m top5: 0.4818097014925373
[2m[36m(func pid=176697)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=176697)[0m f1_macro: 0.1418065776292266
[2m[36m(func pid=176697)[0m f1_weighted: 0.11113683215688254
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.478, 0.176, 0.0, 0.049, 0.016, 0.0, 0.287, 0.197, 0.214]
== Status ==
Current time: 2024-01-07 10:29:47 (running for 00:12:17.38)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.644 |      0.13  |                   51 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.342 |      0.247 |                   49 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.003 |      0.211 |                   50 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 14.936 |      0.142 |                   29 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=171228)[0m top1: 0.28171641791044777
[2m[36m(func pid=171228)[0m top5: 0.8101679104477612
[2m[36m(func pid=171228)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=171228)[0m f1_macro: 0.2878283369762339
[2m[36m(func pid=171228)[0m f1_weighted: 0.2926511532018144
[2m[36m(func pid=171228)[0m f1_per_class: [0.356, 0.387, 0.667, 0.098, 0.101, 0.341, 0.408, 0.423, 0.023, 0.075]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m top1: 0.30970149253731344
[2m[36m(func pid=170672)[0m top5: 0.8987873134328358
[2m[36m(func pid=170672)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=170672)[0m f1_macro: 0.20885037079568872
[2m[36m(func pid=170672)[0m f1_weighted: 0.30989799578564997
[2m[36m(func pid=170672)[0m f1_per_class: [0.183, 0.164, 0.5, 0.32, 0.084, 0.082, 0.573, 0.032, 0.059, 0.091]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m top1: 0.2248134328358209
[2m[36m(func pid=170188)[0m top5: 0.8208955223880597
[2m[36m(func pid=170188)[0m f1_micro: 0.2248134328358209
[2m[36m(func pid=170188)[0m f1_macro: 0.12798849599183215
[2m[36m(func pid=170188)[0m f1_weighted: 0.25107282735246705
[2m[36m(func pid=170188)[0m f1_per_class: [0.048, 0.303, 0.061, 0.277, 0.0, 0.067, 0.35, 0.107, 0.067, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 7.6949 | Steps: 4 | Val loss: 9.9697 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.2364 | Steps: 4 | Val loss: 2.8997 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.1958 | Steps: 4 | Val loss: 2.0233 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 10:29:52 (running for 00:12:22.44)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.834 |      0.128 |                   52 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.287 |      0.209 |                   50 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.1   |      0.288 |                   51 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  7.695 |      0.208 |                   30 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.6752 | Steps: 4 | Val loss: 2.2151 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=176697)[0m top1: 0.1982276119402985
[2m[36m(func pid=176697)[0m top5: 0.7947761194029851
[2m[36m(func pid=176697)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=176697)[0m f1_macro: 0.20753730424506395
[2m[36m(func pid=176697)[0m f1_weighted: 0.16626201324787887
[2m[36m(func pid=176697)[0m f1_per_class: [0.115, 0.475, 0.383, 0.0, 0.092, 0.275, 0.078, 0.362, 0.0, 0.294]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=171228)[0m top1: 0.3148320895522388
[2m[36m(func pid=171228)[0m top5: 0.6539179104477612
[2m[36m(func pid=171228)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=171228)[0m f1_macro: 0.2430514234481521
[2m[36m(func pid=171228)[0m f1_weighted: 0.3001407912429477
[2m[36m(func pid=171228)[0m f1_per_class: [0.116, 0.411, 0.218, 0.0, 0.158, 0.395, 0.519, 0.379, 0.0, 0.234]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m top1: 0.29197761194029853
[2m[36m(func pid=170672)[0m top5: 0.8717350746268657
[2m[36m(func pid=170672)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=170672)[0m f1_macro: 0.1411775888991504
[2m[36m(func pid=170672)[0m f1_weighted: 0.2292820024137649
[2m[36m(func pid=170672)[0m f1_per_class: [0.154, 0.026, 0.256, 0.141, 0.138, 0.047, 0.582, 0.0, 0.0, 0.067]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m top1: 0.23274253731343283
[2m[36m(func pid=170188)[0m top5: 0.8138992537313433
[2m[36m(func pid=170188)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=170188)[0m f1_macro: 0.1473166165573841
[2m[36m(func pid=170188)[0m f1_weighted: 0.26105859702201645
[2m[36m(func pid=170188)[0m f1_per_class: [0.06, 0.325, 0.068, 0.28, 0.0, 0.194, 0.307, 0.148, 0.09, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 11.0299 | Steps: 4 | Val loss: 15.0868 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.2575 | Steps: 4 | Val loss: 3.3683 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 10:29:57 (running for 00:12:27.67)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.675 |      0.147 |                   53 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.196 |      0.141 |                   51 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.236 |      0.243 |                   52 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.03  |      0.154 |                   31 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.4438 | Steps: 4 | Val loss: 2.0795 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=176697)[0m top1: 0.11194029850746269
[2m[36m(func pid=176697)[0m top5: 0.6077425373134329
[2m[36m(func pid=176697)[0m f1_micro: 0.11194029850746269
[2m[36m(func pid=176697)[0m f1_macro: 0.15373800024102352
[2m[36m(func pid=176697)[0m f1_weighted: 0.1163265558270147
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.27, 0.407, 0.118, 0.127, 0.0, 0.0, 0.554, 0.028, 0.032]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.6702 | Steps: 4 | Val loss: 2.2065 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=171228)[0m top1: 0.18097014925373134
[2m[36m(func pid=171228)[0m top5: 0.574160447761194
[2m[36m(func pid=171228)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=171228)[0m f1_macro: 0.1802247038832596
[2m[36m(func pid=171228)[0m f1_weighted: 0.14693863654121808
[2m[36m(func pid=171228)[0m f1_per_class: [0.044, 0.469, 0.153, 0.0, 0.052, 0.239, 0.0, 0.521, 0.131, 0.194]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.24300373134328357
[2m[36m(func pid=170188)[0m top5: 0.820429104477612
[2m[36m(func pid=170188)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=170188)[0m f1_macro: 0.15814493903407298
[2m[36m(func pid=170188)[0m f1_weighted: 0.2696861149414968
[2m[36m(func pid=170188)[0m f1_per_class: [0.067, 0.311, 0.069, 0.31, 0.0, 0.264, 0.291, 0.137, 0.076, 0.057]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m top1: 0.23180970149253732
[2m[36m(func pid=170672)[0m top5: 0.8540111940298507
[2m[36m(func pid=170672)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=170672)[0m f1_macro: 0.12031418695051291
[2m[36m(func pid=170672)[0m f1_weighted: 0.20918361927708615
[2m[36m(func pid=170672)[0m f1_per_class: [0.094, 0.005, 0.096, 0.104, 0.257, 0.008, 0.578, 0.016, 0.0, 0.043]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 11.2188 | Steps: 4 | Val loss: 10.0619 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.2394 | Steps: 4 | Val loss: 2.5054 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.6973 | Steps: 4 | Val loss: 2.2076 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 10:30:03 (running for 00:12:33.15)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.67  |      0.158 |                   54 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.444 |      0.12  |                   52 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.257 |      0.18  |                   53 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.219 |      0.147 |                   32 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.251865671641791
[2m[36m(func pid=176697)[0m top5: 0.8143656716417911
[2m[36m(func pid=176697)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=176697)[0m f1_macro: 0.14672231648632125
[2m[36m(func pid=176697)[0m f1_weighted: 0.21274173876890332
[2m[36m(func pid=176697)[0m f1_per_class: [0.263, 0.256, 0.205, 0.0, 0.036, 0.0, 0.536, 0.0, 0.0, 0.171]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.3789 | Steps: 4 | Val loss: 2.0648 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=171228)[0m top1: 0.18563432835820895
[2m[36m(func pid=171228)[0m top5: 0.7845149253731343
[2m[36m(func pid=171228)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=171228)[0m f1_macro: 0.19368138250671854
[2m[36m(func pid=171228)[0m f1_weighted: 0.19123217041240012
[2m[36m(func pid=171228)[0m f1_per_class: [0.182, 0.403, 0.167, 0.224, 0.035, 0.176, 0.0, 0.483, 0.144, 0.122]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.2294776119402985
[2m[36m(func pid=170188)[0m top5: 0.8120335820895522
[2m[36m(func pid=170188)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=170188)[0m f1_macro: 0.15338828516807376
[2m[36m(func pid=170188)[0m f1_weighted: 0.25719201724360685
[2m[36m(func pid=170188)[0m f1_per_class: [0.062, 0.328, 0.072, 0.27, 0.0, 0.278, 0.273, 0.127, 0.082, 0.041]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m top1: 0.12360074626865672
[2m[36m(func pid=170672)[0m top5: 0.8558768656716418
[2m[36m(func pid=170672)[0m f1_micro: 0.12360074626865672
[2m[36m(func pid=170672)[0m f1_macro: 0.12465380740432286
[2m[36m(func pid=170672)[0m f1_weighted: 0.1553038712524567
[2m[36m(func pid=170672)[0m f1_per_class: [0.165, 0.056, 0.119, 0.119, 0.237, 0.0, 0.317, 0.205, 0.0, 0.028]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 15.1443 | Steps: 4 | Val loss: 13.0527 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.8257 | Steps: 4 | Val loss: 2.1814 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 10:30:08 (running for 00:12:38.27)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.697 |      0.153 |                   55 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.379 |      0.125 |                   53 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.239 |      0.194 |                   54 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 15.144 |      0.195 |                   33 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.25326492537313433
[2m[36m(func pid=176697)[0m top5: 0.761660447761194
[2m[36m(func pid=176697)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=176697)[0m f1_macro: 0.1945500264634931
[2m[36m(func pid=176697)[0m f1_weighted: 0.16279843163443708
[2m[36m(func pid=176697)[0m f1_per_class: [0.085, 0.461, 0.564, 0.146, 0.303, 0.299, 0.0, 0.0, 0.0, 0.086]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.5842 | Steps: 4 | Val loss: 2.1947 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.1570 | Steps: 4 | Val loss: 1.9904 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=171228)[0m top1: 0.35074626865671643
[2m[36m(func pid=171228)[0m top5: 0.8465485074626866
[2m[36m(func pid=171228)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=171228)[0m f1_macro: 0.2820656514150263
[2m[36m(func pid=171228)[0m f1_weighted: 0.33635943316122796
[2m[36m(func pid=171228)[0m f1_per_class: [0.23, 0.255, 0.552, 0.557, 0.187, 0.311, 0.225, 0.412, 0.0, 0.092]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m top1: 0.25
[2m[36m(func pid=170188)[0m top5: 0.8292910447761194
[2m[36m(func pid=170188)[0m f1_micro: 0.25
[2m[36m(func pid=170188)[0m f1_macro: 0.1610301498621896
[2m[36m(func pid=170188)[0m f1_weighted: 0.2758541707340296
[2m[36m(func pid=170188)[0m f1_per_class: [0.076, 0.28, 0.085, 0.323, 0.0, 0.296, 0.308, 0.105, 0.101, 0.036]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 9.4928 | Steps: 4 | Val loss: 7.6469 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=170672)[0m top1: 0.27425373134328357
[2m[36m(func pid=170672)[0m top5: 0.8367537313432836
[2m[36m(func pid=170672)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=170672)[0m f1_macro: 0.21420934495797242
[2m[36m(func pid=170672)[0m f1_weighted: 0.23823026174733142
[2m[36m(func pid=170672)[0m f1_per_class: [0.235, 0.112, 0.312, 0.539, 0.128, 0.276, 0.0, 0.497, 0.0, 0.041]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.9369 | Steps: 4 | Val loss: 1.9130 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 10:30:13 (running for 00:12:43.49)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.584 |      0.161 |                   56 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.157 |      0.214 |                   54 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.826 |      0.282 |                   55 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  9.493 |      0.232 |                   34 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.31669776119402987
[2m[36m(func pid=176697)[0m top5: 0.7472014925373134
[2m[36m(func pid=176697)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=176697)[0m f1_macro: 0.23162447191996782
[2m[36m(func pid=176697)[0m f1_weighted: 0.2610350871178808
[2m[36m(func pid=176697)[0m f1_per_class: [0.29, 0.459, 0.349, 0.537, 0.079, 0.0, 0.0, 0.311, 0.102, 0.189]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.6390 | Steps: 4 | Val loss: 2.1798 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=171228)[0m top1: 0.36473880597014924
[2m[36m(func pid=171228)[0m top5: 0.8432835820895522
[2m[36m(func pid=171228)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=171228)[0m f1_macro: 0.27371232194713907
[2m[36m(func pid=171228)[0m f1_weighted: 0.31634933702030626
[2m[36m(func pid=171228)[0m f1_per_class: [0.202, 0.499, 0.238, 0.035, 0.286, 0.421, 0.471, 0.369, 0.026, 0.19]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.1937 | Steps: 4 | Val loss: 1.9977 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 8.2471 | Steps: 4 | Val loss: 6.6872 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=170188)[0m top1: 0.28125
[2m[36m(func pid=170188)[0m top5: 0.8465485074626866
[2m[36m(func pid=170188)[0m f1_micro: 0.28125
[2m[36m(func pid=170188)[0m f1_macro: 0.1673410535554805
[2m[36m(func pid=170188)[0m f1_weighted: 0.29560806960918307
[2m[36m(func pid=170188)[0m f1_per_class: [0.071, 0.246, 0.093, 0.401, 0.0, 0.295, 0.319, 0.118, 0.102, 0.028]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m top1: 0.32509328358208955
[2m[36m(func pid=170672)[0m top5: 0.7709888059701493
[2m[36m(func pid=170672)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=170672)[0m f1_macro: 0.2487670829553575
[2m[36m(func pid=170672)[0m f1_weighted: 0.26517735998949865
[2m[36m(func pid=170672)[0m f1_per_class: [0.28, 0.21, 0.5, 0.55, 0.082, 0.375, 0.0, 0.382, 0.0, 0.108]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.2758 | Steps: 4 | Val loss: 2.1471 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=176697)[0m top1: 0.25419776119402987
[2m[36m(func pid=176697)[0m top5: 0.8339552238805971
[2m[36m(func pid=176697)[0m f1_micro: 0.25419776119402987
[2m[36m(func pid=176697)[0m f1_macro: 0.26316112969370825
[2m[36m(func pid=176697)[0m f1_weighted: 0.2914237364154792
[2m[36m(func pid=176697)[0m f1_per_class: [0.341, 0.083, 0.609, 0.385, 0.235, 0.128, 0.401, 0.381, 0.027, 0.042]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.6683 | Steps: 4 | Val loss: 2.1742 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 10:30:19 (running for 00:12:49.29)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.639 |      0.167 |                   57 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.194 |      0.249 |                   55 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.276 |      0.228 |                   57 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  8.247 |      0.263 |                   35 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.28218283582089554
[2m[36m(func pid=171228)[0m top5: 0.8339552238805971
[2m[36m(func pid=171228)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=171228)[0m f1_macro: 0.22771469542095518
[2m[36m(func pid=171228)[0m f1_weighted: 0.2365836422346431
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.559, 0.123, 0.156, 0.18, 0.381, 0.057, 0.465, 0.141, 0.214]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.2604 | Steps: 4 | Val loss: 2.0502 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 7.7947 | Steps: 4 | Val loss: 12.6089 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=170188)[0m top1: 0.29244402985074625
[2m[36m(func pid=170188)[0m top5: 0.8568097014925373
[2m[36m(func pid=170188)[0m f1_micro: 0.29244402985074625
[2m[36m(func pid=170188)[0m f1_macro: 0.1696705829670093
[2m[36m(func pid=170188)[0m f1_weighted: 0.30675627973496167
[2m[36m(func pid=170188)[0m f1_per_class: [0.065, 0.265, 0.089, 0.411, 0.0, 0.29, 0.335, 0.148, 0.093, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.5177 | Steps: 4 | Val loss: 1.9264 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=170672)[0m top1: 0.30970149253731344
[2m[36m(func pid=170672)[0m top5: 0.7555970149253731
[2m[36m(func pid=170672)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=170672)[0m f1_macro: 0.239455990457326
[2m[36m(func pid=170672)[0m f1_weighted: 0.26678308554594266
[2m[36m(func pid=170672)[0m f1_per_class: [0.264, 0.254, 0.333, 0.52, 0.084, 0.403, 0.0, 0.36, 0.047, 0.129]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m top1: 0.15764925373134328
[2m[36m(func pid=176697)[0m top5: 0.7611940298507462
[2m[36m(func pid=176697)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=176697)[0m f1_macro: 0.13441169303225628
[2m[36m(func pid=176697)[0m f1_weighted: 0.1604009272457142
[2m[36m(func pid=176697)[0m f1_per_class: [0.145, 0.371, 0.104, 0.235, 0.028, 0.008, 0.0, 0.454, 0.0, 0.0]
[2m[36m(func pid=176697)[0m 
== Status ==
Current time: 2024-01-07 10:30:24 (running for 00:12:54.47)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.668 |      0.17  |                   58 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.26  |      0.239 |                   56 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.518 |      0.272 |                   58 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  7.795 |      0.134 |                   36 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.6896 | Steps: 4 | Val loss: 2.1737 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=171228)[0m top1: 0.3162313432835821
[2m[36m(func pid=171228)[0m top5: 0.8838619402985075
[2m[36m(func pid=171228)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=171228)[0m f1_macro: 0.27243510763332696
[2m[36m(func pid=171228)[0m f1_weighted: 0.31051494130336504
[2m[36m(func pid=171228)[0m f1_per_class: [0.288, 0.078, 0.533, 0.536, 0.043, 0.296, 0.245, 0.497, 0.0, 0.208]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.1502 | Steps: 4 | Val loss: 2.0924 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 15.0924 | Steps: 4 | Val loss: 8.8814 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=170188)[0m top1: 0.2887126865671642
[2m[36m(func pid=170188)[0m top5: 0.8479477611940298
[2m[36m(func pid=170188)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=170188)[0m f1_macro: 0.15275864276955903
[2m[36m(func pid=170188)[0m f1_weighted: 0.2854705289175687
[2m[36m(func pid=170188)[0m f1_per_class: [0.048, 0.198, 0.09, 0.423, 0.0, 0.292, 0.301, 0.115, 0.06, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.0272 | Steps: 4 | Val loss: 2.0824 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=170672)[0m top1: 0.2416044776119403
[2m[36m(func pid=170672)[0m top5: 0.7835820895522388
[2m[36m(func pid=170672)[0m f1_micro: 0.2416044776119403
[2m[36m(func pid=170672)[0m f1_macro: 0.199680708882634
[2m[36m(func pid=170672)[0m f1_weighted: 0.21171090759609165
[2m[36m(func pid=170672)[0m f1_per_class: [0.174, 0.245, 0.154, 0.333, 0.088, 0.42, 0.0, 0.297, 0.136, 0.149]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m top1: 0.27658582089552236
[2m[36m(func pid=176697)[0m top5: 0.8073694029850746
[2m[36m(func pid=176697)[0m f1_micro: 0.27658582089552236
[2m[36m(func pid=176697)[0m f1_macro: 0.19784613099155793
[2m[36m(func pid=176697)[0m f1_weighted: 0.3229072655008226
[2m[36m(func pid=176697)[0m f1_per_class: [0.146, 0.454, 0.092, 0.241, 0.05, 0.0, 0.498, 0.358, 0.14, 0.0]
[2m[36m(func pid=176697)[0m 
== Status ==
Current time: 2024-01-07 10:30:29 (running for 00:12:59.48)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.69  |      0.153 |                   59 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.15  |      0.2   |                   57 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.027 |      0.261 |                   59 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 15.092 |      0.198 |                   37 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.27098880597014924
[2m[36m(func pid=171228)[0m top5: 0.8563432835820896
[2m[36m(func pid=171228)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=171228)[0m f1_macro: 0.26105044327836324
[2m[36m(func pid=171228)[0m f1_weighted: 0.29989961847786945
[2m[36m(func pid=171228)[0m f1_per_class: [0.291, 0.374, 0.556, 0.095, 0.035, 0.194, 0.504, 0.442, 0.0, 0.12]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.6169 | Steps: 4 | Val loss: 2.1666 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.2463 | Steps: 4 | Val loss: 2.1319 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 9.8970 | Steps: 4 | Val loss: 15.2436 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=170188)[0m top1: 0.271455223880597
[2m[36m(func pid=170188)[0m top5: 0.8418843283582089
[2m[36m(func pid=170188)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=170188)[0m f1_macro: 0.12947807820288756
[2m[36m(func pid=170188)[0m f1_weighted: 0.23687100029652786
[2m[36m(func pid=170188)[0m f1_per_class: [0.059, 0.138, 0.128, 0.415, 0.0, 0.262, 0.202, 0.09, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.6059 | Steps: 4 | Val loss: 2.9129 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=170672)[0m top1: 0.1935634328358209
[2m[36m(func pid=170672)[0m top5: 0.7639925373134329
[2m[36m(func pid=170672)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=170672)[0m f1_macro: 0.15746855882420907
[2m[36m(func pid=170672)[0m f1_weighted: 0.12403524760685845
[2m[36m(func pid=170672)[0m f1_per_class: [0.066, 0.274, 0.117, 0.016, 0.078, 0.379, 0.0, 0.352, 0.12, 0.172]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m top1: 0.0960820895522388
[2m[36m(func pid=176697)[0m top5: 0.6385261194029851
[2m[36m(func pid=176697)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=176697)[0m f1_macro: 0.08006993659209116
[2m[36m(func pid=176697)[0m f1_weighted: 0.0709843449747352
[2m[36m(func pid=176697)[0m f1_per_class: [0.133, 0.0, 0.059, 0.125, 0.083, 0.0, 0.048, 0.287, 0.026, 0.039]
[2m[36m(func pid=176697)[0m 
== Status ==
Current time: 2024-01-07 10:30:35 (running for 00:13:04.85)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.617 |      0.129 |                   60 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.246 |      0.157 |                   58 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.606 |      0.171 |                   60 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  9.897 |      0.08  |                   38 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)

[2m[36m(func pid=171228)[0m top1: 0.2080223880597015

[2m[36m(func pid=171228)[0m top5: 0.7229477611940298
[2m[36m(func pid=171228)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=171228)[0m f1_macro: 0.1710475667186724
[2m[36m(func pid=171228)[0m f1_weighted: 0.1470347003900914
[2m[36m(func pid=171228)[0m f1_per_class: [0.107, 0.432, 0.086, 0.007, 0.079, 0.311, 0.0, 0.436, 0.176, 0.077]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.6506 | Steps: 4 | Val loss: 2.1666 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.1910 | Steps: 4 | Val loss: 2.1434 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 13.7240 | Steps: 4 | Val loss: 15.9234 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=170188)[0m top1: 0.26632462686567165
[2m[36m(func pid=170188)[0m top5: 0.8414179104477612
[2m[36m(func pid=170188)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=170188)[0m f1_macro: 0.129252221860065
[2m[36m(func pid=170188)[0m f1_weighted: 0.22300526914076813
[2m[36m(func pid=170188)[0m f1_per_class: [0.076, 0.125, 0.132, 0.414, 0.0, 0.257, 0.157, 0.132, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3330 | Steps: 4 | Val loss: 3.2080 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=170672)[0m top1: 0.2224813432835821
[2m[36m(func pid=170672)[0m top5: 0.7136194029850746
[2m[36m(func pid=170672)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=170672)[0m f1_macro: 0.19463306247632628
[2m[36m(func pid=170672)[0m f1_weighted: 0.1426930008998343
[2m[36m(func pid=170672)[0m f1_per_class: [0.182, 0.352, 0.195, 0.0, 0.122, 0.389, 0.0, 0.441, 0.15, 0.115]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m top1: 0.23367537313432835
[2m[36m(func pid=176697)[0m top5: 0.5331156716417911
[2m[36m(func pid=176697)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=176697)[0m f1_macro: 0.16317305366819035
[2m[36m(func pid=176697)[0m f1_weighted: 0.18464706269553866
[2m[36m(func pid=176697)[0m f1_per_class: [0.116, 0.0, 0.278, 0.0, 0.176, 0.252, 0.461, 0.17, 0.048, 0.13]
[2m[36m(func pid=176697)[0m 
== Status ==
Current time: 2024-01-07 10:30:40 (running for 00:13:10.15)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.651 |      0.129 |                   61 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.191 |      0.195 |                   59 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.333 |      0.165 |                   61 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 13.724 |      0.163 |                   39 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.14085820895522388
[2m[36m(func pid=171228)[0m top5: 0.7122201492537313
[2m[36m(func pid=171228)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=171228)[0m f1_macro: 0.16503793588895066
[2m[36m(func pid=171228)[0m f1_weighted: 0.12548137789615701
[2m[36m(func pid=171228)[0m f1_per_class: [0.154, 0.0, 0.37, 0.226, 0.037, 0.27, 0.0, 0.378, 0.078, 0.138]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.7380 | Steps: 4 | Val loss: 2.1749 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 20.5416 | Steps: 4 | Val loss: 13.4150 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.2374 | Steps: 4 | Val loss: 2.1270 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=170188)[0m top1: 0.24813432835820895
[2m[36m(func pid=170188)[0m top5: 0.8176305970149254
[2m[36m(func pid=170188)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=170188)[0m f1_macro: 0.13026584903809044
[2m[36m(func pid=170188)[0m f1_weighted: 0.20102425502987095
[2m[36m(func pid=170188)[0m f1_per_class: [0.112, 0.085, 0.167, 0.391, 0.0, 0.252, 0.117, 0.179, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 3.0506 | Steps: 4 | Val loss: 1.8154 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=176697)[0m top1: 0.11520522388059702
[2m[36m(func pid=176697)[0m top5: 0.7052238805970149
[2m[36m(func pid=176697)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=176697)[0m f1_macro: 0.15771130575821216
[2m[36m(func pid=176697)[0m f1_weighted: 0.11869226226310746
[2m[36m(func pid=176697)[0m f1_per_class: [0.127, 0.075, 0.267, 0.0, 0.027, 0.164, 0.167, 0.484, 0.071, 0.194]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.23647388059701493
[2m[36m(func pid=170672)[0m top5: 0.7350746268656716
[2m[36m(func pid=170672)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=170672)[0m f1_macro: 0.2440179839375271
[2m[36m(func pid=170672)[0m f1_weighted: 0.15395952792179282
[2m[36m(func pid=170672)[0m f1_per_class: [0.301, 0.393, 0.465, 0.0, 0.178, 0.367, 0.0, 0.488, 0.14, 0.108]
[2m[36m(func pid=170672)[0m 
== Status ==
Current time: 2024-01-07 10:30:45 (running for 00:13:15.52)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.738 |      0.13  |                   62 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.237 |      0.244 |                   60 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  3.051 |      0.303 |                   62 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 20.542 |      0.158 |                   40 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.4398320895522388
[2m[36m(func pid=171228)[0m top5: 0.8675373134328358
[2m[36m(func pid=171228)[0m f1_micro: 0.4398320895522388
[2m[36m(func pid=171228)[0m f1_macro: 0.3033938985523928
[2m[36m(func pid=171228)[0m f1_weighted: 0.39031415297508193
[2m[36m(func pid=171228)[0m f1_per_class: [0.292, 0.037, 0.645, 0.574, 0.324, 0.417, 0.537, 0.0, 0.061, 0.147]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.6414 | Steps: 4 | Val loss: 2.1857 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 17.2085 | Steps: 4 | Val loss: 14.2220 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.1675 | Steps: 4 | Val loss: 2.0564 | Batch size: 32 | lr: 0.001 | Duration: 3.22s
[2m[36m(func pid=170188)[0m top1: 0.23600746268656717
[2m[36m(func pid=170188)[0m top5: 0.8031716417910447
[2m[36m(func pid=170188)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=170188)[0m f1_macro: 0.13466818690576604
[2m[36m(func pid=170188)[0m f1_weighted: 0.19471089058779847
[2m[36m(func pid=170188)[0m f1_per_class: [0.097, 0.084, 0.19, 0.379, 0.0, 0.252, 0.095, 0.25, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.0086 | Steps: 4 | Val loss: 1.9890 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=176697)[0m top1: 0.353544776119403
[2m[36m(func pid=176697)[0m top5: 0.6693097014925373
[2m[36m(func pid=176697)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=176697)[0m f1_macro: 0.19545448299726936
[2m[36m(func pid=176697)[0m f1_weighted: 0.27949160496781744
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.497, 0.207, 0.528, 0.186, 0.314, 0.0, 0.121, 0.0, 0.101]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.2294776119402985
[2m[36m(func pid=170672)[0m top5: 0.8283582089552238
[2m[36m(func pid=170672)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=170672)[0m f1_macro: 0.22707282044974816
[2m[36m(func pid=170672)[0m f1_weighted: 0.14983423844213978
[2m[36m(func pid=170672)[0m f1_per_class: [0.19, 0.379, 0.526, 0.0, 0.163, 0.352, 0.022, 0.437, 0.093, 0.108]
[2m[36m(func pid=170672)[0m 
== Status ==
Current time: 2024-01-07 10:30:51 (running for 00:13:20.81)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.641 |      0.135 |                   63 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.167 |      0.227 |                   61 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.009 |      0.22  |                   63 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 17.208 |      0.195 |                   41 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.30830223880597013
[2m[36m(func pid=171228)[0m top5: 0.8460820895522388
[2m[36m(func pid=171228)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=171228)[0m f1_macro: 0.21976641764566868
[2m[36m(func pid=171228)[0m f1_weighted: 0.25904902591651224
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.461, 0.25, 0.21, 0.0, 0.346, 0.148, 0.497, 0.179, 0.107]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6031 | Steps: 4 | Val loss: 2.1895 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 19.0730 | Steps: 4 | Val loss: 18.1627 | Batch size: 32 | lr: 0.1 | Duration: 2.68s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.3909 | Steps: 4 | Val loss: 1.9811 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=170188)[0m top1: 0.25652985074626866
[2m[36m(func pid=170188)[0m top5: 0.7868470149253731
[2m[36m(func pid=170188)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=170188)[0m f1_macro: 0.1700030541770792
[2m[36m(func pid=170188)[0m f1_weighted: 0.22472224612867028
[2m[36m(func pid=170188)[0m f1_per_class: [0.106, 0.135, 0.25, 0.384, 0.0, 0.286, 0.111, 0.427, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.1429 | Steps: 4 | Val loss: 2.2292 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=176697)[0m top1: 0.11753731343283583
[2m[36m(func pid=176697)[0m top5: 0.6674440298507462
[2m[36m(func pid=176697)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=176697)[0m f1_macro: 0.14342650354346867
[2m[36m(func pid=176697)[0m f1_weighted: 0.08346445037188735
[2m[36m(func pid=176697)[0m f1_per_class: [0.078, 0.0, 0.471, 0.164, 0.205, 0.107, 0.0, 0.317, 0.0, 0.093]
[2m[36m(func pid=176697)[0m 
== Status ==
Current time: 2024-01-07 10:30:56 (running for 00:13:25.98)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.603 |      0.17  |                   64 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.167 |      0.227 |                   61 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.143 |      0.244 |                   64 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 19.073 |      0.143 |                   42 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170672)[0m top1: 0.269589552238806
[2m[36m(func pid=170672)[0m top5: 0.8535447761194029
[2m[36m(func pid=170672)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=170672)[0m f1_macro: 0.22625012939287795
[2m[36m(func pid=170672)[0m f1_weighted: 0.23364126883997569
[2m[36m(func pid=170672)[0m f1_per_class: [0.228, 0.376, 0.345, 0.112, 0.133, 0.362, 0.22, 0.359, 0.026, 0.101]
[2m[36m(func pid=171228)[0m top1: 0.36613805970149255
[2m[36m(func pid=171228)[0m top5: 0.8022388059701493
[2m[36m(func pid=171228)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=171228)[0m f1_macro: 0.24440787271153389
[2m[36m(func pid=171228)[0m f1_weighted: 0.31817188559642023
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.569, 0.111, 0.516, 0.209, 0.427, 0.0, 0.335, 0.13, 0.148]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6821 | Steps: 4 | Val loss: 2.1850 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 18.0103 | Steps: 4 | Val loss: 9.9731 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 3.1888 | Steps: 4 | Val loss: 1.8198 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=170188)[0m top1: 0.2658582089552239
[2m[36m(func pid=170188)[0m top5: 0.7803171641791045
[2m[36m(func pid=170188)[0m f1_micro: 0.2658582089552239
[2m[36m(func pid=170188)[0m f1_macro: 0.17339541901671382
[2m[36m(func pid=170188)[0m f1_weighted: 0.23375205946255098
[2m[36m(func pid=170188)[0m f1_per_class: [0.111, 0.207, 0.25, 0.402, 0.0, 0.301, 0.088, 0.376, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.1372 | Steps: 4 | Val loss: 1.9929 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=176697)[0m top1: 0.3530783582089552
[2m[36m(func pid=176697)[0m top5: 0.777518656716418
[2m[36m(func pid=176697)[0m f1_micro: 0.3530783582089552
[2m[36m(func pid=176697)[0m f1_macro: 0.17123698545229665
[2m[36m(func pid=176697)[0m f1_weighted: 0.31076694546499134
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.0, 0.0, 0.39, 0.184, 0.288, 0.517, 0.201, 0.027, 0.105]
[2m[36m(func pid=176697)[0m 
== Status ==
Current time: 2024-01-07 10:31:01 (running for 00:13:31.22)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.682 |      0.173 |                   65 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.391 |      0.226 |                   62 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  3.189 |      0.302 |                   65 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 18.01  |      0.171 |                   43 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.3512126865671642
[2m[36m(func pid=171228)[0m top5: 0.894589552238806
[2m[36m(func pid=171228)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=171228)[0m f1_macro: 0.30245040866696904
[2m[36m(func pid=171228)[0m f1_weighted: 0.3959313704498027
[2m[36m(func pid=171228)[0m f1_per_class: [0.325, 0.18, 0.444, 0.494, 0.044, 0.254, 0.528, 0.414, 0.154, 0.188]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m top1: 0.2947761194029851
[2m[36m(func pid=170672)[0m top5: 0.8661380597014925
[2m[36m(func pid=170672)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=170672)[0m f1_macro: 0.21533608699487966
[2m[36m(func pid=170672)[0m f1_weighted: 0.30886487490969194
[2m[36m(func pid=170672)[0m f1_per_class: [0.102, 0.316, 0.097, 0.381, 0.111, 0.35, 0.276, 0.299, 0.112, 0.109]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.6279 | Steps: 4 | Val loss: 2.1833 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 10.6860 | Steps: 4 | Val loss: 6.6367 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=170188)[0m top1: 0.2737873134328358
[2m[36m(func pid=170188)[0m top5: 0.7761194029850746
[2m[36m(func pid=170188)[0m f1_micro: 0.2737873134328358
[2m[36m(func pid=170188)[0m f1_macro: 0.17446711789636168
[2m[36m(func pid=170188)[0m f1_weighted: 0.2393518998016542
[2m[36m(func pid=170188)[0m f1_per_class: [0.118, 0.242, 0.242, 0.424, 0.0, 0.29, 0.073, 0.356, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.7832 | Steps: 4 | Val loss: 2.1765 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.2310 | Steps: 4 | Val loss: 2.0258 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=176697)[0m top1: 0.3656716417910448
[2m[36m(func pid=176697)[0m top5: 0.8731343283582089
[2m[36m(func pid=176697)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=176697)[0m f1_macro: 0.28599746802304493
[2m[36m(func pid=176697)[0m f1_weighted: 0.34780363079602883
[2m[36m(func pid=176697)[0m f1_per_class: [0.344, 0.451, 0.44, 0.559, 0.066, 0.328, 0.144, 0.38, 0.0, 0.148]
[2m[36m(func pid=176697)[0m 
== Status ==
Current time: 2024-01-07 10:31:07 (running for 00:13:36.89)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.628 |      0.174 |                   66 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.137 |      0.215 |                   63 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.783 |      0.235 |                   66 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 10.686 |      0.286 |                   44 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.23787313432835822
[2m[36m(func pid=171228)[0m top5: 0.8460820895522388
[2m[36m(func pid=171228)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=171228)[0m f1_macro: 0.23545394042643086
[2m[36m(func pid=171228)[0m f1_weighted: 0.256541922856677
[2m[36m(func pid=171228)[0m f1_per_class: [0.287, 0.359, 0.471, 0.148, 0.184, 0.327, 0.312, 0.214, 0.0, 0.052]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170672)[0m top1: 0.28638059701492535
[2m[36m(func pid=170672)[0m top5: 0.8656716417910447
[2m[36m(func pid=170672)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=170672)[0m f1_macro: 0.22241893392241466
[2m[36m(func pid=170672)[0m f1_weighted: 0.305817688550424
[2m[36m(func pid=170672)[0m f1_per_class: [0.091, 0.149, 0.098, 0.435, 0.072, 0.363, 0.27, 0.475, 0.151, 0.119]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.7305 | Steps: 4 | Val loss: 2.1991 | Batch size: 32 | lr: 0.0001 | Duration: 3.07s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 11.8951 | Steps: 4 | Val loss: 15.5166 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=170188)[0m top1: 0.25279850746268656
[2m[36m(func pid=170188)[0m top5: 0.7425373134328358
[2m[36m(func pid=170188)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=170188)[0m f1_macro: 0.15576487012660617
[2m[36m(func pid=170188)[0m f1_weighted: 0.22101813472447795
[2m[36m(func pid=170188)[0m f1_per_class: [0.097, 0.335, 0.156, 0.379, 0.0, 0.194, 0.039, 0.357, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.6264 | Steps: 4 | Val loss: 1.7633 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.1162 | Steps: 4 | Val loss: 2.0682 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=176697)[0m top1: 0.2140858208955224
[2m[36m(func pid=176697)[0m top5: 0.7915111940298507
[2m[36m(func pid=176697)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=176697)[0m f1_macro: 0.10119706453680999
[2m[36m(func pid=176697)[0m f1_weighted: 0.1691080985481332
[2m[36m(func pid=176697)[0m f1_per_class: [0.145, 0.407, 0.066, 0.04, 0.0, 0.0, 0.273, 0.047, 0.0, 0.034]
[2m[36m(func pid=176697)[0m 
== Status ==
Current time: 2024-01-07 10:31:12 (running for 00:13:42.19)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.731 |      0.156 |                   67 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.231 |      0.222 |                   64 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.626 |      0.328 |                   67 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.895 |      0.101 |                   45 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.4099813432835821
[2m[36m(func pid=171228)[0m top5: 0.8596082089552238
[2m[36m(func pid=171228)[0m f1_micro: 0.4099813432835821
[2m[36m(func pid=171228)[0m f1_macro: 0.32773260266647936
[2m[36m(func pid=171228)[0m f1_weighted: 0.37382481003168033
[2m[36m(func pid=171228)[0m f1_per_class: [0.044, 0.529, 0.714, 0.553, 0.302, 0.348, 0.186, 0.407, 0.0, 0.194]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.5936 | Steps: 4 | Val loss: 2.1901 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=170672)[0m top1: 0.22154850746268656
[2m[36m(func pid=170672)[0m top5: 0.8540111940298507
[2m[36m(func pid=170672)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=170672)[0m f1_macro: 0.22280149292597518
[2m[36m(func pid=170672)[0m f1_weighted: 0.22583234679999784
[2m[36m(func pid=170672)[0m f1_per_class: [0.156, 0.047, 0.5, 0.377, 0.051, 0.305, 0.128, 0.5, 0.092, 0.071]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 18.6676 | Steps: 4 | Val loss: 13.5370 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=170188)[0m top1: 0.25886194029850745
[2m[36m(func pid=170188)[0m top5: 0.75
[2m[36m(func pid=170188)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=170188)[0m f1_macro: 0.1540988317080502
[2m[36m(func pid=170188)[0m f1_weighted: 0.21454257421083175
[2m[36m(func pid=170188)[0m f1_per_class: [0.095, 0.377, 0.15, 0.339, 0.0, 0.162, 0.039, 0.377, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.6567 | Steps: 4 | Val loss: 2.0833 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=176697)[0m top1: 0.33908582089552236
[2m[36m(func pid=176697)[0m top5: 0.8325559701492538
[2m[36m(func pid=176697)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=176697)[0m f1_macro: 0.18592770551403687
[2m[36m(func pid=176697)[0m f1_weighted: 0.24787798433790123
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.364, 0.355, 0.481, 0.146, 0.178, 0.069, 0.078, 0.0, 0.188]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.4392 | Steps: 4 | Val loss: 2.1102 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 10:31:17 (running for 00:13:47.41)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.594 |      0.154 |                   68 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.116 |      0.223 |                   65 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.657 |      0.212 |                   68 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 18.668 |      0.186 |                   46 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.30177238805970147
[2m[36m(func pid=171228)[0m top5: 0.8306902985074627
[2m[36m(func pid=171228)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=171228)[0m f1_macro: 0.2115569853867348
[2m[36m(func pid=171228)[0m f1_weighted: 0.30522412584304004
[2m[36m(func pid=171228)[0m f1_per_class: [0.159, 0.475, 0.154, 0.075, 0.094, 0.063, 0.579, 0.213, 0.103, 0.202]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.6088 | Steps: 4 | Val loss: 2.1908 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=170672)[0m top1: 0.1828358208955224
[2m[36m(func pid=170672)[0m top5: 0.832089552238806
[2m[36m(func pid=170672)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=170672)[0m f1_macro: 0.18059240049224343
[2m[36m(func pid=170672)[0m f1_weighted: 0.16148397633337602
[2m[36m(func pid=170672)[0m f1_per_class: [0.13, 0.067, 0.353, 0.294, 0.056, 0.299, 0.0, 0.447, 0.041, 0.119]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 15.2764 | Steps: 4 | Val loss: 14.4959 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=170188)[0m top1: 0.24860074626865672
[2m[36m(func pid=170188)[0m top5: 0.7560634328358209
[2m[36m(func pid=170188)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=170188)[0m f1_macro: 0.14329490995125
[2m[36m(func pid=170188)[0m f1_weighted: 0.19264356690107792
[2m[36m(func pid=170188)[0m f1_per_class: [0.094, 0.401, 0.14, 0.261, 0.0, 0.133, 0.039, 0.366, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.7579 | Steps: 4 | Val loss: 2.5719 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=176697)[0m top1: 0.15858208955223882
[2m[36m(func pid=176697)[0m top5: 0.6422574626865671
[2m[36m(func pid=176697)[0m f1_micro: 0.15858208955223882
[2m[36m(func pid=176697)[0m f1_macro: 0.19852634613531833
[2m[36m(func pid=176697)[0m f1_weighted: 0.13872423400519698
[2m[36m(func pid=176697)[0m f1_per_class: [0.311, 0.375, 0.545, 0.0, 0.099, 0.0, 0.116, 0.463, 0.076, 0.0]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.2750 | Steps: 4 | Val loss: 2.0986 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 10:31:23 (running for 00:13:52.88)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.609 |      0.143 |                   69 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.439 |      0.181 |                   66 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.758 |      0.246 |                   69 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 15.276 |      0.199 |                   47 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.29011194029850745
[2m[36m(func pid=171228)[0m top5: 0.6693097014925373
[2m[36m(func pid=171228)[0m f1_micro: 0.29011194029850745
[2m[36m(func pid=171228)[0m f1_macro: 0.24626384912937788
[2m[36m(func pid=171228)[0m f1_weighted: 0.3049200262607206
[2m[36m(func pid=171228)[0m f1_per_class: [0.153, 0.409, 0.333, 0.0, 0.073, 0.294, 0.572, 0.342, 0.071, 0.214]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.6239 | Steps: 4 | Val loss: 2.1951 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 14.2460 | Steps: 4 | Val loss: 13.2912 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=170672)[0m top1: 0.20149253731343283
[2m[36m(func pid=170672)[0m top5: 0.784981343283582
[2m[36m(func pid=170672)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=170672)[0m f1_macro: 0.18345968641383412
[2m[36m(func pid=170672)[0m f1_weighted: 0.17838078374355362
[2m[36m(func pid=170672)[0m f1_per_class: [0.19, 0.056, 0.4, 0.367, 0.061, 0.359, 0.0, 0.286, 0.021, 0.093]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m top1: 0.2583955223880597
[2m[36m(func pid=176697)[0m top5: 0.6361940298507462
[2m[36m(func pid=176697)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=176697)[0m f1_macro: 0.17870731301985954
[2m[36m(func pid=176697)[0m f1_weighted: 0.2266132430634623
[2m[36m(func pid=176697)[0m f1_per_class: [0.276, 0.288, 0.308, 0.0, 0.046, 0.0, 0.513, 0.263, 0.0, 0.093]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m top1: 0.2355410447761194
[2m[36m(func pid=170188)[0m top5: 0.746268656716418
[2m[36m(func pid=170188)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=170188)[0m f1_macro: 0.13547471936930613
[2m[36m(func pid=170188)[0m f1_weighted: 0.17219369139878407
[2m[36m(func pid=170188)[0m f1_per_class: [0.093, 0.375, 0.133, 0.201, 0.0, 0.131, 0.04, 0.382, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.1907 | Steps: 4 | Val loss: 2.1868 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.1654 | Steps: 4 | Val loss: 2.1400 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 10:31:28 (running for 00:13:58.20)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.624 |      0.135 |                   70 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.275 |      0.183 |                   67 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.191 |      0.255 |                   70 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 14.246 |      0.179 |                   48 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.27052238805970147
[2m[36m(func pid=171228)[0m top5: 0.7756529850746269
[2m[36m(func pid=171228)[0m f1_micro: 0.27052238805970147
[2m[36m(func pid=171228)[0m f1_macro: 0.2549255474166105
[2m[36m(func pid=171228)[0m f1_weighted: 0.16762645593306036
[2m[36m(func pid=171228)[0m f1_per_class: [0.238, 0.52, 0.615, 0.0, 0.217, 0.412, 0.003, 0.26, 0.069, 0.214]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 11.7300 | Steps: 4 | Val loss: 11.2479 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.6348 | Steps: 4 | Val loss: 2.2050 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=170672)[0m top1: 0.13386194029850745
[2m[36m(func pid=170672)[0m top5: 0.7332089552238806
[2m[36m(func pid=170672)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=170672)[0m f1_macro: 0.12687699738968744
[2m[36m(func pid=170672)[0m f1_weighted: 0.11055334832116162
[2m[36m(func pid=170672)[0m f1_per_class: [0.184, 0.232, 0.171, 0.09, 0.066, 0.226, 0.0, 0.226, 0.022, 0.05]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m top1: 0.269589552238806
[2m[36m(func pid=176697)[0m top5: 0.7625932835820896
[2m[36m(func pid=176697)[0m f1_micro: 0.269589552238806
[2m[36m(func pid=176697)[0m f1_macro: 0.20540066088229203
[2m[36m(func pid=176697)[0m f1_weighted: 0.2519239876457335
[2m[36m(func pid=176697)[0m f1_per_class: [0.165, 0.549, 0.121, 0.419, 0.173, 0.107, 0.006, 0.297, 0.068, 0.149]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m top1: 0.2126865671641791
[2m[36m(func pid=170188)[0m top5: 0.7388059701492538
[2m[36m(func pid=170188)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=170188)[0m f1_macro: 0.11868393397826571
[2m[36m(func pid=170188)[0m f1_weighted: 0.1424730764103226
[2m[36m(func pid=170188)[0m f1_per_class: [0.08, 0.37, 0.107, 0.11, 0.0, 0.099, 0.042, 0.379, 0.0, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.8745 | Steps: 4 | Val loss: 2.0745 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.1972 | Steps: 4 | Val loss: 2.1861 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 10:31:33 (running for 00:14:03.41)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.635 |      0.119 |                   71 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.165 |      0.127 |                   68 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.875 |      0.278 |                   71 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.73  |      0.205 |                   49 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.34841417910447764
[2m[36m(func pid=171228)[0m top5: 0.8344216417910447
[2m[36m(func pid=171228)[0m f1_micro: 0.34841417910447764
[2m[36m(func pid=171228)[0m f1_macro: 0.27814244114399583
[2m[36m(func pid=171228)[0m f1_weighted: 0.3032406104812855
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.353, 0.69, 0.578, 0.255, 0.271, 0.046, 0.513, 0.0, 0.076]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 15.7199 | Steps: 4 | Val loss: 13.3010 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.6180 | Steps: 4 | Val loss: 2.1960 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=170672)[0m top1: 0.11147388059701492
[2m[36m(func pid=170672)[0m top5: 0.6968283582089553
[2m[36m(func pid=170672)[0m f1_micro: 0.11147388059701491
[2m[36m(func pid=170672)[0m f1_macro: 0.10142982247258896
[2m[36m(func pid=170672)[0m f1_weighted: 0.07837801139068448
[2m[36m(func pid=170672)[0m f1_per_class: [0.0, 0.161, 0.206, 0.007, 0.071, 0.281, 0.0, 0.251, 0.0, 0.038]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m top1: 0.2574626865671642
[2m[36m(func pid=176697)[0m top5: 0.6576492537313433
[2m[36m(func pid=176697)[0m f1_micro: 0.2574626865671642
[2m[36m(func pid=176697)[0m f1_macro: 0.2585058350972183
[2m[36m(func pid=176697)[0m f1_weighted: 0.23192931269598197
[2m[36m(func pid=176697)[0m f1_per_class: [0.276, 0.539, 0.643, 0.357, 0.051, 0.0, 0.0, 0.415, 0.098, 0.207]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.6762 | Steps: 4 | Val loss: 2.4736 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=170188)[0m top1: 0.2234141791044776
[2m[36m(func pid=170188)[0m top5: 0.7555970149253731
[2m[36m(func pid=170188)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=170188)[0m f1_macro: 0.1307907445526046
[2m[36m(func pid=170188)[0m f1_weighted: 0.1605374717529846
[2m[36m(func pid=170188)[0m f1_per_class: [0.096, 0.373, 0.133, 0.141, 0.0, 0.147, 0.057, 0.346, 0.015, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.1922 | Steps: 4 | Val loss: 2.1508 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 10:31:38 (running for 00:14:08.61)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.618 |      0.131 |                   72 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.197 |      0.101 |                   69 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.676 |      0.168 |                   72 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 15.72  |      0.259 |                   50 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.261660447761194
[2m[36m(func pid=171228)[0m top5: 0.8041044776119403
[2m[36m(func pid=171228)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=171228)[0m f1_macro: 0.16832630113228692
[2m[36m(func pid=171228)[0m f1_weighted: 0.22966279047638197
[2m[36m(func pid=171228)[0m f1_per_class: [0.215, 0.093, 0.094, 0.51, 0.097, 0.286, 0.085, 0.092, 0.0, 0.211]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 15.8938 | Steps: 4 | Val loss: 13.4446 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.5929 | Steps: 4 | Val loss: 2.1984 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=170672)[0m top1: 0.1553171641791045
[2m[36m(func pid=170672)[0m top5: 0.6725746268656716
[2m[36m(func pid=170672)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=170672)[0m f1_macro: 0.14148798389901732
[2m[36m(func pid=170672)[0m f1_weighted: 0.10909371449991905
[2m[36m(func pid=170672)[0m f1_per_class: [0.044, 0.251, 0.275, 0.0, 0.057, 0.368, 0.0, 0.327, 0.041, 0.052]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.5259 | Steps: 4 | Val loss: 2.6753 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=176697)[0m top1: 0.09421641791044776
[2m[36m(func pid=176697)[0m top5: 0.8180970149253731
[2m[36m(func pid=176697)[0m f1_micro: 0.09421641791044776
[2m[36m(func pid=176697)[0m f1_macro: 0.14655518591639494
[2m[36m(func pid=176697)[0m f1_weighted: 0.06366048348426964
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.12, 0.632, 0.057, 0.308, 0.056, 0.0, 0.23, 0.027, 0.036]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m top1: 0.22434701492537312
[2m[36m(func pid=170188)[0m top5: 0.7742537313432836
[2m[36m(func pid=170188)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=170188)[0m f1_macro: 0.13004776564167864
[2m[36m(func pid=170188)[0m f1_weighted: 0.1692102846980578
[2m[36m(func pid=170188)[0m f1_per_class: [0.085, 0.381, 0.118, 0.139, 0.0, 0.108, 0.098, 0.344, 0.026, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.1558 | Steps: 4 | Val loss: 2.1076 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 10:31:44 (running for 00:14:13.72)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.593 |      0.13  |                   73 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.192 |      0.141 |                   70 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.526 |      0.216 |                   73 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 15.894 |      0.147 |                   51 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.26632462686567165
[2m[36m(func pid=171228)[0m top5: 0.6870335820895522
[2m[36m(func pid=171228)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=171228)[0m f1_macro: 0.21633856560015263
[2m[36m(func pid=171228)[0m f1_weighted: 0.19495206867273593
[2m[36m(func pid=171228)[0m f1_per_class: [0.237, 0.503, 0.407, 0.0, 0.131, 0.295, 0.19, 0.032, 0.161, 0.207]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 15.4205 | Steps: 4 | Val loss: 13.7096 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.5769 | Steps: 4 | Val loss: 2.1954 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=170672)[0m top1: 0.1837686567164179
[2m[36m(func pid=170672)[0m top5: 0.6977611940298507
[2m[36m(func pid=170672)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=170672)[0m f1_macro: 0.18585263589872375
[2m[36m(func pid=170672)[0m f1_weighted: 0.11993177639888103
[2m[36m(func pid=170672)[0m f1_per_class: [0.304, 0.291, 0.408, 0.0, 0.057, 0.332, 0.0, 0.364, 0.023, 0.079]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.3403 | Steps: 4 | Val loss: 2.2419 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=176697)[0m top1: 0.3666044776119403
[2m[36m(func pid=176697)[0m top5: 0.8577425373134329
[2m[36m(func pid=176697)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=176697)[0m f1_macro: 0.1877207383400824
[2m[36m(func pid=176697)[0m f1_weighted: 0.2975097603851814
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.491, 0.084, 0.0, 0.25, 0.33, 0.575, 0.0, 0.0, 0.148]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m top1: 0.23833955223880596
[2m[36m(func pid=170188)[0m top5: 0.7910447761194029
[2m[36m(func pid=170188)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=170188)[0m f1_macro: 0.14305252784059408
[2m[36m(func pid=170188)[0m f1_weighted: 0.1972681066068372
[2m[36m(func pid=170188)[0m f1_per_class: [0.121, 0.407, 0.141, 0.141, 0.0, 0.112, 0.178, 0.317, 0.014, 0.0]
[2m[36m(func pid=170188)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.0258 | Steps: 4 | Val loss: 2.1039 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 10:31:49 (running for 00:14:18.98)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=4
Bracket: Iter 75.000: 0.26325
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (16 PENDING, 4 RUNNING, 4 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00004 | RUNNING    | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.577 |      0.143 |                   74 |
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.156 |      0.186 |                   71 |
| train_952df_00006 | RUNNING    | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  2.34  |      0.248 |                   74 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 15.421 |      0.188 |                   52 |
| train_952df_00008 | PENDING    |                     | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=171228)[0m top1: 0.31949626865671643
[2m[36m(func pid=171228)[0m top5: 0.7803171641791045
[2m[36m(func pid=171228)[0m f1_micro: 0.31949626865671643
[2m[36m(func pid=171228)[0m f1_macro: 0.24794882733764673
[2m[36m(func pid=171228)[0m f1_weighted: 0.28716144127721266
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.491, 0.512, 0.0, 0.063, 0.249, 0.475, 0.42, 0.086, 0.185]
[2m[36m(func pid=171228)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 25.0771 | Steps: 4 | Val loss: 25.2004 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=170188)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.6015 | Steps: 4 | Val loss: 2.1886 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=170672)[0m top1: 0.18330223880597016
[2m[36m(func pid=170672)[0m top5: 0.7588619402985075
[2m[36m(func pid=170672)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=170672)[0m f1_macro: 0.19381446565477078
[2m[36m(func pid=170672)[0m f1_weighted: 0.11653231025001133
[2m[36m(func pid=170672)[0m f1_per_class: [0.184, 0.275, 0.6, 0.0, 0.074, 0.318, 0.0, 0.415, 0.0, 0.071]
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=171228)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.9711 | Steps: 4 | Val loss: 2.5485 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=176697)[0m top1: 0.16138059701492538
[2m[36m(func pid=176697)[0m top5: 0.5998134328358209
[2m[36m(func pid=176697)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=176697)[0m f1_macro: 0.19650065760023377
[2m[36m(func pid=176697)[0m f1_weighted: 0.10997753315441607
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.388, 0.846, 0.0, 0.035, 0.087, 0.0, 0.376, 0.156, 0.077]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170188)[0m top1: 0.24953358208955223
[2m[36m(func pid=170188)[0m top5: 0.8190298507462687
[2m[36m(func pid=170188)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=170188)[0m f1_macro: 0.15776468030077667
[2m[36m(func pid=170188)[0m f1_weighted: 0.2322004653887297
[2m[36m(func pid=170188)[0m f1_per_class: [0.112, 0.391, 0.218, 0.184, 0.0, 0.071, 0.282, 0.301, 0.019, 0.0]
[2m[36m(func pid=171228)[0m top1: 0.22108208955223882
[2m[36m(func pid=171228)[0m top5: 0.7807835820895522
[2m[36m(func pid=171228)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=171228)[0m f1_macro: 0.15476363017763192
[2m[36m(func pid=171228)[0m f1_weighted: 0.1820255553280109
[2m[36m(func pid=171228)[0m f1_per_class: [0.0, 0.0, 0.375, 0.519, 0.038, 0.053, 0.027, 0.31, 0.0, 0.225]
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.3255 | Steps: 4 | Val loss: 2.0662 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 27.6442 | Steps: 4 | Val loss: 12.6356 | Batch size: 32 | lr: 0.1 | Duration: 2.56s
[2m[36m(func pid=170672)[0m top1: 0.1884328358208955
[2m[36m(func pid=170672)[0m top5: 0.8339552238805971
[2m[36m(func pid=170672)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=170672)[0m f1_macro: 0.20629599493348647
[2m[36m(func pid=170672)[0m f1_weighted: 0.13529640965406836
[2m[36m(func pid=170672)[0m f1_per_class: [0.147, 0.281, 0.6, 0.057, 0.104, 0.303, 0.0, 0.463, 0.044, 0.065]
[2m[36m(func pid=176697)[0m top1: 0.208955223880597
[2m[36m(func pid=176697)[0m top5: 0.8106343283582089
[2m[36m(func pid=176697)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=176697)[0m f1_macro: 0.13641383171219643
[2m[36m(func pid=176697)[0m f1_weighted: 0.2163700052374609
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.061, 0.202, 0.083, 0.042, 0.328, 0.469, 0.0, 0.09, 0.09]
== Status ==
Current time: 2024-01-07 10:31:54 (running for 00:14:24.11)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.23399999999999999
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 3 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.026 |      0.194 |                   72 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 25.077 |      0.197 |                   53 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1142)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=1142)[0m Configuration completed!
[2m[36m(func pid=1142)[0m New optimizer parameters:
[2m[36m(func pid=1142)[0m SGD (
[2m[36m(func pid=1142)[0m Parameter Group 0
[2m[36m(func pid=1142)[0m     dampening: 0
[2m[36m(func pid=1142)[0m     differentiable: False
[2m[36m(func pid=1142)[0m     foreach: None
[2m[36m(func pid=1142)[0m     lr: 0.0001
[2m[36m(func pid=1142)[0m     maximize: False
[2m[36m(func pid=1142)[0m     momentum: 0.99
[2m[36m(func pid=1142)[0m     nesterov: False
[2m[36m(func pid=1142)[0m     weight_decay: 0.0001
[2m[36m(func pid=1142)[0m )
[2m[36m(func pid=1142)[0m 
== Status ==
Current time: 2024-01-07 10:32:01 (running for 00:14:31.06)
Memory usage on this node: 20.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.23399999999999999
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (15 PENDING, 3 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.026 |      0.194 |                   72 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 27.644 |      0.136 |                   54 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | PENDING    |                     | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 21.0668 | Steps: 4 | Val loss: 14.2253 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.1956 | Steps: 4 | Val loss: 1.9866 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 2.9925 | Steps: 4 | Val loss: 2.3305 | Batch size: 32 | lr: 0.0001 | Duration: 4.32s
[2m[36m(func pid=176697)[0m top1: 0.17350746268656717
[2m[36m(func pid=176697)[0m top5: 0.7220149253731343
[2m[36m(func pid=176697)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=176697)[0m f1_macro: 0.18085682069441011
[2m[36m(func pid=176697)[0m f1_weighted: 0.15359023458011845
[2m[36m(func pid=176697)[0m f1_per_class: [0.343, 0.0, 0.632, 0.016, 0.096, 0.277, 0.349, 0.0, 0.039, 0.057]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m top1: 0.23647388059701493
[2m[36m(func pid=170672)[0m top5: 0.8666044776119403
[2m[36m(func pid=170672)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=170672)[0m f1_macro: 0.2319577370182831
[2m[36m(func pid=170672)[0m f1_weighted: 0.22609754980082108
[2m[36m(func pid=170672)[0m f1_per_class: [0.317, 0.196, 0.426, 0.402, 0.065, 0.301, 0.033, 0.352, 0.153, 0.074]
[2m[36m(func pid=1142)[0m top1: 0.14319029850746268
[2m[36m(func pid=1142)[0m top5: 0.5401119402985075
[2m[36m(func pid=1142)[0m f1_micro: 0.14319029850746268
[2m[36m(func pid=1142)[0m f1_macro: 0.042163819334268396
[2m[36m(func pid=1142)[0m f1_weighted: 0.0795548996183919
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.172, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 10:32:06 (running for 00:14:36.42)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.23399999999999999
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.326 |      0.206 |                   73 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 21.067 |      0.181 |                   55 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |        |            |                      |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 10.1790 | Steps: 4 | Val loss: 35.9415 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=1670)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1670)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=1670)[0m Configuration completed!
[2m[36m(func pid=1670)[0m New optimizer parameters:
[2m[36m(func pid=1670)[0m SGD (
[2m[36m(func pid=1670)[0m Parameter Group 0
[2m[36m(func pid=1670)[0m     dampening: 0
[2m[36m(func pid=1670)[0m     differentiable: False
[2m[36m(func pid=1670)[0m     foreach: None
[2m[36m(func pid=1670)[0m     lr: 0.001
[2m[36m(func pid=1670)[0m     maximize: False
[2m[36m(func pid=1670)[0m     momentum: 0.99
[2m[36m(func pid=1670)[0m     nesterov: False
[2m[36m(func pid=1670)[0m     weight_decay: 0.0001
[2m[36m(func pid=1670)[0m )
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=170672)[0m 
[2m[36m(func pid=1142)[0m 
== Status ==
Current time: 2024-01-07 10:32:12 (running for 00:14:41.92)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=6
Bracket: Iter 75.000: 0.23399999999999999
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (14 PENDING, 4 RUNNING, 6 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00005 | RUNNING    | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.196 |      0.232 |                   74 |
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 10.179 |      0.178 |                   56 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.992 |      0.042 |                    1 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |        |            |                      |
| train_952df_00010 | PENDING    |                     | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.10727611940298508
[2m[36m(func pid=176697)[0m top5: 0.6110074626865671
[2m[36m(func pid=176697)[0m f1_micro: 0.10727611940298508
[2m[36m(func pid=176697)[0m f1_macro: 0.17766033079909013
[2m[36m(func pid=176697)[0m f1_weighted: 0.05919736538000016
[2m[36m(func pid=176697)[0m f1_per_class: [0.313, 0.102, 0.556, 0.0, 0.211, 0.13, 0.0, 0.171, 0.094, 0.2]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=170672)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.1548 | Steps: 4 | Val loss: 1.9092 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0427 | Steps: 4 | Val loss: 2.3165 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1186 | Steps: 4 | Val loss: 2.3184 | Batch size: 32 | lr: 0.001 | Duration: 4.64s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 16.4830 | Steps: 4 | Val loss: 34.3362 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=170672)[0m top1: 0.3050373134328358
[2m[36m(func pid=170672)[0m top5: 0.9104477611940298
[2m[36m(func pid=170672)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=170672)[0m f1_macro: 0.1913839955527987
[2m[36m(func pid=170672)[0m f1_weighted: 0.3052330431274159
[2m[36m(func pid=170672)[0m f1_per_class: [0.115, 0.082, 0.192, 0.508, 0.059, 0.343, 0.325, 0.062, 0.15, 0.077]
[2m[36m(func pid=1142)[0m top1: 0.19542910447761194
[2m[36m(func pid=1142)[0m top5: 0.5783582089552238
[2m[36m(func pid=1142)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=1142)[0m f1_macro: 0.04073960267731894
[2m[36m(func pid=1142)[0m f1_weighted: 0.10082606649189824
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.0, 0.0, 0.349, 0.0, 0.0, 0.0, 0.058, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.14039179104477612
[2m[36m(func pid=1670)[0m top5: 0.5415111940298507
[2m[36m(func pid=1670)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=1670)[0m f1_macro: 0.062464553888104554
[2m[36m(func pid=1670)[0m f1_weighted: 0.09558647132591568
[2m[36m(func pid=1670)[0m f1_per_class: [0.029, 0.0, 0.03, 0.281, 0.0, 0.0, 0.0, 0.284, 0.0, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=176697)[0m top1: 0.22061567164179105
[2m[36m(func pid=176697)[0m top5: 0.5811567164179104
[2m[36m(func pid=176697)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=176697)[0m f1_macro: 0.17293633037391348
[2m[36m(func pid=176697)[0m f1_weighted: 0.1650674011389036
[2m[36m(func pid=176697)[0m f1_per_class: [0.175, 0.477, 0.1, 0.0, 0.143, 0.296, 0.088, 0.214, 0.128, 0.109]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0905 | Steps: 4 | Val loss: 2.3234 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9159 | Steps: 4 | Val loss: 2.2808 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 16.9931 | Steps: 4 | Val loss: 29.3707 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=1142)[0m top1: 0.21361940298507462
[2m[36m(func pid=1142)[0m top5: 0.5764925373134329
[2m[36m(func pid=1142)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=1142)[0m f1_macro: 0.042451497808263065
[2m[36m(func pid=1142)[0m f1_weighted: 0.1053981221617023
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.0, 0.0, 0.371, 0.0, 0.0, 0.0, 0.027, 0.0, 0.027]
[2m[36m(func pid=1670)[0m top1: 0.006996268656716418
[2m[36m(func pid=1670)[0m top5: 0.6324626865671642
[2m[36m(func pid=1670)[0m f1_micro: 0.006996268656716418
[2m[36m(func pid=1670)[0m f1_macro: 0.001867858917039245
[2m[36m(func pid=1670)[0m f1_weighted: 0.001902466060161191
[2m[36m(func pid=1670)[0m f1_per_class: [0.0, 0.0, 0.012, 0.007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=176697)[0m top1: 0.11380597014925373
[2m[36m(func pid=176697)[0m top5: 0.5638992537313433
[2m[36m(func pid=176697)[0m f1_micro: 0.11380597014925373
[2m[36m(func pid=176697)[0m f1_macro: 0.15430038321800965
[2m[36m(func pid=176697)[0m f1_weighted: 0.12461007619351971
[2m[36m(func pid=176697)[0m f1_per_class: [0.344, 0.308, 0.4, 0.0, 0.032, 0.085, 0.156, 0.0, 0.147, 0.071]
== Status ==
Current time: 2024-01-07 10:32:17 (running for 00:14:47.16)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 16.483 |      0.173 |                   57 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  3.043 |      0.041 |                    2 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  3.119 |      0.062 |                    1 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


== Status ==
Current time: 2024-01-07 10:32:24 (running for 00:14:53.77)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 16.993 |      0.154 |                   58 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  3.043 |      0.041 |                    2 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  3.119 |      0.062 |                    1 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=2489)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=2489)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=2489)[0m Configuration completed!
[2m[36m(func pid=2489)[0m New optimizer parameters:
[2m[36m(func pid=2489)[0m SGD (
[2m[36m(func pid=2489)[0m Parameter Group 0
[2m[36m(func pid=2489)[0m     dampening: 0
[2m[36m(func pid=2489)[0m     differentiable: False
[2m[36m(func pid=2489)[0m     foreach: None
[2m[36m(func pid=2489)[0m     lr: 0.01
[2m[36m(func pid=2489)[0m     maximize: False
[2m[36m(func pid=2489)[0m     momentum: 0.99
[2m[36m(func pid=2489)[0m     nesterov: False
[2m[36m(func pid=2489)[0m     weight_decay: 0.0001
[2m[36m(func pid=2489)[0m )
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 13.7106 | Steps: 4 | Val loss: 11.0324 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8958 | Steps: 4 | Val loss: 2.2873 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9814 | Steps: 4 | Val loss: 2.3236 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.2156 | Steps: 4 | Val loss: 2.3375 | Batch size: 32 | lr: 0.01 | Duration: 4.78s
== Status ==
Current time: 2024-01-07 10:32:29 (running for 00:14:58.79)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 16.993 |      0.154 |                   58 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  3.091 |      0.042 |                    3 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.916 |      0.002 |                    2 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |        |            |                      |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.006063432835820896
[2m[36m(func pid=1670)[0m top5: 0.7523320895522388
[2m[36m(func pid=1670)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=1670)[0m f1_macro: 0.001205377839592026
[2m[36m(func pid=1670)[0m f1_weighted: 7.308727572153144e-05
[2m[36m(func pid=1670)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=176697)[0m top1: 0.17957089552238806
[2m[36m(func pid=176697)[0m top5: 0.7625932835820896
[2m[36m(func pid=176697)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=176697)[0m f1_macro: 0.19454386363357273
[2m[36m(func pid=176697)[0m f1_weighted: 0.14044289533211823
[2m[36m(func pid=176697)[0m f1_per_class: [0.221, 0.131, 0.267, 0.127, 0.057, 0.292, 0.021, 0.51, 0.133, 0.186]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.2234141791044776
[2m[36m(func pid=1142)[0m top5: 0.558768656716418
[2m[36m(func pid=1142)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=1142)[0m f1_macro: 0.048074867090133144
[2m[36m(func pid=1142)[0m f1_weighted: 0.11613546060956405
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.005, 0.019, 0.405, 0.0, 0.0, 0.0, 0.033, 0.0, 0.019]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.09001865671641791
[2m[36m(func pid=2489)[0m top5: 0.3987873134328358
[2m[36m(func pid=2489)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=2489)[0m f1_macro: 0.04523479632475562
[2m[36m(func pid=2489)[0m f1_weighted: 0.05409561391010883
[2m[36m(func pid=2489)[0m f1_per_class: [0.055, 0.304, 0.068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 6.4284 | Steps: 4 | Val loss: 8.5044 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9420 | Steps: 4 | Val loss: 2.2486 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9675 | Steps: 4 | Val loss: 2.3330 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.1594 | Steps: 4 | Val loss: 2.1476 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 10:32:34 (running for 00:15:04.30)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 13.711 |      0.195 |                   59 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.981 |      0.048 |                    4 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.942 |      0.024 |                    4 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.216 |      0.045 |                    1 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.030783582089552237
[2m[36m(func pid=1670)[0m top5: 0.6697761194029851
[2m[36m(func pid=1670)[0m f1_micro: 0.030783582089552237
[2m[36m(func pid=1670)[0m f1_macro: 0.024248226886123613
[2m[36m(func pid=1670)[0m f1_weighted: 0.03408195506055349
[2m[36m(func pid=1670)[0m f1_per_class: [0.12, 0.0, 0.015, 0.041, 0.0, 0.0, 0.068, 0.0, 0.0, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=176697)[0m top1: 0.376865671641791
[2m[36m(func pid=176697)[0m top5: 0.8908582089552238
[2m[36m(func pid=176697)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=176697)[0m f1_macro: 0.20212376257411052
[2m[36m(func pid=176697)[0m f1_weighted: 0.32871609785628797
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.5, 0.186, 0.432, 0.0, 0.091, 0.28, 0.455, 0.0, 0.077]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.13526119402985073
[2m[36m(func pid=1142)[0m top5: 0.5368470149253731
[2m[36m(func pid=1142)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=1142)[0m f1_macro: 0.04195853347512646
[2m[36m(func pid=1142)[0m f1_weighted: 0.10133006253594398
[2m[36m(func pid=1142)[0m f1_per_class: [0.023, 0.0, 0.023, 0.358, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.279384328358209
[2m[36m(func pid=2489)[0m top5: 0.8544776119402985
[2m[36m(func pid=2489)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=2489)[0m f1_macro: 0.09170396667561502
[2m[36m(func pid=2489)[0m f1_weighted: 0.14832968663549653
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.0, 0.37, 0.0, 0.0, 0.0, 0.488, 0.0, 0.0, 0.059]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.7860 | Steps: 4 | Val loss: 2.1985 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 7.2063 | Steps: 4 | Val loss: 9.7197 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8859 | Steps: 4 | Val loss: 2.3404 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8177 | Steps: 4 | Val loss: 2.4092 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 10:32:39 (running for 00:15:09.60)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  7.206 |      0.263 |                   61 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.968 |      0.042 |                    5 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.942 |      0.024 |                    4 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.159 |      0.092 |                    2 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.2294776119402985
[2m[36m(func pid=176697)[0m top5: 0.8316231343283582
[2m[36m(func pid=176697)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=176697)[0m f1_macro: 0.26270254856626013
[2m[36m(func pid=176697)[0m f1_weighted: 0.2660040026893628
[2m[36m(func pid=176697)[0m f1_per_class: [0.249, 0.309, 0.571, 0.035, 0.029, 0.256, 0.444, 0.495, 0.05, 0.188]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1670)[0m top1: 0.08442164179104478
[2m[36m(func pid=1670)[0m top5: 0.7663246268656716
[2m[36m(func pid=1670)[0m f1_micro: 0.08442164179104478
[2m[36m(func pid=1670)[0m f1_macro: 0.04706626706626707
[2m[36m(func pid=1670)[0m f1_weighted: 0.04089691403124239
[2m[36m(func pid=1670)[0m f1_per_class: [0.077, 0.0, 0.0, 0.0, 0.0, 0.324, 0.0, 0.0, 0.07, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.026119402985074626
[2m[36m(func pid=1142)[0m top5: 0.5340485074626866
[2m[36m(func pid=1142)[0m f1_micro: 0.026119402985074626
[2m[36m(func pid=1142)[0m f1_macro: 0.015851178944869922
[2m[36m(func pid=1142)[0m f1_weighted: 0.029550576815239402
[2m[36m(func pid=1142)[0m f1_per_class: [0.04, 0.005, 0.014, 0.099, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.010261194029850746
[2m[36m(func pid=2489)[0m top5: 0.71875
[2m[36m(func pid=2489)[0m f1_micro: 0.010261194029850746
[2m[36m(func pid=2489)[0m f1_macro: 0.011966616021811975
[2m[36m(func pid=2489)[0m f1_weighted: 0.002255782506494729
[2m[36m(func pid=2489)[0m f1_per_class: [0.104, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 9.7856 | Steps: 4 | Val loss: 10.1525 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8008 | Steps: 4 | Val loss: 2.1599 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7976 | Steps: 4 | Val loss: 2.3430 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0960 | Steps: 4 | Val loss: 2.6036 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 10:32:45 (running for 00:15:14.86)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  9.786 |      0.235 |                   62 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.886 |      0.016 |                    6 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.786 |      0.047 |                    5 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  2.818 |      0.012 |                    3 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.23600746268656717
[2m[36m(func pid=176697)[0m top5: 0.8180970149253731
[2m[36m(func pid=176697)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=176697)[0m f1_macro: 0.234638474558092
[2m[36m(func pid=176697)[0m f1_weighted: 0.22779933493812396
[2m[36m(func pid=176697)[0m f1_per_class: [0.2, 0.148, 0.56, 0.147, 0.0, 0.315, 0.294, 0.416, 0.135, 0.13]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1670)[0m top1: 0.08115671641791045
[2m[36m(func pid=1670)[0m top5: 0.8628731343283582
[2m[36m(func pid=1670)[0m f1_micro: 0.08115671641791045
[2m[36m(func pid=1670)[0m f1_macro: 0.040673233932742844
[2m[36m(func pid=1670)[0m f1_weighted: 0.040646118096193194
[2m[36m(func pid=1670)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.335, 0.0, 0.0, 0.072, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.006996268656716418
[2m[36m(func pid=1142)[0m top5: 0.519589552238806
[2m[36m(func pid=1142)[0m f1_micro: 0.006996268656716418
[2m[36m(func pid=1142)[0m f1_macro: 0.004224574711453432
[2m[36m(func pid=1142)[0m f1_weighted: 0.0015319530701577684
[2m[36m(func pid=1142)[0m f1_per_class: [0.027, 0.0, 0.012, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.11613805970149253
[2m[36m(func pid=2489)[0m top5: 0.5177238805970149
[2m[36m(func pid=2489)[0m f1_micro: 0.11613805970149253
[2m[36m(func pid=2489)[0m f1_macro: 0.05348050519120247
[2m[36m(func pid=2489)[0m f1_weighted: 0.026018654139553545
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.0, 0.324, 0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 11.1647 | Steps: 4 | Val loss: 9.3526 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8430 | Steps: 4 | Val loss: 2.0481 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.9249 | Steps: 4 | Val loss: 2.3649 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.4600 | Steps: 4 | Val loss: 2.0299 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=176697)[0m top1: 0.2971082089552239
[2m[36m(func pid=176697)[0m top5: 0.8647388059701493
[2m[36m(func pid=176697)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=176697)[0m f1_macro: 0.17169415923880627
[2m[36m(func pid=176697)[0m f1_weighted: 0.26456291211629585
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.448, 0.143, 0.533, 0.111, 0.0, 0.073, 0.221, 0.022, 0.167]
== Status ==
Current time: 2024-01-07 10:32:50 (running for 00:15:20.31)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.165 |      0.172 |                   63 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.798 |      0.004 |                    7 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.801 |      0.041 |                    6 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.096 |      0.053 |                    4 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1670)[0m top1: 0.31576492537313433
[2m[36m(func pid=1670)[0m top5: 0.7052238805970149
[2m[36m(func pid=1670)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=1670)[0m f1_macro: 0.09031037590862885
[2m[36m(func pid=1670)[0m f1_weighted: 0.18459266096594454
[2m[36m(func pid=1670)[0m f1_per_class: [0.0, 0.0, 0.0, 0.53, 0.0, 0.302, 0.0, 0.0, 0.071, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.006063432835820896
[2m[36m(func pid=1142)[0m top5: 0.46875
[2m[36m(func pid=1142)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=1142)[0m f1_macro: 0.0012160898035547241
[2m[36m(func pid=1142)[0m f1_weighted: 7.373678846180695e-05
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.1515858208955224
[2m[36m(func pid=2489)[0m top5: 0.8022388059701493
[2m[36m(func pid=2489)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=2489)[0m f1_macro: 0.09721615908244291
[2m[36m(func pid=2489)[0m f1_weighted: 0.13405198501121834
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.353, 0.333, 0.21, 0.0, 0.0, 0.041, 0.0, 0.0, 0.035]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 11.9577 | Steps: 4 | Val loss: 11.0898 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7985 | Steps: 4 | Val loss: 2.3694 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7524 | Steps: 4 | Val loss: 2.0679 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 10:32:55 (running for 00:15:25.57)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.165 |      0.172 |                   63 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.798 |      0.001 |                    9 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.843 |      0.09  |                    7 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.46  |      0.097 |                    5 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.25279850746268656
[2m[36m(func pid=176697)[0m top5: 0.7807835820895522
[2m[36m(func pid=176697)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=176697)[0m f1_macro: 0.15210635497019137
[2m[36m(func pid=176697)[0m f1_weighted: 0.16440845860148665
[2m[36m(func pid=176697)[0m f1_per_class: [0.341, 0.506, 0.0, 0.007, 0.086, 0.0, 0.172, 0.245, 0.026, 0.138]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.006063432835820896
[2m[36m(func pid=1142)[0m top5: 0.46968283582089554
[2m[36m(func pid=1142)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=1142)[0m f1_macro: 0.0012269938650306747
[2m[36m(func pid=1142)[0m f1_weighted: 7.439794890577785e-05
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.4096 | Steps: 4 | Val loss: 2.8250 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=1670)[0m top1: 0.3199626865671642
[2m[36m(func pid=1670)[0m top5: 0.6473880597014925
[2m[36m(func pid=1670)[0m f1_micro: 0.3199626865671642
[2m[36m(func pid=1670)[0m f1_macro: 0.14084677344079594
[2m[36m(func pid=1670)[0m f1_weighted: 0.20815346275985502
[2m[36m(func pid=1670)[0m f1_per_class: [0.0, 0.095, 0.296, 0.538, 0.121, 0.327, 0.0, 0.032, 0.0, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m top1: 0.08069029850746269
[2m[36m(func pid=2489)[0m top5: 0.6725746268656716
[2m[36m(func pid=2489)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=2489)[0m f1_macro: 0.04162871772020687
[2m[36m(func pid=2489)[0m f1_weighted: 0.06201551951306257
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.241, 0.0, 0.0, 0.0, 0.0, 0.058, 0.0, 0.086, 0.031]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 9.6687 | Steps: 4 | Val loss: 11.1547 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8168 | Steps: 4 | Val loss: 2.3682 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.5507 | Steps: 4 | Val loss: 2.2298 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 10:33:01 (running for 00:15:30.71)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  9.669 |      0.202 |                   65 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.798 |      0.001 |                    9 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.752 |      0.141 |                    8 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.41  |      0.042 |                    6 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.32276119402985076
[2m[36m(func pid=176697)[0m top5: 0.8316231343283582
[2m[36m(func pid=176697)[0m f1_micro: 0.32276119402985076
[2m[36m(func pid=176697)[0m f1_macro: 0.20198584847348555
[2m[36m(func pid=176697)[0m f1_weighted: 0.270282221993234
[2m[36m(func pid=176697)[0m f1_per_class: [0.044, 0.275, 0.324, 0.571, 0.076, 0.088, 0.074, 0.468, 0.0, 0.1]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.007462686567164179
[2m[36m(func pid=1142)[0m top5: 0.5041977611940298
[2m[36m(func pid=1142)[0m f1_micro: 0.007462686567164179
[2m[36m(func pid=1142)[0m f1_macro: 0.004730959631185886
[2m[36m(func pid=1142)[0m f1_weighted: 0.0007854432226191906
[2m[36m(func pid=1142)[0m f1_per_class: [0.034, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.09701492537313433
[2m[36m(func pid=1670)[0m top5: 0.6394589552238806
[2m[36m(func pid=1670)[0m f1_micro: 0.09701492537313433
[2m[36m(func pid=1670)[0m f1_macro: 0.09753927935411591
[2m[36m(func pid=1670)[0m f1_weighted: 0.0619026859124612
[2m[36m(func pid=1670)[0m f1_per_class: [0.0, 0.212, 0.348, 0.0, 0.024, 0.008, 0.0, 0.383, 0.0, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 4.1585 | Steps: 4 | Val loss: 3.3421 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 15.6593 | Steps: 4 | Val loss: 13.9898 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.7054 | Steps: 4 | Val loss: 2.3543 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=2489)[0m top1: 0.14085820895522388
[2m[36m(func pid=2489)[0m top5: 0.6152052238805971
[2m[36m(func pid=2489)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=2489)[0m f1_macro: 0.12323632430112981
[2m[36m(func pid=2489)[0m f1_weighted: 0.06243767983380383
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.0, 0.381, 0.053, 0.229, 0.265, 0.0, 0.2, 0.026, 0.079]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7808 | Steps: 4 | Val loss: 2.4164 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 10:33:06 (running for 00:15:35.75)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 15.659 |      0.198 |                   66 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.817 |      0.005 |                   10 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.551 |      0.098 |                    9 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.159 |      0.123 |                    7 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.30130597014925375
[2m[36m(func pid=176697)[0m top5: 0.8460820895522388
[2m[36m(func pid=176697)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=176697)[0m f1_macro: 0.1981097261389005
[2m[36m(func pid=176697)[0m f1_weighted: 0.26821916446512956
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.493, 0.058, 0.416, 0.118, 0.372, 0.0, 0.379, 0.0, 0.146]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.014925373134328358
[2m[36m(func pid=1142)[0m top5: 0.5205223880597015
[2m[36m(func pid=1142)[0m f1_micro: 0.014925373134328358
[2m[36m(func pid=1142)[0m f1_macro: 0.00802828985751915
[2m[36m(func pid=1142)[0m f1_weighted: 0.00651977169247907
[2m[36m(func pid=1142)[0m f1_per_class: [0.044, 0.005, 0.015, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.08442164179104478
[2m[36m(func pid=1670)[0m top5: 0.46595149253731344
[2m[36m(func pid=1670)[0m f1_micro: 0.08442164179104478
[2m[36m(func pid=1670)[0m f1_macro: 0.10372501025095335
[2m[36m(func pid=1670)[0m f1_weighted: 0.06410947795648451
[2m[36m(func pid=1670)[0m f1_per_class: [0.0, 0.2, 0.34, 0.0, 0.023, 0.0, 0.0, 0.474, 0.0, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.5944 | Steps: 4 | Val loss: 5.0775 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 12.1582 | Steps: 4 | Val loss: 7.7865 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9591 | Steps: 4 | Val loss: 2.3565 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=2489)[0m top1: 0.04664179104477612
[2m[36m(func pid=2489)[0m top5: 0.3805970149253731
[2m[36m(func pid=2489)[0m f1_micro: 0.04664179104477612
[2m[36m(func pid=2489)[0m f1_macro: 0.08617443640302722
[2m[36m(func pid=2489)[0m f1_weighted: 0.06062648699567268
[2m[36m(func pid=2489)[0m f1_per_class: [0.04, 0.0, 0.417, 0.147, 0.019, 0.04, 0.0, 0.199, 0.0, 0.0]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.6772 | Steps: 4 | Val loss: 2.4816 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 10:33:11 (running for 00:15:40.97)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 12.158 |      0.272 |                   67 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.705 |      0.008 |                   11 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.781 |      0.104 |                   10 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.594 |      0.086 |                    8 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.4388992537313433
[2m[36m(func pid=176697)[0m top5: 0.8857276119402985
[2m[36m(func pid=176697)[0m f1_micro: 0.4388992537313433
[2m[36m(func pid=176697)[0m f1_macro: 0.2721738082271066
[2m[36m(func pid=176697)[0m f1_weighted: 0.3625241727887486
[2m[36m(func pid=176697)[0m f1_per_class: [0.185, 0.527, 0.545, 0.283, 0.286, 0.0, 0.58, 0.147, 0.02, 0.148]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.033582089552238806
[2m[36m(func pid=1142)[0m top5: 0.5097947761194029
[2m[36m(func pid=1142)[0m f1_micro: 0.033582089552238806
[2m[36m(func pid=1142)[0m f1_macro: 0.018391284614160765
[2m[36m(func pid=1142)[0m f1_weighted: 0.03247497360510764
[2m[36m(func pid=1142)[0m f1_per_class: [0.035, 0.031, 0.03, 0.0, 0.0, 0.0, 0.088, 0.0, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.05083955223880597
[2m[36m(func pid=1670)[0m top5: 0.416044776119403
[2m[36m(func pid=1670)[0m f1_micro: 0.05083955223880597
[2m[36m(func pid=1670)[0m f1_macro: 0.104042753134196
[2m[36m(func pid=1670)[0m f1_weighted: 0.05144024824216579
[2m[36m(func pid=1670)[0m f1_per_class: [0.113, 0.12, 0.333, 0.0, 0.019, 0.0, 0.0, 0.455, 0.0, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.4754 | Steps: 4 | Val loss: 4.5433 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 8.4597 | Steps: 4 | Val loss: 17.1737 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.9772 | Steps: 4 | Val loss: 2.3553 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=2489)[0m top1: 0.12173507462686567
[2m[36m(func pid=2489)[0m top5: 0.48134328358208955
[2m[36m(func pid=2489)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=2489)[0m f1_macro: 0.18195563422669497
[2m[36m(func pid=2489)[0m f1_weighted: 0.11738706485207753
[2m[36m(func pid=2489)[0m f1_per_class: [0.235, 0.236, 0.476, 0.121, 0.026, 0.0, 0.0, 0.517, 0.134, 0.074]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.7671 | Steps: 4 | Val loss: 2.4898 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 10:33:16 (running for 00:15:46.21)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  8.46  |      0.204 |                   68 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.959 |      0.018 |                   12 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.677 |      0.104 |                   11 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.475 |      0.182 |                    9 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.17630597014925373
[2m[36m(func pid=176697)[0m top5: 0.6142723880597015
[2m[36m(func pid=176697)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=176697)[0m f1_macro: 0.2040743383255513
[2m[36m(func pid=176697)[0m f1_weighted: 0.13988794225824558
[2m[36m(func pid=176697)[0m f1_per_class: [0.152, 0.428, 0.556, 0.0, 0.04, 0.278, 0.0, 0.452, 0.0, 0.136]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.0853544776119403
[2m[36m(func pid=1142)[0m top5: 0.5055970149253731
[2m[36m(func pid=1142)[0m f1_micro: 0.0853544776119403
[2m[36m(func pid=1142)[0m f1_macro: 0.05685024736696002
[2m[36m(func pid=1142)[0m f1_weighted: 0.10041944139537622
[2m[36m(func pid=1142)[0m f1_per_class: [0.037, 0.166, 0.13, 0.0, 0.0, 0.0, 0.236, 0.0, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.043843283582089554
[2m[36m(func pid=1670)[0m top5: 0.5960820895522388
[2m[36m(func pid=1670)[0m f1_micro: 0.043843283582089554
[2m[36m(func pid=1670)[0m f1_macro: 0.13168683991398908
[2m[36m(func pid=1670)[0m f1_weighted: 0.03571388942168756
[2m[36m(func pid=1670)[0m f1_per_class: [0.246, 0.0, 0.571, 0.0, 0.022, 0.0, 0.0, 0.465, 0.0, 0.012]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.2971 | Steps: 4 | Val loss: 4.7647 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 13.3326 | Steps: 4 | Val loss: 14.5178 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.6414 | Steps: 4 | Val loss: 2.3140 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=2489)[0m top1: 0.15298507462686567
[2m[36m(func pid=2489)[0m top5: 0.5736940298507462
[2m[36m(func pid=2489)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=2489)[0m f1_macro: 0.15951935365432465
[2m[36m(func pid=2489)[0m f1_weighted: 0.1653408140919332
[2m[36m(func pid=2489)[0m f1_per_class: [0.043, 0.29, 0.4, 0.293, 0.0, 0.0, 0.0, 0.498, 0.04, 0.032]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6287 | Steps: 4 | Val loss: 2.3165 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 10:33:21 (running for 00:15:51.31)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 13.333 |      0.173 |                   69 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.977 |      0.057 |                   13 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.767 |      0.132 |                   12 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.297 |      0.16  |                   10 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.20055970149253732
[2m[36m(func pid=176697)[0m top5: 0.5890858208955224
[2m[36m(func pid=176697)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=176697)[0m f1_macro: 0.1730507630017645
[2m[36m(func pid=176697)[0m f1_weighted: 0.14235456164790933
[2m[36m(func pid=176697)[0m f1_per_class: [0.15, 0.411, 0.156, 0.0, 0.114, 0.33, 0.003, 0.455, 0.025, 0.088]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.15671641791044777
[2m[36m(func pid=1142)[0m top5: 0.5293843283582089
[2m[36m(func pid=1142)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=1142)[0m f1_macro: 0.09885332059245103
[2m[36m(func pid=1142)[0m f1_weighted: 0.16532386740449687
[2m[36m(func pid=1142)[0m f1_per_class: [0.054, 0.246, 0.286, 0.0, 0.0, 0.0, 0.403, 0.0, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.06996268656716417
[2m[36m(func pid=1670)[0m top5: 0.5909514925373134
[2m[36m(func pid=1670)[0m f1_micro: 0.06996268656716417
[2m[36m(func pid=1670)[0m f1_macro: 0.13656127114547592
[2m[36m(func pid=1670)[0m f1_weighted: 0.05473259118892826
[2m[36m(func pid=1670)[0m f1_per_class: [0.207, 0.0, 0.571, 0.0, 0.027, 0.02, 0.059, 0.466, 0.0, 0.017]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 5.7589 | Steps: 4 | Val loss: 3.8350 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 9.9983 | Steps: 4 | Val loss: 9.8743 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.6951 | Steps: 4 | Val loss: 2.2707 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=2489)[0m top1: 0.26399253731343286
[2m[36m(func pid=2489)[0m top5: 0.5918843283582089
[2m[36m(func pid=2489)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=2489)[0m f1_macro: 0.16651985627996338
[2m[36m(func pid=2489)[0m f1_weighted: 0.19202395670434994
[2m[36m(func pid=2489)[0m f1_per_class: [0.274, 0.0, 0.379, 0.576, 0.0, 0.0, 0.0, 0.396, 0.0, 0.04]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.2918 | Steps: 4 | Val loss: 2.0946 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 10:33:26 (running for 00:15:56.64)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  9.998 |      0.236 |                   70 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.641 |      0.099 |                   14 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.629 |      0.137 |                   13 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  5.759 |      0.167 |                   11 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.4552238805970149
[2m[36m(func pid=176697)[0m top5: 0.8558768656716418
[2m[36m(func pid=176697)[0m f1_micro: 0.4552238805970149
[2m[36m(func pid=176697)[0m f1_macro: 0.23558058586550593
[2m[36m(func pid=176697)[0m f1_weighted: 0.40134775531199346
[2m[36m(func pid=176697)[0m f1_per_class: [0.043, 0.466, 0.264, 0.56, 0.204, 0.0, 0.5, 0.175, 0.0, 0.143]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.19776119402985073
[2m[36m(func pid=1142)[0m top5: 0.5764925373134329
[2m[36m(func pid=1142)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=1142)[0m f1_macro: 0.12574244275039512
[2m[36m(func pid=1142)[0m f1_weighted: 0.1944359290308625
[2m[36m(func pid=1142)[0m f1_per_class: [0.066, 0.318, 0.4, 0.0, 0.0, 0.0, 0.454, 0.0, 0.019, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.22388059701492538
[2m[36m(func pid=1670)[0m top5: 0.695429104477612
[2m[36m(func pid=1670)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=1670)[0m f1_macro: 0.15617489786538713
[2m[36m(func pid=1670)[0m f1_weighted: 0.15586553104079523
[2m[36m(func pid=1670)[0m f1_per_class: [0.31, 0.0, 0.455, 0.003, 0.063, 0.318, 0.358, 0.016, 0.038, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 3.5209 | Steps: 4 | Val loss: 1.8735 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 14.7465 | Steps: 4 | Val loss: 14.6323 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8768 | Steps: 4 | Val loss: 2.2383 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=2489)[0m top1: 0.416044776119403
[2m[36m(func pid=2489)[0m top5: 0.9328358208955224
[2m[36m(func pid=2489)[0m f1_micro: 0.416044776119403
[2m[36m(func pid=2489)[0m f1_macro: 0.22099191015279107
[2m[36m(func pid=2489)[0m f1_weighted: 0.3406430797477226
[2m[36m(func pid=2489)[0m f1_per_class: [0.242, 0.506, 0.259, 0.254, 0.229, 0.054, 0.557, 0.0, 0.037, 0.071]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.5402 | Steps: 4 | Val loss: 1.9871 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 10:33:32 (running for 00:16:01.89)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 14.747 |      0.19  |                   71 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.695 |      0.126 |                   15 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.292 |      0.156 |                   14 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.521 |      0.221 |                   12 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.16884328358208955
[2m[36m(func pid=176697)[0m top5: 0.7210820895522388
[2m[36m(func pid=176697)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=176697)[0m f1_macro: 0.19031863841434787
[2m[36m(func pid=176697)[0m f1_weighted: 0.13409127916339564
[2m[36m(func pid=176697)[0m f1_per_class: [0.078, 0.438, 0.435, 0.0, 0.176, 0.227, 0.0, 0.437, 0.027, 0.085]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.23694029850746268
[2m[36m(func pid=1142)[0m top5: 0.6096082089552238
[2m[36m(func pid=1142)[0m f1_micro: 0.23694029850746268
[2m[36m(func pid=1142)[0m f1_macro: 0.12300760160254698
[2m[36m(func pid=1142)[0m f1_weighted: 0.21449855736124518
[2m[36m(func pid=1142)[0m f1_per_class: [0.08, 0.347, 0.25, 0.0, 0.0, 0.0, 0.503, 0.0, 0.05, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.22901119402985073
[2m[36m(func pid=1670)[0m top5: 0.7751865671641791
[2m[36m(func pid=1670)[0m f1_micro: 0.22901119402985073
[2m[36m(func pid=1670)[0m f1_macro: 0.126711250722562
[2m[36m(func pid=1670)[0m f1_weighted: 0.16691876293478192
[2m[36m(func pid=1670)[0m f1_per_class: [0.083, 0.0, 0.367, 0.032, 0.0, 0.319, 0.385, 0.0, 0.081, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 4.7645 | Steps: 4 | Val loss: 3.4840 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 8.4279 | Steps: 4 | Val loss: 25.0052 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6221 | Steps: 4 | Val loss: 2.1985 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=2489)[0m top1: 0.23787313432835822
[2m[36m(func pid=2489)[0m top5: 0.7686567164179104
[2m[36m(func pid=2489)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=2489)[0m f1_macro: 0.181787964574671
[2m[36m(func pid=2489)[0m f1_weighted: 0.19252380459233281
[2m[36m(func pid=2489)[0m f1_per_class: [0.082, 0.297, 0.516, 0.0, 0.144, 0.324, 0.315, 0.0, 0.141, 0.0]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.7134 | Steps: 4 | Val loss: 1.9043 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 10:33:37 (running for 00:16:07.18)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  8.428 |      0.13  |                   72 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.877 |      0.123 |                   16 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.54  |      0.127 |                   15 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.764 |      0.182 |                   13 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.13899253731343283
[2m[36m(func pid=176697)[0m top5: 0.539179104477612
[2m[36m(func pid=176697)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=176697)[0m f1_macro: 0.1295844973019759
[2m[36m(func pid=176697)[0m f1_weighted: 0.09204347173150644
[2m[36m(func pid=176697)[0m f1_per_class: [0.241, 0.0, 0.138, 0.0, 0.092, 0.24, 0.135, 0.201, 0.149, 0.098]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.26492537313432835
[2m[36m(func pid=1142)[0m top5: 0.6721082089552238
[2m[36m(func pid=1142)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=1142)[0m f1_macro: 0.11759520388603548
[2m[36m(func pid=1142)[0m f1_weighted: 0.22106340853544124
[2m[36m(func pid=1142)[0m f1_per_class: [0.102, 0.375, 0.133, 0.0, 0.0, 0.0, 0.509, 0.0, 0.057, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.30363805970149255
[2m[36m(func pid=1670)[0m top5: 0.8036380597014925
[2m[36m(func pid=1670)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=1670)[0m f1_macro: 0.18971482269416817
[2m[36m(func pid=1670)[0m f1_weighted: 0.25096332032140817
[2m[36m(func pid=1670)[0m f1_per_class: [0.148, 0.0, 0.349, 0.534, 0.0, 0.3, 0.134, 0.344, 0.089, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 4.5466 | Steps: 4 | Val loss: 6.2709 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 14.6144 | Steps: 4 | Val loss: 18.2923 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.9044 | Steps: 4 | Val loss: 2.1636 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=2489)[0m top1: 0.0228544776119403
[2m[36m(func pid=2489)[0m top5: 0.5909514925373134
[2m[36m(func pid=2489)[0m f1_micro: 0.0228544776119403
[2m[36m(func pid=2489)[0m f1_macro: 0.08443493989846426
[2m[36m(func pid=2489)[0m f1_weighted: 0.023172088074174407
[2m[36m(func pid=2489)[0m f1_per_class: [0.113, 0.076, 0.571, 0.0, 0.021, 0.01, 0.003, 0.032, 0.0, 0.018]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8256 | Steps: 4 | Val loss: 1.9436 | Batch size: 32 | lr: 0.001 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 10:33:42 (running for 00:16:12.37)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 14.614 |      0.183 |                   73 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.622 |      0.118 |                   17 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.713 |      0.19  |                   16 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.547 |      0.084 |                   14 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.25513059701492535
[2m[36m(func pid=176697)[0m top5: 0.6277985074626866
[2m[36m(func pid=176697)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=176697)[0m f1_macro: 0.1826691950731508
[2m[36m(func pid=176697)[0m f1_weighted: 0.23539024883248502
[2m[36m(func pid=176697)[0m f1_per_class: [0.146, 0.0, 0.114, 0.054, 0.039, 0.277, 0.538, 0.345, 0.027, 0.286]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.28824626865671643
[2m[36m(func pid=1142)[0m top5: 0.7257462686567164
[2m[36m(func pid=1142)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=1142)[0m f1_macro: 0.14743209149024852
[2m[36m(func pid=1142)[0m f1_weighted: 0.21723464177212218
[2m[36m(func pid=1142)[0m f1_per_class: [0.145, 0.326, 0.421, 0.0, 0.0, 0.0, 0.515, 0.0, 0.068, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.34281716417910446
[2m[36m(func pid=1670)[0m top5: 0.871268656716418
[2m[36m(func pid=1670)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=1670)[0m f1_macro: 0.1705536461211431
[2m[36m(func pid=1670)[0m f1_weighted: 0.22827519000930144
[2m[36m(func pid=1670)[0m f1_per_class: [0.125, 0.0, 0.232, 0.56, 0.0, 0.411, 0.0, 0.351, 0.027, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 4.9234 | Steps: 4 | Val loss: 4.9027 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 6.8397 | Steps: 4 | Val loss: 10.7745 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.6197 | Steps: 4 | Val loss: 2.1264 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=2489)[0m top1: 0.06902985074626866
[2m[36m(func pid=2489)[0m top5: 0.7154850746268657
[2m[36m(func pid=2489)[0m f1_micro: 0.06902985074626866
[2m[36m(func pid=2489)[0m f1_macro: 0.1117098565586252
[2m[36m(func pid=2489)[0m f1_weighted: 0.05287865006172887
[2m[36m(func pid=2489)[0m f1_per_class: [0.085, 0.0, 0.297, 0.0, 0.034, 0.168, 0.0, 0.512, 0.0, 0.021]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4391 | Steps: 4 | Val loss: 2.0863 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 10:33:47 (running for 00:16:17.64)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.221
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  6.84  |      0.287 |                   74 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.904 |      0.147 |                   18 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.826 |      0.171 |                   17 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.923 |      0.112 |                   15 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.32742537313432835
[2m[36m(func pid=176697)[0m top5: 0.8386194029850746
[2m[36m(func pid=176697)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=176697)[0m f1_macro: 0.2873494951839502
[2m[36m(func pid=176697)[0m f1_weighted: 0.2540254341221736
[2m[36m(func pid=176697)[0m f1_per_class: [0.185, 0.021, 0.6, 0.541, 0.244, 0.437, 0.018, 0.477, 0.147, 0.203]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.29990671641791045
[2m[36m(func pid=1142)[0m top5: 0.7784514925373134
[2m[36m(func pid=1142)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=1142)[0m f1_macro: 0.15107239217251806
[2m[36m(func pid=1142)[0m f1_weighted: 0.2119142517784255
[2m[36m(func pid=1142)[0m f1_per_class: [0.193, 0.282, 0.435, 0.003, 0.0, 0.0, 0.513, 0.0, 0.084, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.2905783582089552
[2m[36m(func pid=1670)[0m top5: 0.7705223880597015
[2m[36m(func pid=1670)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=1670)[0m f1_macro: 0.1625683319210453
[2m[36m(func pid=1670)[0m f1_weighted: 0.22766789630919274
[2m[36m(func pid=1670)[0m f1_per_class: [0.173, 0.088, 0.218, 0.539, 0.0, 0.391, 0.0, 0.216, 0.0, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 3.3624 | Steps: 4 | Val loss: 2.5053 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 5.7169 | Steps: 4 | Val loss: 12.2064 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.6623 | Steps: 4 | Val loss: 2.0954 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=2489)[0m top1: 0.38013059701492535
[2m[36m(func pid=2489)[0m top5: 0.8642723880597015
[2m[36m(func pid=2489)[0m f1_micro: 0.38013059701492535
[2m[36m(func pid=2489)[0m f1_macro: 0.27144970700161486
[2m[36m(func pid=2489)[0m f1_weighted: 0.3164492590917937
[2m[36m(func pid=2489)[0m f1_per_class: [0.176, 0.389, 0.189, 0.567, 0.339, 0.507, 0.003, 0.343, 0.136, 0.065]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.4262 | Steps: 4 | Val loss: 2.2011 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 10:33:53 (running for 00:16:22.95)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  5.717 |      0.261 |                   75 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.62  |      0.151 |                   19 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.439 |      0.163 |                   18 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.362 |      0.271 |                   16 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.23041044776119404
[2m[36m(func pid=176697)[0m top5: 0.710820895522388
[2m[36m(func pid=176697)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=176697)[0m f1_macro: 0.26101943659700433
[2m[36m(func pid=176697)[0m f1_weighted: 0.15535606470661403
[2m[36m(func pid=176697)[0m f1_per_class: [0.321, 0.42, 0.667, 0.0, 0.185, 0.305, 0.009, 0.493, 0.118, 0.093]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.30363805970149255
[2m[36m(func pid=1142)[0m top5: 0.8069029850746269
[2m[36m(func pid=1142)[0m f1_micro: 0.30363805970149255
[2m[36m(func pid=1142)[0m f1_macro: 0.14724773772773003
[2m[36m(func pid=1142)[0m f1_weighted: 0.21183360047664548
[2m[36m(func pid=1142)[0m f1_per_class: [0.239, 0.271, 0.345, 0.01, 0.0, 0.0, 0.511, 0.0, 0.097, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.2994402985074627
[2m[36m(func pid=1670)[0m top5: 0.6739738805970149
[2m[36m(func pid=1670)[0m f1_micro: 0.2994402985074627
[2m[36m(func pid=1670)[0m f1_macro: 0.22317305294115153
[2m[36m(func pid=1670)[0m f1_weighted: 0.2781955443710436
[2m[36m(func pid=1670)[0m f1_per_class: [0.227, 0.486, 0.314, 0.493, 0.182, 0.327, 0.0, 0.202, 0.0, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 3.5727 | Steps: 4 | Val loss: 3.0012 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 4.4019 | Steps: 4 | Val loss: 19.1127 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.6187 | Steps: 4 | Val loss: 2.0705 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=2489)[0m top1: 0.29384328358208955
[2m[36m(func pid=2489)[0m top5: 0.9146455223880597
[2m[36m(func pid=2489)[0m f1_micro: 0.29384328358208955
[2m[36m(func pid=2489)[0m f1_macro: 0.1766517039867437
[2m[36m(func pid=2489)[0m f1_weighted: 0.26531925249572524
[2m[36m(func pid=2489)[0m f1_per_class: [0.214, 0.518, 0.234, 0.003, 0.0, 0.0, 0.552, 0.0, 0.097, 0.148]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.3734 | Steps: 4 | Val loss: 2.3793 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 10:33:58 (running for 00:16:28.40)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  4.402 |      0.271 |                   76 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.662 |      0.147 |                   20 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.426 |      0.223 |                   19 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.573 |      0.177 |                   17 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.2621268656716418
[2m[36m(func pid=176697)[0m top5: 0.6487873134328358
[2m[36m(func pid=176697)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=176697)[0m f1_macro: 0.2711262452960762
[2m[36m(func pid=176697)[0m f1_weighted: 0.2550734822099385
[2m[36m(func pid=176697)[0m f1_per_class: [0.326, 0.483, 0.588, 0.444, 0.051, 0.031, 0.0, 0.493, 0.097, 0.198]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.30736940298507465
[2m[36m(func pid=1142)[0m top5: 0.8246268656716418
[2m[36m(func pid=1142)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=1142)[0m f1_macro: 0.15424790479049388
[2m[36m(func pid=1142)[0m f1_weighted: 0.21807464805521398
[2m[36m(func pid=1142)[0m f1_per_class: [0.239, 0.202, 0.431, 0.08, 0.0, 0.0, 0.506, 0.0, 0.085, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.21361940298507462
[2m[36m(func pid=1670)[0m top5: 0.6184701492537313
[2m[36m(func pid=1670)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=1670)[0m f1_macro: 0.16803649979269958
[2m[36m(func pid=1670)[0m f1_weighted: 0.123847507744915
[2m[36m(func pid=1670)[0m f1_per_class: [0.218, 0.41, 0.253, 0.035, 0.123, 0.167, 0.0, 0.262, 0.0, 0.213]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 5.8160 | Steps: 4 | Val loss: 3.0038 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 10.5357 | Steps: 4 | Val loss: 17.5539 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5764 | Steps: 4 | Val loss: 2.0586 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.3969 | Steps: 4 | Val loss: 2.5486 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=2489)[0m top1: 0.324160447761194
[2m[36m(func pid=2489)[0m top5: 0.8791977611940298
[2m[36m(func pid=2489)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=2489)[0m f1_macro: 0.1920450130000988
[2m[36m(func pid=2489)[0m f1_weighted: 0.2768436824229063
[2m[36m(func pid=2489)[0m f1_per_class: [0.13, 0.497, 0.483, 0.048, 0.0, 0.0, 0.564, 0.0, 0.093, 0.105]
[2m[36m(func pid=2489)[0m 
== Status ==
Current time: 2024-01-07 10:34:03 (running for 00:16:33.55)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 10.536 |      0.226 |                   77 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.619 |      0.154 |                   21 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.373 |      0.168 |                   20 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  5.816 |      0.192 |                   18 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.25513059701492535
[2m[36m(func pid=176697)[0m top5: 0.6343283582089553
[2m[36m(func pid=176697)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=176697)[0m f1_macro: 0.22633836673429425
[2m[36m(func pid=176697)[0m f1_weighted: 0.21544677646406654
[2m[36m(func pid=176697)[0m f1_per_class: [0.235, 0.503, 0.157, 0.202, 0.1, 0.296, 0.0, 0.437, 0.145, 0.188]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.33955223880597013
[2m[36m(func pid=1142)[0m top5: 0.8502798507462687
[2m[36m(func pid=1142)[0m f1_micro: 0.33955223880597013
[2m[36m(func pid=1142)[0m f1_macro: 0.13350834387629906
[2m[36m(func pid=1142)[0m f1_weighted: 0.27119282032804515
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.19, 0.247, 0.285, 0.0, 0.0, 0.517, 0.0, 0.095, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.13619402985074627
[2m[36m(func pid=1670)[0m top5: 0.5666977611940298
[2m[36m(func pid=1670)[0m f1_micro: 0.13619402985074627
[2m[36m(func pid=1670)[0m f1_macro: 0.13002542431422884
[2m[36m(func pid=1670)[0m f1_weighted: 0.07889985606288083
[2m[36m(func pid=1670)[0m f1_per_class: [0.247, 0.289, 0.301, 0.0, 0.059, 0.0, 0.0, 0.369, 0.0, 0.034]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 5.2464 | Steps: 4 | Val loss: 3.6932 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 13.3753 | Steps: 4 | Val loss: 13.0999 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.6829 | Steps: 4 | Val loss: 2.0588 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.2419 | Steps: 4 | Val loss: 2.6504 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=2489)[0m top1: 0.25046641791044777
[2m[36m(func pid=2489)[0m top5: 0.7989738805970149
[2m[36m(func pid=2489)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=2489)[0m f1_macro: 0.15389866696671772
[2m[36m(func pid=2489)[0m f1_weighted: 0.22608279310205429
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.286, 0.192, 0.527, 0.0, 0.0, 0.003, 0.446, 0.051, 0.035]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m top1: 0.2126865671641791
[2m[36m(func pid=176697)[0m top5: 0.7574626865671642
[2m[36m(func pid=176697)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=176697)[0m f1_macro: 0.14850094902950325
[2m[36m(func pid=176697)[0m f1_weighted: 0.22468686281521852
[2m[36m(func pid=176697)[0m f1_per_class: [0.2, 0.011, 0.136, 0.212, 0.242, 0.0, 0.51, 0.0, 0.134, 0.039]
[2m[36m(func pid=176697)[0m 
== Status ==
Current time: 2024-01-07 10:34:09 (running for 00:16:38.77)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 13.375 |      0.149 |                   78 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.576 |      0.134 |                   22 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.397 |      0.13  |                   21 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  5.246 |      0.154 |                   19 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.37406716417910446
[2m[36m(func pid=1142)[0m top5: 0.8838619402985075
[2m[36m(func pid=1142)[0m f1_micro: 0.37406716417910446
[2m[36m(func pid=1142)[0m f1_macro: 0.13736616907832572
[2m[36m(func pid=1142)[0m f1_weighted: 0.3170871068790266
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.153, 0.163, 0.461, 0.0, 0.0, 0.534, 0.0, 0.062, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.09188432835820895
[2m[36m(func pid=1670)[0m top5: 0.5219216417910447
[2m[36m(func pid=1670)[0m f1_micro: 0.09188432835820894
[2m[36m(func pid=1670)[0m f1_macro: 0.1441630585156913
[2m[36m(func pid=1670)[0m f1_weighted: 0.07881414601038676
[2m[36m(func pid=1670)[0m f1_per_class: [0.214, 0.213, 0.275, 0.0, 0.033, 0.0, 0.0, 0.531, 0.148, 0.029]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 3.1347 | Steps: 4 | Val loss: 3.8343 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 9.2825 | Steps: 4 | Val loss: 12.9762 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.6329 | Steps: 4 | Val loss: 2.0629 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4707 | Steps: 4 | Val loss: 2.6303 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=2489)[0m top1: 0.11380597014925373
[2m[36m(func pid=2489)[0m top5: 0.8446828358208955
[2m[36m(func pid=2489)[0m f1_micro: 0.11380597014925373
[2m[36m(func pid=2489)[0m f1_macro: 0.08767974889464128
[2m[36m(func pid=2489)[0m f1_weighted: 0.0665791394380836
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.291, 0.178, 0.0, 0.143, 0.008, 0.003, 0.206, 0.0, 0.047]
[2m[36m(func pid=2489)[0m 
== Status ==
Current time: 2024-01-07 10:34:14 (running for 00:16:44.00)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  9.282 |      0.207 |                   79 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.683 |      0.137 |                   23 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.242 |      0.144 |                   22 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.135 |      0.088 |                   20 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.41884328358208955
[2m[36m(func pid=176697)[0m top5: 0.7742537313432836
[2m[36m(func pid=176697)[0m f1_micro: 0.41884328358208955
[2m[36m(func pid=176697)[0m f1_macro: 0.20698838861035046
[2m[36m(func pid=176697)[0m f1_weighted: 0.34987055740456857
[2m[36m(func pid=176697)[0m f1_per_class: [0.292, 0.011, 0.0, 0.57, 0.143, 0.0, 0.537, 0.319, 0.053, 0.145]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.3810634328358209
[2m[36m(func pid=1142)[0m top5: 0.8936567164179104
[2m[36m(func pid=1142)[0m f1_micro: 0.3810634328358209
[2m[36m(func pid=1142)[0m f1_macro: 0.139683320456044
[2m[36m(func pid=1142)[0m f1_weighted: 0.32796444242820494
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.097, 0.124, 0.516, 0.053, 0.008, 0.551, 0.0, 0.019, 0.028]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.06949626865671642
[2m[36m(func pid=1670)[0m top5: 0.542910447761194
[2m[36m(func pid=1670)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=1670)[0m f1_macro: 0.11418827888791597
[2m[36m(func pid=1670)[0m f1_weighted: 0.04785541835451188
[2m[36m(func pid=1670)[0m f1_per_class: [0.118, 0.056, 0.262, 0.0, 0.028, 0.0, 0.0, 0.514, 0.108, 0.056]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 3.8843 | Steps: 4 | Val loss: 4.4016 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 14.8147 | Steps: 4 | Val loss: 14.3015 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.6147 | Steps: 4 | Val loss: 2.0743 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.4711 | Steps: 4 | Val loss: 2.4781 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=2489)[0m top1: 0.24113805970149255
[2m[36m(func pid=2489)[0m top5: 0.8013059701492538
[2m[36m(func pid=2489)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=2489)[0m f1_macro: 0.2230467659998412
[2m[36m(func pid=2489)[0m f1_weighted: 0.2165800359897502
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.302, 0.7, 0.0, 0.058, 0.34, 0.348, 0.255, 0.0, 0.227]
[2m[36m(func pid=2489)[0m 
== Status ==
Current time: 2024-01-07 10:34:19 (running for 00:16:49.16)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 14.815 |      0.195 |                   80 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.633 |      0.14  |                   24 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.471 |      0.114 |                   23 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.884 |      0.223 |                   21 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.18610074626865672
[2m[36m(func pid=176697)[0m top5: 0.8451492537313433
[2m[36m(func pid=176697)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=176697)[0m f1_macro: 0.19481188130977675
[2m[36m(func pid=176697)[0m f1_weighted: 0.1636776034860672
[2m[36m(func pid=176697)[0m f1_per_class: [0.251, 0.494, 0.6, 0.202, 0.063, 0.0, 0.0, 0.195, 0.0, 0.143]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.3358208955223881
[2m[36m(func pid=1142)[0m top5: 0.882929104477612
[2m[36m(func pid=1142)[0m f1_micro: 0.3358208955223881
[2m[36m(func pid=1142)[0m f1_macro: 0.1433451988103789
[2m[36m(func pid=1142)[0m f1_weighted: 0.3156709756744234
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.047, 0.118, 0.514, 0.061, 0.008, 0.517, 0.122, 0.027, 0.02]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.06763059701492537
[2m[36m(func pid=1670)[0m top5: 0.6441231343283582
[2m[36m(func pid=1670)[0m f1_micro: 0.06763059701492537
[2m[36m(func pid=1670)[0m f1_macro: 0.1368982235223843
[2m[36m(func pid=1670)[0m f1_weighted: 0.044587950598821806
[2m[36m(func pid=1670)[0m f1_per_class: [0.263, 0.027, 0.385, 0.0, 0.027, 0.0, 0.0, 0.481, 0.097, 0.089]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 4.0913 | Steps: 4 | Val loss: 7.5784 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 13.4621 | Steps: 4 | Val loss: 15.0215 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.5986 | Steps: 4 | Val loss: 2.0915 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=2489)[0m top1: 0.08069029850746269
[2m[36m(func pid=2489)[0m top5: 0.6777052238805971
[2m[36m(func pid=2489)[0m f1_micro: 0.08069029850746269
[2m[36m(func pid=2489)[0m f1_macro: 0.14180401806188286
[2m[36m(func pid=2489)[0m f1_weighted: 0.0596698890870593
[2m[36m(func pid=2489)[0m f1_per_class: [0.197, 0.021, 0.5, 0.0, 0.027, 0.213, 0.003, 0.337, 0.12, 0.0]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.1375 | Steps: 4 | Val loss: 2.2311 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:34:24 (running for 00:16:54.28)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 13.462 |      0.145 |                   81 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.615 |      0.143 |                   25 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.471 |      0.137 |                   24 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.091 |      0.142 |                   22 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=176697)[0m top1: 0.23041044776119404
[2m[36m(func pid=176697)[0m top5: 0.8572761194029851
[2m[36m(func pid=176697)[0m f1_micro: 0.23041044776119404
[2m[36m(func pid=176697)[0m f1_macro: 0.14520825635081333
[2m[36m(func pid=176697)[0m f1_weighted: 0.20535583194001364
[2m[36m(func pid=176697)[0m f1_per_class: [0.078, 0.462, 0.09, 0.003, 0.108, 0.258, 0.304, 0.0, 0.0, 0.148]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m top1: 0.23880597014925373
[2m[36m(func pid=1142)[0m top5: 0.8418843283582089
[2m[36m(func pid=1142)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=1142)[0m f1_macro: 0.1303243140069526
[2m[36m(func pid=1142)[0m f1_weighted: 0.2366701486528849
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.026, 0.121, 0.487, 0.038, 0.008, 0.246, 0.36, 0.0, 0.016]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.09794776119402986
[2m[36m(func pid=1670)[0m top5: 0.8017723880597015
[2m[36m(func pid=1670)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=1670)[0m f1_macro: 0.13542376872056222
[2m[36m(func pid=1670)[0m f1_weighted: 0.10489955552237425
[2m[36m(func pid=1670)[0m f1_per_class: [0.214, 0.021, 0.522, 0.095, 0.031, 0.128, 0.156, 0.032, 0.102, 0.054]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 5.5877 | Steps: 4 | Val loss: 7.0595 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 11.2424 | Steps: 4 | Val loss: 13.2027 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.5970 | Steps: 4 | Val loss: 2.1250 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=2489)[0m top1: 0.18703358208955223
[2m[36m(func pid=2489)[0m top5: 0.5620335820895522
[2m[36m(func pid=2489)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=2489)[0m f1_macro: 0.18709046405285484
[2m[36m(func pid=2489)[0m f1_weighted: 0.17583332138211952
[2m[36m(func pid=2489)[0m f1_per_class: [0.184, 0.0, 0.136, 0.373, 0.044, 0.291, 0.0, 0.449, 0.138, 0.255]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.2649 | Steps: 4 | Val loss: 1.9994 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=176697)[0m top1: 0.22061567164179105
[2m[36m(func pid=176697)[0m top5: 0.7574626865671642
[2m[36m(func pid=176697)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=176697)[0m f1_macro: 0.1773491874884195
[2m[36m(func pid=176697)[0m f1_weighted: 0.19934595583335996
[2m[36m(func pid=176697)[0m f1_per_class: [0.254, 0.011, 0.178, 0.474, 0.061, 0.242, 0.012, 0.439, 0.026, 0.075]
[2m[36m(func pid=176697)[0m 
== Status ==
Current time: 2024-01-07 10:34:29 (running for 00:16:59.43)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.242 |      0.177 |                   82 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.599 |      0.13  |                   26 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.138 |      0.135 |                   25 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  5.588 |      0.187 |                   23 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.16791044776119404
[2m[36m(func pid=1142)[0m top5: 0.7700559701492538
[2m[36m(func pid=1142)[0m f1_micro: 0.16791044776119404
[2m[36m(func pid=1142)[0m f1_macro: 0.0961717005121599
[2m[36m(func pid=1142)[0m f1_weighted: 0.14761188597235078
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.016, 0.109, 0.44, 0.03, 0.0, 0.003, 0.349, 0.0, 0.015]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.2513992537313433
[2m[36m(func pid=1670)[0m top5: 0.8577425373134329
[2m[36m(func pid=1670)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=1670)[0m f1_macro: 0.2100906845831819
[2m[36m(func pid=1670)[0m f1_weighted: 0.2693118167025894
[2m[36m(func pid=1670)[0m f1_per_class: [0.267, 0.0, 0.5, 0.346, 0.049, 0.345, 0.402, 0.0, 0.114, 0.077]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 5.3554 | Steps: 4 | Val loss: 7.8384 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 12.6819 | Steps: 4 | Val loss: 14.3397 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6228 | Steps: 4 | Val loss: 2.1526 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=2489)[0m top1: 0.2126865671641791
[2m[36m(func pid=2489)[0m top5: 0.5251865671641791
[2m[36m(func pid=2489)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=2489)[0m f1_macro: 0.16187119880192608
[2m[36m(func pid=2489)[0m f1_weighted: 0.20039670176873248
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.0, 0.097, 0.538, 0.214, 0.129, 0.0, 0.526, 0.078, 0.037]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.8970 | Steps: 4 | Val loss: 1.8910 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=176697)[0m top1: 0.2681902985074627
[2m[36m(func pid=176697)[0m top5: 0.8558768656716418
[2m[36m(func pid=176697)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=176697)[0m f1_macro: 0.310531381915396
[2m[36m(func pid=176697)[0m f1_weighted: 0.30262508094771623
[2m[36m(func pid=176697)[0m f1_per_class: [0.094, 0.351, 0.632, 0.343, 0.276, 0.449, 0.18, 0.488, 0.092, 0.2]
[2m[36m(func pid=176697)[0m 
== Status ==
Current time: 2024-01-07 10:34:36 (running for 00:17:05.86)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 12.682 |      0.311 |                   83 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.623 |      0.094 |                   28 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.265 |      0.21  |                   26 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  5.355 |      0.162 |                   24 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.1515858208955224
[2m[36m(func pid=1142)[0m top5: 0.7052238805970149
[2m[36m(func pid=1142)[0m f1_micro: 0.1515858208955224
[2m[36m(func pid=1142)[0m f1_macro: 0.0935863726060234
[2m[36m(func pid=1142)[0m f1_weighted: 0.1393250947489638
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.026, 0.098, 0.403, 0.027, 0.008, 0.0, 0.353, 0.0, 0.021]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.3316231343283582
[2m[36m(func pid=1670)[0m top5: 0.8847947761194029
[2m[36m(func pid=1670)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=1670)[0m f1_macro: 0.2199053322515347
[2m[36m(func pid=1670)[0m f1_weighted: 0.32425908145242344
[2m[36m(func pid=1670)[0m f1_per_class: [0.267, 0.0, 0.4, 0.448, 0.084, 0.406, 0.471, 0.0, 0.124, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 5.5427 | Steps: 4 | Val loss: 5.8241 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 9.5292 | Steps: 4 | Val loss: 12.6195 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.5797 | Steps: 4 | Val loss: 2.1652 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.0599 | Steps: 4 | Val loss: 1.8057 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=2489)[0m top1: 0.26632462686567165
[2m[36m(func pid=2489)[0m top5: 0.6389925373134329
[2m[36m(func pid=2489)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=2489)[0m f1_macro: 0.23943420442265015
[2m[36m(func pid=2489)[0m f1_weighted: 0.238180136144887
[2m[36m(func pid=2489)[0m f1_per_class: [0.331, 0.0, 0.474, 0.547, 0.0, 0.39, 0.0, 0.428, 0.184, 0.041]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m top1: 0.39365671641791045
[2m[36m(func pid=176697)[0m top5: 0.941231343283582
[2m[36m(func pid=176697)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=176697)[0m f1_macro: 0.17514703181745192
[2m[36m(func pid=176697)[0m f1_weighted: 0.36558065904745907
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.51, 0.064, 0.323, 0.0, 0.151, 0.565, 0.0, 0.0, 0.138]
[2m[36m(func pid=176697)[0m 
== Status ==
Current time: 2024-01-07 10:34:41 (running for 00:17:11.14)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  9.529 |      0.175 |                   84 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.58  |      0.086 |                   29 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.897 |      0.22  |                   27 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  5.543 |      0.239 |                   25 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.13386194029850745
[2m[36m(func pid=1142)[0m top5: 0.6511194029850746
[2m[36m(func pid=1142)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=1142)[0m f1_macro: 0.08580135137672404
[2m[36m(func pid=1142)[0m f1_weighted: 0.12570845737153552
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.055, 0.11, 0.346, 0.026, 0.007, 0.0, 0.314, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.36473880597014924
[2m[36m(func pid=1670)[0m top5: 0.9090485074626866
[2m[36m(func pid=1670)[0m f1_micro: 0.36473880597014924
[2m[36m(func pid=1670)[0m f1_macro: 0.19042597212429266
[2m[36m(func pid=1670)[0m f1_weighted: 0.3304932452063185
[2m[36m(func pid=1670)[0m f1_per_class: [0.044, 0.016, 0.286, 0.536, 0.107, 0.368, 0.432, 0.0, 0.115, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 15.9879 | Steps: 4 | Val loss: 11.6756 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 5.0617 | Steps: 4 | Val loss: 5.3957 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.5534 | Steps: 4 | Val loss: 2.1907 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.3814 | Steps: 4 | Val loss: 1.8047 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=176697)[0m top1: 0.34654850746268656
[2m[36m(func pid=176697)[0m top5: 0.8041044776119403
[2m[36m(func pid=176697)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=176697)[0m f1_macro: 0.1979123644759864
[2m[36m(func pid=176697)[0m f1_weighted: 0.36287450466126736
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.335, 0.186, 0.542, 0.174, 0.0, 0.484, 0.0, 0.197, 0.06]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=2489)[0m top1: 0.26632462686567165
[2m[36m(func pid=2489)[0m top5: 0.6357276119402985
[2m[36m(func pid=2489)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=2489)[0m f1_macro: 0.22092569039280266
[2m[36m(func pid=2489)[0m f1_weighted: 0.18012790337443857
[2m[36m(func pid=2489)[0m f1_per_class: [0.107, 0.539, 0.381, 0.0, 0.0, 0.458, 0.0, 0.438, 0.087, 0.2]
[2m[36m(func pid=2489)[0m 
== Status ==
Current time: 2024-01-07 10:34:46 (running for 00:17:16.41)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 15.988 |      0.198 |                   85 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.553 |      0.08  |                   30 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.06  |      0.19  |                   28 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  5.062 |      0.221 |                   26 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.11520522388059702
[2m[36m(func pid=1142)[0m top5: 0.5984141791044776
[2m[36m(func pid=1142)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=1142)[0m f1_macro: 0.07994439827359423
[2m[36m(func pid=1142)[0m f1_weighted: 0.10654266287755922
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.108, 0.115, 0.248, 0.028, 0.007, 0.0, 0.293, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.3451492537313433
[2m[36m(func pid=1670)[0m top5: 0.875
[2m[36m(func pid=1670)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=1670)[0m f1_macro: 0.2083630503987509
[2m[36m(func pid=1670)[0m f1_weighted: 0.33037430025088194
[2m[36m(func pid=1670)[0m f1_per_class: [0.214, 0.172, 0.275, 0.495, 0.108, 0.326, 0.382, 0.047, 0.065, 0.0]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 6.2775 | Steps: 4 | Val loss: 11.3145 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 4.9135 | Steps: 4 | Val loss: 5.0686 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.5040 | Steps: 4 | Val loss: 2.2143 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=176697)[0m top1: 0.20335820895522388
[2m[36m(func pid=176697)[0m top5: 0.7966417910447762
[2m[36m(func pid=176697)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=176697)[0m f1_macro: 0.2193977136328627
[2m[36m(func pid=176697)[0m f1_weighted: 0.19851361572888906
[2m[36m(func pid=176697)[0m f1_per_class: [0.298, 0.394, 0.5, 0.341, 0.035, 0.0, 0.009, 0.311, 0.086, 0.219]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.2752 | Steps: 4 | Val loss: 1.9779 | Batch size: 32 | lr: 0.001 | Duration: 2.73s
[2m[36m(func pid=2489)[0m top1: 0.33908582089552236
[2m[36m(func pid=2489)[0m top5: 0.7835820895522388
[2m[36m(func pid=2489)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=2489)[0m f1_macro: 0.1728654799299175
[2m[36m(func pid=2489)[0m f1_weighted: 0.269215706893444
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.444, 0.186, 0.0, 0.0, 0.446, 0.449, 0.0, 0.203, 0.0]
[2m[36m(func pid=2489)[0m 
== Status ==
Current time: 2024-01-07 10:34:52 (running for 00:17:21.80)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  6.278 |      0.219 |                   86 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.504 |      0.084 |                   31 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.381 |      0.208 |                   29 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.914 |      0.173 |                   27 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.0960820895522388
[2m[36m(func pid=1142)[0m top5: 0.5522388059701493
[2m[36m(func pid=1142)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=1142)[0m f1_macro: 0.08416323597004711
[2m[36m(func pid=1142)[0m f1_weighted: 0.07838711022872487
[2m[36m(func pid=1142)[0m f1_per_class: [0.094, 0.148, 0.174, 0.12, 0.026, 0.0, 0.0, 0.278, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.23180970149253732
[2m[36m(func pid=1670)[0m top5: 0.8166977611940298
[2m[36m(func pid=1670)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=1670)[0m f1_macro: 0.21683235329320763
[2m[36m(func pid=1670)[0m f1_weighted: 0.17914359856746337
[2m[36m(func pid=1670)[0m f1_per_class: [0.301, 0.274, 0.392, 0.007, 0.119, 0.338, 0.171, 0.531, 0.0, 0.036]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 7.7951 | Steps: 4 | Val loss: 10.1016 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 6.5224 | Steps: 4 | Val loss: 3.4976 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.5104 | Steps: 4 | Val loss: 2.2417 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 1.9672 | Steps: 4 | Val loss: 2.2340 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=176697)[0m top1: 0.3521455223880597
[2m[36m(func pid=176697)[0m top5: 0.7854477611940298
[2m[36m(func pid=176697)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=176697)[0m f1_macro: 0.29915675363690863
[2m[36m(func pid=176697)[0m f1_weighted: 0.29755393379707357
[2m[36m(func pid=176697)[0m f1_per_class: [0.253, 0.51, 0.545, 0.0, 0.348, 0.364, 0.465, 0.231, 0.081, 0.194]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=2489)[0m top1: 0.41138059701492535
[2m[36m(func pid=2489)[0m top5: 0.9239738805970149
[2m[36m(func pid=2489)[0m f1_micro: 0.41138059701492535
[2m[36m(func pid=2489)[0m f1_macro: 0.17133983939861142
[2m[36m(func pid=2489)[0m f1_weighted: 0.33654591422522284
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.555, 0.107, 0.122, 0.0, 0.346, 0.557, 0.0, 0.027, 0.0]
[2m[36m(func pid=2489)[0m 
== Status ==
Current time: 2024-01-07 10:34:57 (running for 00:17:27.19)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  7.795 |      0.299 |                   87 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.51  |      0.104 |                   32 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.275 |      0.217 |                   30 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  6.522 |      0.171 |                   28 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.08115671641791045
[2m[36m(func pid=1142)[0m top5: 0.511660447761194
[2m[36m(func pid=1142)[0m f1_micro: 0.08115671641791045
[2m[36m(func pid=1142)[0m f1_macro: 0.10425090732581191
[2m[36m(func pid=1142)[0m f1_weighted: 0.04709409444246846
[2m[36m(func pid=1142)[0m f1_per_class: [0.194, 0.124, 0.408, 0.01, 0.024, 0.0, 0.0, 0.282, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.22014925373134328
[2m[36m(func pid=1670)[0m top5: 0.7541977611940298
[2m[36m(func pid=1670)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=1670)[0m f1_macro: 0.22117713786937626
[2m[36m(func pid=1670)[0m f1_weighted: 0.14271161045607336
[2m[36m(func pid=1670)[0m f1_per_class: [0.263, 0.347, 0.545, 0.0, 0.149, 0.416, 0.0, 0.432, 0.0, 0.059]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 10.2697 | Steps: 4 | Val loss: 9.6866 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 9.2640 | Steps: 4 | Val loss: 3.6466 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.7017 | Steps: 4 | Val loss: 2.2761 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=176697)[0m top1: 0.386660447761194
[2m[36m(func pid=176697)[0m top5: 0.8843283582089553
[2m[36m(func pid=176697)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=176697)[0m f1_macro: 0.26176233648395897
[2m[36m(func pid=176697)[0m f1_weighted: 0.36025786235210155
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.389, 0.136, 0.557, 0.308, 0.38, 0.226, 0.36, 0.0, 0.262]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6903 | Steps: 4 | Val loss: 2.3119 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=2489)[0m top1: 0.4435634328358209
[2m[36m(func pid=2489)[0m top5: 0.871268656716418
[2m[36m(func pid=2489)[0m f1_micro: 0.4435634328358209
[2m[36m(func pid=2489)[0m f1_macro: 0.23846698852985124
[2m[36m(func pid=2489)[0m f1_weighted: 0.372174078581888
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.073, 0.393, 0.576, 0.105, 0.485, 0.406, 0.319, 0.027, 0.0]
[2m[36m(func pid=2489)[0m 
== Status ==
Current time: 2024-01-07 10:35:02 (running for 00:17:32.33)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 10.27  |      0.262 |                   88 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.702 |      0.105 |                   33 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.967 |      0.221 |                   31 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  9.264 |      0.238 |                   29 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.0732276119402985
[2m[36m(func pid=1142)[0m top5: 0.4822761194029851
[2m[36m(func pid=1142)[0m f1_micro: 0.0732276119402985
[2m[36m(func pid=1142)[0m f1_macro: 0.10512975391450755
[2m[36m(func pid=1142)[0m f1_weighted: 0.0372586827858586
[2m[36m(func pid=1142)[0m f1_per_class: [0.203, 0.076, 0.455, 0.0, 0.023, 0.0, 0.0, 0.294, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.22388059701492538
[2m[36m(func pid=1670)[0m top5: 0.7336753731343284
[2m[36m(func pid=1670)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=1670)[0m f1_macro: 0.22311402306049105
[2m[36m(func pid=1670)[0m f1_weighted: 0.14593800706626656
[2m[36m(func pid=1670)[0m f1_per_class: [0.243, 0.399, 0.632, 0.0, 0.129, 0.399, 0.0, 0.364, 0.0, 0.065]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 12.2874 | Steps: 4 | Val loss: 12.4543 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 5.9514 | Steps: 4 | Val loss: 6.8053 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.5747 | Steps: 4 | Val loss: 2.2929 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=176697)[0m top1: 0.21222014925373134
[2m[36m(func pid=176697)[0m top5: 0.8115671641791045
[2m[36m(func pid=176697)[0m f1_micro: 0.21222014925373134
[2m[36m(func pid=176697)[0m f1_macro: 0.1766209037027761
[2m[36m(func pid=176697)[0m f1_weighted: 0.1993925999996597
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.453, 0.393, 0.288, 0.119, 0.186, 0.003, 0.256, 0.0, 0.068]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.1405 | Steps: 4 | Val loss: 2.2420 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=2489)[0m top1: 0.15951492537313433
[2m[36m(func pid=2489)[0m top5: 0.5321828358208955
[2m[36m(func pid=2489)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=2489)[0m f1_macro: 0.11862316470395469
[2m[36m(func pid=2489)[0m f1_weighted: 0.093104906988746
[2m[36m(func pid=2489)[0m f1_per_class: [0.042, 0.073, 0.015, 0.048, 0.159, 0.374, 0.0, 0.311, 0.11, 0.054]
[2m[36m(func pid=2489)[0m 
== Status ==
Current time: 2024-01-07 10:35:07 (running for 00:17:37.68)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 12.287 |      0.177 |                   89 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.575 |      0.093 |                   34 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.69  |      0.223 |                   32 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  5.951 |      0.119 |                   30 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.07182835820895522
[2m[36m(func pid=1142)[0m top5: 0.49347014925373134
[2m[36m(func pid=1142)[0m f1_micro: 0.07182835820895522
[2m[36m(func pid=1142)[0m f1_macro: 0.09292664137103526
[2m[36m(func pid=1142)[0m f1_weighted: 0.03548867431840409
[2m[36m(func pid=1142)[0m f1_per_class: [0.164, 0.061, 0.353, 0.0, 0.024, 0.007, 0.0, 0.321, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.2140858208955224
[2m[36m(func pid=1670)[0m top5: 0.7677238805970149
[2m[36m(func pid=1670)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=1670)[0m f1_macro: 0.19537911008228784
[2m[36m(func pid=1670)[0m f1_weighted: 0.13828956996925287
[2m[36m(func pid=1670)[0m f1_per_class: [0.229, 0.405, 0.5, 0.0, 0.096, 0.323, 0.012, 0.327, 0.0, 0.062]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 16.8029 | Steps: 4 | Val loss: 13.4417 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 4.4230 | Steps: 4 | Val loss: 12.9625 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=176697)[0m top1: 0.25886194029850745
[2m[36m(func pid=176697)[0m top5: 0.8507462686567164
[2m[36m(func pid=176697)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=176697)[0m f1_macro: 0.2196028302521936
[2m[36m(func pid=176697)[0m f1_weighted: 0.23711767917013288
[2m[36m(func pid=176697)[0m f1_per_class: [0.212, 0.471, 0.545, 0.0, 0.079, 0.255, 0.381, 0.045, 0.0, 0.207]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5640 | Steps: 4 | Val loss: 2.3192 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 1.9810 | Steps: 4 | Val loss: 2.0860 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=2489)[0m top1: 0.09001865671641791
[2m[36m(func pid=2489)[0m top5: 0.3736007462686567
[2m[36m(func pid=2489)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=2489)[0m f1_macro: 0.12869442235656103
[2m[36m(func pid=2489)[0m f1_weighted: 0.07239660750062216
[2m[36m(func pid=2489)[0m f1_per_class: [0.093, 0.13, 0.25, 0.0, 0.031, 0.112, 0.0, 0.519, 0.086, 0.067]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m top1: 0.24300373134328357
[2m[36m(func pid=1670)[0m top5: 0.8218283582089553
[2m[36m(func pid=1670)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=1670)[0m f1_macro: 0.16939538469418952
[2m[36m(func pid=1670)[0m f1_weighted: 0.19626775953755243
[2m[36m(func pid=1670)[0m f1_per_class: [0.217, 0.396, 0.169, 0.0, 0.067, 0.032, 0.314, 0.414, 0.0, 0.085]
== Status ==
Current time: 2024-01-07 10:35:13 (running for 00:17:43.01)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 16.803 |      0.22  |                   90 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.575 |      0.093 |                   34 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.981 |      0.169 |                   34 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.423 |      0.129 |                   31 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.06623134328358209
[2m[36m(func pid=1142)[0m top5: 0.4925373134328358
[2m[36m(func pid=1142)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=1142)[0m f1_macro: 0.0726411677087555
[2m[36m(func pid=1142)[0m f1_weighted: 0.03437148132900076
[2m[36m(func pid=1142)[0m f1_per_class: [0.142, 0.048, 0.143, 0.0, 0.024, 0.013, 0.0, 0.356, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 18.8935 | Steps: 4 | Val loss: 10.2286 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 7.6176 | Steps: 4 | Val loss: 15.5699 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=176697)[0m top1: 0.27238805970149255
[2m[36m(func pid=176697)[0m top5: 0.8344216417910447
[2m[36m(func pid=176697)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=176697)[0m f1_macro: 0.2434914732690872
[2m[36m(func pid=176697)[0m f1_weighted: 0.3039239437580656
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.393, 0.606, 0.465, 0.065, 0.2, 0.175, 0.462, 0.0, 0.07]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 1.9995 | Steps: 4 | Val loss: 1.9669 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.7117 | Steps: 4 | Val loss: 2.2977 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=2489)[0m top1: 0.09934701492537314
[2m[36m(func pid=2489)[0m top5: 0.3880597014925373
[2m[36m(func pid=2489)[0m f1_micro: 0.09934701492537314
[2m[36m(func pid=2489)[0m f1_macro: 0.13630233796029137
[2m[36m(func pid=2489)[0m f1_weighted: 0.08714134093083492
[2m[36m(func pid=2489)[0m f1_per_class: [0.37, 0.284, 0.04, 0.0, 0.027, 0.0, 0.0, 0.432, 0.132, 0.077]
[2m[36m(func pid=2489)[0m 
== Status ==
Current time: 2024-01-07 10:35:18 (running for 00:17:48.09)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 18.893 |      0.243 |                   91 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.564 |      0.073 |                   35 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2     |      0.215 |                   35 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  7.618 |      0.136 |                   32 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.32742537313432835
[2m[36m(func pid=1670)[0m top5: 0.8661380597014925
[2m[36m(func pid=1670)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=1670)[0m f1_macro: 0.2147718005351867
[2m[36m(func pid=1670)[0m f1_weighted: 0.3507254455279497
[2m[36m(func pid=1670)[0m f1_per_class: [0.12, 0.402, 0.104, 0.41, 0.057, 0.0, 0.45, 0.489, 0.0, 0.115]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 9.4417 | Steps: 4 | Val loss: 18.2870 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=1142)[0m top1: 0.06436567164179105
[2m[36m(func pid=1142)[0m top5: 0.5139925373134329
[2m[36m(func pid=1142)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=1142)[0m f1_macro: 0.05867833704536192
[2m[36m(func pid=1142)[0m f1_weighted: 0.03203494528743793
[2m[36m(func pid=1142)[0m f1_per_class: [0.154, 0.031, 0.0, 0.0, 0.022, 0.026, 0.0, 0.354, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 9.7608 | Steps: 4 | Val loss: 15.6558 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=176697)[0m top1: 0.14598880597014927
[2m[36m(func pid=176697)[0m top5: 0.7140858208955224
[2m[36m(func pid=176697)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=176697)[0m f1_macro: 0.21206300397006378
[2m[36m(func pid=176697)[0m f1_weighted: 0.12198534083738241
[2m[36m(func pid=176697)[0m f1_per_class: [0.319, 0.46, 0.449, 0.0, 0.341, 0.0, 0.039, 0.254, 0.076, 0.182]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.7839 | Steps: 4 | Val loss: 1.9310 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.6979 | Steps: 4 | Val loss: 2.2963 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=2489)[0m top1: 0.12546641791044777
[2m[36m(func pid=2489)[0m top5: 0.3885261194029851
[2m[36m(func pid=2489)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=2489)[0m f1_macro: 0.1345027191819035
[2m[36m(func pid=2489)[0m f1_weighted: 0.1227513205307907
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.444, 0.026, 0.0, 0.054, 0.204, 0.0, 0.316, 0.031, 0.269]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 9.3255 | Steps: 4 | Val loss: 9.7274 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 10:35:23 (running for 00:17:53.37)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  9.442 |      0.212 |                   92 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.712 |      0.059 |                   36 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.784 |      0.234 |                   36 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  9.761 |      0.135 |                   33 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.36613805970149255
[2m[36m(func pid=1670)[0m top5: 0.8861940298507462
[2m[36m(func pid=1670)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=1670)[0m f1_macro: 0.23356064388978282
[2m[36m(func pid=1670)[0m f1_weighted: 0.37262864887734476
[2m[36m(func pid=1670)[0m f1_per_class: [0.083, 0.213, 0.125, 0.544, 0.056, 0.0, 0.485, 0.505, 0.182, 0.143]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.06203358208955224
[2m[36m(func pid=1142)[0m top5: 0.5396455223880597
[2m[36m(func pid=1142)[0m f1_micro: 0.06203358208955224
[2m[36m(func pid=1142)[0m f1_macro: 0.07495135808106519
[2m[36m(func pid=1142)[0m f1_weighted: 0.03580685133475509
[2m[36m(func pid=1142)[0m f1_per_class: [0.146, 0.045, 0.143, 0.0, 0.023, 0.024, 0.0, 0.369, 0.0, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 6.7620 | Steps: 4 | Val loss: 11.0995 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=176697)[0m top1: 0.40345149253731344
[2m[36m(func pid=176697)[0m top5: 0.8847947761194029
[2m[36m(func pid=176697)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=176697)[0m f1_macro: 0.28418245666562886
[2m[36m(func pid=176697)[0m f1_weighted: 0.4048172176428702
[2m[36m(func pid=176697)[0m f1_per_class: [0.174, 0.331, 0.175, 0.506, 0.29, 0.239, 0.496, 0.377, 0.0, 0.254]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.4828 | Steps: 4 | Val loss: 2.2600 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.2094 | Steps: 4 | Val loss: 1.9646 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=2489)[0m top1: 0.18050373134328357
[2m[36m(func pid=2489)[0m top5: 0.40625
[2m[36m(func pid=2489)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=2489)[0m f1_macro: 0.1885661893200014
[2m[36m(func pid=2489)[0m f1_weighted: 0.14384657778240842
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.401, 0.323, 0.0, 0.2, 0.365, 0.0, 0.443, 0.099, 0.054]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 6.9895 | Steps: 4 | Val loss: 8.1382 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 10:35:28 (running for 00:17:58.66)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  9.326 |      0.284 |                   93 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.698 |      0.075 |                   37 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.209 |      0.207 |                   37 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  6.762 |      0.189 |                   34 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.3316231343283582
[2m[36m(func pid=1670)[0m top5: 0.8978544776119403
[2m[36m(func pid=1670)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=1670)[0m f1_macro: 0.20682420092132775
[2m[36m(func pid=1670)[0m f1_weighted: 0.336041550220159
[2m[36m(func pid=1670)[0m f1_per_class: [0.0, 0.117, 0.386, 0.511, 0.066, 0.031, 0.494, 0.239, 0.146, 0.077]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.06343283582089553
[2m[36m(func pid=1142)[0m top5: 0.5848880597014925
[2m[36m(func pid=1142)[0m f1_micro: 0.06343283582089553
[2m[36m(func pid=1142)[0m f1_macro: 0.10576172195249947
[2m[36m(func pid=1142)[0m f1_weighted: 0.03820500855203829
[2m[36m(func pid=1142)[0m f1_per_class: [0.16, 0.03, 0.375, 0.0, 0.023, 0.028, 0.0, 0.385, 0.057, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 3.6024 | Steps: 4 | Val loss: 6.1656 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=176697)[0m top1: 0.3614738805970149
[2m[36m(func pid=176697)[0m top5: 0.8899253731343284
[2m[36m(func pid=176697)[0m f1_micro: 0.3614738805970149
[2m[36m(func pid=176697)[0m f1_macro: 0.29812575520702295
[2m[36m(func pid=176697)[0m f1_weighted: 0.3247416770206669
[2m[36m(func pid=176697)[0m f1_per_class: [0.313, 0.443, 0.5, 0.545, 0.182, 0.344, 0.072, 0.396, 0.0, 0.188]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.0319 | Steps: 4 | Val loss: 1.9044 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.6170 | Steps: 4 | Val loss: 2.2353 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=2489)[0m top1: 0.23460820895522388
[2m[36m(func pid=2489)[0m top5: 0.6245335820895522
[2m[36m(func pid=2489)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=2489)[0m f1_macro: 0.21841171082849872
[2m[36m(func pid=2489)[0m f1_weighted: 0.2024173530757328
[2m[36m(func pid=2489)[0m f1_per_class: [0.152, 0.083, 0.158, 0.367, 0.281, 0.385, 0.0, 0.522, 0.127, 0.11]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 9.8225 | Steps: 4 | Val loss: 10.6185 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=1670)[0m top1: 0.3423507462686567
[2m[36m(func pid=1670)[0m top5: 0.914179104477612
[2m[36m(func pid=1670)[0m f1_micro: 0.3423507462686567
[2m[36m(func pid=1670)[0m f1_macro: 0.22371051761004143
[2m[36m(func pid=1670)[0m f1_weighted: 0.32589600154727605
[2m[36m(func pid=1670)[0m f1_per_class: [0.0, 0.063, 0.769, 0.516, 0.073, 0.054, 0.513, 0.016, 0.156, 0.077]
== Status ==
Current time: 2024-01-07 10:35:34 (running for 00:18:03.98)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  6.989 |      0.298 |                   94 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.483 |      0.106 |                   38 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.032 |      0.224 |                   38 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.602 |      0.218 |                   35 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.06669776119402986
[2m[36m(func pid=1142)[0m top5: 0.6478544776119403
[2m[36m(func pid=1142)[0m f1_micro: 0.06669776119402986
[2m[36m(func pid=1142)[0m f1_macro: 0.12352717587774846
[2m[36m(func pid=1142)[0m f1_weighted: 0.039867844521524386
[2m[36m(func pid=1142)[0m f1_per_class: [0.188, 0.031, 0.5, 0.0, 0.024, 0.011, 0.0, 0.413, 0.067, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 3.2464 | Steps: 4 | Val loss: 6.0447 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=176697)[0m top1: 0.2943097014925373
[2m[36m(func pid=176697)[0m top5: 0.8111007462686567
[2m[36m(func pid=176697)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=176697)[0m f1_macro: 0.2247529112278815
[2m[36m(func pid=176697)[0m f1_weighted: 0.27380353358878723
[2m[36m(func pid=176697)[0m f1_per_class: [0.235, 0.474, 0.556, 0.007, 0.051, 0.054, 0.537, 0.245, 0.027, 0.063]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.8869 | Steps: 4 | Val loss: 1.7635 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.5142 | Steps: 4 | Val loss: 2.2043 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=2489)[0m top1: 0.38386194029850745
[2m[36m(func pid=2489)[0m top5: 0.7635261194029851
[2m[36m(func pid=2489)[0m f1_micro: 0.38386194029850745
[2m[36m(func pid=2489)[0m f1_macro: 0.23321783685216646
[2m[36m(func pid=2489)[0m f1_weighted: 0.3086044794337285
[2m[36m(func pid=2489)[0m f1_per_class: [0.277, 0.0, 0.538, 0.505, 0.0, 0.333, 0.373, 0.118, 0.028, 0.16]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 5.1585 | Steps: 4 | Val loss: 11.3270 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 10:35:39 (running for 00:18:09.36)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  9.823 |      0.225 |                   95 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.617 |      0.124 |                   39 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.887 |      0.263 |                   39 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.246 |      0.233 |                   36 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.4039179104477612
[2m[36m(func pid=1670)[0m top5: 0.9230410447761194
[2m[36m(func pid=1670)[0m f1_micro: 0.4039179104477612
[2m[36m(func pid=1670)[0m f1_macro: 0.2633420923499244
[2m[36m(func pid=1670)[0m f1_weighted: 0.38005243369814806
[2m[36m(func pid=1670)[0m f1_per_class: [0.333, 0.18, 0.556, 0.564, 0.073, 0.106, 0.537, 0.032, 0.174, 0.077]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.07276119402985075
[2m[36m(func pid=1142)[0m top5: 0.7042910447761194
[2m[36m(func pid=1142)[0m f1_micro: 0.07276119402985075
[2m[36m(func pid=1142)[0m f1_macro: 0.12255933309133485
[2m[36m(func pid=1142)[0m f1_weighted: 0.03940203737040391
[2m[36m(func pid=1142)[0m f1_per_class: [0.218, 0.016, 0.439, 0.0, 0.026, 0.019, 0.0, 0.419, 0.089, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 6.0090 | Steps: 4 | Val loss: 5.0917 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=176697)[0m top1: 0.2681902985074627
[2m[36m(func pid=176697)[0m top5: 0.8073694029850746
[2m[36m(func pid=176697)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=176697)[0m f1_macro: 0.29705065003149417
[2m[36m(func pid=176697)[0m f1_weighted: 0.24456118168495014
[2m[36m(func pid=176697)[0m f1_per_class: [0.327, 0.335, 0.511, 0.363, 0.218, 0.301, 0.009, 0.504, 0.139, 0.262]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.4099 | Steps: 4 | Val loss: 2.1579 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.0540 | Steps: 4 | Val loss: 1.7803 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=2489)[0m top1: 0.4295708955223881
[2m[36m(func pid=2489)[0m top5: 0.7961753731343284
[2m[36m(func pid=2489)[0m f1_micro: 0.4295708955223881
[2m[36m(func pid=2489)[0m f1_macro: 0.2263152693063651
[2m[36m(func pid=2489)[0m f1_weighted: 0.36058876825218783
[2m[36m(func pid=2489)[0m f1_per_class: [0.286, 0.0, 0.161, 0.506, 0.255, 0.295, 0.585, 0.015, 0.0, 0.16]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 5.6678 | Steps: 4 | Val loss: 24.1833 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 10:35:45 (running for 00:18:14.76)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  5.159 |      0.297 |                   96 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.514 |      0.123 |                   40 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.054 |      0.311 |                   40 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  6.009 |      0.226 |                   37 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.07742537313432836
[2m[36m(func pid=1142)[0m top5: 0.7541977611940298
[2m[36m(func pid=1142)[0m f1_micro: 0.07742537313432836
[2m[36m(func pid=1142)[0m f1_macro: 0.11134129508549906
[2m[36m(func pid=1142)[0m f1_weighted: 0.03762134121498843
[2m[36m(func pid=1142)[0m f1_per_class: [0.242, 0.0, 0.319, 0.003, 0.028, 0.034, 0.0, 0.388, 0.1, 0.0]
[2m[36m(func pid=1670)[0m top1: 0.40718283582089554
[2m[36m(func pid=1670)[0m top5: 0.9011194029850746
[2m[36m(func pid=1670)[0m f1_micro: 0.40718283582089554
[2m[36m(func pid=1670)[0m f1_macro: 0.3111668861335225
[2m[36m(func pid=1670)[0m f1_weighted: 0.43007882691727956
[2m[36m(func pid=1670)[0m f1_per_class: [0.192, 0.362, 0.562, 0.501, 0.088, 0.272, 0.549, 0.398, 0.0, 0.188]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 8.9241 | Steps: 4 | Val loss: 6.6236 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=176697)[0m top1: 0.17444029850746268
[2m[36m(func pid=176697)[0m top5: 0.6273320895522388
[2m[36m(func pid=176697)[0m f1_micro: 0.17444029850746268
[2m[36m(func pid=176697)[0m f1_macro: 0.18825971118802523
[2m[36m(func pid=176697)[0m f1_weighted: 0.1922464393069723
[2m[36m(func pid=176697)[0m f1_per_class: [0.071, 0.459, 0.07, 0.0, 0.229, 0.039, 0.243, 0.479, 0.079, 0.214]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.4126 | Steps: 4 | Val loss: 2.1139 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 1.9505 | Steps: 4 | Val loss: 1.8846 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=2489)[0m top1: 0.24766791044776118
[2m[36m(func pid=2489)[0m top5: 0.6473880597014925
[2m[36m(func pid=2489)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=2489)[0m f1_macro: 0.15528219578401678
[2m[36m(func pid=2489)[0m f1_weighted: 0.20921741097748825
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.011, 0.17, 0.007, 0.229, 0.261, 0.542, 0.133, 0.068, 0.133]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 8.1852 | Steps: 4 | Val loss: 14.4625 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 10:35:50 (running for 00:18:20.09)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  5.668 |      0.188 |                   97 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.41  |      0.111 |                   41 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.95  |      0.286 |                   41 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  8.924 |      0.155 |                   38 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.322294776119403
[2m[36m(func pid=1670)[0m top5: 0.8610074626865671
[2m[36m(func pid=1670)[0m f1_micro: 0.322294776119403
[2m[36m(func pid=1670)[0m f1_macro: 0.28554347532784313
[2m[36m(func pid=1670)[0m f1_weighted: 0.31635202466719914
[2m[36m(func pid=1670)[0m f1_per_class: [0.17, 0.481, 0.5, 0.255, 0.112, 0.403, 0.266, 0.477, 0.0, 0.192]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.07369402985074627
[2m[36m(func pid=1142)[0m top5: 0.7924440298507462
[2m[36m(func pid=1142)[0m f1_micro: 0.07369402985074627
[2m[36m(func pid=1142)[0m f1_macro: 0.07731674868509204
[2m[36m(func pid=1142)[0m f1_weighted: 0.034000289832909925
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.0, 0.224, 0.016, 0.028, 0.021, 0.0, 0.382, 0.101, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 5.9412 | Steps: 4 | Val loss: 8.1471 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=176697)[0m top1: 0.240205223880597
[2m[36m(func pid=176697)[0m top5: 0.6763059701492538
[2m[36m(func pid=176697)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=176697)[0m f1_macro: 0.19434823325877223
[2m[36m(func pid=176697)[0m f1_weighted: 0.2419423802595836
[2m[36m(func pid=176697)[0m f1_per_class: [0.19, 0.496, 0.186, 0.003, 0.078, 0.0, 0.41, 0.409, 0.101, 0.069]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.7991 | Steps: 4 | Val loss: 2.1280 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.4931 | Steps: 4 | Val loss: 2.1001 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=2489)[0m top1: 0.17583955223880596
[2m[36m(func pid=2489)[0m top5: 0.5774253731343284
[2m[36m(func pid=2489)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=2489)[0m f1_macro: 0.16397592868963354
[2m[36m(func pid=2489)[0m f1_weighted: 0.10724960400495413
[2m[36m(func pid=2489)[0m f1_per_class: [0.043, 0.108, 0.173, 0.0, 0.204, 0.375, 0.057, 0.323, 0.115, 0.241]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 7.9432 | Steps: 4 | Val loss: 8.8767 | Batch size: 32 | lr: 0.1 | Duration: 2.64s
== Status ==
Current time: 2024-01-07 10:35:55 (running for 00:18:25.40)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  8.185 |      0.194 |                   98 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.413 |      0.077 |                   42 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.799 |      0.194 |                   42 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  5.941 |      0.164 |                   39 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.2513992537313433
[2m[36m(func pid=1670)[0m top5: 0.7957089552238806
[2m[36m(func pid=1670)[0m f1_micro: 0.2513992537313433
[2m[36m(func pid=1670)[0m f1_macro: 0.19447409926996426
[2m[36m(func pid=1670)[0m f1_weighted: 0.154940131263586
[2m[36m(func pid=1670)[0m f1_per_class: [0.195, 0.456, 0.286, 0.0, 0.131, 0.395, 0.003, 0.39, 0.0, 0.088]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.08162313432835822
[2m[36m(func pid=1142)[0m top5: 0.8064365671641791
[2m[36m(func pid=1142)[0m f1_micro: 0.08162313432835822
[2m[36m(func pid=1142)[0m f1_macro: 0.07929851415759495
[2m[36m(func pid=1142)[0m f1_weighted: 0.048339200029213956
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.0, 0.182, 0.057, 0.028, 0.021, 0.009, 0.385, 0.111, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 3.9524 | Steps: 4 | Val loss: 7.9483 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=176697)[0m top1: 0.3451492537313433
[2m[36m(func pid=176697)[0m top5: 0.8526119402985075
[2m[36m(func pid=176697)[0m f1_micro: 0.3451492537313433
[2m[36m(func pid=176697)[0m f1_macro: 0.290347449441135
[2m[36m(func pid=176697)[0m f1_weighted: 0.31690519113485494
[2m[36m(func pid=176697)[0m f1_per_class: [0.345, 0.54, 0.471, 0.418, 0.103, 0.28, 0.116, 0.462, 0.07, 0.1]
[2m[36m(func pid=176697)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.8460 | Steps: 4 | Val loss: 2.2340 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.4502 | Steps: 4 | Val loss: 2.0908 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=2489)[0m top1: 0.24347014925373134
[2m[36m(func pid=2489)[0m top5: 0.5732276119402985
[2m[36m(func pid=2489)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=2489)[0m f1_macro: 0.2271462082654668
[2m[36m(func pid=2489)[0m f1_weighted: 0.17101644135832555
[2m[36m(func pid=2489)[0m f1_per_class: [0.182, 0.513, 0.13, 0.0, 0.206, 0.285, 0.025, 0.509, 0.112, 0.31]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=176697)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 11.2243 | Steps: 4 | Val loss: 11.3732 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 10:36:00 (running for 00:18:30.55)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=7
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (13 PENDING, 4 RUNNING, 7 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00007 | RUNNING    | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  7.943 |      0.29  |                   99 |
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.493 |      0.079 |                   43 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.846 |      0.195 |                   43 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.952 |      0.227 |                   40 |
| train_952df_00011 | PENDING    |                     | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.21315298507462688
[2m[36m(func pid=1670)[0m top5: 0.7924440298507462
[2m[36m(func pid=1670)[0m f1_micro: 0.2131529850746269
[2m[36m(func pid=1670)[0m f1_macro: 0.19466040298932888
[2m[36m(func pid=1670)[0m f1_weighted: 0.1520962251568324
[2m[36m(func pid=1670)[0m f1_per_class: [0.262, 0.422, 0.255, 0.0, 0.133, 0.376, 0.009, 0.437, 0.0, 0.053]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.09328358208955224
[2m[36m(func pid=1142)[0m top5: 0.832089552238806
[2m[36m(func pid=1142)[0m f1_micro: 0.09328358208955224
[2m[36m(func pid=1142)[0m f1_macro: 0.08681674699764164
[2m[36m(func pid=1142)[0m f1_weighted: 0.07445992098543254
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.0, 0.156, 0.106, 0.031, 0.039, 0.045, 0.383, 0.108, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 4.6582 | Steps: 4 | Val loss: 9.2918 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=176697)[0m top1: 0.2080223880597015
[2m[36m(func pid=176697)[0m top5: 0.9104477611940298
[2m[36m(func pid=176697)[0m f1_micro: 0.2080223880597015
[2m[36m(func pid=176697)[0m f1_macro: 0.19512771901479734
[2m[36m(func pid=176697)[0m f1_weighted: 0.22857562928051708
[2m[36m(func pid=176697)[0m f1_per_class: [0.0, 0.345, 0.636, 0.228, 0.057, 0.212, 0.246, 0.0, 0.058, 0.17]
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.2224 | Steps: 4 | Val loss: 2.1505 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.3508 | Steps: 4 | Val loss: 2.0676 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=2489)[0m top1: 0.21875
[2m[36m(func pid=2489)[0m top5: 0.5527052238805971
[2m[36m(func pid=2489)[0m f1_micro: 0.21875
[2m[36m(func pid=2489)[0m f1_macro: 0.17805596342715102
[2m[36m(func pid=2489)[0m f1_weighted: 0.1558714786854213
[2m[36m(func pid=2489)[0m f1_per_class: [0.247, 0.428, 0.476, 0.0, 0.103, 0.173, 0.159, 0.062, 0.051, 0.08]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m top1: 0.21735074626865672
[2m[36m(func pid=1670)[0m top5: 0.8152985074626866
[2m[36m(func pid=1670)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=1670)[0m f1_macro: 0.17584821286813915
[2m[36m(func pid=1670)[0m f1_weighted: 0.151099077198478
[2m[36m(func pid=1670)[0m f1_per_class: [0.044, 0.408, 0.314, 0.0, 0.112, 0.369, 0.034, 0.42, 0.0, 0.057]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.12639925373134328
[2m[36m(func pid=1142)[0m top5: 0.8227611940298507
[2m[36m(func pid=1142)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=1142)[0m f1_macro: 0.10499389157414854
[2m[36m(func pid=1142)[0m f1_weighted: 0.12947448892990449
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.0, 0.161, 0.184, 0.034, 0.068, 0.154, 0.341, 0.108, 0.0]
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 6.0309 | Steps: 4 | Val loss: 10.7183 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.5639 | Steps: 4 | Val loss: 1.9452 | Batch size: 32 | lr: 0.001 | Duration: 2.76s
[2m[36m(func pid=2489)[0m top1: 0.1166044776119403
[2m[36m(func pid=2489)[0m top5: 0.7145522388059702
[2m[36m(func pid=2489)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=2489)[0m f1_macro: 0.1630437362568407
[2m[36m(func pid=2489)[0m f1_weighted: 0.09649785698300879
[2m[36m(func pid=2489)[0m f1_per_class: [0.345, 0.366, 0.667, 0.0, 0.066, 0.0, 0.062, 0.0, 0.082, 0.042]
== Status ==
Current time: 2024-01-07 10:36:05 (running for 00:18:35.66)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.45  |      0.087 |                   44 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.222 |      0.176 |                   44 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.658 |      0.178 |                   41 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=12671)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=12671)[0m Configuration completed!
[2m[36m(func pid=12671)[0m New optimizer parameters:
[2m[36m(func pid=12671)[0m SGD (
[2m[36m(func pid=12671)[0m Parameter Group 0
[2m[36m(func pid=12671)[0m     dampening: 0
[2m[36m(func pid=12671)[0m     differentiable: False
[2m[36m(func pid=12671)[0m     foreach: None
[2m[36m(func pid=12671)[0m     lr: 0.1
[2m[36m(func pid=12671)[0m     maximize: False
[2m[36m(func pid=12671)[0m     momentum: 0.99
[2m[36m(func pid=12671)[0m     nesterov: False
[2m[36m(func pid=12671)[0m     weight_decay: 0.0001
[2m[36m(func pid=12671)[0m )
[2m[36m(func pid=12671)[0m 
== Status ==
Current time: 2024-01-07 10:36:11 (running for 00:18:40.74)
Memory usage on this node: 24.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.351 |      0.105 |                   45 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.564 |      0.212 |                   45 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  6.031 |      0.163 |                   42 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.23833955223880596
[2m[36m(func pid=1670)[0m top5: 0.8465485074626866
[2m[36m(func pid=1670)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=1670)[0m f1_macro: 0.2120980385733368
[2m[36m(func pid=1670)[0m f1_weighted: 0.2024169809497955
[2m[36m(func pid=1670)[0m f1_per_class: [0.087, 0.331, 0.386, 0.056, 0.075, 0.337, 0.189, 0.456, 0.076, 0.128]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.2836 | Steps: 4 | Val loss: 2.0424 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 6.9360 | Steps: 4 | Val loss: 8.7802 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.7513 | Steps: 4 | Val loss: 1.9059 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 6.4402 | Steps: 4 | Val loss: 4.1966 | Batch size: 32 | lr: 0.1 | Duration: 4.35s
[2m[36m(func pid=1142)[0m top1: 0.16744402985074627
[2m[36m(func pid=1142)[0m top5: 0.8288246268656716
[2m[36m(func pid=1142)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=1142)[0m f1_macro: 0.12500255847595482
[2m[36m(func pid=1142)[0m f1_weighted: 0.18703461898878795
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.0, 0.171, 0.284, 0.037, 0.088, 0.252, 0.309, 0.109, 0.0]
[2m[36m(func pid=1142)[0m 
== Status ==
Current time: 2024-01-07 10:36:16 (running for 00:18:45.77)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.284 |      0.125 |                   46 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.564 |      0.212 |                   45 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  6.936 |      0.177 |                   43 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 |        |            |                      |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=2489)[0m top1: 0.20009328358208955
[2m[36m(func pid=2489)[0m top5: 0.738339552238806
[2m[36m(func pid=2489)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=2489)[0m f1_macro: 0.17746526381271904
[2m[36m(func pid=2489)[0m f1_weighted: 0.2276553465522172
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.3, 0.556, 0.43, 0.029, 0.0, 0.153, 0.0, 0.158, 0.15]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m top1: 0.29151119402985076
[2m[36m(func pid=1670)[0m top5: 0.9011194029850746
[2m[36m(func pid=1670)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=1670)[0m f1_macro: 0.27520929064193794
[2m[36m(func pid=1670)[0m f1_weighted: 0.3239495386498652
[2m[36m(func pid=1670)[0m f1_per_class: [0.0, 0.218, 0.733, 0.435, 0.054, 0.309, 0.305, 0.502, 0.121, 0.074]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=12671)[0m top1: 0.23600746268656717
[2m[36m(func pid=12671)[0m top5: 0.5573694029850746
[2m[36m(func pid=12671)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=12671)[0m f1_macro: 0.0551681302489081
[2m[36m(func pid=12671)[0m f1_weighted: 0.11151387442039314
[2m[36m(func pid=12671)[0m f1_per_class: [0.164, 0.0, 0.0, 0.388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.5321 | Steps: 4 | Val loss: 2.0272 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 6.0770 | Steps: 4 | Val loss: 8.1432 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.0149 | Steps: 4 | Val loss: 1.9624 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 18.3791 | Steps: 4 | Val loss: 13.9549 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=1142)[0m top1: 0.22154850746268656== Status ==
Current time: 2024-01-07 10:36:21 (running for 00:18:50.77)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.532 |      0.142 |                   47 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.751 |      0.275 |                   46 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  6.936 |      0.177 |                   43 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 |  6.44  |      0.055 |                    1 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)



[2m[36m(func pid=1142)[0m top5: 0.8353544776119403
[2m[36m(func pid=1142)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=1142)[0m f1_macro: 0.14232594605426377
[2m[36m(func pid=1142)[0m f1_weighted: 0.24394563610867062
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.0, 0.147, 0.376, 0.044, 0.085, 0.36, 0.303, 0.109, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.18796641791044777
[2m[36m(func pid=2489)[0m top5: 0.777518656716418
[2m[36m(func pid=2489)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=2489)[0m f1_macro: 0.2091487222393042
[2m[36m(func pid=2489)[0m f1_weighted: 0.21318116006678955
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.253, 0.632, 0.452, 0.035, 0.105, 0.024, 0.216, 0.161, 0.214]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m top1: 0.2751865671641791
[2m[36m(func pid=1670)[0m top5: 0.9076492537313433
[2m[36m(func pid=1670)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=1670)[0m f1_macro: 0.24679358158327314
[2m[36m(func pid=1670)[0m f1_weighted: 0.29765649557597085
[2m[36m(func pid=1670)[0m f1_per_class: [0.0, 0.021, 0.688, 0.488, 0.043, 0.261, 0.308, 0.465, 0.117, 0.077]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=12671)[0m top1: 0.10354477611940298
[2m[36m(func pid=12671)[0m top5: 0.36473880597014924
[2m[36m(func pid=12671)[0m f1_micro: 0.10354477611940298
[2m[36m(func pid=12671)[0m f1_macro: 0.03259854728078602
[2m[36m(func pid=12671)[0m f1_weighted: 0.03485085914529682
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.303, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.4620 | Steps: 4 | Val loss: 2.0152 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.5800 | Steps: 4 | Val loss: 1.9919 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.8278 | Steps: 4 | Val loss: 7.5042 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 35.5285 | Steps: 4 | Val loss: 17.7440 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 10:36:26 (running for 00:18:56.11)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.462 |      0.139 |                   48 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.015 |      0.247 |                   47 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  6.077 |      0.209 |                   44 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 18.379 |      0.033 |                    2 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.24207089552238806
[2m[36m(func pid=1142)[0m top5: 0.8386194029850746
[2m[36m(func pid=1142)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=1142)[0m f1_macro: 0.13936400609284652
[2m[36m(func pid=1142)[0m f1_weighted: 0.24669468215029464
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.0, 0.138, 0.402, 0.058, 0.024, 0.369, 0.289, 0.113, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.30130597014925375
[2m[36m(func pid=1670)[0m top5: 0.8833955223880597
[2m[36m(func pid=1670)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=1670)[0m f1_macro: 0.28215938368393123
[2m[36m(func pid=1670)[0m f1_weighted: 0.28839892059375116
[2m[36m(func pid=1670)[0m f1_per_class: [0.38, 0.0, 0.667, 0.555, 0.042, 0.249, 0.196, 0.486, 0.17, 0.077]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m top1: 0.1875
[2m[36m(func pid=2489)[0m top5: 0.7066231343283582
[2m[36m(func pid=2489)[0m f1_micro: 0.1875
[2m[36m(func pid=2489)[0m f1_macro: 0.23361720513551343
[2m[36m(func pid=2489)[0m f1_weighted: 0.15750220274629478
[2m[36m(func pid=2489)[0m f1_per_class: [0.218, 0.428, 0.552, 0.06, 0.079, 0.206, 0.003, 0.513, 0.063, 0.214]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m top1: 0.17117537313432835
[2m[36m(func pid=12671)[0m top5: 0.5522388059701493
[2m[36m(func pid=12671)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=12671)[0m f1_macro: 0.03171515537622991
[2m[36m(func pid=12671)[0m f1_weighted: 0.05131397324272669
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.5108 | Steps: 4 | Val loss: 1.9805 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.1577 | Steps: 4 | Val loss: 2.0634 | Batch size: 32 | lr: 0.001 | Duration: 2.74s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.6475 | Steps: 4 | Val loss: 7.7740 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 38.1405 | Steps: 4 | Val loss: 12.8160 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 10:36:31 (running for 00:19:01.45)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.462 |      0.139 |                   48 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.158 |      0.236 |                   49 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  2.828 |      0.234 |                   45 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 35.528 |      0.032 |                    3 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.23973880597014927
[2m[36m(func pid=1142)[0m top5: 0.8526119402985075
[2m[36m(func pid=1142)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=1142)[0m f1_macro: 0.135646905209039
[2m[36m(func pid=1142)[0m f1_weighted: 0.2366415298153946
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.016, 0.176, 0.426, 0.059, 0.024, 0.312, 0.258, 0.085, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.2905783582089552
[2m[36m(func pid=1670)[0m top5: 0.8726679104477612
[2m[36m(func pid=1670)[0m f1_micro: 0.2905783582089552
[2m[36m(func pid=1670)[0m f1_macro: 0.23630012487948931
[2m[36m(func pid=1670)[0m f1_weighted: 0.2515556603916244
[2m[36m(func pid=1670)[0m f1_per_class: [0.296, 0.0, 0.56, 0.553, 0.046, 0.254, 0.103, 0.448, 0.026, 0.077]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m top1: 0.21128731343283583
[2m[36m(func pid=2489)[0m top5: 0.6632462686567164
[2m[36m(func pid=2489)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=2489)[0m f1_macro: 0.2023568923840377
[2m[36m(func pid=2489)[0m f1_weighted: 0.16263856935555934
[2m[36m(func pid=2489)[0m f1_per_class: [0.098, 0.513, 0.286, 0.013, 0.184, 0.392, 0.0, 0.299, 0.032, 0.207]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m top1: 0.03171641791044776
[2m[36m(func pid=12671)[0m top5: 0.6972947761194029
[2m[36m(func pid=12671)[0m f1_micro: 0.03171641791044776
[2m[36m(func pid=12671)[0m f1_macro: 0.006153846153846154
[2m[36m(func pid=12671)[0m f1_weighted: 0.0020378874856486796
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.062, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.2335 | Steps: 4 | Val loss: 2.0925 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.5087 | Steps: 4 | Val loss: 1.9765 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 3.9266 | Steps: 4 | Val loss: 6.9273 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 39.8080 | Steps: 4 | Val loss: 17.7603 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
== Status ==
Current time: 2024-01-07 10:36:37 (running for 00:19:06.74)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.511 |      0.136 |                   49 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.234 |      0.221 |                   50 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  2.647 |      0.202 |                   46 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 38.141 |      0.006 |                    4 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.27845149253731344
[2m[36m(func pid=1670)[0m top5: 0.8638059701492538
[2m[36m(func pid=1670)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=1670)[0m f1_macro: 0.22081070618344553
[2m[36m(func pid=1670)[0m f1_weighted: 0.2471749701430371
[2m[36m(func pid=1670)[0m f1_per_class: [0.198, 0.0, 0.483, 0.518, 0.054, 0.326, 0.107, 0.427, 0.027, 0.069]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.24113805970149255
[2m[36m(func pid=1142)[0m top5: 0.863339552238806
[2m[36m(func pid=1142)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=1142)[0m f1_macro: 0.1327395502077786
[2m[36m(func pid=1142)[0m f1_weighted: 0.23597510619678608
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.057, 0.156, 0.442, 0.063, 0.008, 0.281, 0.247, 0.074, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.2019589552238806
[2m[36m(func pid=2489)[0m top5: 0.7271455223880597
[2m[36m(func pid=2489)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=2489)[0m f1_macro: 0.19452519885768832
[2m[36m(func pid=2489)[0m f1_weighted: 0.16948592705111867
[2m[36m(func pid=2489)[0m f1_per_class: [0.082, 0.511, 0.085, 0.042, 0.34, 0.365, 0.018, 0.213, 0.107, 0.182]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m top1: 0.12173507462686567
[2m[36m(func pid=12671)[0m top5: 0.8148320895522388
[2m[36m(func pid=12671)[0m f1_micro: 0.12173507462686567
[2m[36m(func pid=12671)[0m f1_macro: 0.04735549110569849
[2m[36m(func pid=12671)[0m f1_weighted: 0.12513709372957083
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.0, 0.0, 0.448, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.8880 | Steps: 4 | Val loss: 2.0101 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.4924 | Steps: 4 | Val loss: 1.9741 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 4.9887 | Steps: 4 | Val loss: 4.9908 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 44.9574 | Steps: 4 | Val loss: 31.6459 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=1670)[0m top1: 0.28451492537313433
[2m[36m(func pid=1670)[0m top5: 0.8624067164179104
[2m[36m(func pid=1670)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=1670)[0m f1_macro: 0.2146833254658616
[2m[36m(func pid=1670)[0m f1_weighted: 0.2741928559697432
[2m[36m(func pid=1670)[0m f1_per_class: [0.183, 0.032, 0.186, 0.481, 0.07, 0.362, 0.204, 0.412, 0.027, 0.189]
== Status ==
Current time: 2024-01-07 10:36:42 (running for 00:19:12.04)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.509 |      0.133 |                   50 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.888 |      0.215 |                   51 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.927 |      0.195 |                   47 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 39.808 |      0.047 |                    5 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1142)[0m top1: 0.25093283582089554
[2m[36m(func pid=1142)[0m top5: 0.867070895522388
[2m[36m(func pid=1142)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=1142)[0m f1_macro: 0.1387300818591671
[2m[36m(func pid=1142)[0m f1_weighted: 0.2524305419710986
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.172, 0.15, 0.464, 0.067, 0.0, 0.258, 0.236, 0.04, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=12671)[0m top1: 0.17210820895522388
[2m[36m(func pid=12671)[0m top5: 0.6730410447761194
[2m[36m(func pid=12671)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=12671)[0m f1_macro: 0.02939068100358423
[2m[36m(func pid=12671)[0m f1_weighted: 0.05058377467501204
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=2489)[0m top1: 0.40345149253731344
[2m[36m(func pid=2489)[0m top5: 0.8931902985074627
[2m[36m(func pid=2489)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=2489)[0m f1_macro: 0.26345700523199694
[2m[36m(func pid=2489)[0m f1_weighted: 0.4236587353941122
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.459, 0.093, 0.479, 0.378, 0.249, 0.557, 0.157, 0.055, 0.208]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.3541 | Steps: 4 | Val loss: 1.9793 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.4324 | Steps: 4 | Val loss: 2.1583 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 50.0897 | Steps: 4 | Val loss: 43.1525 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 3.8319 | Steps: 4 | Val loss: 5.1115 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 10:36:47 (running for 00:19:17.41)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.492 |      0.139 |                   51 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.888 |      0.215 |                   51 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.989 |      0.263 |                   48 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 44.957 |      0.029 |                    6 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.1921641791044776
[2m[36m(func pid=1670)[0m top5: 0.8031716417910447
[2m[36m(func pid=1670)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=1670)[0m f1_macro: 0.19465164250644085
[2m[36m(func pid=1670)[0m f1_weighted: 0.20819356711744305
[2m[36m(func pid=1670)[0m f1_per_class: [0.25, 0.284, 0.113, 0.094, 0.143, 0.326, 0.204, 0.465, 0.024, 0.044]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.27005597014925375
[2m[36m(func pid=1142)[0m top5: 0.8605410447761194
[2m[36m(func pid=1142)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=1142)[0m f1_macro: 0.17559758455339644
[2m[36m(func pid=1142)[0m f1_weighted: 0.28051712235437193
[2m[36m(func pid=1142)[0m f1_per_class: [0.086, 0.26, 0.154, 0.457, 0.074, 0.0, 0.295, 0.249, 0.023, 0.158]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=12671)[0m top1: 0.1142723880597015
[2m[36m(func pid=12671)[0m top5: 0.6543843283582089
[2m[36m(func pid=12671)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=12671)[0m f1_macro: 0.06737862061392563
[2m[36m(func pid=12671)[0m f1_weighted: 0.032518452835671605
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.0, 0.368, 0.0, 0.017, 0.239, 0.0, 0.049, 0.0, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=2489)[0m top1: 0.45009328358208955
[2m[36m(func pid=2489)[0m top5: 0.8824626865671642
[2m[36m(func pid=2489)[0m f1_micro: 0.45009328358208955
[2m[36m(func pid=2489)[0m f1_macro: 0.2671596673687709
[2m[36m(func pid=2489)[0m f1_weighted: 0.39923178539519166
[2m[36m(func pid=2489)[0m f1_per_class: [0.246, 0.24, 0.4, 0.58, 0.25, 0.278, 0.512, 0.0, 0.022, 0.144]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.6481 | Steps: 4 | Val loss: 2.5504 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 61.3906 | Steps: 4 | Val loss: 56.9594 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.3196 | Steps: 4 | Val loss: 1.9854 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 6.4683 | Steps: 4 | Val loss: 6.2343 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 10:36:53 (running for 00:19:22.91)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.354 |      0.176 |                   52 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.648 |      0.162 |                   53 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.832 |      0.267 |                   49 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 50.09  |      0.067 |                    7 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.11240671641791045
[2m[36m(func pid=1670)[0m top5: 0.7444029850746269
[2m[36m(func pid=1670)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=1670)[0m f1_macro: 0.161925050536498
[2m[36m(func pid=1670)[0m f1_weighted: 0.1046482887935421
[2m[36m(func pid=1670)[0m f1_per_class: [0.044, 0.269, 0.333, 0.0, 0.295, 0.066, 0.049, 0.532, 0.0, 0.03]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.27098880597014924
[2m[36m(func pid=1142)[0m top5: 0.8521455223880597
[2m[36m(func pid=1142)[0m f1_micro: 0.27098880597014924
[2m[36m(func pid=1142)[0m f1_macro: 0.19618011545917965
[2m[36m(func pid=1142)[0m f1_weighted: 0.2800735295296012
[2m[36m(func pid=1142)[0m f1_per_class: [0.205, 0.329, 0.275, 0.391, 0.08, 0.0, 0.306, 0.27, 0.0, 0.107]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=12671)[0m top1: 0.025652985074626867
[2m[36m(func pid=12671)[0m top5: 0.6655783582089553
[2m[36m(func pid=12671)[0m f1_micro: 0.025652985074626867
[2m[36m(func pid=12671)[0m f1_macro: 0.025024565231390468
[2m[36m(func pid=12671)[0m f1_weighted: 0.026605991919934104
[2m[36m(func pid=12671)[0m f1_per_class: [0.156, 0.0, 0.0, 0.0, 0.017, 0.0, 0.078, 0.0, 0.0, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=2489)[0m top1: 0.34794776119402987
[2m[36m(func pid=2489)[0m top5: 0.8115671641791045
[2m[36m(func pid=2489)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=2489)[0m f1_macro: 0.30331650632054663
[2m[36m(func pid=2489)[0m f1_weighted: 0.34616610700020506
[2m[36m(func pid=2489)[0m f1_per_class: [0.133, 0.224, 0.632, 0.537, 0.296, 0.421, 0.265, 0.319, 0.052, 0.154]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 74.3386 | Steps: 4 | Val loss: 30.5696 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.7876 | Steps: 4 | Val loss: 2.6377 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.5625 | Steps: 4 | Val loss: 2.0054 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 4.2916 | Steps: 4 | Val loss: 7.8862 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 10:36:58 (running for 00:19:28.08)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.32  |      0.196 |                   53 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.648 |      0.162 |                   53 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  6.468 |      0.303 |                   50 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 74.339 |      0.059 |                    9 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.14365671641791045
[2m[36m(func pid=1670)[0m top5: 0.7089552238805971
[2m[36m(func pid=1670)[0m f1_micro: 0.14365671641791045
[2m[36m(func pid=1670)[0m f1_macro: 0.21013736277442896
[2m[36m(func pid=1670)[0m f1_weighted: 0.10904974673749537
[2m[36m(func pid=1670)[0m f1_per_class: [0.044, 0.305, 0.667, 0.0, 0.304, 0.105, 0.009, 0.535, 0.094, 0.038]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=12671)[0m top1: 0.12360074626865672
[2m[36m(func pid=12671)[0m top5: 0.7672574626865671
[2m[36m(func pid=12671)[0m f1_micro: 0.12360074626865672
[2m[36m(func pid=12671)[0m f1_macro: 0.05883396062339195
[2m[36m(func pid=12671)[0m f1_weighted: 0.09722263015538195
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.0, 0.0, 0.301, 0.039, 0.0, 0.0, 0.187, 0.061, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1142)[0m top1: 0.23973880597014927
[2m[36m(func pid=1142)[0m top5: 0.8414179104477612
[2m[36m(func pid=1142)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=1142)[0m f1_macro: 0.1943720996503903
[2m[36m(func pid=1142)[0m f1_weighted: 0.2365683392149584
[2m[36m(func pid=1142)[0m f1_per_class: [0.197, 0.353, 0.417, 0.243, 0.083, 0.0, 0.28, 0.284, 0.0, 0.088]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.23134328358208955
[2m[36m(func pid=2489)[0m top5: 0.6702425373134329
[2m[36m(func pid=2489)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=2489)[0m f1_macro: 0.2458720404986674
[2m[36m(func pid=2489)[0m f1_weighted: 0.22360207367128268
[2m[36m(func pid=2489)[0m f1_per_class: [0.115, 0.436, 0.375, 0.262, 0.261, 0.303, 0.0, 0.541, 0.048, 0.118]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 63.5625 | Steps: 4 | Val loss: 42.1061 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.0704 | Steps: 4 | Val loss: 2.5667 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.6571 | Steps: 4 | Val loss: 2.0443 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 4.9397 | Steps: 4 | Val loss: 9.9816 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=12671)[0m top1: 0.25652985074626866
[2m[36m(func pid=12671)[0m top5: 0.7611940298507462
[2m[36m(func pid=12671)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=12671)[0m f1_macro: 0.06635518952592123
[2m[36m(func pid=12671)[0m f1_weighted: 0.12169848413932868
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.0, 0.146, 0.411, 0.0, 0.0, 0.0, 0.106, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 10:37:03 (running for 00:19:33.14)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.562 |      0.194 |                   54 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.788 |      0.21  |                   54 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.292 |      0.246 |                   51 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 63.562 |      0.066 |                   10 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.20662313432835822
[2m[36m(func pid=1670)[0m top5: 0.6702425373134329
[2m[36m(func pid=1670)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=1670)[0m f1_macro: 0.2269491310590081
[2m[36m(func pid=1670)[0m f1_weighted: 0.15291840309935198
[2m[36m(func pid=1670)[0m f1_per_class: [0.143, 0.399, 0.274, 0.0, 0.367, 0.31, 0.012, 0.572, 0.116, 0.076]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.20988805970149255
[2m[36m(func pid=1142)[0m top5: 0.8311567164179104
[2m[36m(func pid=1142)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=1142)[0m f1_macro: 0.18780851470085042
[2m[36m(func pid=1142)[0m f1_weighted: 0.19489780109654462
[2m[36m(func pid=1142)[0m f1_per_class: [0.174, 0.343, 0.5, 0.092, 0.098, 0.0, 0.277, 0.334, 0.0, 0.059]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.22621268656716417
[2m[36m(func pid=2489)[0m top5: 0.5928171641791045
[2m[36m(func pid=2489)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=2489)[0m f1_macro: 0.2700644422742257
[2m[36m(func pid=2489)[0m f1_weighted: 0.15920651967903404
[2m[36m(func pid=2489)[0m f1_per_class: [0.412, 0.463, 0.632, 0.0, 0.19, 0.273, 0.0, 0.505, 0.128, 0.098]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 59.0095 | Steps: 4 | Val loss: 54.1566 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.4595 | Steps: 4 | Val loss: 2.3252 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.4853 | Steps: 4 | Val loss: 2.0741 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 6.0416 | Steps: 4 | Val loss: 10.8980 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 10:37:08 (running for 00:19:38.57)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.657 |      0.188 |                   55 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.07  |      0.227 |                   55 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.94  |      0.27  |                   52 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 59.01  |      0.132 |                   11 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.18703358208955223
[2m[36m(func pid=12671)[0m top5: 0.6203358208955224
[2m[36m(func pid=12671)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=12671)[0m f1_macro: 0.13205676230574148
[2m[36m(func pid=12671)[0m f1_weighted: 0.0934545261716234
[2m[36m(func pid=12671)[0m f1_per_class: [0.233, 0.39, 0.439, 0.0, 0.0, 0.1, 0.0, 0.121, 0.0, 0.037]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.23414179104477612
[2m[36m(func pid=1670)[0m top5: 0.7532649253731343
[2m[36m(func pid=1670)[0m f1_micro: 0.23414179104477612
[2m[36m(func pid=1670)[0m f1_macro: 0.24336794380212107
[2m[36m(func pid=1670)[0m f1_weighted: 0.2147131797807852
[2m[36m(func pid=1670)[0m f1_per_class: [0.273, 0.461, 0.072, 0.0, 0.24, 0.422, 0.144, 0.526, 0.116, 0.181]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.17350746268656717
[2m[36m(func pid=1142)[0m top5: 0.8134328358208955
[2m[36m(func pid=1142)[0m f1_micro: 0.17350746268656717
[2m[36m(func pid=1142)[0m f1_macro: 0.1702485535755596
[2m[36m(func pid=1142)[0m f1_weighted: 0.14066514593825086
[2m[36m(func pid=1142)[0m f1_per_class: [0.172, 0.337, 0.48, 0.013, 0.107, 0.016, 0.161, 0.372, 0.0, 0.044]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.24766791044776118
[2m[36m(func pid=2489)[0m top5: 0.5806902985074627
[2m[36m(func pid=2489)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=2489)[0m f1_macro: 0.2351563559519744
[2m[36m(func pid=2489)[0m f1_weighted: 0.15119615504904504
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.445, 0.621, 0.0, 0.265, 0.302, 0.0, 0.497, 0.139, 0.083]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.9668 | Steps: 4 | Val loss: 2.2054 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 66.0880 | Steps: 4 | Val loss: 79.0901 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.4506 | Steps: 4 | Val loss: 2.0972 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 3.8041 | Steps: 4 | Val loss: 8.8160 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 10:37:14 (running for 00:19:43.94)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.485 |      0.17  |                   56 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.459 |      0.243 |                   56 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  6.042 |      0.235 |                   53 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 66.088 |      0.097 |                   12 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.2271455223880597
[2m[36m(func pid=12671)[0m top5: 0.6744402985074627
[2m[36m(func pid=12671)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=12671)[0m f1_macro: 0.0968080403229018
[2m[36m(func pid=12671)[0m f1_weighted: 0.12736128127134874
[2m[36m(func pid=12671)[0m f1_per_class: [0.168, 0.378, 0.0, 0.0, 0.0, 0.364, 0.058, 0.0, 0.0, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.24253731343283583
[2m[36m(func pid=1670)[0m top5: 0.8222947761194029
[2m[36m(func pid=1670)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=1670)[0m f1_macro: 0.2342800884147898
[2m[36m(func pid=1670)[0m f1_weighted: 0.25453038358315516
[2m[36m(func pid=1670)[0m f1_per_class: [0.202, 0.461, 0.045, 0.032, 0.202, 0.414, 0.295, 0.316, 0.116, 0.26]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.15298507462686567
[2m[36m(func pid=1142)[0m top5: 0.8017723880597015
[2m[36m(func pid=1142)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=1142)[0m f1_macro: 0.17734549415376666
[2m[36m(func pid=1142)[0m f1_weighted: 0.11604620603694586
[2m[36m(func pid=1142)[0m f1_per_class: [0.172, 0.336, 0.545, 0.003, 0.111, 0.096, 0.045, 0.429, 0.0, 0.035]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.27472014925373134
[2m[36m(func pid=2489)[0m top5: 0.6674440298507462
[2m[36m(func pid=2489)[0m f1_micro: 0.27472014925373134
[2m[36m(func pid=2489)[0m f1_macro: 0.2211400640068495
[2m[36m(func pid=2489)[0m f1_weighted: 0.2575347713136284
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.462, 0.242, 0.415, 0.057, 0.238, 0.0, 0.46, 0.122, 0.215]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 104.0741 | Steps: 4 | Val loss: 82.4480 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.3459 | Steps: 4 | Val loss: 1.8901 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.4320 | Steps: 4 | Val loss: 2.1237 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 7.1514 | Steps: 4 | Val loss: 9.8500 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=12671)[0m top1: 0.291044776119403
[2m[36m(func pid=12671)[0m top5: 0.6865671641791045
[2m[36m(func pid=12671)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=12671)[0m f1_macro: 0.19630568387525904
[2m[36m(func pid=12671)[0m f1_weighted: 0.2179379697852904
[2m[36m(func pid=12671)[0m f1_per_class: [0.206, 0.0, 0.6, 0.0, 0.0, 0.341, 0.526, 0.195, 0.096, 0.0]
== Status ==
Current time: 2024-01-07 10:37:19 (running for 00:19:49.16)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.451 |      0.177 |                   57 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.967 |      0.234 |                   57 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   3.804 |      0.221 |                   54 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 104.074 |      0.196 |                   13 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.38572761194029853
[2m[36m(func pid=1670)[0m top5: 0.8819962686567164
[2m[36m(func pid=1670)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=1670)[0m f1_macro: 0.2905968337676308
[2m[36m(func pid=1670)[0m f1_weighted: 0.4026654716076891
[2m[36m(func pid=1670)[0m f1_per_class: [0.216, 0.344, 0.153, 0.516, 0.16, 0.407, 0.399, 0.365, 0.106, 0.24]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.14412313432835822
[2m[36m(func pid=1142)[0m top5: 0.7728544776119403
[2m[36m(func pid=1142)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=1142)[0m f1_macro: 0.1801884469082991
[2m[36m(func pid=1142)[0m f1_weighted: 0.10756698550503889
[2m[36m(func pid=1142)[0m f1_per_class: [0.174, 0.322, 0.545, 0.0, 0.113, 0.151, 0.0, 0.462, 0.0, 0.034]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.22574626865671643
[2m[36m(func pid=2489)[0m top5: 0.7532649253731343
[2m[36m(func pid=2489)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=2489)[0m f1_macro: 0.17229339265144383
[2m[36m(func pid=2489)[0m f1_weighted: 0.2048208772529923
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.063, 0.183, 0.512, 0.033, 0.124, 0.006, 0.493, 0.072, 0.235]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 72.4798 | Steps: 4 | Val loss: 94.7541 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.8621 | Steps: 4 | Val loss: 1.8044 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.2739 | Steps: 4 | Val loss: 2.1113 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 6.9385 | Steps: 4 | Val loss: 8.4345 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 10:37:24 (running for 00:19:54.32)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.432 |      0.18  |                   58 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.346 |      0.291 |                   58 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  7.151 |      0.172 |                   55 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 72.48  |      0.085 |                   14 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.2957089552238806
[2m[36m(func pid=12671)[0m top5: 0.6464552238805971
[2m[36m(func pid=12671)[0m f1_micro: 0.2957089552238806
[2m[36m(func pid=12671)[0m f1_macro: 0.0852973627592512
[2m[36m(func pid=12671)[0m f1_weighted: 0.16317967918108878
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.0, 0.133, 0.0, 0.0, 0.052, 0.49, 0.177, 0.0, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.4076492537313433
[2m[36m(func pid=1670)[0m top5: 0.8838619402985075
[2m[36m(func pid=1670)[0m f1_micro: 0.4076492537313433
[2m[36m(func pid=1670)[0m f1_macro: 0.31143224423745985
[2m[36m(func pid=1670)[0m f1_weighted: 0.3679057594211895
[2m[36m(func pid=1670)[0m f1_per_class: [0.274, 0.15, 0.483, 0.569, 0.14, 0.416, 0.31, 0.513, 0.054, 0.205]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.1478544776119403
[2m[36m(func pid=1142)[0m top5: 0.7481343283582089
[2m[36m(func pid=1142)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=1142)[0m f1_macro: 0.18482226967119222
[2m[36m(func pid=1142)[0m f1_weighted: 0.11644537246642148
[2m[36m(func pid=1142)[0m f1_per_class: [0.199, 0.304, 0.5, 0.003, 0.104, 0.252, 0.0, 0.453, 0.0, 0.033]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.2583955223880597
[2m[36m(func pid=2489)[0m top5: 0.8610074626865671
[2m[36m(func pid=2489)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=2489)[0m f1_macro: 0.17694044121131555
[2m[36m(func pid=2489)[0m f1_weighted: 0.2971188419181696
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.005, 0.164, 0.467, 0.033, 0.159, 0.411, 0.321, 0.137, 0.071]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 80.8927 | Steps: 4 | Val loss: 95.9401 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.7414 | Steps: 4 | Val loss: 1.8604 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.3373 | Steps: 4 | Val loss: 2.1263 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 5.9159 | Steps: 4 | Val loss: 7.7013 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 10:37:29 (running for 00:19:59.60)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.274 |      0.185 |                   59 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.862 |      0.311 |                   59 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  6.938 |      0.177 |                   56 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 80.893 |      0.144 |                   15 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.10914179104477612
[2m[36m(func pid=12671)[0m top5: 0.5573694029850746
[2m[36m(func pid=12671)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=12671)[0m f1_macro: 0.14381882571927823
[2m[36m(func pid=12671)[0m f1_weighted: 0.08240155474782677
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.0, 0.156, 0.0, 0.308, 0.429, 0.0, 0.516, 0.0, 0.03]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.37593283582089554
[2m[36m(func pid=1670)[0m top5: 0.8838619402985075
[2m[36m(func pid=1670)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=1670)[0m f1_macro: 0.29543323681760236
[2m[36m(func pid=1670)[0m f1_weighted: 0.2982297799224194
[2m[36m(func pid=1670)[0m f1_per_class: [0.328, 0.098, 0.632, 0.569, 0.112, 0.392, 0.115, 0.504, 0.028, 0.176]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.15904850746268656
[2m[36m(func pid=1142)[0m top5: 0.722481343283582
[2m[36m(func pid=1142)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=1142)[0m f1_macro: 0.18960151626192204
[2m[36m(func pid=1142)[0m f1_weighted: 0.12367470883382053
[2m[36m(func pid=1142)[0m f1_per_class: [0.209, 0.284, 0.462, 0.01, 0.098, 0.336, 0.0, 0.414, 0.047, 0.037]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.23740671641791045
[2m[36m(func pid=2489)[0m top5: 0.8572761194029851
[2m[36m(func pid=2489)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=2489)[0m f1_macro: 0.1646254356057951
[2m[36m(func pid=2489)[0m f1_weighted: 0.2388958560787252
[2m[36m(func pid=2489)[0m f1_per_class: [0.222, 0.058, 0.17, 0.142, 0.041, 0.223, 0.513, 0.0, 0.076, 0.2]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.4791 | Steps: 4 | Val loss: 1.8598 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 77.4441 | Steps: 4 | Val loss: 115.5557 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3849 | Steps: 4 | Val loss: 2.1121 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 3.3344 | Steps: 4 | Val loss: 7.7064 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 10:37:35 (running for 00:20:04.94)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.337 |      0.19  |                   60 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.741 |      0.295 |                   60 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  5.916 |      0.165 |                   57 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 77.444 |      0.014 |                   16 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.33348880597014924
[2m[36m(func pid=1670)[0m top5: 0.8857276119402985
[2m[36m(func pid=1670)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=1670)[0m f1_macro: 0.2715476648999474
[2m[36m(func pid=1670)[0m f1_weighted: 0.2706110882979541
[2m[36m(func pid=1670)[0m f1_per_class: [0.324, 0.078, 0.632, 0.57, 0.067, 0.355, 0.066, 0.414, 0.027, 0.182]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=12671)[0m top1: 0.03311567164179104
[2m[36m(func pid=12671)[0m top5: 0.396455223880597
[2m[36m(func pid=12671)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=12671)[0m f1_macro: 0.01388782911939635
[2m[36m(func pid=12671)[0m f1_weighted: 0.006548248210157109
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.019, 0.0, 0.0, 0.027, 0.0, 0.0, 0.0, 0.093, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1142)[0m top1: 0.16277985074626866
[2m[36m(func pid=1142)[0m top5: 0.7010261194029851
[2m[36m(func pid=1142)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=1142)[0m f1_macro: 0.20249131160231376
[2m[36m(func pid=1142)[0m f1_weighted: 0.12314070032661334
[2m[36m(func pid=1142)[0m f1_per_class: [0.258, 0.246, 0.513, 0.013, 0.078, 0.351, 0.0, 0.407, 0.124, 0.035]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.25513059701492535
[2m[36m(func pid=2489)[0m top5: 0.7513992537313433
[2m[36m(func pid=2489)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=2489)[0m f1_macro: 0.24522044595649045
[2m[36m(func pid=2489)[0m f1_weighted: 0.2706732772005513
[2m[36m(func pid=2489)[0m f1_per_class: [0.122, 0.373, 0.571, 0.003, 0.102, 0.324, 0.52, 0.0, 0.081, 0.356]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 95.0793 | Steps: 4 | Val loss: 67.4304 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.6758 | Steps: 4 | Val loss: 1.8910 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.2875 | Steps: 4 | Val loss: 2.1195 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 9.2598 | Steps: 4 | Val loss: 7.8959 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=12671)[0m top1: 0.09561567164179105
[2m[36m(func pid=12671)[0m top5: 0.6007462686567164
[2m[36m(func pid=12671)[0m f1_micro: 0.09561567164179104
[2m[36m(func pid=12671)[0m f1_macro: 0.13832841274586308
[2m[36m(func pid=12671)[0m f1_weighted: 0.07366973336075752
[2m[36m(func pid=12671)[0m f1_per_class: [0.27, 0.22, 0.545, 0.0, 0.029, 0.189, 0.0, 0.032, 0.098, 0.0]
[2m[36m(func pid=12671)[0m 
== Status ==
Current time: 2024-01-07 10:37:40 (running for 00:20:10.12)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.385 |      0.202 |                   61 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.479 |      0.272 |                   61 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.334 |      0.245 |                   58 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 95.079 |      0.138 |                   17 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=1670)[0m top1: 0.2980410447761194
[2m[36m(func pid=1670)[0m top5: 0.8805970149253731
[2m[36m(func pid=1670)[0m f1_micro: 0.2980410447761194
[2m[36m(func pid=1670)[0m f1_macro: 0.25544903140188024
[2m[36m(func pid=1670)[0m f1_weighted: 0.2603615853863201
[2m[36m(func pid=1670)[0m f1_per_class: [0.297, 0.161, 0.583, 0.567, 0.052, 0.297, 0.024, 0.334, 0.051, 0.188]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.1609141791044776
[2m[36m(func pid=1142)[0m top5: 0.7010261194029851
[2m[36m(func pid=1142)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=1142)[0m f1_macro: 0.18498754557380023
[2m[36m(func pid=1142)[0m f1_weighted: 0.11819443726231156
[2m[36m(func pid=1142)[0m f1_per_class: [0.281, 0.182, 0.361, 0.033, 0.078, 0.371, 0.0, 0.386, 0.121, 0.037]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.28544776119402987
[2m[36m(func pid=2489)[0m top5: 0.7350746268656716
[2m[36m(func pid=2489)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=2489)[0m f1_macro: 0.27536998482113145
[2m[36m(func pid=2489)[0m f1_weighted: 0.28223622510883417
[2m[36m(func pid=2489)[0m f1_per_class: [0.103, 0.583, 0.375, 0.0, 0.247, 0.392, 0.36, 0.28, 0.117, 0.296]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 59.2068 | Steps: 4 | Val loss: 58.0303 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.6915 | Steps: 4 | Val loss: 1.9710 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.3463 | Steps: 4 | Val loss: 2.1289 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 3.9846 | Steps: 4 | Val loss: 4.4014 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 10:37:45 (running for 00:20:15.17)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.287 |      0.185 |                   62 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.676 |      0.255 |                   62 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  9.26  |      0.275 |                   59 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 59.207 |      0.217 |                   18 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.29850746268656714
[2m[36m(func pid=12671)[0m top5: 0.835820895522388
[2m[36m(func pid=12671)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=12671)[0m f1_macro: 0.21742800388449277
[2m[36m(func pid=12671)[0m f1_weighted: 0.2109066234206559
[2m[36m(func pid=12671)[0m f1_per_class: [0.336, 0.0, 0.571, 0.521, 0.081, 0.347, 0.0, 0.243, 0.0, 0.074]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.24347014925373134
[2m[36m(func pid=1670)[0m top5: 0.8586753731343284
[2m[36m(func pid=1670)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=1670)[0m f1_macro: 0.24636911477507453
[2m[36m(func pid=1670)[0m f1_weighted: 0.24744486017297326
[2m[36m(func pid=1670)[0m f1_per_class: [0.331, 0.314, 0.476, 0.412, 0.045, 0.302, 0.043, 0.288, 0.066, 0.186]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.15391791044776118
[2m[36m(func pid=1142)[0m top5: 0.6856343283582089
[2m[36m(func pid=1142)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=1142)[0m f1_macro: 0.16768278341214288
[2m[36m(func pid=1142)[0m f1_weighted: 0.11492416058953855
[2m[36m(func pid=1142)[0m f1_per_class: [0.226, 0.146, 0.282, 0.063, 0.064, 0.325, 0.0, 0.42, 0.11, 0.042]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.3400186567164179
[2m[36m(func pid=2489)[0m top5: 0.8171641791044776
[2m[36m(func pid=2489)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=2489)[0m f1_macro: 0.3185688020676272
[2m[36m(func pid=2489)[0m f1_weighted: 0.30181250528665954
[2m[36m(func pid=2489)[0m f1_per_class: [0.299, 0.528, 0.538, 0.37, 0.353, 0.39, 0.08, 0.389, 0.093, 0.145]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 96.7688 | Steps: 4 | Val loss: 45.6959 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.3769 | Steps: 4 | Val loss: 2.1673 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.1775 | Steps: 4 | Val loss: 2.1348 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 3.5685 | Steps: 4 | Val loss: 6.1186 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 10:37:50 (running for 00:20:20.30)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.346 |      0.168 |                   63 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.692 |      0.246 |                   63 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.985 |      0.319 |                   60 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 96.769 |      0.228 |                   19 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.376865671641791
[2m[36m(func pid=12671)[0m top5: 0.8129664179104478
[2m[36m(func pid=12671)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=12671)[0m f1_macro: 0.22814599359705165
[2m[36m(func pid=12671)[0m f1_weighted: 0.3653976097629068
[2m[36m(func pid=12671)[0m f1_per_class: [0.109, 0.0, 0.114, 0.543, 0.174, 0.336, 0.492, 0.406, 0.0, 0.107]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.18889925373134328
[2m[36m(func pid=1670)[0m top5: 0.8059701492537313
[2m[36m(func pid=1670)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=1670)[0m f1_macro: 0.21021518766907357
[2m[36m(func pid=1670)[0m f1_weighted: 0.156548904764755
[2m[36m(func pid=1670)[0m f1_per_class: [0.196, 0.328, 0.353, 0.036, 0.046, 0.318, 0.068, 0.332, 0.136, 0.29]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.1525186567164179
[2m[36m(func pid=1142)[0m top5: 0.6669776119402985
[2m[36m(func pid=1142)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=1142)[0m f1_macro: 0.15178569549577509
[2m[36m(func pid=1142)[0m f1_weighted: 0.10976851439519367
[2m[36m(func pid=1142)[0m f1_per_class: [0.083, 0.11, 0.289, 0.084, 0.061, 0.307, 0.0, 0.422, 0.107, 0.055]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.3269589552238806
[2m[36m(func pid=2489)[0m top5: 0.8073694029850746
[2m[36m(func pid=2489)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=2489)[0m f1_macro: 0.24725435714084312
[2m[36m(func pid=2489)[0m f1_weighted: 0.2748160892663008
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.248, 0.606, 0.6, 0.276, 0.266, 0.016, 0.403, 0.0, 0.058]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 61.1876 | Steps: 4 | Val loss: 79.4655 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.5713 | Steps: 4 | Val loss: 2.3941 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.2453 | Steps: 4 | Val loss: 2.1483 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 4.1395 | Steps: 4 | Val loss: 5.2751 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 10:37:56 (running for 00:20:25.73)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.177 |      0.152 |                   64 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.377 |      0.21  |                   64 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.569 |      0.247 |                   61 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 61.188 |      0.066 |                   20 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.09981343283582089
[2m[36m(func pid=12671)[0m top5: 0.6389925373134329
[2m[36m(func pid=12671)[0m f1_micro: 0.0998134328358209
[2m[36m(func pid=12671)[0m f1_macro: 0.06648581028103831
[2m[36m(func pid=12671)[0m f1_weighted: 0.06690936333975918
[2m[36m(func pid=12671)[0m f1_per_class: [0.082, 0.211, 0.062, 0.0, 0.0, 0.0, 0.057, 0.188, 0.0, 0.065]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.19402985074626866
[2m[36m(func pid=1670)[0m top5: 0.7201492537313433
[2m[36m(func pid=1670)[0m f1_micro: 0.19402985074626866
[2m[36m(func pid=1670)[0m f1_macro: 0.1687542848372105
[2m[36m(func pid=1670)[0m f1_weighted: 0.1421367497691703
[2m[36m(func pid=1670)[0m f1_per_class: [0.0, 0.351, 0.144, 0.0, 0.061, 0.327, 0.042, 0.381, 0.188, 0.194]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.15391791044776118
[2m[36m(func pid=1142)[0m top5: 0.6515858208955224
[2m[36m(func pid=1142)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=1142)[0m f1_macro: 0.14279582247370023
[2m[36m(func pid=1142)[0m f1_weighted: 0.10715256715007448
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.081, 0.247, 0.095, 0.062, 0.303, 0.0, 0.442, 0.112, 0.087]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.38199626865671643
[2m[36m(func pid=2489)[0m top5: 0.8414179104477612
[2m[36m(func pid=2489)[0m f1_micro: 0.3819962686567165
[2m[36m(func pid=2489)[0m f1_macro: 0.2731459257175199
[2m[36m(func pid=2489)[0m f1_weighted: 0.327371941993618
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.312, 0.465, 0.613, 0.323, 0.383, 0.095, 0.392, 0.047, 0.102]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 108.9078 | Steps: 4 | Val loss: 91.2821 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.6081 | Steps: 4 | Val loss: 2.5526 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.3075 | Steps: 4 | Val loss: 2.1759 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 7.3646 | Steps: 4 | Val loss: 5.2227 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 10:38:01 (running for 00:20:31.10)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.143 |                   65 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.571 |      0.169 |                   65 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   4.139 |      0.273 |                   62 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 108.908 |      0.179 |                   21 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.37453358208955223
[2m[36m(func pid=12671)[0m top5: 0.6609141791044776
[2m[36m(func pid=12671)[0m f1_micro: 0.3745335820895522
[2m[36m(func pid=12671)[0m f1_macro: 0.17923862489503178
[2m[36m(func pid=12671)[0m f1_weighted: 0.275805217465229
[2m[36m(func pid=12671)[0m f1_per_class: [0.251, 0.455, 0.189, 0.0, 0.0, 0.0, 0.588, 0.264, 0.0, 0.044]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.1865671641791045
[2m[36m(func pid=1670)[0m top5: 0.7084888059701493
[2m[36m(func pid=1670)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=1670)[0m f1_macro: 0.161507287722101
[2m[36m(func pid=1670)[0m f1_weighted: 0.1489706443307707
[2m[36m(func pid=1670)[0m f1_per_class: [0.0, 0.324, 0.082, 0.0, 0.106, 0.342, 0.069, 0.446, 0.174, 0.073]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.13852611940298507
[2m[36m(func pid=1142)[0m top5: 0.6352611940298507
[2m[36m(func pid=1142)[0m f1_micro: 0.13852611940298507
[2m[36m(func pid=1142)[0m f1_macro: 0.1405590599412887
[2m[36m(func pid=1142)[0m f1_weighted: 0.10330986413467425
[2m[36m(func pid=1142)[0m f1_per_class: [0.0, 0.082, 0.212, 0.087, 0.056, 0.276, 0.0, 0.465, 0.103, 0.125]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.28171641791044777
[2m[36m(func pid=2489)[0m top5: 0.8456156716417911
[2m[36m(func pid=2489)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=2489)[0m f1_macro: 0.24803041318092198
[2m[36m(func pid=2489)[0m f1_weighted: 0.30466031977243213
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.475, 0.15, 0.301, 0.341, 0.4, 0.228, 0.283, 0.091, 0.211]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 100.2466 | Steps: 4 | Val loss: 90.6668 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.4309 | Steps: 4 | Val loss: 2.5669 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.3086 | Steps: 4 | Val loss: 2.1574 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 3.2429 | Steps: 4 | Val loss: 8.9941 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 10:38:06 (running for 00:20:36.43)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.307 |      0.141 |                   66 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.608 |      0.162 |                   66 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   7.365 |      0.248 |                   63 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 100.247 |      0.21  |                   22 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |         |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |         |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.3596082089552239
[2m[36m(func pid=12671)[0m top5: 0.6665111940298507
[2m[36m(func pid=12671)[0m f1_micro: 0.35960820895522383
[2m[36m(func pid=12671)[0m f1_macro: 0.20960726007985458
[2m[36m(func pid=12671)[0m f1_weighted: 0.2871825479184684
[2m[36m(func pid=12671)[0m f1_per_class: [0.224, 0.445, 0.471, 0.0, 0.0, 0.336, 0.55, 0.0, 0.0, 0.071]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.1875
[2m[36m(func pid=1670)[0m top5: 0.7364738805970149
[2m[36m(func pid=1670)[0m f1_micro: 0.1875
[2m[36m(func pid=1670)[0m f1_macro: 0.16630229037156757
[2m[36m(func pid=1670)[0m f1_weighted: 0.16387426024888477
[2m[36m(func pid=1670)[0m f1_per_class: [0.0, 0.352, 0.071, 0.0, 0.144, 0.313, 0.11, 0.511, 0.092, 0.071]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.13526119402985073
[2m[36m(func pid=1142)[0m top5: 0.6380597014925373
[2m[36m(func pid=1142)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=1142)[0m f1_macro: 0.14722635147123764
[2m[36m(func pid=1142)[0m f1_weighted: 0.09872958338693229
[2m[36m(func pid=1142)[0m f1_per_class: [0.044, 0.037, 0.31, 0.095, 0.05, 0.262, 0.0, 0.488, 0.108, 0.077]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.23460820895522388
[2m[36m(func pid=2489)[0m top5: 0.664179104477612
[2m[36m(func pid=2489)[0m f1_micro: 0.23460820895522388
[2m[36m(func pid=2489)[0m f1_macro: 0.2139676279302963
[2m[36m(func pid=2489)[0m f1_weighted: 0.2011896676649071
[2m[36m(func pid=2489)[0m f1_per_class: [0.043, 0.53, 0.115, 0.0, 0.252, 0.422, 0.118, 0.29, 0.092, 0.276]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 99.6284 | Steps: 4 | Val loss: 82.8608 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.7201 | Steps: 4 | Val loss: 2.3038 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.2623 | Steps: 4 | Val loss: 2.1537 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 4.6653 | Steps: 4 | Val loss: 11.1506 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 10:38:11 (running for 00:20:41.56)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.309 |      0.147 |                   67 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.431 |      0.166 |                   67 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.243 |      0.214 |                   64 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 99.628 |      0.156 |                   23 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.18190298507462688
[2m[36m(func pid=12671)[0m top5: 0.6082089552238806
[2m[36m(func pid=12671)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=12671)[0m f1_macro: 0.15621625811988757
[2m[36m(func pid=12671)[0m f1_weighted: 0.1227617171479812
[2m[36m(func pid=12671)[0m f1_per_class: [0.203, 0.244, 0.086, 0.0, 0.2, 0.338, 0.036, 0.43, 0.0, 0.026]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.22108208955223882
[2m[36m(func pid=1670)[0m top5: 0.7677238805970149
[2m[36m(func pid=1670)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=1670)[0m f1_macro: 0.19470096262456862
[2m[36m(func pid=1670)[0m f1_weighted: 0.18197172316832844
[2m[36m(func pid=1670)[0m f1_per_class: [0.087, 0.368, 0.079, 0.0, 0.141, 0.333, 0.135, 0.517, 0.175, 0.111]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.13805970149253732
[2m[36m(func pid=1142)[0m top5: 0.6604477611940298
[2m[36m(func pid=1142)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=1142)[0m f1_macro: 0.15440189433688029
[2m[36m(func pid=1142)[0m f1_weighted: 0.09975853425380826
[2m[36m(func pid=1142)[0m f1_per_class: [0.189, 0.021, 0.314, 0.098, 0.052, 0.265, 0.0, 0.496, 0.109, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.29524253731343286
[2m[36m(func pid=2489)[0m top5: 0.6333955223880597
[2m[36m(func pid=2489)[0m f1_micro: 0.29524253731343286
[2m[36m(func pid=2489)[0m f1_macro: 0.23832706204295598
[2m[36m(func pid=2489)[0m f1_weighted: 0.28558873782961497
[2m[36m(func pid=2489)[0m f1_per_class: [0.135, 0.526, 0.103, 0.0, 0.12, 0.331, 0.426, 0.33, 0.102, 0.311]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 88.5554 | Steps: 4 | Val loss: 77.2974 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.0057 | Steps: 4 | Val loss: 2.0305 | Batch size: 32 | lr: 0.001 | Duration: 2.80s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.2510 | Steps: 4 | Val loss: 2.1506 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 4.9395 | Steps: 4 | Val loss: 15.3790 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 10:38:17 (running for 00:20:46.81)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.262 |      0.154 |                   68 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.72  |      0.195 |                   68 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.665 |      0.238 |                   65 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 88.555 |      0.096 |                   24 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.10914179104477612
[2m[36m(func pid=12671)[0m top5: 0.6786380597014925
[2m[36m(func pid=12671)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=12671)[0m f1_macro: 0.09584672705154282
[2m[36m(func pid=12671)[0m f1_weighted: 0.06636056255764834
[2m[36m(func pid=12671)[0m f1_per_class: [0.132, 0.0, 0.049, 0.0, 0.035, 0.356, 0.0, 0.387, 0.0, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.29151119402985076
[2m[36m(func pid=1670)[0m top5: 0.8222947761194029
[2m[36m(func pid=1670)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=1670)[0m f1_macro: 0.26410056563491485
[2m[36m(func pid=1670)[0m f1_weighted: 0.26262820901900374
[2m[36m(func pid=1670)[0m f1_per_class: [0.253, 0.451, 0.154, 0.212, 0.16, 0.341, 0.135, 0.525, 0.188, 0.222]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.14272388059701493
[2m[36m(func pid=1142)[0m top5: 0.6791044776119403
[2m[36m(func pid=1142)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=1142)[0m f1_macro: 0.16224335131401602
[2m[36m(func pid=1142)[0m f1_weighted: 0.10623238873528741
[2m[36m(func pid=1142)[0m f1_per_class: [0.262, 0.011, 0.306, 0.124, 0.052, 0.263, 0.0, 0.494, 0.112, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.19916044776119404
[2m[36m(func pid=2489)[0m top5: 0.6077425373134329
[2m[36m(func pid=2489)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=2489)[0m f1_macro: 0.2119138380079732
[2m[36m(func pid=2489)[0m f1_weighted: 0.21706833941188985
[2m[36m(func pid=2489)[0m f1_per_class: [0.085, 0.368, 0.282, 0.0, 0.071, 0.33, 0.302, 0.305, 0.0, 0.377]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 84.8551 | Steps: 4 | Val loss: 68.6904 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.2362 | Steps: 4 | Val loss: 1.8959 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.3613 | Steps: 4 | Val loss: 2.1569 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 8.0931 | Steps: 4 | Val loss: 16.7356 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:38:22 (running for 00:20:52.04)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.251 |      0.162 |                   69 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.006 |      0.264 |                   69 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.94  |      0.212 |                   66 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 84.855 |      0.171 |                   25 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.2271455223880597
[2m[36m(func pid=12671)[0m top5: 0.707089552238806
[2m[36m(func pid=12671)[0m f1_micro: 0.2271455223880597
[2m[36m(func pid=12671)[0m f1_macro: 0.17072377566399832
[2m[36m(func pid=12671)[0m f1_weighted: 0.18175020912468212
[2m[36m(func pid=12671)[0m f1_per_class: [0.179, 0.0, 0.489, 0.526, 0.028, 0.0, 0.0, 0.485, 0.0, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1142)[0m top1: 0.14365671641791045
[2m[36m(func pid=1142)[0m top5: 0.6842350746268657
[2m[36m(func pid=1142)[0m f1_micro: 0.14365671641791045
[2m[36m(func pid=1142)[0m f1_macro: 0.15963771422516404
[2m[36m(func pid=1142)[0m f1_weighted: 0.1130170652022037
[2m[36m(func pid=1142)[0m f1_per_class: [0.26, 0.005, 0.293, 0.163, 0.048, 0.251, 0.0, 0.457, 0.119, 0.0]
[2m[36m(func pid=1670)[0m top1: 0.373134328358209
[2m[36m(func pid=1670)[0m top5: 0.8530783582089553
[2m[36m(func pid=1670)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=1670)[0m f1_macro: 0.3222599866381971
[2m[36m(func pid=1670)[0m f1_weighted: 0.36452048497129985
[2m[36m(func pid=1670)[0m f1_per_class: [0.269, 0.49, 0.5, 0.526, 0.107, 0.34, 0.169, 0.504, 0.102, 0.215]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.13899253731343283
[2m[36m(func pid=2489)[0m top5: 0.5354477611940298
[2m[36m(func pid=2489)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=2489)[0m f1_macro: 0.16175971041139403
[2m[36m(func pid=2489)[0m f1_weighted: 0.12153854556810623
[2m[36m(func pid=2489)[0m f1_per_class: [0.101, 0.296, 0.22, 0.0, 0.056, 0.236, 0.05, 0.37, 0.0, 0.288]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 83.8324 | Steps: 4 | Val loss: 112.4039 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.6501 | Steps: 4 | Val loss: 2.0510 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.0914 | Steps: 4 | Val loss: 2.1566 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 3.8525 | Steps: 4 | Val loss: 15.2110 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 10:38:27 (running for 00:20:57.27)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.361 |      0.16  |                   70 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.236 |      0.322 |                   70 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  8.093 |      0.162 |                   67 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 83.832 |      0.1   |                   26 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.04011194029850746
[2m[36m(func pid=12671)[0m top5: 0.7486007462686567
[2m[36m(func pid=12671)[0m f1_micro: 0.04011194029850746
[2m[36m(func pid=12671)[0m f1_macro: 0.10046787429352769
[2m[36m(func pid=12671)[0m f1_weighted: 0.009412554682084179
[2m[36m(func pid=12671)[0m f1_per_class: [0.085, 0.0, 0.632, 0.0, 0.222, 0.0, 0.0, 0.0, 0.066, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.35074626865671643
[2m[36m(func pid=1670)[0m top5: 0.8465485074626866
[2m[36m(func pid=1670)[0m f1_micro: 0.35074626865671643
[2m[36m(func pid=1670)[0m f1_macro: 0.3112113274278612
[2m[36m(func pid=1670)[0m f1_weighted: 0.3406453332441497
[2m[36m(func pid=1670)[0m f1_per_class: [0.322, 0.397, 0.632, 0.539, 0.062, 0.269, 0.167, 0.457, 0.052, 0.216]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.1623134328358209
[2m[36m(func pid=1142)[0m top5: 0.7075559701492538
[2m[36m(func pid=1142)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=1142)[0m f1_macro: 0.15776055770099373
[2m[36m(func pid=1142)[0m f1_weighted: 0.1420546993597534
[2m[36m(func pid=1142)[0m f1_per_class: [0.216, 0.011, 0.214, 0.274, 0.047, 0.245, 0.0, 0.441, 0.131, 0.0]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.13059701492537312
[2m[36m(func pid=2489)[0m top5: 0.5018656716417911
[2m[36m(func pid=2489)[0m f1_micro: 0.13059701492537312
[2m[36m(func pid=2489)[0m f1_macro: 0.1529619791132278
[2m[36m(func pid=2489)[0m f1_weighted: 0.10131182404070851
[2m[36m(func pid=2489)[0m f1_per_class: [0.278, 0.254, 0.173, 0.0, 0.046, 0.187, 0.0, 0.482, 0.0, 0.11]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 82.9513 | Steps: 4 | Val loss: 34.3564 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.1349 | Steps: 4 | Val loss: 2.0684 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.4436 | Steps: 4 | Val loss: 2.1640 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 6.4664 | Steps: 4 | Val loss: 13.9946 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 10:38:32 (running for 00:21:02.62)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.091 |      0.158 |                   71 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.65  |      0.311 |                   71 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  3.852 |      0.153 |                   68 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 82.951 |      0.201 |                   27 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.3694029850746269
[2m[36m(func pid=12671)[0m top5: 0.9057835820895522
[2m[36m(func pid=12671)[0m f1_micro: 0.3694029850746269
[2m[36m(func pid=12671)[0m f1_macro: 0.200871698877839
[2m[36m(func pid=12671)[0m f1_weighted: 0.37780202978506167
[2m[36m(func pid=12671)[0m f1_per_class: [0.249, 0.385, 0.143, 0.52, 0.063, 0.032, 0.513, 0.0, 0.103, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.3474813432835821
[2m[36m(func pid=1670)[0m top5: 0.8614738805970149
[2m[36m(func pid=1670)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=1670)[0m f1_macro: 0.30357711860265746
[2m[36m(func pid=1670)[0m f1_weighted: 0.360228197748973
[2m[36m(func pid=1670)[0m f1_per_class: [0.322, 0.345, 0.632, 0.545, 0.047, 0.163, 0.309, 0.358, 0.133, 0.182]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=1142)[0m top1: 0.17630597014925373
[2m[36m(func pid=1142)[0m top5: 0.7327425373134329
[2m[36m(func pid=1142)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=1142)[0m f1_macro: 0.16348929000739199
[2m[36m(func pid=1142)[0m f1_weighted: 0.16809517334998447
[2m[36m(func pid=1142)[0m f1_per_class: [0.146, 0.027, 0.174, 0.37, 0.045, 0.216, 0.0, 0.449, 0.131, 0.077]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=2489)[0m top1: 0.12733208955223882
[2m[36m(func pid=2489)[0m top5: 0.5125932835820896
[2m[36m(func pid=2489)[0m f1_micro: 0.12733208955223882
[2m[36m(func pid=2489)[0m f1_macro: 0.137307326593001
[2m[36m(func pid=2489)[0m f1_weighted: 0.09656561270329855
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.261, 0.289, 0.0, 0.046, 0.174, 0.0, 0.469, 0.048, 0.086]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 80.4848 | Steps: 4 | Val loss: 35.1869 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.6883 | Steps: 4 | Val loss: 2.1622 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.1831 | Steps: 4 | Val loss: 2.1518 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 4.9418 | Steps: 4 | Val loss: 10.3995 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
== Status ==
Current time: 2024-01-07 10:38:38 (running for 00:21:07.78)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.444 |      0.163 |                   72 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  2.135 |      0.304 |                   72 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  6.466 |      0.137 |                   69 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 80.485 |      0.333 |                   28 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.435634328358209
[2m[36m(func pid=12671)[0m top5: 0.8889925373134329
[2m[36m(func pid=12671)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=12671)[0m f1_macro: 0.3325917723119039
[2m[36m(func pid=12671)[0m f1_weighted: 0.4069499133284301
[2m[36m(func pid=12671)[0m f1_per_class: [0.339, 0.536, 0.556, 0.493, 0.237, 0.435, 0.304, 0.426, 0.0, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1670)[0m top1: 0.302705223880597
[2m[36m(func pid=1670)[0m top5: 0.8722014925373134
[2m[36m(func pid=1670)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=1670)[0m f1_macro: 0.27808990935541406
[2m[36m(func pid=1670)[0m f1_weighted: 0.3425573517345873
[2m[36m(func pid=1670)[0m f1_per_class: [0.308, 0.297, 0.6, 0.521, 0.034, 0.0, 0.373, 0.303, 0.153, 0.194]
[2m[36m(func pid=1142)[0m top1: 0.18516791044776118
[2m[36m(func pid=1142)[0m top5: 0.7691231343283582
[2m[36m(func pid=1142)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=1142)[0m f1_macro: 0.16779282088152753
[2m[36m(func pid=1142)[0m f1_weighted: 0.17883018460439973
[2m[36m(func pid=1142)[0m f1_per_class: [0.163, 0.046, 0.186, 0.4, 0.041, 0.215, 0.0, 0.429, 0.135, 0.062]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m top1: 0.1935634328358209
[2m[36m(func pid=2489)[0m top5: 0.6842350746268657
[2m[36m(func pid=2489)[0m f1_micro: 0.1935634328358209
[2m[36m(func pid=2489)[0m f1_macro: 0.22865123760116196
[2m[36m(func pid=2489)[0m f1_weighted: 0.17032198209144805
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.348, 0.667, 0.186, 0.042, 0.178, 0.0, 0.449, 0.146, 0.271]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 68.8585 | Steps: 4 | Val loss: 55.4695 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.3335 | Steps: 4 | Val loss: 2.1543 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.7595 | Steps: 4 | Val loss: 2.2367 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 4.3047 | Steps: 4 | Val loss: 7.6496 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 10:38:43 (running for 00:21:12.90)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.183 |      0.168 |                   73 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.688 |      0.278 |                   73 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.942 |      0.229 |                   70 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 68.859 |      0.278 |                   29 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.34701492537313433
[2m[36m(func pid=12671)[0m top5: 0.7490671641791045
[2m[36m(func pid=12671)[0m f1_micro: 0.34701492537313433
[2m[36m(func pid=12671)[0m f1_macro: 0.27782245508021886
[2m[36m(func pid=12671)[0m f1_weighted: 0.2803768904303354
[2m[36m(func pid=12671)[0m f1_per_class: [0.197, 0.484, 0.667, 0.433, 0.227, 0.388, 0.0, 0.383, 0.0, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1142)[0m top1: 0.18563432835820895
[2m[36m(func pid=1142)[0m top5: 0.789179104477612
[2m[36m(func pid=1142)[0m f1_micro: 0.18563432835820895
[2m[36m(func pid=1142)[0m f1_macro: 0.1710409934202905
[2m[36m(func pid=1142)[0m f1_weighted: 0.18366874887089057
[2m[36m(func pid=1142)[0m f1_per_class: [0.147, 0.11, 0.171, 0.376, 0.041, 0.219, 0.0, 0.44, 0.114, 0.091]
[2m[36m(func pid=1142)[0m 
[2m[36m(func pid=1670)[0m top1: 0.26632462686567165
[2m[36m(func pid=1670)[0m top5: 0.8619402985074627
[2m[36m(func pid=1670)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=1670)[0m f1_macro: 0.2667655015747938
[2m[36m(func pid=1670)[0m f1_weighted: 0.3238387450437147
[2m[36m(func pid=1670)[0m f1_per_class: [0.333, 0.385, 0.522, 0.337, 0.036, 0.0, 0.433, 0.307, 0.139, 0.176]
[2m[36m(func pid=1670)[0m 
[2m[36m(func pid=2489)[0m top1: 0.32742537313432835
[2m[36m(func pid=2489)[0m top5: 0.8041044776119403
[2m[36m(func pid=2489)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=2489)[0m f1_macro: 0.27025289365236727
[2m[36m(func pid=2489)[0m f1_weighted: 0.26745061482332366
[2m[36m(func pid=2489)[0m f1_per_class: [0.0, 0.279, 0.632, 0.534, 0.067, 0.21, 0.019, 0.487, 0.129, 0.346]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 52.6108 | Steps: 4 | Val loss: 82.4344 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=1142)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.2447 | Steps: 4 | Val loss: 2.1515 | Batch size: 32 | lr: 0.0001 | Duration: 2.79s
[2m[36m(func pid=1670)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.5383 | Steps: 4 | Val loss: 2.3195 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 9.9422 | Steps: 4 | Val loss: 6.8959 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 10:38:48 (running for 00:21:18.12)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=8
Bracket: Iter 75.000: 0.2535
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (12 PENDING, 4 RUNNING, 8 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00008 | RUNNING    | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.334 |      0.171 |                   74 |
| train_952df_00009 | RUNNING    | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.76  |      0.267 |                   74 |
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.305 |      0.27  |                   71 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 52.611 |      0.142 |                   30 |
| train_952df_00012 | PENDING    |                     | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | PENDING    |                     | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 PENDING)


[2m[36m(func pid=12671)[0m top1: 0.11613805970149253
[2m[36m(func pid=12671)[0m top5: 0.6511194029850746
[2m[36m(func pid=12671)[0m f1_micro: 0.11613805970149253
[2m[36m(func pid=12671)[0m f1_macro: 0.14151356225321737
[2m[36m(func pid=12671)[0m f1_weighted: 0.10589309560856601
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.212, 0.268, 0.0, 0.049, 0.325, 0.0, 0.514, 0.0, 0.047]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=1142)[0m top1: 0.17630597014925373
[2m[36m(func pid=1142)[0m top5: 0.7994402985074627
[2m[36m(func pid=1142)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=1142)[0m f1_macro: 0.16769673435069438
[2m[36m(func pid=1142)[0m f1_weighted: 0.1855563713745875
[2m[36m(func pid=1142)[0m f1_per_class: [0.133, 0.207, 0.175, 0.342, 0.039, 0.187, 0.0, 0.434, 0.099, 0.062]
[2m[36m(func pid=1670)[0m top1: 0.26725746268656714
[2m[36m(func pid=1670)[0m top5: 0.8138992537313433
[2m[36m(func pid=1670)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=1670)[0m f1_macro: 0.2326454687827392
[2m[36m(func pid=1670)[0m f1_weighted: 0.2703240675678285
[2m[36m(func pid=1670)[0m f1_per_class: [0.34, 0.398, 0.377, 0.032, 0.041, 0.0, 0.535, 0.307, 0.119, 0.176]
[2m[36m(func pid=2489)[0m top1: 0.3903917910447761
[2m[36m(func pid=2489)[0m top5: 0.8997201492537313
[2m[36m(func pid=2489)[0m f1_micro: 0.39039179104477606
[2m[36m(func pid=2489)[0m f1_macro: 0.2937310006802735
[2m[36m(func pid=2489)[0m f1_weighted: 0.33867952408085644
[2m[36m(func pid=2489)[0m f1_per_class: [0.085, 0.258, 0.75, 0.515, 0.133, 0.16, 0.333, 0.366, 0.026, 0.311]
[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 97.1620 | Steps: 4 | Val loss: 116.4697 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 4.9744 | Steps: 4 | Val loss: 4.4124 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=12671)[0m top1: 0.058768656716417914
[2m[36m(func pid=12671)[0m top5: 0.4146455223880597
[2m[36m(func pid=12671)[0m f1_micro: 0.05876865671641791
[2m[36m(func pid=12671)[0m f1_macro: 0.08326475950097113
[2m[36m(func pid=12671)[0m f1_weighted: 0.05077271688959669
[2m[36m(func pid=12671)[0m f1_per_class: [0.083, 0.119, 0.065, 0.0, 0.034, 0.0, 0.0, 0.472, 0.0, 0.061]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=2489)[0m top1: 0.435634328358209
[2m[36m(func pid=2489)[0m top5: 0.917910447761194
[2m[36m(func pid=2489)[0m f1_micro: 0.435634328358209
[2m[36m(func pid=2489)[0m f1_macro: 0.2853195113209862
[2m[36m(func pid=2489)[0m f1_weighted: 0.4176276599165047
[2m[36m(func pid=2489)[0m f1_per_class: [0.25, 0.455, 0.5, 0.5, 0.1, 0.16, 0.563, 0.0, 0.053, 0.273]
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 88.7074 | Steps: 4 | Val loss: 103.2823 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=20296)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20296)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=20296)[0m Configuration completed!
[2m[36m(func pid=20296)[0m New optimizer parameters:
[2m[36m(func pid=20296)[0m SGD (
[2m[36m(func pid=20296)[0m Parameter Group 0
[2m[36m(func pid=20296)[0m     dampening: 0
[2m[36m(func pid=20296)[0m     differentiable: False
[2m[36m(func pid=20296)[0m     foreach: None
[2m[36m(func pid=20296)[0m     lr: 0.0001
[2m[36m(func pid=20296)[0m     maximize: False
[2m[36m(func pid=20296)[0m     momentum: 0.9
[2m[36m(func pid=20296)[0m     nesterov: False
[2m[36m(func pid=20296)[0m     weight_decay: 0.0001
[2m[36m(func pid=20296)[0m )
[2m[36m(func pid=20296)[0m 
== Status ==
Current time: 2024-01-07 10:38:53 (running for 00:21:23.59)
Memory usage on this node: 19.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.2465
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  9.942 |      0.294 |                   72 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 97.162 |      0.083 |                   31 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=2489)[0m 
[2m[36m(func pid=20299)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=20299)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=20299)[0m Configuration completed!
[2m[36m(func pid=20299)[0m New optimizer parameters:
[2m[36m(func pid=20299)[0m SGD (
[2m[36m(func pid=20299)[0m Parameter Group 0
[2m[36m(func pid=20299)[0m     dampening: 0
[2m[36m(func pid=20299)[0m     differentiable: False
[2m[36m(func pid=20299)[0m     foreach: None
[2m[36m(func pid=20299)[0m     lr: 0.001
[2m[36m(func pid=20299)[0m     maximize: False
[2m[36m(func pid=20299)[0m     momentum: 0.9
[2m[36m(func pid=20299)[0m     nesterov: False
[2m[36m(func pid=20299)[0m     weight_decay: 0.0001
[2m[36m(func pid=20299)[0m )
[2m[36m(func pid=20299)[0m 
== Status ==
Current time: 2024-01-07 10:38:59 (running for 00:21:28.92)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.2465
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  4.974 |      0.285 |                   73 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 88.707 |      0.115 |                   32 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.11707089552238806
[2m[36m(func pid=12671)[0m top5: 0.5009328358208955
[2m[36m(func pid=12671)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=12671)[0m f1_macro: 0.11533569596567986
[2m[36m(func pid=12671)[0m f1_weighted: 0.11449261284573996
[2m[36m(func pid=12671)[0m f1_per_class: [0.05, 0.444, 0.13, 0.0, 0.039, 0.0, 0.083, 0.092, 0.099, 0.214]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 6.7278 | Steps: 4 | Val loss: 7.4693 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 49.4194 | Steps: 4 | Val loss: 92.1701 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1219 | Steps: 4 | Val loss: 2.3251 | Batch size: 32 | lr: 0.0001 | Duration: 4.53s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1131 | Steps: 4 | Val loss: 2.3244 | Batch size: 32 | lr: 0.001 | Duration: 4.45s
[2m[36m(func pid=2489)[0m top1: 0.30736940298507465
[2m[36m(func pid=2489)[0m top5: 0.710820895522388
[2m[36m(func pid=2489)[0m f1_micro: 0.30736940298507465
[2m[36m(func pid=2489)[0m f1_macro: 0.2238477413945338
[2m[36m(func pid=2489)[0m f1_weighted: 0.2952293445027181
[2m[36m(func pid=2489)[0m f1_per_class: [0.268, 0.431, 0.155, 0.02, 0.115, 0.348, 0.51, 0.237, 0.045, 0.109]
[2m[36m(func pid=2489)[0m 
== Status ==
Current time: 2024-01-07 10:39:04 (running for 00:21:34.19)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=10
Bracket: Iter 75.000: 0.2465
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 4 RUNNING, 10 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00010 | RUNNING    | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  6.728 |      0.224 |                   74 |
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 49.419 |      0.182 |                   33 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |        |            |                      |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |        |            |                      |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (2 PENDING, 2 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.27705223880597013
[2m[36m(func pid=12671)[0m top5: 0.6604477611940298
[2m[36m(func pid=12671)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=12671)[0m f1_macro: 0.1816830010868749
[2m[36m(func pid=12671)[0m f1_weighted: 0.24788843860344917
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.494, 0.267, 0.0, 0.162, 0.282, 0.412, 0.0, 0.123, 0.077]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20296)[0m top1: 0.1417910447761194
[2m[36m(func pid=20296)[0m top5: 0.5583022388059702
[2m[36m(func pid=20296)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=20296)[0m f1_macro: 0.042229636722724284
[2m[36m(func pid=20296)[0m f1_weighted: 0.08060403254837191
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.0, 0.0, 0.254, 0.0, 0.0, 0.0, 0.168, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.14738805970149255
[2m[36m(func pid=20299)[0m top5: 0.48274253731343286
[2m[36m(func pid=20299)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=20299)[0m f1_macro: 0.05395692632784279
[2m[36m(func pid=20299)[0m f1_weighted: 0.09050139412376151
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.0, 0.007, 0.273, 0.0, 0.0, 0.0, 0.242, 0.0, 0.018]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=2489)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 0.9704 | Steps: 4 | Val loss: 13.9540 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 121.4586 | Steps: 4 | Val loss: 57.0574 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0524 | Steps: 4 | Val loss: 2.3073 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9413 | Steps: 4 | Val loss: 2.3135 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=2489)[0m top1: 0.15485074626865672
[2m[36m(func pid=2489)[0m top5: 0.5368470149253731
[2m[36m(func pid=2489)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=2489)[0m f1_macro: 0.14240229529050663
[2m[36m(func pid=2489)[0m f1_weighted: 0.09958515863711537
[2m[36m(func pid=2489)[0m f1_per_class: [0.229, 0.191, 0.09, 0.0, 0.106, 0.315, 0.0, 0.401, 0.023, 0.07]
== Status ==
Current time: 2024-01-07 10:39:09 (running for 00:21:39.36)
Memory usage on this node: 23.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (10 PENDING, 3 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 121.459 |      0.236 |                   34 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   3.122 |      0.042 |                    1 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   3.113 |      0.054 |                    1 |
| train_952df_00014 | PENDING    |                     | 0.01   |       0.9  |         0.0001 |         |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 2 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.3353544776119403
[2m[36m(func pid=12671)[0m top5: 0.855410447761194
[2m[36m(func pid=12671)[0m f1_micro: 0.3353544776119403
[2m[36m(func pid=12671)[0m f1_macro: 0.2355566194466241
[2m[36m(func pid=12671)[0m f1_weighted: 0.3308447231358027
[2m[36m(func pid=12671)[0m f1_per_class: [0.05, 0.54, 0.375, 0.079, 0.0, 0.457, 0.516, 0.0, 0.124, 0.214]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20296)[0m top1: 0.20242537313432835
[2m[36m(func pid=20296)[0m top5: 0.5923507462686567
[2m[36m(func pid=20296)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=20296)[0m f1_macro: 0.044812932185625536
[2m[36m(func pid=20296)[0m f1_weighted: 0.10543151317969905
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.011, 0.0, 0.354, 0.0, 0.0, 0.0, 0.083, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.006063432835820896
[2m[36m(func pid=20299)[0m top5: 0.5363805970149254
[2m[36m(func pid=20299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=20299)[0m f1_macro: 0.0012059369202226345
[2m[36m(func pid=20299)[0m f1_weighted: 7.312117520006647e-05
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 57.4669 | Steps: 4 | Val loss: 53.2752 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0266 | Steps: 4 | Val loss: 2.3153 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8786 | Steps: 4 | Val loss: 2.2925 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=12671)[0m top1: 0.38992537313432835
[2m[36m(func pid=12671)[0m top5: 0.8418843283582089
[2m[36m(func pid=12671)[0m f1_micro: 0.38992537313432835
[2m[36m(func pid=12671)[0m f1_macro: 0.23824527199941778
[2m[36m(func pid=12671)[0m f1_weighted: 0.32121285868034616
[2m[36m(func pid=12671)[0m f1_per_class: [0.226, 0.027, 0.426, 0.538, 0.0, 0.473, 0.296, 0.26, 0.027, 0.111]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20296)[0m top1: 0.2224813432835821
[2m[36m(func pid=20296)[0m top5: 0.5797574626865671
[2m[36m(func pid=20296)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=20296)[0m f1_macro: 0.05585715441287268
[2m[36m(func pid=20296)[0m f1_weighted: 0.11022469569839004
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.005, 0.111, 0.382, 0.0, 0.0, 0.0, 0.028, 0.0, 0.032]
[2m[36m(func pid=20299)[0m top1: 0.006063432835820896
[2m[36m(func pid=20299)[0m top5: 0.6324626865671642
[2m[36m(func pid=20299)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=20299)[0m f1_macro: 0.0012059369202226345
[2m[36m(func pid=20299)[0m f1_weighted: 7.312117520006647e-05
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 93.9217 | Steps: 4 | Val loss: 66.0919 | Batch size: 32 | lr: 0.1 | Duration: 2.71s
== Status ==
Current time: 2024-01-07 10:39:14 (running for 00:21:44.55)
Memory usage on this node: 22.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 57.467 |      0.238 |                   35 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  3.052 |      0.045 |                    2 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.941 |      0.001 |                    2 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=21639)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=21639)[0m Configuration completed!
[2m[36m(func pid=21639)[0m New optimizer parameters:
[2m[36m(func pid=21639)[0m SGD (
[2m[36m(func pid=21639)[0m Parameter Group 0
[2m[36m(func pid=21639)[0m     dampening: 0
[2m[36m(func pid=21639)[0m     differentiable: False
[2m[36m(func pid=21639)[0m     foreach: None
[2m[36m(func pid=21639)[0m     lr: 0.01
[2m[36m(func pid=21639)[0m     maximize: False
[2m[36m(func pid=21639)[0m     momentum: 0.9
[2m[36m(func pid=21639)[0m     nesterov: False
[2m[36m(func pid=21639)[0m     weight_decay: 0.0001
[2m[36m(func pid=21639)[0m )
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20299)[0m 
== Status ==
Current time: 2024-01-07 10:39:19 (running for 00:21:49.61)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 93.922 |      0.197 |                   36 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  3.027 |      0.056 |                    3 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.879 |      0.001 |                    3 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |        |            |                      |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.34328358208955223
[2m[36m(func pid=12671)[0m top5: 0.8003731343283582
[2m[36m(func pid=12671)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=12671)[0m f1_macro: 0.19735278414287113
[2m[36m(func pid=12671)[0m f1_weighted: 0.236668077258702
[2m[36m(func pid=12671)[0m f1_per_class: [0.22, 0.016, 0.149, 0.546, 0.0, 0.449, 0.0, 0.39, 0.0, 0.203]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9616 | Steps: 4 | Val loss: 2.3185 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8581 | Steps: 4 | Val loss: 2.2671 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0548 | Steps: 4 | Val loss: 2.3792 | Batch size: 32 | lr: 0.01 | Duration: 4.35s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 66.1384 | Steps: 4 | Val loss: 86.1504 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=20296)[0m top1: 0.23740671641791045
[2m[36m(func pid=20296)[0m top5: 0.5699626865671642
[2m[36m(func pid=20296)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=20296)[0m f1_macro: 0.056141436589383954
[2m[36m(func pid=20296)[0m f1_weighted: 0.1196813753433839
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.044, 0.071, 0.394, 0.0, 0.0, 0.0, 0.024, 0.0, 0.028]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.15764925373134328
[2m[36m(func pid=20299)[0m top5: 0.554570895522388
[2m[36m(func pid=20299)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=20299)[0m f1_macro: 0.06413694754954666
[2m[36m(func pid=20299)[0m f1_weighted: 0.056167321378643664
[2m[36m(func pid=20299)[0m f1_per_class: [0.133, 0.291, 0.178, 0.0, 0.0, 0.0, 0.006, 0.0, 0.0, 0.033]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.020522388059701493
[2m[36m(func pid=21639)[0m top5: 0.4468283582089552
[2m[36m(func pid=21639)[0m f1_micro: 0.020522388059701493
[2m[36m(func pid=21639)[0m f1_macro: 0.004322200392927309
[2m[36m(func pid=21639)[0m f1_weighted: 0.0008870187373544849
[2m[36m(func pid=21639)[0m f1_per_class: [0.043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=21639)[0m 
== Status ==
Current time: 2024-01-07 10:39:25 (running for 00:21:54.76)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 66.138 |      0.119 |                   37 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.962 |      0.056 |                    4 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.858 |      0.064 |                    4 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  3.055 |      0.004 |                    1 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.16557835820895522
[2m[36m(func pid=12671)[0m top5: 0.7000932835820896
[2m[36m(func pid=12671)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=12671)[0m f1_macro: 0.11915587611778503
[2m[36m(func pid=12671)[0m f1_weighted: 0.11086616747579392
[2m[36m(func pid=12671)[0m f1_per_class: [0.085, 0.316, 0.127, 0.0, 0.0, 0.329, 0.0, 0.27, 0.0, 0.065]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9305 | Steps: 4 | Val loss: 2.3281 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.8848 | Steps: 4 | Val loss: 2.2194 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9067 | Steps: 4 | Val loss: 2.3026 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 72.5410 | Steps: 4 | Val loss: 102.2875 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=20296)[0m top1: 0.24113805970149255
[2m[36m(func pid=20296)[0m top5: 0.5541044776119403
[2m[36m(func pid=20296)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=20296)[0m f1_macro: 0.05717654057618623
[2m[36m(func pid=20296)[0m f1_weighted: 0.11920336193131863
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.038, 0.123, 0.399, 0.0, 0.0, 0.0, 0.011, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.1394589552238806
[2m[36m(func pid=20299)[0m top5: 0.7434701492537313
[2m[36m(func pid=20299)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=20299)[0m f1_macro: 0.0626441339627117
[2m[36m(func pid=20299)[0m f1_weighted: 0.1204624974676308
[2m[36m(func pid=20299)[0m f1_per_class: [0.09, 0.225, 0.0, 0.031, 0.0, 0.0, 0.237, 0.0, 0.0, 0.043]
[2m[36m(func pid=20299)[0m 
== Status ==
Current time: 2024-01-07 10:39:30 (running for 00:21:59.98)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 72.541 |      0.159 |                   38 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.931 |      0.057 |                    5 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.885 |      0.063 |                    5 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  3.055 |      0.004 |                    1 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.14925373134328357
[2m[36m(func pid=12671)[0m top5: 0.6068097014925373
[2m[36m(func pid=12671)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=12671)[0m f1_macro: 0.15887910278422007
[2m[36m(func pid=12671)[0m f1_weighted: 0.10087913609562908
[2m[36m(func pid=12671)[0m f1_per_class: [0.27, 0.285, 0.256, 0.0, 0.227, 0.158, 0.018, 0.326, 0.0, 0.048]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=21639)[0m top1: 0.017257462686567165
[2m[36m(func pid=21639)[0m top5: 0.4510261194029851
[2m[36m(func pid=21639)[0m f1_micro: 0.017257462686567165
[2m[36m(func pid=21639)[0m f1_macro: 0.024176289403505063
[2m[36m(func pid=21639)[0m f1_weighted: 0.0066151909784695345
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.0, 0.114, 0.0, 0.021, 0.0, 0.0, 0.098, 0.0, 0.009]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.0032 | Steps: 4 | Val loss: 2.3352 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7384 | Steps: 4 | Val loss: 2.1549 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 71.7944 | Steps: 4 | Val loss: 115.9666 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.6845 | Steps: 4 | Val loss: 2.0688 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=20296)[0m top1: 0.23087686567164178
[2m[36m(func pid=20296)[0m top5: 0.5433768656716418
[2m[36m(func pid=20296)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=20296)[0m f1_macro: 0.050605553966130555
[2m[36m(func pid=20296)[0m f1_weighted: 0.11397388491141634
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.02, 0.092, 0.394, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.2453358208955224
[2m[36m(func pid=20299)[0m top5: 0.8736007462686567
[2m[36m(func pid=20299)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=20299)[0m f1_macro: 0.07748798887220222
[2m[36m(func pid=20299)[0m f1_weighted: 0.18250601153617757
[2m[36m(func pid=20299)[0m f1_per_class: [0.135, 0.0, 0.0, 0.369, 0.0, 0.0, 0.258, 0.0, 0.0, 0.014]
[2m[36m(func pid=20299)[0m 
== Status ==
Current time: 2024-01-07 10:39:35 (running for 00:22:05.37)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 71.794 |      0.139 |                   39 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  3.003 |      0.051 |                    6 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.738 |      0.077 |                    6 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.907 |      0.024 |                    2 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.23134328358208955
[2m[36m(func pid=12671)[0m top5: 0.6077425373134329
[2m[36m(func pid=12671)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=12671)[0m f1_macro: 0.1389462690625425
[2m[36m(func pid=12671)[0m f1_weighted: 0.21085732253380265
[2m[36m(func pid=12671)[0m f1_per_class: [0.126, 0.111, 0.222, 0.0, 0.046, 0.164, 0.557, 0.016, 0.0, 0.146]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=21639)[0m top1: 0.24300373134328357
[2m[36m(func pid=21639)[0m top5: 0.9169776119402985
[2m[36m(func pid=21639)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=21639)[0m f1_macro: 0.055511041009463725
[2m[36m(func pid=21639)[0m f1_weighted: 0.11348695206930648
[2m[36m(func pid=21639)[0m f1_per_class: [0.16, 0.0, 0.0, 0.395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.9078 | Steps: 4 | Val loss: 2.3338 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.6872 | Steps: 4 | Val loss: 2.1487 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 66.7091 | Steps: 4 | Val loss: 125.4897 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.0435 | Steps: 4 | Val loss: 2.2189 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=20296)[0m top1: 0.2140858208955224
[2m[36m(func pid=20296)[0m top5: 0.5499067164179104
[2m[36m(func pid=20296)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=20296)[0m f1_macro: 0.05095881271333019
[2m[36m(func pid=20296)[0m f1_weighted: 0.11111395841113383
[2m[36m(func pid=20296)[0m f1_per_class: [0.028, 0.02, 0.059, 0.377, 0.0, 0.0, 0.0, 0.025, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.30130597014925375
[2m[36m(func pid=20299)[0m top5: 0.8362873134328358
[2m[36m(func pid=20299)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=20299)[0m f1_macro: 0.12995457421099751
[2m[36m(func pid=20299)[0m f1_weighted: 0.20319246927007484
[2m[36m(func pid=20299)[0m f1_per_class: [0.158, 0.0, 0.414, 0.438, 0.0, 0.0, 0.251, 0.0, 0.0, 0.039]
[2m[36m(func pid=20299)[0m 
== Status ==
Current time: 2024-01-07 10:39:41 (running for 00:22:10.89)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 66.709 |      0.152 |                   40 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.908 |      0.051 |                    7 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.687 |      0.13  |                    7 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.684 |      0.056 |                    3 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.23740671641791045
[2m[36m(func pid=12671)[0m top5: 0.5611007462686567
[2m[36m(func pid=12671)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=12671)[0m f1_macro: 0.15231016430465563
[2m[36m(func pid=12671)[0m f1_weighted: 0.21034098921415784
[2m[36m(func pid=12671)[0m f1_per_class: [0.207, 0.021, 0.153, 0.0, 0.034, 0.32, 0.541, 0.0, 0.025, 0.222]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=21639)[0m top1: 0.01166044776119403
[2m[36m(func pid=21639)[0m top5: 0.6548507462686567
[2m[36m(func pid=21639)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=21639)[0m f1_macro: 0.002311604253351826
[2m[36m(func pid=21639)[0m f1_weighted: 0.000269543406407629
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8646 | Steps: 4 | Val loss: 2.3360 | Batch size: 32 | lr: 0.0001 | Duration: 2.70s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6927 | Steps: 4 | Val loss: 2.1845 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 67.0725 | Steps: 4 | Val loss: 150.3310 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.6649 | Steps: 4 | Val loss: 2.4476 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=20296)[0m top1: 0.1730410447761194
[2m[36m(func pid=20296)[0m top5: 0.5457089552238806
[2m[36m(func pid=20296)[0m f1_micro: 0.1730410447761194
[2m[36m(func pid=20296)[0m f1_macro: 0.040485498123315186
[2m[36m(func pid=20296)[0m f1_weighted: 0.09905824654640602
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.005, 0.037, 0.348, 0.0, 0.0, 0.0, 0.014, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.08861940298507463
[2m[36m(func pid=20299)[0m top5: 0.820429104477612
[2m[36m(func pid=20299)[0m f1_micro: 0.08861940298507463
[2m[36m(func pid=20299)[0m f1_macro: 0.06497016904109103
[2m[36m(func pid=20299)[0m f1_weighted: 0.1147123241937078
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.0, 0.176, 0.148, 0.067, 0.0, 0.24, 0.0, 0.0, 0.018]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=12671)[0m top1: 0.12826492537313433
[2m[36m(func pid=12671)[0m top5: 0.4701492537313433
[2m[36m(func pid=12671)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=12671)[0m f1_macro: 0.17428552896870636
[2m[36m(func pid=12671)[0m f1_weighted: 0.07917347458728724
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.0, 0.571, 0.0, 0.057, 0.331, 0.009, 0.507, 0.105, 0.162]
[2m[36m(func pid=12671)[0m 
== Status ==
Current time: 2024-01-07 10:39:46 (running for 00:22:16.21)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 67.073 |      0.174 |                   41 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.865 |      0.04  |                    8 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.693 |      0.065 |                    8 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  3.044 |      0.002 |                    4 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=21639)[0m top1: 0.01166044776119403
[2m[36m(func pid=21639)[0m top5: 0.4878731343283582
[2m[36m(func pid=21639)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=21639)[0m f1_macro: 0.002321262766945218
[2m[36m(func pid=21639)[0m f1_weighted: 0.00027066963233969424
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8370 | Steps: 4 | Val loss: 2.3354 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6602 | Steps: 4 | Val loss: 2.2625 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 76.4334 | Steps: 4 | Val loss: 125.5727 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.5793 | Steps: 4 | Val loss: 2.1666 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=20296)[0m top1: 0.12733208955223882
[2m[36m(func pid=20296)[0m top5: 0.5438432835820896
[2m[36m(func pid=20296)[0m f1_micro: 0.12733208955223882
[2m[36m(func pid=20296)[0m f1_macro: 0.03580727367370691
[2m[36m(func pid=20296)[0m f1_weighted: 0.08857797925303952
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.011, 0.024, 0.307, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.01585820895522388
[2m[36m(func pid=20299)[0m top5: 0.7248134328358209
[2m[36m(func pid=20299)[0m f1_micro: 0.01585820895522388
[2m[36m(func pid=20299)[0m f1_macro: 0.02015207015207015
[2m[36m(func pid=20299)[0m f1_weighted: 0.0032023212384990453
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.0, 0.137, 0.0, 0.038, 0.0, 0.006, 0.0, 0.0, 0.02]
[2m[36m(func pid=20299)[0m 
== Status ==
Current time: 2024-01-07 10:39:51 (running for 00:22:21.41)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 76.433 |      0.191 |                   42 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.837 |      0.036 |                    9 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.66  |      0.02  |                    9 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.665 |      0.002 |                    5 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.16277985074626866
[2m[36m(func pid=12671)[0m top5: 0.4001865671641791
[2m[36m(func pid=12671)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=12671)[0m f1_macro: 0.19058964946553034
[2m[36m(func pid=12671)[0m f1_weighted: 0.08629540700973468
[2m[36m(func pid=12671)[0m f1_per_class: [0.29, 0.005, 0.327, 0.0, 0.096, 0.349, 0.0, 0.534, 0.11, 0.194]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=21639)[0m top1: 0.0914179104477612
[2m[36m(func pid=21639)[0m top5: 0.7322761194029851
[2m[36m(func pid=21639)[0m f1_micro: 0.0914179104477612
[2m[36m(func pid=21639)[0m f1_macro: 0.11400067433381653
[2m[36m(func pid=21639)[0m f1_weighted: 0.11200205989423447
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.0, 0.4, 0.31, 0.018, 0.0, 0.0, 0.374, 0.038, 0.0]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8952 | Steps: 4 | Val loss: 2.3411 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7581 | Steps: 4 | Val loss: 2.2678 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 52.1759 | Steps: 4 | Val loss: 89.9217 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=20296)[0m top1: 0.0625
[2m[36m(func pid=20296)[0m top5: 0.5368470149253731
[2m[36m(func pid=20296)[0m f1_micro: 0.0625
[2m[36m(func pid=20296)[0m f1_macro: 0.03063342450439225
[2m[36m(func pid=20296)[0m f1_weighted: 0.059895151492106224
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.005, 0.017, 0.192, 0.0, 0.0, 0.0, 0.092, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.7217 | Steps: 4 | Val loss: 2.0319 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=20299)[0m top1: 0.02332089552238806
[2m[36m(func pid=20299)[0m top5: 0.613339552238806
[2m[36m(func pid=20299)[0m f1_micro: 0.02332089552238806
[2m[36m(func pid=20299)[0m f1_macro: 0.04488409249398708
[2m[36m(func pid=20299)[0m f1_weighted: 0.012993794297119169
[2m[36m(func pid=20299)[0m f1_per_class: [0.155, 0.0, 0.244, 0.0, 0.0, 0.0, 0.027, 0.0, 0.0, 0.022]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=12671)[0m top1: 0.20475746268656717
[2m[36m(func pid=12671)[0m top5: 0.6236007462686567
[2m[36m(func pid=12671)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=12671)[0m f1_macro: 0.18808880930121863
[2m[36m(func pid=12671)[0m f1_weighted: 0.19579421600416355
[2m[36m(func pid=12671)[0m f1_per_class: [0.125, 0.248, 0.085, 0.286, 0.178, 0.353, 0.0, 0.427, 0.084, 0.094]
== Status ==
Current time: 2024-01-07 10:39:57 (running for 00:22:26.82)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 52.176 |      0.188 |                   43 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.895 |      0.031 |                   10 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.758 |      0.045 |                   10 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.579 |      0.114 |                    6 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=21639)[0m top1: 0.17117537313432835
[2m[36m(func pid=21639)[0m top5: 0.8157649253731343
[2m[36m(func pid=21639)[0m f1_micro: 0.17117537313432835
[2m[36m(func pid=21639)[0m f1_macro: 0.13246358101109593
[2m[36m(func pid=21639)[0m f1_weighted: 0.2042937539255238
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.146, 0.381, 0.175, 0.116, 0.0, 0.419, 0.0, 0.056, 0.031]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8146 | Steps: 4 | Val loss: 2.3377 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.5855 | Steps: 4 | Val loss: 2.2112 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 37.6052 | Steps: 4 | Val loss: 81.8124 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=20296)[0m top1: 0.037779850746268655
[2m[36m(func pid=20296)[0m top5: 0.5289179104477612
[2m[36m(func pid=20296)[0m f1_micro: 0.037779850746268655
[2m[36m(func pid=20296)[0m f1_macro: 0.017799757220885525
[2m[36m(func pid=20296)[0m f1_weighted: 0.03856044722856672
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.0, 0.015, 0.131, 0.0, 0.0, 0.0, 0.032, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.1473 | Steps: 4 | Val loss: 2.3047 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=20299)[0m top1: 0.10680970149253731
[2m[36m(func pid=20299)[0m top5: 0.6021455223880597
[2m[36m(func pid=20299)[0m f1_micro: 0.10680970149253732
[2m[36m(func pid=20299)[0m f1_macro: 0.13359688835788475
[2m[36m(func pid=20299)[0m f1_weighted: 0.1168144006142307
[2m[36m(func pid=20299)[0m f1_per_class: [0.219, 0.0, 0.444, 0.0, 0.042, 0.0, 0.318, 0.188, 0.1, 0.026]
[2m[36m(func pid=20299)[0m 
== Status ==
Current time: 2024-01-07 10:40:02 (running for 00:22:31.83)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 37.605 |      0.204 |                   44 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.815 |      0.018 |                   11 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.585 |      0.134 |                   11 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.722 |      0.132 |                    7 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.3148320895522388
[2m[36m(func pid=12671)[0m top5: 0.6506529850746269
[2m[36m(func pid=12671)[0m f1_micro: 0.3148320895522388
[2m[36m(func pid=12671)[0m f1_macro: 0.20442381222823508
[2m[36m(func pid=12671)[0m f1_weighted: 0.23732007089846466
[2m[36m(func pid=12671)[0m f1_per_class: [0.258, 0.443, 0.211, 0.447, 0.118, 0.0, 0.0, 0.481, 0.0, 0.087]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8452 | Steps: 4 | Val loss: 2.3463 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=21639)[0m top1: 0.07835820895522388
[2m[36m(func pid=21639)[0m top5: 0.5956156716417911
[2m[36m(func pid=21639)[0m f1_micro: 0.07835820895522388
[2m[36m(func pid=21639)[0m f1_macro: 0.10534494564052095
[2m[36m(func pid=21639)[0m f1_weighted: 0.0651688492882255
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.204, 0.333, 0.0, 0.0, 0.0, 0.0, 0.468, 0.019, 0.029]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.7065 | Steps: 4 | Val loss: 2.1532 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 55.1154 | Steps: 4 | Val loss: 87.7751 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=20296)[0m top1: 0.018656716417910446
[2m[36m(func pid=20296)[0m top5: 0.5167910447761194
[2m[36m(func pid=20296)[0m f1_micro: 0.018656716417910446
[2m[36m(func pid=20296)[0m f1_macro: 0.008144035974224653
[2m[36m(func pid=20296)[0m f1_weighted: 0.019097522534054494
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.0, 0.013, 0.068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.3653 | Steps: 4 | Val loss: 2.2774 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=20299)[0m top1: 0.2392723880597015
[2m[36m(func pid=20299)[0m top5: 0.6842350746268657
[2m[36m(func pid=20299)[0m f1_micro: 0.2392723880597015
[2m[36m(func pid=20299)[0m f1_macro: 0.12649415821981855
[2m[36m(func pid=20299)[0m f1_weighted: 0.19389589821358716
[2m[36m(func pid=20299)[0m f1_per_class: [0.184, 0.165, 0.133, 0.003, 0.0, 0.0, 0.495, 0.161, 0.082, 0.041]
[2m[36m(func pid=20299)[0m 
== Status ==
Current time: 2024-01-07 10:40:07 (running for 00:22:37.16)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 55.115 |      0.154 |                   45 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.845 |      0.008 |                   12 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.707 |      0.126 |                   12 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  3.147 |      0.105 |                    8 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.261660447761194
[2m[36m(func pid=12671)[0m top5: 0.6305970149253731
[2m[36m(func pid=12671)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=12671)[0m f1_macro: 0.15410105968920332
[2m[36m(func pid=12671)[0m f1_weighted: 0.17584440832610398
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.436, 0.314, 0.245, 0.0, 0.0, 0.003, 0.502, 0.0, 0.04]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.8669 | Steps: 4 | Val loss: 2.3572 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=21639)[0m top1: 0.15671641791044777
[2m[36m(func pid=21639)[0m top5: 0.6963619402985075
[2m[36m(func pid=21639)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=21639)[0m f1_macro: 0.13138300335934225
[2m[36m(func pid=21639)[0m f1_weighted: 0.1484432899236077
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.0, 0.383, 0.423, 0.022, 0.0, 0.0, 0.486, 0.0, 0.0]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6594 | Steps: 4 | Val loss: 2.1064 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 68.2411 | Steps: 4 | Val loss: 89.0666 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
[2m[36m(func pid=20296)[0m top1: 0.013992537313432836
[2m[36m(func pid=20296)[0m top5: 0.49953358208955223
[2m[36m(func pid=20296)[0m f1_micro: 0.013992537313432836
[2m[36m(func pid=20296)[0m f1_macro: 0.005851816957738435
[2m[36m(func pid=20296)[0m f1_weighted: 0.01279054785257785
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.0, 0.013, 0.046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.28125
[2m[36m(func pid=20299)[0m top5: 0.7705223880597015
[2m[36m(func pid=20299)[0m f1_micro: 0.28125
[2m[36m(func pid=20299)[0m f1_macro: 0.1608101471857843
[2m[36m(func pid=20299)[0m f1_weighted: 0.22950354558939232
[2m[36m(func pid=20299)[0m f1_per_class: [0.193, 0.331, 0.393, 0.016, 0.0, 0.0, 0.52, 0.085, 0.044, 0.026]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 3.1856 | Steps: 4 | Val loss: 2.5318 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 10:40:12 (running for 00:22:42.24)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 68.241 |      0.134 |                   46 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.867 |      0.006 |                   13 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.659 |      0.161 |                   13 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  3.365 |      0.131 |                    9 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.1669776119402985
[2m[36m(func pid=12671)[0m top5: 0.7056902985074627
[2m[36m(func pid=12671)[0m f1_micro: 0.1669776119402985
[2m[36m(func pid=12671)[0m f1_macro: 0.13390560816561872
[2m[36m(func pid=12671)[0m f1_weighted: 0.22103208206011024
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.48, 0.021, 0.247, 0.296, 0.0, 0.222, 0.0, 0.0, 0.072]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8705 | Steps: 4 | Val loss: 2.3466 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=21639)[0m top1: 0.02005597014925373
[2m[36m(func pid=21639)[0m top5: 0.7803171641791045
[2m[36m(func pid=21639)[0m f1_micro: 0.02005597014925373
[2m[36m(func pid=21639)[0m f1_macro: 0.027886223730985916
[2m[36m(func pid=21639)[0m f1_weighted: 0.01613038009673159
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.017, 0.0, 0.012, 0.163, 0.087, 0.0]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5815 | Steps: 4 | Val loss: 2.0919 | Batch size: 32 | lr: 0.001 | Duration: 2.81s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 95.0530 | Steps: 4 | Val loss: 65.6919 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=20296)[0m top1: 0.014925373134328358
[2m[36m(func pid=20296)[0m top5: 0.49720149253731344
[2m[36m(func pid=20296)[0m f1_micro: 0.014925373134328358
[2m[36m(func pid=20296)[0m f1_macro: 0.006113779804097637
[2m[36m(func pid=20296)[0m f1_weighted: 0.013429231568491155
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.0, 0.013, 0.048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.22994402985074627
[2m[36m(func pid=20299)[0m top5: 0.7630597014925373
[2m[36m(func pid=20299)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=20299)[0m f1_macro: 0.10912072853631818
[2m[36m(func pid=20299)[0m f1_weighted: 0.20488629369394107
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.429, 0.126, 0.0, 0.0, 0.0, 0.413, 0.122, 0.0, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.4901 | Steps: 4 | Val loss: 1.9058 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=12671)[0m top1: 0.30130597014925375
[2m[36m(func pid=12671)[0m top5: 0.7611940298507462
[2m[36m(func pid=12671)[0m f1_micro: 0.30130597014925375
[2m[36m(func pid=12671)[0m f1_macro: 0.18010365393536434
[2m[36m(func pid=12671)[0m f1_weighted: 0.32351748953937043
[2m[36m(func pid=12671)[0m f1_per_class: [0.304, 0.277, 0.069, 0.436, 0.151, 0.0, 0.488, 0.0, 0.0, 0.076]
[2m[36m(func pid=12671)[0m 
== Status ==
Current time: 2024-01-07 10:40:17 (running for 00:22:47.50)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 95.053 |      0.18  |                   47 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.87  |      0.006 |                   14 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.581 |      0.109 |                   14 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  3.186 |      0.028 |                   10 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.8448 | Steps: 4 | Val loss: 2.3473 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7225 | Steps: 4 | Val loss: 2.1369 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=21639)[0m top1: 0.36986940298507465
[2m[36m(func pid=21639)[0m top5: 0.8689365671641791
[2m[36m(func pid=21639)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=21639)[0m f1_macro: 0.2073432677536397
[2m[36m(func pid=21639)[0m f1_weighted: 0.2777024912675251
[2m[36m(func pid=21639)[0m f1_per_class: [0.237, 0.425, 0.529, 0.0, 0.0, 0.181, 0.585, 0.0, 0.0, 0.116]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 42.8564 | Steps: 4 | Val loss: 75.9637 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=20296)[0m top1: 0.01912313432835821
[2m[36m(func pid=20296)[0m top5: 0.4869402985074627
[2m[36m(func pid=20296)[0m f1_micro: 0.01912313432835821
[2m[36m(func pid=20296)[0m f1_macro: 0.008421466253324978
[2m[36m(func pid=20296)[0m f1_weighted: 0.01922212463314347
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.005, 0.014, 0.065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.22201492537313433
[2m[36m(func pid=20299)[0m top5: 0.7481343283582089
[2m[36m(func pid=20299)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=20299)[0m f1_macro: 0.10637705703008034
[2m[36m(func pid=20299)[0m f1_weighted: 0.18629739905965373
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.492, 0.088, 0.0, 0.0, 0.016, 0.3, 0.168, 0.0, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9464 | Steps: 4 | Val loss: 1.7598 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:40:22 (running for 00:22:52.61)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 42.856 |      0.16  |                   48 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.845 |      0.008 |                   15 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.723 |      0.106 |                   15 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.49  |      0.207 |                   11 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.2523320895522388
[2m[36m(func pid=12671)[0m top5: 0.792910447761194
[2m[36m(func pid=12671)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=12671)[0m f1_macro: 0.15983979738989554
[2m[36m(func pid=12671)[0m f1_weighted: 0.26472379311098887
[2m[36m(func pid=12671)[0m f1_per_class: [0.168, 0.053, 0.25, 0.299, 0.043, 0.047, 0.53, 0.0, 0.061, 0.148]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.9019 | Steps: 4 | Val loss: 2.3416 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=21639)[0m top1: 0.3493470149253731
[2m[36m(func pid=21639)[0m top5: 0.9085820895522388
[2m[36m(func pid=21639)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=21639)[0m f1_macro: 0.20657117277416867
[2m[36m(func pid=21639)[0m f1_weighted: 0.2667400592523951
[2m[36m(func pid=21639)[0m f1_per_class: [0.337, 0.063, 0.373, 0.558, 0.0, 0.008, 0.215, 0.436, 0.0, 0.076]
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.6080 | Steps: 4 | Val loss: 2.1731 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 79.4865 | Steps: 4 | Val loss: 96.4383 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=20296)[0m top1: 0.028917910447761194
[2m[36m(func pid=20296)[0m top5: 0.48274253731343286
[2m[36m(func pid=20296)[0m f1_micro: 0.028917910447761194
[2m[36m(func pid=20296)[0m f1_macro: 0.01740059947213444
[2m[36m(func pid=20296)[0m f1_weighted: 0.030771753573975717
[2m[36m(func pid=20296)[0m f1_per_class: [0.0, 0.005, 0.015, 0.094, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.24766791044776118
[2m[36m(func pid=20299)[0m top5: 0.6963619402985075
[2m[36m(func pid=20299)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=20299)[0m f1_macro: 0.13907643304740433
[2m[36m(func pid=20299)[0m f1_weighted: 0.17994981150330802
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.426, 0.091, 0.0, 0.0, 0.249, 0.185, 0.315, 0.124, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.1803 | Steps: 4 | Val loss: 2.5081 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 10:40:28 (running for 00:22:57.73)
Memory usage on this node: 24.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 79.487 |      0.097 |                   49 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.902 |      0.017 |                   16 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.608 |      0.139 |                   16 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.946 |      0.207 |                   12 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.1357276119402985
[2m[36m(func pid=12671)[0m top5: 0.6721082089552238
[2m[36m(func pid=12671)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=12671)[0m f1_macro: 0.09731158201082132
[2m[36m(func pid=12671)[0m f1_weighted: 0.15343488875658462
[2m[36m(func pid=12671)[0m f1_per_class: [0.225, 0.005, 0.0, 0.092, 0.029, 0.188, 0.325, 0.0, 0.109, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.7511 | Steps: 4 | Val loss: 2.3278 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.5151 | Steps: 4 | Val loss: 2.2256 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=21639)[0m top1: 0.09048507462686567
[2m[36m(func pid=21639)[0m top5: 0.6618470149253731
[2m[36m(func pid=21639)[0m f1_micro: 0.09048507462686567
[2m[36m(func pid=21639)[0m f1_macro: 0.15102370212140967
[2m[36m(func pid=21639)[0m f1_weighted: 0.03314938548299679
[2m[36m(func pid=21639)[0m f1_per_class: [0.25, 0.0, 0.533, 0.0, 0.214, 0.0, 0.0, 0.343, 0.065, 0.105]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 74.3544 | Steps: 4 | Val loss: 91.7883 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=20296)[0m top1: 0.0480410447761194
[2m[36m(func pid=20296)[0m top5: 0.48973880597014924
[2m[36m(func pid=20296)[0m f1_micro: 0.0480410447761194
[2m[36m(func pid=20296)[0m f1_macro: 0.03734752953609126
[2m[36m(func pid=20296)[0m f1_weighted: 0.05008336686765548
[2m[36m(func pid=20296)[0m f1_per_class: [0.075, 0.031, 0.017, 0.13, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.1912313432835821
[2m[36m(func pid=20299)[0m top5: 0.632929104477612
[2m[36m(func pid=20299)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=20299)[0m f1_macro: 0.16762971631518353
[2m[36m(func pid=20299)[0m f1_weighted: 0.14734918913941394
[2m[36m(func pid=20299)[0m f1_per_class: [0.036, 0.448, 0.182, 0.0, 0.065, 0.328, 0.003, 0.439, 0.099, 0.077]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=12671)[0m top1: 0.19542910447761194
[2m[36m(func pid=12671)[0m top5: 0.6044776119402985
[2m[36m(func pid=12671)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=12671)[0m f1_macro: 0.16127581741490996
[2m[36m(func pid=12671)[0m f1_weighted: 0.18677680801817842
[2m[36m(func pid=12671)[0m f1_per_class: [0.286, 0.0, 0.0, 0.382, 0.054, 0.267, 0.042, 0.473, 0.109, 0.0]
== Status ==
Current time: 2024-01-07 10:40:33 (running for 00:23:02.99)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 74.354 |      0.161 |                   50 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.751 |      0.037 |                   17 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.515 |      0.168 |                   17 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.18  |      0.151 |                   13 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.4488 | Steps: 4 | Val loss: 2.4086 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8717 | Steps: 4 | Val loss: 2.3313 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4780 | Steps: 4 | Val loss: 2.2561 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=21639)[0m top1: 0.12360074626865672
[2m[36m(func pid=21639)[0m top5: 0.7168843283582089
[2m[36m(func pid=21639)[0m f1_micro: 0.12360074626865672
[2m[36m(func pid=21639)[0m f1_macro: 0.10855253558343977
[2m[36m(func pid=21639)[0m f1_weighted: 0.11143261203225252
[2m[36m(func pid=21639)[0m f1_per_class: [0.23, 0.0, 0.143, 0.308, 0.025, 0.0, 0.0, 0.329, 0.0, 0.05]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 91.5298 | Steps: 4 | Val loss: 104.9983 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=20296)[0m top1: 0.039645522388059705
[2m[36m(func pid=20296)[0m top5: 0.490205223880597
[2m[36m(func pid=20296)[0m f1_micro: 0.039645522388059705
[2m[36m(func pid=20296)[0m f1_macro: 0.028949842413296983
[2m[36m(func pid=20296)[0m f1_weighted: 0.04225827215688204
[2m[36m(func pid=20296)[0m f1_per_class: [0.06, 0.044, 0.017, 0.107, 0.0, 0.0, 0.0, 0.062, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.17397388059701493
[2m[36m(func pid=20299)[0m top5: 0.5970149253731343
[2m[36m(func pid=20299)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=20299)[0m f1_macro: 0.17325061577666603
[2m[36m(func pid=20299)[0m f1_weighted: 0.10980901444242584
[2m[36m(func pid=20299)[0m f1_per_class: [0.18, 0.235, 0.455, 0.0, 0.063, 0.32, 0.0, 0.401, 0.077, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=12671)[0m top1: 0.2681902985074627
[2m[36m(func pid=12671)[0m top5: 0.6086753731343284
[2m[36m(func pid=12671)[0m f1_micro: 0.2681902985074627
[2m[36m(func pid=12671)[0m f1_macro: 0.13625655314797286
[2m[36m(func pid=12671)[0m f1_weighted: 0.19980406234116752
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.016, 0.0, 0.497, 0.124, 0.312, 0.0, 0.328, 0.085, 0.0]
== Status ==
Current time: 2024-01-07 10:40:38 (running for 00:23:08.18)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 91.53  |      0.136 |                   51 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.872 |      0.029 |                   18 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.478 |      0.173 |                   18 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.449 |      0.109 |                   14 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.6222 | Steps: 4 | Val loss: 2.1164 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7790 | Steps: 4 | Val loss: 2.3198 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.5968 | Steps: 4 | Val loss: 2.2006 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 138.8182 | Steps: 4 | Val loss: 87.8060 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
[2m[36m(func pid=21639)[0m top1: 0.21688432835820895
[2m[36m(func pid=21639)[0m top5: 0.8302238805970149
[2m[36m(func pid=21639)[0m f1_micro: 0.21688432835820895
[2m[36m(func pid=21639)[0m f1_macro: 0.18189923714854458
[2m[36m(func pid=21639)[0m f1_weighted: 0.19745645165982015
[2m[36m(func pid=21639)[0m f1_per_class: [0.044, 0.037, 0.541, 0.509, 0.03, 0.175, 0.0, 0.411, 0.0, 0.071]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m top1: 0.055970149253731345
[2m[36m(func pid=20296)[0m top5: 0.503731343283582
[2m[36m(func pid=20296)[0m f1_micro: 0.055970149253731345
[2m[36m(func pid=20296)[0m f1_macro: 0.0381238697704747
[2m[36m(func pid=20296)[0m f1_weighted: 0.05768168098731359
[2m[36m(func pid=20296)[0m f1_per_class: [0.043, 0.061, 0.019, 0.141, 0.0, 0.0, 0.0, 0.116, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=12671)[0m top1: 0.3003731343283582
[2m[36m(func pid=12671)[0m top5: 0.6632462686567164
[2m[36m(func pid=12671)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=12671)[0m f1_macro: 0.1795483050483281
[2m[36m(func pid=12671)[0m f1_weighted: 0.25060234334699516
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.163, 0.0, 0.545, 0.109, 0.312, 0.003, 0.488, 0.135, 0.039]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m top1: 0.16044776119402984
[2m[36m(func pid=20299)[0m top5: 0.7220149253731343
[2m[36m(func pid=20299)[0m f1_micro: 0.16044776119402984
[2m[36m(func pid=20299)[0m f1_macro: 0.11802580845384551
[2m[36m(func pid=20299)[0m f1_weighted: 0.07259745764705668
[2m[36m(func pid=20299)[0m f1_per_class: [0.126, 0.021, 0.375, 0.042, 0.0, 0.296, 0.0, 0.32, 0.0, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 1.9459 | Steps: 4 | Val loss: 2.2748 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7829 | Steps: 4 | Val loss: 2.3116 | Batch size: 32 | lr: 0.0001 | Duration: 2.64s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 112.7310 | Steps: 4 | Val loss: 57.6370 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.5842 | Steps: 4 | Val loss: 2.1084 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 10:40:46 (running for 00:23:16.25)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 138.818 |      0.18  |                   52 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.779 |      0.038 |                   19 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.597 |      0.118 |                   19 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.946 |      0.126 |                   16 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=21639)[0m top1: 0.20522388059701493
[2m[36m(func pid=21639)[0m top5: 0.7663246268656716
[2m[36m(func pid=21639)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=21639)[0m f1_macro: 0.12648505679703975
[2m[36m(func pid=21639)[0m f1_weighted: 0.17024636813680888
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.0, 0.224, 0.0, 0.138, 0.409, 0.395, 0.0, 0.098, 0.0]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m top1: 0.07229477611940298
[2m[36m(func pid=20296)[0m top5: 0.507929104477612
[2m[36m(func pid=20296)[0m f1_micro: 0.07229477611940298
[2m[36m(func pid=20296)[0m f1_macro: 0.04269208495461865
[2m[36m(func pid=20296)[0m f1_weighted: 0.06662499976213083
[2m[36m(func pid=20296)[0m f1_per_class: [0.051, 0.071, 0.024, 0.167, 0.0, 0.0, 0.0, 0.113, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.3050373134328358
[2m[36m(func pid=20299)[0m top5: 0.7430037313432836
[2m[36m(func pid=20299)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=20299)[0m f1_macro: 0.2179333230488726
[2m[36m(func pid=20299)[0m f1_weighted: 0.2414986238720986
[2m[36m(func pid=20299)[0m f1_per_class: [0.232, 0.215, 0.552, 0.487, 0.0, 0.364, 0.0, 0.33, 0.0, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=12671)[0m top1: 0.30177238805970147
[2m[36m(func pid=12671)[0m top5: 0.8180970149253731
[2m[36m(func pid=12671)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=12671)[0m f1_macro: 0.2626922398229534
[2m[36m(func pid=12671)[0m f1_weighted: 0.32219253945877796
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.559, 0.632, 0.351, 0.105, 0.258, 0.229, 0.384, 0.11, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8108 | Steps: 4 | Val loss: 2.3404 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.8121 | Steps: 4 | Val loss: 2.3154 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 122.2832 | Steps: 4 | Val loss: 119.6302 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.5574 | Steps: 4 | Val loss: 2.0265 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 10:40:51 (running for 00:23:21.51)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 112.731 |      0.263 |                   53 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.783 |      0.043 |                   20 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.584 |      0.218 |                   20 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   2.811 |      0.139 |                   17 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=21639)[0m top1: 0.24067164179104478
[2m[36m(func pid=21639)[0m top5: 0.6725746268656716
[2m[36m(func pid=21639)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=21639)[0m f1_macro: 0.13880241954368525
[2m[36m(func pid=21639)[0m f1_weighted: 0.18949225452958113
[2m[36m(func pid=21639)[0m f1_per_class: [0.145, 0.037, 0.474, 0.0, 0.1, 0.0, 0.591, 0.0, 0.0, 0.042]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m top1: 0.07462686567164178
[2m[36m(func pid=20296)[0m top5: 0.5060634328358209
[2m[36m(func pid=20296)[0m f1_micro: 0.07462686567164178
[2m[36m(func pid=20296)[0m f1_macro: 0.04529375871304559
[2m[36m(func pid=20296)[0m f1_weighted: 0.06894367716604807
[2m[36m(func pid=20296)[0m f1_per_class: [0.041, 0.073, 0.029, 0.171, 0.0, 0.0, 0.0, 0.127, 0.013, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=12671)[0m top1: 0.26399253731343286
[2m[36m(func pid=12671)[0m top5: 0.5676305970149254
[2m[36m(func pid=12671)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=12671)[0m f1_macro: 0.13072221945246734
[2m[36m(func pid=12671)[0m f1_weighted: 0.18109819385445766
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.353, 0.512, 0.0, 0.0, 0.08, 0.363, 0.0, 0.0, 0.0]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m top1: 0.3558768656716418
[2m[36m(func pid=20299)[0m top5: 0.8180970149253731
[2m[36m(func pid=20299)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=20299)[0m f1_macro: 0.20635820265781266
[2m[36m(func pid=20299)[0m f1_weighted: 0.2898791238567145
[2m[36m(func pid=20299)[0m f1_per_class: [0.114, 0.512, 0.214, 0.464, 0.0, 0.441, 0.0, 0.319, 0.0, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4786 | Steps: 4 | Val loss: 1.8581 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.8171 | Steps: 4 | Val loss: 2.3084 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 164.7090 | Steps: 4 | Val loss: 124.0852 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.4655 | Steps: 4 | Val loss: 2.0431 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:40:57 (running for 00:23:26.78)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 122.283 |      0.131 |                   54 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.812 |      0.045 |                   21 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.557 |      0.206 |                   21 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   2.479 |      0.243 |                   18 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=21639)[0m top1: 0.291044776119403
[2m[36m(func pid=21639)[0m top5: 0.8647388059701493
[2m[36m(func pid=21639)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=21639)[0m f1_macro: 0.24337403478369762
[2m[36m(func pid=21639)[0m f1_weighted: 0.1870061949268783
[2m[36m(func pid=21639)[0m f1_per_class: [0.281, 0.424, 0.593, 0.104, 0.0, 0.377, 0.006, 0.5, 0.0, 0.148]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m top1: 0.09001865671641791
[2m[36m(func pid=20296)[0m top5: 0.523320895522388
[2m[36m(func pid=20296)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=20296)[0m f1_macro: 0.049521410831842284
[2m[36m(func pid=20296)[0m f1_weighted: 0.07873182475992424
[2m[36m(func pid=20296)[0m f1_per_class: [0.043, 0.091, 0.039, 0.194, 0.0, 0.008, 0.003, 0.088, 0.029, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=12671)[0m top1: 0.18470149253731344
[2m[36m(func pid=12671)[0m top5: 0.5093283582089553
[2m[36m(func pid=12671)[0m f1_micro: 0.18470149253731344
[2m[36m(func pid=12671)[0m f1_macro: 0.1353269102826558
[2m[36m(func pid=12671)[0m f1_weighted: 0.15754709406464187
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.508, 0.107, 0.0, 0.029, 0.014, 0.147, 0.355, 0.044, 0.148]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m top1: 0.25652985074626866
[2m[36m(func pid=20299)[0m top5: 0.8138992537313433
[2m[36m(func pid=20299)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=20299)[0m f1_macro: 0.14985508375425688
[2m[36m(func pid=20299)[0m f1_weighted: 0.1544530432785709
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.357, 0.22, 0.0, 0.0, 0.314, 0.086, 0.521, 0.0, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 3.1680 | Steps: 4 | Val loss: 1.9629 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.8021 | Steps: 4 | Val loss: 2.2936 | Batch size: 32 | lr: 0.0001 | Duration: 2.75s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 86.7221 | Steps: 4 | Val loss: 117.3221 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4902 | Steps: 4 | Val loss: 2.0884 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 10:41:02 (running for 00:23:32.09)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 164.709 |      0.135 |                   55 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.817 |      0.05  |                   22 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.465 |      0.15  |                   22 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   3.168 |      0.213 |                   19 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=21639)[0m top1: 0.31529850746268656
[2m[36m(func pid=21639)[0m top5: 0.8894589552238806
[2m[36m(func pid=21639)[0m f1_micro: 0.31529850746268656
[2m[36m(func pid=21639)[0m f1_macro: 0.2130496466906498
[2m[36m(func pid=21639)[0m f1_weighted: 0.2564756577640188
[2m[36m(func pid=21639)[0m f1_per_class: [0.242, 0.011, 0.135, 0.53, 0.08, 0.354, 0.078, 0.545, 0.155, 0.0]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m top1: 0.10914179104477612
[2m[36m(func pid=20296)[0m top5: 0.5625
[2m[36m(func pid=20296)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=20296)[0m f1_macro: 0.05172351928037024
[2m[36m(func pid=20296)[0m f1_weighted: 0.08659098310410682
[2m[36m(func pid=20296)[0m f1_per_class: [0.047, 0.099, 0.055, 0.225, 0.0, 0.008, 0.0, 0.074, 0.01, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=12671)[0m top1: 0.08815298507462686
[2m[36m(func pid=12671)[0m top5: 0.6371268656716418
[2m[36m(func pid=12671)[0m f1_micro: 0.08815298507462686
[2m[36m(func pid=12671)[0m f1_macro: 0.11982000311209458
[2m[36m(func pid=12671)[0m f1_weighted: 0.10390829055916638
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.286, 0.075, 0.054, 0.024, 0.028, 0.009, 0.486, 0.107, 0.129]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m top1: 0.2332089552238806
[2m[36m(func pid=20299)[0m top5: 0.8031716417910447
[2m[36m(func pid=20299)[0m f1_micro: 0.2332089552238806
[2m[36m(func pid=20299)[0m f1_macro: 0.14063329500889296
[2m[36m(func pid=20299)[0m f1_weighted: 0.1508921123594792
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.333, 0.383, 0.0, 0.0, 0.212, 0.166, 0.289, 0.023, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.3438 | Steps: 4 | Val loss: 2.0652 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7559 | Steps: 4 | Val loss: 2.2844 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 53.3520 | Steps: 4 | Val loss: 102.5050 | Batch size: 32 | lr: 0.1 | Duration: 2.73s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.4436 | Steps: 4 | Val loss: 2.1716 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 10:41:07 (running for 00:23:37.44)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 86.722 |      0.12  |                   56 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.802 |      0.052 |                   23 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.49  |      0.141 |                   23 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.344 |      0.223 |                   20 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=21639)[0m top1: 0.24113805970149255
[2m[36m(func pid=21639)[0m top5: 0.8777985074626866
[2m[36m(func pid=21639)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=21639)[0m f1_macro: 0.22308460851943757
[2m[36m(func pid=21639)[0m f1_weighted: 0.2815600659949753
[2m[36m(func pid=21639)[0m f1_per_class: [0.262, 0.0, 0.556, 0.298, 0.042, 0.207, 0.493, 0.257, 0.116, 0.0]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m top1: 0.13432835820895522
[2m[36m(func pid=20296)[0m top5: 0.5750932835820896
[2m[36m(func pid=20296)[0m f1_micro: 0.13432835820895522
[2m[36m(func pid=20296)[0m f1_macro: 0.04933982621249068
[2m[36m(func pid=20296)[0m f1_weighted: 0.09388728247121685
[2m[36m(func pid=20296)[0m f1_per_class: [0.048, 0.091, 0.066, 0.272, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=12671)[0m top1: 0.23647388059701493
[2m[36m(func pid=12671)[0m top5: 0.5834888059701493
[2m[36m(func pid=12671)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=12671)[0m f1_macro: 0.21639423697251298
[2m[36m(func pid=12671)[0m f1_weighted: 0.21031533374540978
[2m[36m(func pid=12671)[0m f1_per_class: [0.235, 0.042, 0.37, 0.485, 0.12, 0.223, 0.0, 0.531, 0.079, 0.078]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m top1: 0.20242537313432835
[2m[36m(func pid=20299)[0m top5: 0.6935634328358209
[2m[36m(func pid=20299)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=20299)[0m f1_macro: 0.18611286161775425
[2m[36m(func pid=20299)[0m f1_weighted: 0.14300618717860578
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.468, 0.609, 0.0, 0.067, 0.353, 0.021, 0.119, 0.104, 0.121]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 1.8561 | Steps: 4 | Val loss: 2.7052 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7372 | Steps: 4 | Val loss: 2.2714 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 70.3364 | Steps: 4 | Val loss: 152.9813 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.4310 | Steps: 4 | Val loss: 2.2163 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 10:41:13 (running for 00:23:42.74)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 53.352 |      0.216 |                   57 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.756 |      0.049 |                   24 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.444 |      0.186 |                   24 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.856 |      0.116 |                   21 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=21639)[0m top1: 0.07182835820895522
[2m[36m(func pid=21639)[0m top5: 0.7280783582089553
[2m[36m(func pid=21639)[0m f1_micro: 0.07182835820895522
[2m[36m(func pid=21639)[0m f1_macro: 0.115858744763507
[2m[36m(func pid=21639)[0m f1_weighted: 0.07936089949966822
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.0, 0.212, 0.0, 0.259, 0.0, 0.156, 0.507, 0.0, 0.025]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m top1: 0.15625
[2m[36m(func pid=20296)[0m top5: 0.605410447761194
[2m[36m(func pid=20296)[0m f1_micro: 0.15625
[2m[36m(func pid=20296)[0m f1_macro: 0.05845153390765399
[2m[36m(func pid=20296)[0m f1_weighted: 0.10545377116064272
[2m[36m(func pid=20296)[0m f1_per_class: [0.056, 0.107, 0.099, 0.299, 0.0, 0.008, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=12671)[0m top1: 0.14039179104477612
[2m[36m(func pid=12671)[0m top5: 0.5023320895522388
[2m[36m(func pid=12671)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=12671)[0m f1_macro: 0.19134239930221517
[2m[36m(func pid=12671)[0m f1_weighted: 0.14377487185489696
[2m[36m(func pid=12671)[0m f1_per_class: [0.079, 0.0, 0.632, 0.313, 0.167, 0.191, 0.0, 0.469, 0.0, 0.063]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m top1: 0.12220149253731344
[2m[36m(func pid=20299)[0m top5: 0.6501865671641791
[2m[36m(func pid=20299)[0m f1_micro: 0.12220149253731344
[2m[36m(func pid=20299)[0m f1_macro: 0.14909342123751687
[2m[36m(func pid=20299)[0m f1_weighted: 0.07975242574421261
[2m[36m(func pid=20299)[0m f1_per_class: [0.044, 0.071, 0.476, 0.0, 0.099, 0.361, 0.003, 0.295, 0.102, 0.04]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5918 | Steps: 4 | Val loss: 1.8402 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.7649 | Steps: 4 | Val loss: 2.2660 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 166.3306 | Steps: 4 | Val loss: 156.6411 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.5878 | Steps: 4 | Val loss: 2.1883 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 10:41:18 (running for 00:23:48.25)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 70.336 |      0.191 |                   58 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.737 |      0.058 |                   25 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.431 |      0.149 |                   25 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.592 |      0.253 |                   22 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=21639)[0m top1: 0.30923507462686567
[2m[36m(func pid=21639)[0m top5: 0.9053171641791045
[2m[36m(func pid=21639)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=21639)[0m f1_macro: 0.2534676745388415
[2m[36m(func pid=21639)[0m f1_weighted: 0.2730425003636291
[2m[36m(func pid=21639)[0m f1_per_class: [0.251, 0.435, 0.56, 0.048, 0.068, 0.091, 0.457, 0.478, 0.0, 0.148]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m top1: 0.16557835820895522
[2m[36m(func pid=20296)[0m top5: 0.6114738805970149
[2m[36m(func pid=20296)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=20296)[0m f1_macro: 0.05990324739144037
[2m[36m(func pid=20296)[0m f1_weighted: 0.1012857150101279
[2m[36m(func pid=20296)[0m f1_per_class: [0.082, 0.062, 0.134, 0.313, 0.0, 0.008, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=12671)[0m top1: 0.25886194029850745
[2m[36m(func pid=12671)[0m top5: 0.5223880597014925
[2m[36m(func pid=12671)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=12671)[0m f1_macro: 0.20999051430869992
[2m[36m(func pid=12671)[0m f1_weighted: 0.19719334149344964
[2m[36m(func pid=12671)[0m f1_per_class: [0.132, 0.0, 0.632, 0.445, 0.0, 0.348, 0.0, 0.444, 0.0, 0.1]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m top1: 0.1357276119402985
[2m[36m(func pid=20299)[0m top5: 0.6865671641791045
[2m[36m(func pid=20299)[0m f1_micro: 0.1357276119402985
[2m[36m(func pid=20299)[0m f1_macro: 0.13759587400828824
[2m[36m(func pid=20299)[0m f1_weighted: 0.06686792037067739
[2m[36m(func pid=20299)[0m f1_per_class: [0.178, 0.026, 0.48, 0.0, 0.0, 0.302, 0.0, 0.363, 0.0, 0.027]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4110 | Steps: 4 | Val loss: 1.9649 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.7489 | Steps: 4 | Val loss: 2.2574 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 104.0599 | Steps: 4 | Val loss: 134.8499 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.4723 | Steps: 4 | Val loss: 2.0787 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 10:41:24 (running for 00:23:53.83)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 166.331 |      0.21  |                   59 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.765 |      0.06  |                   26 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.588 |      0.138 |                   26 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   2.411 |      0.216 |                   23 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.17210820895522388
[2m[36m(func pid=20296)[0m top5: 0.6245335820895522
[2m[36m(func pid=20296)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=20296)[0m f1_macro: 0.058753608976870556
[2m[36m(func pid=20296)[0m f1_weighted: 0.10226696854695984
[2m[36m(func pid=20296)[0m f1_per_class: [0.075, 0.048, 0.126, 0.323, 0.0, 0.016, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m top1: 0.2653917910447761
[2m[36m(func pid=21639)[0m top5: 0.8805970149253731
[2m[36m(func pid=21639)[0m f1_micro: 0.2653917910447761
[2m[36m(func pid=21639)[0m f1_macro: 0.21614874513963694
[2m[36m(func pid=21639)[0m f1_weighted: 0.2967753452209383
[2m[36m(func pid=21639)[0m f1_per_class: [0.218, 0.266, 0.143, 0.107, 0.046, 0.263, 0.514, 0.495, 0.11, 0.0]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=12671)[0m top1: 0.32882462686567165
[2m[36m(func pid=12671)[0m top5: 0.6707089552238806
[2m[36m(func pid=12671)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=12671)[0m f1_macro: 0.19030526776047996
[2m[36m(func pid=12671)[0m f1_weighted: 0.21171792339612472
[2m[36m(func pid=12671)[0m f1_per_class: [0.044, 0.0, 0.488, 0.513, 0.0, 0.357, 0.0, 0.382, 0.021, 0.097]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m top1: 0.2583955223880597
[2m[36m(func pid=20299)[0m top5: 0.7845149253731343
[2m[36m(func pid=20299)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=20299)[0m f1_macro: 0.188472308243189
[2m[36m(func pid=20299)[0m f1_weighted: 0.20042746957583704
[2m[36m(func pid=20299)[0m f1_per_class: [0.209, 0.076, 0.385, 0.453, 0.0, 0.284, 0.0, 0.352, 0.0, 0.125]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.7114 | Steps: 4 | Val loss: 2.2510 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.4590 | Steps: 4 | Val loss: 3.2897 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 92.9333 | Steps: 4 | Val loss: 102.4034 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.6530 | Steps: 4 | Val loss: 2.0470 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 10:41:29 (running for 00:23:59.20)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 104.06  |      0.19  |                   60 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.711 |      0.059 |                   28 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.472 |      0.188 |                   27 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   2.411 |      0.216 |                   23 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.177705223880597
[2m[36m(func pid=20296)[0m top5: 0.6413246268656716
[2m[36m(func pid=20296)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=20296)[0m f1_macro: 0.05883134172477698
[2m[36m(func pid=20296)[0m f1_weighted: 0.10720882662058243
[2m[36m(func pid=20296)[0m f1_per_class: [0.075, 0.055, 0.102, 0.333, 0.0, 0.023, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m top1: 0.24860074626865672
[2m[36m(func pid=21639)[0m top5: 0.5680970149253731
[2m[36m(func pid=21639)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=21639)[0m f1_macro: 0.14779222633716546
[2m[36m(func pid=21639)[0m f1_weighted: 0.1460726955562719
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.462, 0.09, 0.0, 0.222, 0.419, 0.0, 0.284, 0.0, 0.0]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=12671)[0m top1: 0.291044776119403
[2m[36m(func pid=12671)[0m top5: 0.7308768656716418
[2m[36m(func pid=12671)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=12671)[0m f1_macro: 0.19762595440096867
[2m[36m(func pid=12671)[0m f1_weighted: 0.27587738263366657
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.0, 0.18, 0.493, 0.0, 0.337, 0.238, 0.367, 0.114, 0.247]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m top1: 0.2933768656716418
[2m[36m(func pid=20299)[0m top5: 0.7518656716417911
[2m[36m(func pid=20299)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=20299)[0m f1_macro: 0.17355055870604053
[2m[36m(func pid=20299)[0m f1_weighted: 0.2381290278565716
[2m[36m(func pid=20299)[0m f1_per_class: [0.167, 0.238, 0.178, 0.494, 0.0, 0.296, 0.0, 0.363, 0.0, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.7105 | Steps: 4 | Val loss: 2.2506 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.3124 | Steps: 4 | Val loss: 2.7159 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 122.1787 | Steps: 4 | Val loss: 105.5468 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.6077 | Steps: 4 | Val loss: 2.0639 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 10:41:34 (running for 00:24:04.53)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 92.933 |      0.198 |                   61 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.711 |      0.061 |                   29 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.653 |      0.174 |                   28 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.459 |      0.148 |                   24 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.1707089552238806
[2m[36m(func pid=20296)[0m top5: 0.6520522388059702
[2m[36m(func pid=20296)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=20296)[0m f1_macro: 0.0608443173081364
[2m[36m(func pid=20296)[0m f1_weighted: 0.11466722821392203
[2m[36m(func pid=20296)[0m f1_per_class: [0.056, 0.081, 0.076, 0.32, 0.0, 0.07, 0.006, 0.0, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=12671)[0m top1: 0.2966417910447761
[2m[36m(func pid=12671)[0m top5: 0.710820895522388
[2m[36m(func pid=12671)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=12671)[0m f1_macro: 0.1913919110544958
[2m[36m(func pid=12671)[0m f1_weighted: 0.266095920029342
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.149, 0.087, 0.092, 0.2, 0.123, 0.575, 0.376, 0.081, 0.23]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=21639)[0m top1: 0.20755597014925373
[2m[36m(func pid=21639)[0m top5: 0.5396455223880597
[2m[36m(func pid=21639)[0m f1_micro: 0.20755597014925375
[2m[36m(func pid=21639)[0m f1_macro: 0.16472140211250322
[2m[36m(func pid=21639)[0m f1_weighted: 0.12892888481277145
[2m[36m(func pid=21639)[0m f1_per_class: [0.208, 0.376, 0.175, 0.0, 0.126, 0.324, 0.0, 0.345, 0.0, 0.095]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20299)[0m top1: 0.22761194029850745
[2m[36m(func pid=20299)[0m top5: 0.7476679104477612
[2m[36m(func pid=20299)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=20299)[0m f1_macro: 0.11081539536416782
[2m[36m(func pid=20299)[0m f1_weighted: 0.1802677074982215
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.193, 0.167, 0.399, 0.0, 0.259, 0.0, 0.091, 0.0, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.8009 | Steps: 4 | Val loss: 2.2583 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 125.0148 | Steps: 4 | Val loss: 137.2749 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.1408 | Steps: 4 | Val loss: 2.1715 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.4034 | Steps: 4 | Val loss: 2.1314 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 10:41:40 (running for 00:24:09.78)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 122.179 |      0.191 |                   62 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.801 |      0.06  |                   30 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.608 |      0.111 |                   29 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   2.312 |      0.165 |                   25 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.15811567164179105
[2m[36m(func pid=20296)[0m top5: 0.648320895522388
[2m[36m(func pid=20296)[0m f1_micro: 0.15811567164179105
[2m[36m(func pid=20296)[0m f1_macro: 0.060024852297128825
[2m[36m(func pid=20296)[0m f1_weighted: 0.11455412217012317
[2m[36m(func pid=20296)[0m f1_per_class: [0.043, 0.08, 0.053, 0.307, 0.0, 0.115, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=12671)[0m top1: 0.283115671641791
[2m[36m(func pid=12671)[0m top5: 0.6296641791044776
[2m[36m(func pid=12671)[0m f1_micro: 0.283115671641791
[2m[36m(func pid=12671)[0m f1_macro: 0.1876921018663391
[2m[36m(func pid=12671)[0m f1_weighted: 0.23799870764353157
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.423, 0.058, 0.0, 0.286, 0.03, 0.447, 0.406, 0.0, 0.227]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=21639)[0m top1: 0.2835820895522388
[2m[36m(func pid=21639)[0m top5: 0.8614738805970149
[2m[36m(func pid=21639)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=21639)[0m f1_macro: 0.15298442221690453
[2m[36m(func pid=21639)[0m f1_weighted: 0.2679730978532542
[2m[36m(func pid=21639)[0m f1_per_class: [0.044, 0.0, 0.0, 0.562, 0.047, 0.0, 0.27, 0.451, 0.079, 0.077]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20299)[0m top1: 0.1455223880597015
[2m[36m(func pid=20299)[0m top5: 0.6553171641791045
[2m[36m(func pid=20299)[0m f1_micro: 0.1455223880597015
[2m[36m(func pid=20299)[0m f1_macro: 0.08321833369131895
[2m[36m(func pid=20299)[0m f1_weighted: 0.0715903673934632
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.159, 0.25, 0.035, 0.0, 0.241, 0.0, 0.016, 0.13, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 92.2372 | Steps: 4 | Val loss: 142.1697 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.7221 | Steps: 4 | Val loss: 2.2562 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.1454 | Steps: 4 | Val loss: 2.1506 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.5168 | Steps: 4 | Val loss: 2.1529 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 10:41:45 (running for 00:24:14.92)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 125.015 |      0.188 |                   63 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.722 |      0.062 |                   31 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.403 |      0.083 |                   30 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   2.141 |      0.153 |                   26 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.15391791044776118
[2m[36m(func pid=20296)[0m top5: 0.6669776119402985
[2m[36m(func pid=20296)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=20296)[0m f1_macro: 0.061541933709898224
[2m[36m(func pid=20296)[0m f1_weighted: 0.11749688981716916
[2m[36m(func pid=20296)[0m f1_per_class: [0.042, 0.104, 0.048, 0.3, 0.0, 0.096, 0.009, 0.016, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=12671)[0m top1: 0.1828358208955224
[2m[36m(func pid=12671)[0m top5: 0.6473880597014925
[2m[36m(func pid=12671)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=12671)[0m f1_macro: 0.1435458932096832
[2m[36m(func pid=12671)[0m f1_weighted: 0.1433533034770562
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.527, 0.085, 0.0, 0.234, 0.006, 0.113, 0.19, 0.081, 0.2]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=21639)[0m top1: 0.22014925373134328
[2m[36m(func pid=21639)[0m top5: 0.8362873134328358
[2m[36m(func pid=21639)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=21639)[0m f1_macro: 0.14308292010957635
[2m[36m(func pid=21639)[0m f1_weighted: 0.24346396191044942
[2m[36m(func pid=21639)[0m f1_per_class: [0.044, 0.0, 0.435, 0.269, 0.047, 0.024, 0.539, 0.0, 0.024, 0.05]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20299)[0m top1: 0.14738805970149255
[2m[36m(func pid=20299)[0m top5: 0.6394589552238806
[2m[36m(func pid=20299)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=20299)[0m f1_macro: 0.11328975221069056
[2m[36m(func pid=20299)[0m f1_weighted: 0.08331961822706896
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.187, 0.224, 0.01, 0.052, 0.264, 0.0, 0.2, 0.124, 0.071]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6751 | Steps: 4 | Val loss: 2.2615 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 56.4457 | Steps: 4 | Val loss: 144.4646 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 1.6382 | Steps: 4 | Val loss: 1.9554 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.3440 | Steps: 4 | Val loss: 2.1883 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 10:41:50 (running for 00:24:20.30)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 92.237 |      0.144 |                   64 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.675 |      0.065 |                   32 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.517 |      0.113 |                   31 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.145 |      0.143 |                   27 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.1417910447761194
[2m[36m(func pid=20296)[0m top5: 0.667910447761194
[2m[36m(func pid=20296)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=20296)[0m f1_macro: 0.06517911853760897
[2m[36m(func pid=20296)[0m f1_weighted: 0.11827150311835954
[2m[36m(func pid=20296)[0m f1_per_class: [0.06, 0.154, 0.04, 0.272, 0.0, 0.084, 0.009, 0.031, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=12671)[0m top1: 0.22388059701492538
[2m[36m(func pid=12671)[0m top5: 0.6198694029850746
[2m[36m(func pid=12671)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=12671)[0m f1_macro: 0.1917654716983772
[2m[36m(func pid=12671)[0m f1_weighted: 0.16754018060391399
[2m[36m(func pid=12671)[0m f1_per_class: [0.291, 0.525, 0.267, 0.0, 0.064, 0.05, 0.132, 0.301, 0.155, 0.133]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=21639)[0m top1: 0.3050373134328358
[2m[36m(func pid=21639)[0m top5: 0.8885261194029851
[2m[36m(func pid=21639)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=21639)[0m f1_macro: 0.21111764466418798
[2m[36m(func pid=21639)[0m f1_weighted: 0.2792339662914165
[2m[36m(func pid=21639)[0m f1_per_class: [0.152, 0.489, 0.178, 0.196, 0.227, 0.391, 0.291, 0.016, 0.0, 0.17]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20299)[0m top1: 0.07882462686567164
[2m[36m(func pid=20299)[0m top5: 0.6497201492537313
[2m[36m(func pid=20299)[0m f1_micro: 0.07882462686567164
[2m[36m(func pid=20299)[0m f1_macro: 0.12603627313161805
[2m[36m(func pid=20299)[0m f1_weighted: 0.06438043509781845
[2m[36m(func pid=20299)[0m f1_per_class: [0.3, 0.065, 0.256, 0.0, 0.043, 0.205, 0.0, 0.371, 0.0, 0.02]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 71.9814 | Steps: 4 | Val loss: 166.8120 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6926 | Steps: 4 | Val loss: 2.2629 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.7406 | Steps: 4 | Val loss: 2.2347 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.4405 | Steps: 4 | Val loss: 2.2003 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 10:41:55 (running for 00:24:25.47)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 71.981 |      0.252 |                   66 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.675 |      0.065 |                   32 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.344 |      0.126 |                   32 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.638 |      0.211 |                   28 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=12671)[0m top1: 0.20988805970149255
[2m[36m(func pid=12671)[0m top5: 0.574160447761194
[2m[36m(func pid=12671)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=12671)[0m f1_macro: 0.2524529533196902
[2m[36m(func pid=12671)[0m f1_weighted: 0.21449962092280567
[2m[36m(func pid=12671)[0m f1_per_class: [0.273, 0.537, 0.632, 0.0, 0.028, 0.106, 0.223, 0.508, 0.092, 0.125]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20296)[0m top1: 0.13712686567164178
[2m[36m(func pid=20296)[0m top5: 0.6590485074626866
[2m[36m(func pid=20296)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=20296)[0m f1_macro: 0.06829031014653901
[2m[36m(func pid=20296)[0m f1_weighted: 0.11408764558989544
[2m[36m(func pid=20296)[0m f1_per_class: [0.08, 0.149, 0.038, 0.265, 0.0, 0.057, 0.003, 0.092, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m top1: 0.279384328358209
[2m[36m(func pid=21639)[0m top5: 0.8027052238805971
[2m[36m(func pid=21639)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=21639)[0m f1_macro: 0.21277948480594971
[2m[36m(func pid=21639)[0m f1_weighted: 0.21490573728710083
[2m[36m(func pid=21639)[0m f1_per_class: [0.117, 0.492, 0.092, 0.175, 0.118, 0.39, 0.0, 0.476, 0.115, 0.154]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20299)[0m top1: 0.06110074626865672
[2m[36m(func pid=20299)[0m top5: 0.6847014925373134
[2m[36m(func pid=20299)[0m f1_micro: 0.06110074626865672
[2m[36m(func pid=20299)[0m f1_macro: 0.13219889003484733
[2m[36m(func pid=20299)[0m f1_weighted: 0.0493586473379328
[2m[36m(func pid=20299)[0m f1_per_class: [0.226, 0.041, 0.462, 0.0, 0.031, 0.055, 0.0, 0.484, 0.0, 0.022]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 86.5295 | Steps: 4 | Val loss: 175.8924 | Batch size: 32 | lr: 0.1 | Duration: 2.67s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.7410 | Steps: 4 | Val loss: 2.2560 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.1802 | Steps: 4 | Val loss: 2.5840 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.4712 | Steps: 4 | Val loss: 2.1429 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=12671)[0m top1: 0.17024253731343283
[2m[36m(func pid=12671)[0m top5: 0.566231343283582
[2m[36m(func pid=12671)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=12671)[0m f1_macro: 0.17712599088140848
[2m[36m(func pid=12671)[0m f1_weighted: 0.17516436795089727
[2m[36m(func pid=12671)[0m f1_per_class: [0.219, 0.497, 0.267, 0.0, 0.039, 0.167, 0.166, 0.163, 0.106, 0.148]
[2m[36m(func pid=12671)[0m 
== Status ==
Current time: 2024-01-07 10:42:01 (running for 00:24:30.94)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 86.53  |      0.177 |                   67 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.741 |      0.075 |                   34 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.44  |      0.132 |                   33 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.741 |      0.213 |                   29 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.1478544776119403
[2m[36m(func pid=20296)[0m top5: 0.667910447761194
[2m[36m(func pid=20296)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=20296)[0m f1_macro: 0.07479086799748227
[2m[36m(func pid=20296)[0m f1_weighted: 0.12522111485972728
[2m[36m(func pid=20296)[0m f1_per_class: [0.074, 0.159, 0.038, 0.28, 0.0, 0.066, 0.012, 0.119, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m top1: 0.1259328358208955
[2m[36m(func pid=21639)[0m top5: 0.7014925373134329
[2m[36m(func pid=21639)[0m f1_micro: 0.1259328358208955
[2m[36m(func pid=21639)[0m f1_macro: 0.17391759906659282
[2m[36m(func pid=21639)[0m f1_weighted: 0.12176532365540561
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.0, 0.615, 0.287, 0.286, 0.104, 0.0, 0.4, 0.017, 0.03]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20299)[0m top1: 0.12546641791044777
[2m[36m(func pid=20299)[0m top5: 0.6851679104477612
[2m[36m(func pid=20299)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=20299)[0m f1_macro: 0.17673822396369848
[2m[36m(func pid=20299)[0m f1_weighted: 0.09555126069991741
[2m[36m(func pid=20299)[0m f1_per_class: [0.224, 0.279, 0.5, 0.016, 0.029, 0.008, 0.0, 0.498, 0.137, 0.077]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 78.8406 | Steps: 4 | Val loss: 170.8955 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.6330 | Steps: 4 | Val loss: 2.2487 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.0639 | Steps: 4 | Val loss: 1.9322 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.2981 | Steps: 4 | Val loss: 2.0203 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=12671)[0m top1: 0.14972014925373134
[2m[36m(func pid=12671)[0m top5: 0.5573694029850746
[2m[36m(func pid=12671)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=12671)[0m f1_macro: 0.18991158842787084
[2m[36m(func pid=12671)[0m f1_weighted: 0.1382117817039846
[2m[36m(func pid=12671)[0m f1_per_class: [0.383, 0.446, 0.556, 0.0, 0.077, 0.143, 0.095, 0.0, 0.123, 0.076]
[2m[36m(func pid=12671)[0m 
== Status ==
Current time: 2024-01-07 10:42:06 (running for 00:24:36.18)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 78.841 |      0.19  |                   68 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.633 |      0.077 |                   35 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.471 |      0.177 |                   34 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.18  |      0.174 |                   30 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.1599813432835821
[2m[36m(func pid=20296)[0m top5: 0.6809701492537313
[2m[36m(func pid=20296)[0m f1_micro: 0.1599813432835821
[2m[36m(func pid=20296)[0m f1_macro: 0.07653367426302107
[2m[36m(func pid=20296)[0m f1_weighted: 0.1324799767978495
[2m[36m(func pid=20296)[0m f1_per_class: [0.056, 0.186, 0.043, 0.291, 0.0, 0.045, 0.019, 0.125, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m top1: 0.36986940298507465
[2m[36m(func pid=21639)[0m top5: 0.8647388059701493
[2m[36m(func pid=21639)[0m f1_micro: 0.36986940298507465
[2m[36m(func pid=21639)[0m f1_macro: 0.2728325814417591
[2m[36m(func pid=21639)[0m f1_weighted: 0.33968629546070583
[2m[36m(func pid=21639)[0m f1_per_class: [0.259, 0.0, 0.571, 0.568, 0.064, 0.232, 0.392, 0.461, 0.0, 0.182]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20299)[0m top1: 0.25093283582089554
[2m[36m(func pid=20299)[0m top5: 0.7915111940298507
[2m[36m(func pid=20299)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=20299)[0m f1_macro: 0.18187983467291371
[2m[36m(func pid=20299)[0m f1_weighted: 0.22561835903558455
[2m[36m(func pid=20299)[0m f1_per_class: [0.241, 0.263, 0.253, 0.512, 0.03, 0.0, 0.003, 0.517, 0.0, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 69.3665 | Steps: 4 | Val loss: 155.6613 | Batch size: 32 | lr: 0.1 | Duration: 2.75s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.6971 | Steps: 4 | Val loss: 2.2516 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.2680 | Steps: 4 | Val loss: 2.1792 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=12671)[0m top1: 0.15438432835820895
[2m[36m(func pid=12671)[0m top5: 0.5573694029850746
[2m[36m(func pid=12671)[0m f1_micro: 0.15438432835820895
[2m[36m(func pid=12671)[0m f1_macro: 0.18019297174086374
[2m[36m(func pid=12671)[0m f1_weighted: 0.15344369947454375
[2m[36m(func pid=12671)[0m f1_per_class: [0.2, 0.45, 0.556, 0.0, 0.141, 0.116, 0.167, 0.0, 0.108, 0.064]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.4106 | Steps: 4 | Val loss: 2.0257 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 10:42:11 (running for 00:24:41.37)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 69.366 |      0.18  |                   69 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.697 |      0.071 |                   36 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.298 |      0.182 |                   35 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.064 |      0.273 |                   31 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.15671641791044777
[2m[36m(func pid=20296)[0m top5: 0.6753731343283582
[2m[36m(func pid=20296)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=20296)[0m f1_macro: 0.07137681818710546
[2m[36m(func pid=20296)[0m f1_weighted: 0.12561740889654735
[2m[36m(func pid=20296)[0m f1_per_class: [0.06, 0.18, 0.042, 0.291, 0.0, 0.016, 0.012, 0.113, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m top1: 0.26865671641791045
[2m[36m(func pid=21639)[0m top5: 0.8703358208955224
[2m[36m(func pid=21639)[0m f1_micro: 0.26865671641791045
[2m[36m(func pid=21639)[0m f1_macro: 0.23044795469716903
[2m[36m(func pid=21639)[0m f1_weighted: 0.24546913000957826
[2m[36m(func pid=21639)[0m f1_per_class: [0.16, 0.348, 0.556, 0.0, 0.05, 0.188, 0.427, 0.498, 0.0, 0.077]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20299)[0m top1: 0.21548507462686567
[2m[36m(func pid=20299)[0m top5: 0.8684701492537313
[2m[36m(func pid=20299)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=20299)[0m f1_macro: 0.13049198776655008
[2m[36m(func pid=20299)[0m f1_weighted: 0.20349014368066642
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.231, 0.142, 0.486, 0.027, 0.0, 0.012, 0.407, 0.0, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 108.8809 | Steps: 4 | Val loss: 108.3550 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.6571 | Steps: 4 | Val loss: 2.2558 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 3.1889 | Steps: 4 | Val loss: 2.9415 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=12671)[0m top1: 0.2178171641791045
[2m[36m(func pid=12671)[0m top5: 0.6277985074626866
[2m[36m(func pid=12671)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=12671)[0m f1_macro: 0.22218141876803096
[2m[36m(func pid=12671)[0m f1_weighted: 0.19911215706569088
[2m[36m(func pid=12671)[0m f1_per_class: [0.235, 0.493, 0.741, 0.0, 0.157, 0.12, 0.286, 0.0, 0.127, 0.064]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.4058 | Steps: 4 | Val loss: 1.9737 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 10:42:17 (running for 00:24:46.89)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 108.881 |      0.222 |                   70 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.657 |      0.075 |                   37 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.411 |      0.13  |                   36 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   2.268 |      0.23  |                   32 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.15391791044776118
[2m[36m(func pid=20296)[0m top5: 0.6632462686567164
[2m[36m(func pid=20296)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=20296)[0m f1_macro: 0.07491734133109995
[2m[36m(func pid=20296)[0m f1_weighted: 0.12536435425769465
[2m[36m(func pid=20296)[0m f1_per_class: [0.079, 0.169, 0.041, 0.29, 0.0, 0.023, 0.015, 0.102, 0.0, 0.029]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m top1: 0.060167910447761194
[2m[36m(func pid=21639)[0m top5: 0.7588619402985075
[2m[36m(func pid=21639)[0m f1_micro: 0.060167910447761194
[2m[36m(func pid=21639)[0m f1_macro: 0.09688355442860794
[2m[36m(func pid=21639)[0m f1_weighted: 0.0468652777708888
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.036, 0.195, 0.013, 0.023, 0.006, 0.006, 0.483, 0.129, 0.077]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20299)[0m top1: 0.2234141791044776
[2m[36m(func pid=20299)[0m top5: 0.9188432835820896
[2m[36m(func pid=20299)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=20299)[0m f1_macro: 0.11768271390824787
[2m[36m(func pid=20299)[0m f1_weighted: 0.2629050855972798
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.295, 0.121, 0.328, 0.031, 0.0, 0.402, 0.0, 0.0, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 131.5845 | Steps: 4 | Val loss: 76.1903 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.6583 | Steps: 4 | Val loss: 2.2546 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=12671)[0m top1: 0.3824626865671642
[2m[36m(func pid=12671)[0m top5: 0.8092350746268657
[2m[36m(func pid=12671)[0m f1_micro: 0.38246268656716415
[2m[36m(func pid=12671)[0m f1_macro: 0.2231732682738413
[2m[36m(func pid=12671)[0m f1_weighted: 0.35750800302915764
[2m[36m(func pid=12671)[0m f1_per_class: [0.085, 0.548, 0.255, 0.489, 0.154, 0.291, 0.293, 0.0, 0.026, 0.091]
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.4967 | Steps: 4 | Val loss: 2.9818 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.2780 | Steps: 4 | Val loss: 2.0118 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 10:42:22 (running for 00:24:52.32)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 131.585 |      0.223 |                   71 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.658 |      0.088 |                   38 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.406 |      0.118 |                   37 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   3.189 |      0.097 |                   33 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |         |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |         |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |         |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |         |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.15671641791044777
[2m[36m(func pid=20296)[0m top5: 0.6791044776119403
[2m[36m(func pid=20296)[0m f1_micro: 0.15671641791044777
[2m[36m(func pid=20296)[0m f1_macro: 0.0883546605780963
[2m[36m(func pid=20296)[0m f1_weighted: 0.1357030199051195
[2m[36m(func pid=20296)[0m f1_per_class: [0.1, 0.185, 0.041, 0.278, 0.0, 0.008, 0.031, 0.241, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.27705223880597013
[2m[36m(func pid=20299)[0m top5: 0.9169776119402985
[2m[36m(func pid=20299)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=20299)[0m f1_macro: 0.11705057205041199
[2m[36m(func pid=20299)[0m f1_weighted: 0.2673627915463815
[2m[36m(func pid=20299)[0m f1_per_class: [0.0, 0.302, 0.073, 0.164, 0.043, 0.0, 0.564, 0.0, 0.024, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.23600746268656717
[2m[36m(func pid=21639)[0m top5: 0.7374067164179104
[2m[36m(func pid=21639)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=21639)[0m f1_macro: 0.19004609876185874
[2m[36m(func pid=21639)[0m f1_weighted: 0.1926350260719177
[2m[36m(func pid=21639)[0m f1_per_class: [0.25, 0.0, 0.333, 0.457, 0.074, 0.291, 0.009, 0.338, 0.0, 0.147]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 68.2409 | Steps: 4 | Val loss: 90.2564 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.7478 | Steps: 4 | Val loss: 2.2556 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=12671)[0m top1: 0.41651119402985076
[2m[36m(func pid=12671)[0m top5: 0.8390858208955224
[2m[36m(func pid=12671)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=12671)[0m f1_macro: 0.22937932053014848
[2m[36m(func pid=12671)[0m f1_weighted: 0.383476331782196
[2m[36m(func pid=12671)[0m f1_per_class: [0.0, 0.512, 0.192, 0.539, 0.178, 0.344, 0.328, 0.062, 0.0, 0.138]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.2994 | Steps: 4 | Val loss: 2.0854 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5405 | Steps: 4 | Val loss: 2.1783 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 10:42:28 (running for 00:24:57.70)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 68.241 |      0.229 |                   72 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.748 |      0.079 |                   39 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.278 |      0.117 |                   38 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.497 |      0.19  |                   34 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.15111940298507462
[2m[36m(func pid=20296)[0m top5: 0.667910447761194
[2m[36m(func pid=20296)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=20296)[0m f1_macro: 0.0785122964536999
[2m[36m(func pid=20296)[0m f1_weighted: 0.12796193865019148
[2m[36m(func pid=20296)[0m f1_per_class: [0.065, 0.209, 0.045, 0.268, 0.0, 0.023, 0.012, 0.163, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.25326492537313433
[2m[36m(func pid=20299)[0m top5: 0.8819962686567164
[2m[36m(func pid=20299)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=20299)[0m f1_macro: 0.10183826964925487
[2m[36m(func pid=20299)[0m f1_weighted: 0.23388829160086505
[2m[36m(func pid=20299)[0m f1_per_class: [0.03, 0.221, 0.068, 0.108, 0.04, 0.0, 0.552, 0.0, 0.0, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.4216417910447761
[2m[36m(func pid=21639)[0m top5: 0.7919776119402985
[2m[36m(func pid=21639)[0m f1_micro: 0.42164179104477617
[2m[36m(func pid=21639)[0m f1_macro: 0.2804100858966069
[2m[36m(func pid=21639)[0m f1_weighted: 0.3908804416519463
[2m[36m(func pid=21639)[0m f1_per_class: [0.288, 0.0, 0.478, 0.518, 0.089, 0.422, 0.555, 0.389, 0.0, 0.065]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 85.1341 | Steps: 4 | Val loss: 92.9548 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.6854 | Steps: 4 | Val loss: 2.2541 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=12671)[0m top1: 0.3689365671641791
[2m[36m(func pid=12671)[0m top5: 0.8059701492537313
[2m[36m(func pid=12671)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=12671)[0m f1_macro: 0.247142340288819
[2m[36m(func pid=12671)[0m f1_weighted: 0.3540853114930207
[2m[36m(func pid=12671)[0m f1_per_class: [0.098, 0.512, 0.119, 0.504, 0.111, 0.298, 0.194, 0.486, 0.0, 0.148]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.4602 | Steps: 4 | Val loss: 2.1357 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.4943 | Steps: 4 | Val loss: 2.2435 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 10:42:33 (running for 00:25:03.20)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 85.134 |      0.247 |                   73 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.685 |      0.086 |                   40 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.299 |      0.102 |                   39 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.541 |      0.28  |                   35 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.15111940298507462
[2m[36m(func pid=20296)[0m top5: 0.6856343283582089
[2m[36m(func pid=20296)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=20296)[0m f1_macro: 0.0860753639357548
[2m[36m(func pid=20296)[0m f1_weighted: 0.1316698899985586
[2m[36m(func pid=20296)[0m f1_per_class: [0.039, 0.238, 0.053, 0.236, 0.0, 0.007, 0.03, 0.216, 0.041, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 96.9910 | Steps: 4 | Val loss: 133.0602 | Batch size: 32 | lr: 0.1 | Duration: 2.66s
[2m[36m(func pid=20299)[0m top1: 0.2178171641791045
[2m[36m(func pid=20299)[0m top5: 0.8222947761194029
[2m[36m(func pid=20299)[0m f1_micro: 0.2178171641791045
[2m[36m(func pid=20299)[0m f1_macro: 0.13893254987561243
[2m[36m(func pid=20299)[0m f1_weighted: 0.21466641783182466
[2m[36m(func pid=20299)[0m f1_per_class: [0.131, 0.171, 0.112, 0.051, 0.043, 0.0, 0.488, 0.352, 0.042, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.27845149253731344
[2m[36m(func pid=21639)[0m top5: 0.8647388059701493
[2m[36m(func pid=21639)[0m f1_micro: 0.27845149253731344
[2m[36m(func pid=21639)[0m f1_macro: 0.21699137440232596
[2m[36m(func pid=21639)[0m f1_weighted: 0.3138195239943299
[2m[36m(func pid=21639)[0m f1_per_class: [0.18, 0.127, 0.261, 0.41, 0.073, 0.103, 0.453, 0.342, 0.117, 0.105]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.6763 | Steps: 4 | Val loss: 2.2559 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=12671)[0m top1: 0.1623134328358209
[2m[36m(func pid=12671)[0m top5: 0.6888992537313433
[2m[36m(func pid=12671)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=12671)[0m f1_macro: 0.13380395307314363
[2m[36m(func pid=12671)[0m f1_weighted: 0.13010256758560942
[2m[36m(func pid=12671)[0m f1_per_class: [0.079, 0.516, 0.145, 0.054, 0.069, 0.043, 0.0, 0.284, 0.0, 0.148]
[2m[36m(func pid=12671)[0m 
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.3917 | Steps: 4 | Val loss: 2.0601 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.3445 | Steps: 4 | Val loss: 2.4181 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 10:42:38 (running for 00:25:08.52)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=11
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (9 PENDING, 4 RUNNING, 11 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00011 | RUNNING    | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 96.991 |      0.134 |                   74 |
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.676 |      0.092 |                   41 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.46  |      0.139 |                   40 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.494 |      0.217 |                   36 |
| train_952df_00015 | PENDING    |                     | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (1 PENDING, 3 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.1571828358208955
[2m[36m(func pid=20296)[0m top5: 0.6935634328358209
[2m[36m(func pid=20296)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=20296)[0m f1_macro: 0.09152326396184965
[2m[36m(func pid=20296)[0m f1_weighted: 0.13961343402470905
[2m[36m(func pid=20296)[0m f1_per_class: [0.034, 0.229, 0.054, 0.258, 0.0, 0.015, 0.03, 0.275, 0.021, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=12671)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 163.8541 | Steps: 4 | Val loss: 161.9094 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
[2m[36m(func pid=20299)[0m top1: 0.16277985074626866
[2m[36m(func pid=20299)[0m top5: 0.8171641791044776
[2m[36m(func pid=20299)[0m f1_micro: 0.16277985074626866
[2m[36m(func pid=20299)[0m f1_macro: 0.13377559550851598
[2m[36m(func pid=20299)[0m f1_weighted: 0.13386011175951854
[2m[36m(func pid=20299)[0m f1_per_class: [0.21, 0.032, 0.4, 0.381, 0.04, 0.0, 0.0, 0.244, 0.031, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.20988805970149255
[2m[36m(func pid=21639)[0m top5: 0.7770522388059702
[2m[36m(func pid=21639)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=21639)[0m f1_macro: 0.18789847138631016
[2m[36m(func pid=21639)[0m f1_weighted: 0.12160040054675561
[2m[36m(func pid=21639)[0m f1_per_class: [0.163, 0.44, 0.513, 0.016, 0.0, 0.008, 0.0, 0.495, 0.113, 0.13]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.6709 | Steps: 4 | Val loss: 2.2462 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=12671)[0m top1: 0.1417910447761194
[2m[36m(func pid=12671)[0m top5: 0.6222014925373134
[2m[36m(func pid=12671)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=12671)[0m f1_macro: 0.14159816002856235
[2m[36m(func pid=12671)[0m f1_weighted: 0.10881651140738934
[2m[36m(func pid=12671)[0m f1_per_class: [0.086, 0.42, 0.224, 0.0, 0.073, 0.125, 0.0, 0.281, 0.0, 0.207]
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.7124 | Steps: 4 | Val loss: 1.8519 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.3228 | Steps: 4 | Val loss: 1.9602 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=20296)[0m top1: 0.17630597014925373
[2m[36m(func pid=20296)[0m top5: 0.7196828358208955
[2m[36m(func pid=20296)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=20296)[0m f1_macro: 0.1063347988797898
[2m[36m(func pid=20296)[0m f1_weighted: 0.1676145488899116
[2m[36m(func pid=20296)[0m f1_per_class: [0.035, 0.237, 0.057, 0.268, 0.0, 0.015, 0.098, 0.335, 0.018, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m top1: 0.3283582089552239
[2m[36m(func pid=21639)[0m top5: 0.8568097014925373
[2m[36m(func pid=21639)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=21639)[0m f1_macro: 0.21694381913612698
[2m[36m(func pid=21639)[0m f1_weighted: 0.2857054168364883
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.492, 0.304, 0.525, 0.24, 0.0, 0.085, 0.392, 0.07, 0.062]
[2m[36m(func pid=20299)[0m top1: 0.2756529850746269
[2m[36m(func pid=20299)[0m top5: 0.8535447761194029
[2m[36m(func pid=20299)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=20299)[0m f1_macro: 0.19465304703733508
[2m[36m(func pid=20299)[0m f1_weighted: 0.20547624691328728
[2m[36m(func pid=20299)[0m f1_per_class: [0.333, 0.0, 0.5, 0.559, 0.048, 0.18, 0.0, 0.327, 0.0, 0.0]
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.6696 | Steps: 4 | Val loss: 2.2550 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=20296)[0m top1: 0.16744402985074627
[2m[36m(func pid=20296)[0m top5: 0.7168843283582089
[2m[36m(func pid=20296)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=20296)[0m f1_macro: 0.10753086901867268
[2m[36m(func pid=20296)[0m f1_weighted: 0.16161274545637838
[2m[36m(func pid=20296)[0m f1_per_class: [0.048, 0.221, 0.061, 0.257, 0.0, 0.0, 0.095, 0.378, 0.016, 0.0]
== Status ==
Current time: 2024-01-07 10:42:44 (running for 00:25:13.76)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.671 |      0.106 |                   42 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.392 |      0.134 |                   41 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.345 |      0.188 |                   37 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 10:42:50 (running for 00:25:20.29)
Memory usage on this node: 23.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.671 |      0.106 |                   42 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.392 |      0.134 |                   41 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.712 |      0.217 |                   38 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=31048)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=31048)[0m Configuration completed!
[2m[36m(func pid=31048)[0m New optimizer parameters:
[2m[36m(func pid=31048)[0m SGD (
[2m[36m(func pid=31048)[0m Parameter Group 0
[2m[36m(func pid=31048)[0m     dampening: 0
[2m[36m(func pid=31048)[0m     differentiable: False
[2m[36m(func pid=31048)[0m     foreach: None
[2m[36m(func pid=31048)[0m     lr: 0.1
[2m[36m(func pid=31048)[0m     maximize: False
[2m[36m(func pid=31048)[0m     momentum: 0.9
[2m[36m(func pid=31048)[0m     nesterov: False
[2m[36m(func pid=31048)[0m     weight_decay: 0.0001
[2m[36m(func pid=31048)[0m )
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.7107 | Steps: 4 | Val loss: 2.2566 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.2111 | Steps: 4 | Val loss: 2.2496 | Batch size: 32 | lr: 0.01 | Duration: 3.17s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.3075 | Steps: 4 | Val loss: 1.9338 | Batch size: 32 | lr: 0.001 | Duration: 3.24s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 4.8222 | Steps: 4 | Val loss: 3.7314 | Batch size: 32 | lr: 0.1 | Duration: 4.59s
== Status ==
Current time: 2024-01-07 10:42:55 (running for 00:25:25.33)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.67  |      0.108 |                   43 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.323 |      0.195 |                   42 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.712 |      0.217 |                   38 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |        |            |                      |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.16977611940298507
[2m[36m(func pid=20296)[0m top5: 0.7056902985074627
[2m[36m(func pid=20296)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=20296)[0m f1_macro: 0.11397409281125144
[2m[36m(func pid=20296)[0m f1_weighted: 0.16495067033722952
[2m[36m(func pid=20296)[0m f1_per_class: [0.039, 0.222, 0.08, 0.247, 0.0, 0.023, 0.1, 0.403, 0.026, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.324160447761194
[2m[36m(func pid=20299)[0m top5: 0.8689365671641791
[2m[36m(func pid=20299)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=20299)[0m f1_macro: 0.22396795434322683
[2m[36m(func pid=20299)[0m f1_weighted: 0.23360545136195937
[2m[36m(func pid=20299)[0m f1_per_class: [0.145, 0.0, 0.486, 0.561, 0.075, 0.335, 0.006, 0.459, 0.094, 0.077]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.23367537313432835
[2m[36m(func pid=21639)[0m top5: 0.8348880597014925
[2m[36m(func pid=21639)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=21639)[0m f1_macro: 0.16708674303327392
[2m[36m(func pid=21639)[0m f1_weighted: 0.2351677204532651
[2m[36m(func pid=21639)[0m f1_per_class: [0.186, 0.165, 0.243, 0.084, 0.051, 0.344, 0.458, 0.0, 0.0, 0.139]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m top1: 0.17210820895522388
[2m[36m(func pid=31048)[0m top5: 0.6119402985074627
[2m[36m(func pid=31048)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=31048)[0m f1_macro: 0.029367290091524074
[2m[36m(func pid=31048)[0m f1_weighted: 0.05054351699520701
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.6764 | Steps: 4 | Val loss: 2.2611 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.1384 | Steps: 4 | Val loss: 2.2034 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3878 | Steps: 4 | Val loss: 1.9937 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 23.2052 | Steps: 4 | Val loss: 7.4746 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 10:43:01 (running for 00:25:31.20)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.676 |      0.114 |                   45 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.307 |      0.224 |                   43 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.211 |      0.167 |                   39 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  4.822 |      0.029 |                    1 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.1609141791044776
[2m[36m(func pid=20296)[0m top5: 0.6968283582089553
[2m[36m(func pid=20296)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=20296)[0m f1_macro: 0.11369196200588746
[2m[36m(func pid=20296)[0m f1_weighted: 0.15993681608120192
[2m[36m(func pid=20296)[0m f1_per_class: [0.056, 0.205, 0.104, 0.232, 0.0, 0.045, 0.102, 0.379, 0.013, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.2224813432835821
[2m[36m(func pid=20299)[0m top5: 0.8395522388059702
[2m[36m(func pid=20299)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=20299)[0m f1_macro: 0.21958686097097382
[2m[36m(func pid=20299)[0m f1_weighted: 0.18858339826000153
[2m[36m(func pid=20299)[0m f1_per_class: [0.187, 0.027, 0.486, 0.361, 0.086, 0.382, 0.0, 0.475, 0.115, 0.077]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.24067164179104478
[2m[36m(func pid=21639)[0m top5: 0.8563432835820896
[2m[36m(func pid=21639)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=21639)[0m f1_macro: 0.1751176091638598
[2m[36m(func pid=21639)[0m f1_weighted: 0.2198913784407053
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.369, 0.093, 0.301, 0.123, 0.301, 0.067, 0.197, 0.072, 0.228]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m top1: 0.01912313432835821
[2m[36m(func pid=31048)[0m top5: 0.45382462686567165
[2m[36m(func pid=31048)[0m f1_micro: 0.01912313432835821
[2m[36m(func pid=31048)[0m f1_macro: 0.018131578947368422
[2m[36m(func pid=31048)[0m f1_weighted: 0.003501080125687353
[2m[36m(func pid=31048)[0m f1_per_class: [0.164, 0.0, 0.0, 0.0, 0.017, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.7712 | Steps: 4 | Val loss: 2.2678 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.8128 | Steps: 4 | Val loss: 2.7523 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.3329 | Steps: 4 | Val loss: 2.0980 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 26.7606 | Steps: 4 | Val loss: 13.0125 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=20296)[0m top1: 0.1525186567164179
[2m[36m(func pid=20296)[0m top5: 0.667910447761194
[2m[36m(func pid=20296)[0m f1_micro: 0.1525186567164179
[2m[36m(func pid=20296)[0m f1_macro: 0.11703321074696815
[2m[36m(func pid=20296)[0m f1_weighted: 0.14595590922030666
[2m[36m(func pid=20296)[0m f1_per_class: [0.062, 0.198, 0.17, 0.213, 0.0, 0.059, 0.07, 0.369, 0.028, 0.0]
[2m[36m(func pid=20296)[0m 
== Status ==
Current time: 2024-01-07 10:43:06 (running for 00:25:36.47)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.771 |      0.117 |                   46 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.388 |      0.22  |                   44 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.138 |      0.175 |                   40 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 23.205 |      0.018 |                    2 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20299)[0m top1: 0.146455223880597
[2m[36m(func pid=20299)[0m top5: 0.7611940298507462
[2m[36m(func pid=20299)[0m f1_micro: 0.146455223880597
[2m[36m(func pid=20299)[0m f1_macro: 0.17651818178158638
[2m[36m(func pid=20299)[0m f1_weighted: 0.09454404904862689
[2m[36m(func pid=20299)[0m f1_per_class: [0.256, 0.021, 0.393, 0.033, 0.074, 0.374, 0.0, 0.466, 0.105, 0.044]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.292910447761194
[2m[36m(func pid=21639)[0m top5: 0.6716417910447762
[2m[36m(func pid=21639)[0m f1_micro: 0.292910447761194
[2m[36m(func pid=21639)[0m f1_macro: 0.2602545705303414
[2m[36m(func pid=21639)[0m f1_weighted: 0.27042124810354345
[2m[36m(func pid=21639)[0m f1_per_class: [0.044, 0.32, 0.6, 0.509, 0.112, 0.306, 0.0, 0.468, 0.141, 0.102]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m top1: 0.012593283582089552
[2m[36m(func pid=31048)[0m top5: 0.45382462686567165
[2m[36m(func pid=31048)[0m f1_micro: 0.012593283582089552
[2m[36m(func pid=31048)[0m f1_macro: 0.008213355272178802
[2m[36m(func pid=31048)[0m f1_weighted: 0.0014790046067482416
[2m[36m(func pid=31048)[0m f1_per_class: [0.059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7574 | Steps: 4 | Val loss: 2.2738 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.4501 | Steps: 4 | Val loss: 2.1864 | Batch size: 32 | lr: 0.001 | Duration: 2.78s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.8591 | Steps: 4 | Val loss: 3.3111 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 22.5686 | Steps: 4 | Val loss: 8.4902 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 10:43:12 (running for 00:25:41.74)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.757 |      0.122 |                   47 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.333 |      0.177 |                   45 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.813 |      0.26  |                   41 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 26.761 |      0.008 |                    3 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.15111940298507462
[2m[36m(func pid=20296)[0m top5: 0.6413246268656716
[2m[36m(func pid=20296)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=20296)[0m f1_macro: 0.12158812829983737
[2m[36m(func pid=20296)[0m f1_weighted: 0.14186948488005807
[2m[36m(func pid=20296)[0m f1_per_class: [0.063, 0.199, 0.214, 0.211, 0.0, 0.1, 0.045, 0.349, 0.034, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.07695895522388059
[2m[36m(func pid=20299)[0m top5: 0.7901119402985075
[2m[36m(func pid=20299)[0m f1_micro: 0.07695895522388059
[2m[36m(func pid=20299)[0m f1_macro: 0.11095640550051626
[2m[36m(func pid=20299)[0m f1_weighted: 0.06397254194075351
[2m[36m(func pid=20299)[0m f1_per_class: [0.038, 0.005, 0.13, 0.0, 0.126, 0.263, 0.0, 0.521, 0.0, 0.025]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.22807835820895522
[2m[36m(func pid=21639)[0m top5: 0.5405783582089553
[2m[36m(func pid=21639)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=21639)[0m f1_macro: 0.15325896060976424
[2m[36m(func pid=21639)[0m f1_weighted: 0.17070050286421065
[2m[36m(func pid=21639)[0m f1_per_class: [0.184, 0.0, 0.157, 0.5, 0.065, 0.0, 0.0, 0.331, 0.161, 0.136]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m top1: 0.2042910447761194
[2m[36m(func pid=31048)[0m top5: 0.5918843283582089
[2m[36m(func pid=31048)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=31048)[0m f1_macro: 0.05336742813888943
[2m[36m(func pid=31048)[0m f1_weighted: 0.11709082334672928
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.297, 0.0, 0.236, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.7616 | Steps: 4 | Val loss: 2.2744 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.3427 | Steps: 4 | Val loss: 2.1457 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.0547 | Steps: 4 | Val loss: 2.6967 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 24.6227 | Steps: 4 | Val loss: 19.2458 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 10:43:17 (running for 00:25:47.08)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.762 |      0.114 |                   48 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.45  |      0.111 |                   46 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.859 |      0.153 |                   42 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 22.569 |      0.053 |                    4 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.1501865671641791
[2m[36m(func pid=20296)[0m top5: 0.6222014925373134
[2m[36m(func pid=20296)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=20296)[0m f1_macro: 0.11428402976542376
[2m[36m(func pid=20296)[0m f1_weighted: 0.13203947133735663
[2m[36m(func pid=20296)[0m f1_per_class: [0.066, 0.155, 0.226, 0.245, 0.0, 0.059, 0.024, 0.329, 0.038, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.10167910447761194
[2m[36m(func pid=20299)[0m top5: 0.7915111940298507
[2m[36m(func pid=20299)[0m f1_micro: 0.10167910447761194
[2m[36m(func pid=20299)[0m f1_macro: 0.12558293803132786
[2m[36m(func pid=20299)[0m f1_weighted: 0.07735333153594015
[2m[36m(func pid=20299)[0m f1_per_class: [0.07, 0.011, 0.141, 0.0, 0.129, 0.371, 0.0, 0.512, 0.0, 0.022]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.24813432835820895
[2m[36m(func pid=21639)[0m top5: 0.6819029850746269
[2m[36m(func pid=21639)[0m f1_micro: 0.24813432835820895
[2m[36m(func pid=21639)[0m f1_macro: 0.24365030428431944
[2m[36m(func pid=21639)[0m f1_weighted: 0.19452024300891482
[2m[36m(func pid=21639)[0m f1_per_class: [0.406, 0.0, 0.64, 0.537, 0.075, 0.0, 0.0, 0.441, 0.116, 0.222]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m top1: 0.015391791044776119
[2m[36m(func pid=31048)[0m top5: 0.3582089552238806
[2m[36m(func pid=31048)[0m f1_micro: 0.015391791044776119
[2m[36m(func pid=31048)[0m f1_macro: 0.04260603540079523
[2m[36m(func pid=31048)[0m f1_weighted: 0.007096657678356173
[2m[36m(func pid=31048)[0m f1_per_class: [0.197, 0.0, 0.182, 0.0, 0.016, 0.0, 0.0, 0.032, 0.0, 0.0]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.7374 | Steps: 4 | Val loss: 2.2515 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.4388 | Steps: 4 | Val loss: 2.0932 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.7345 | Steps: 4 | Val loss: 1.9024 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 28.8583 | Steps: 4 | Val loss: 15.3290 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 10:43:22 (running for 00:25:52.28)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.737 |      0.117 |                   49 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.343 |      0.126 |                   47 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.055 |      0.244 |                   43 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 24.623 |      0.043 |                    5 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.17863805970149255
[2m[36m(func pid=20296)[0m top5: 0.6501865671641791
[2m[36m(func pid=20296)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=20296)[0m f1_macro: 0.11674443972306818
[2m[36m(func pid=20296)[0m f1_weighted: 0.14300912934588048
[2m[36m(func pid=20296)[0m f1_per_class: [0.083, 0.118, 0.261, 0.311, 0.0, 0.045, 0.03, 0.319, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.17210820895522388
[2m[36m(func pid=20299)[0m top5: 0.7817164179104478
[2m[36m(func pid=20299)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=20299)[0m f1_macro: 0.16115222274381558
[2m[36m(func pid=20299)[0m f1_weighted: 0.12643989738829137
[2m[36m(func pid=20299)[0m f1_per_class: [0.193, 0.286, 0.187, 0.003, 0.086, 0.354, 0.012, 0.448, 0.0, 0.042]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.3666044776119403
[2m[36m(func pid=21639)[0m top5: 0.8652052238805971
[2m[36m(func pid=21639)[0m f1_micro: 0.3666044776119403
[2m[36m(func pid=21639)[0m f1_macro: 0.23755276412738544
[2m[36m(func pid=21639)[0m f1_weighted: 0.29965630171573665
[2m[36m(func pid=21639)[0m f1_per_class: [0.259, 0.45, 0.444, 0.078, 0.189, 0.0, 0.591, 0.215, 0.028, 0.121]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m top1: 0.16138059701492538
[2m[36m(func pid=31048)[0m top5: 0.5457089552238806
[2m[36m(func pid=31048)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=31048)[0m f1_macro: 0.03241139215609142
[2m[36m(func pid=31048)[0m f1_weighted: 0.05140681850473842
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.027]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.7370 | Steps: 4 | Val loss: 2.2385 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.1803 | Steps: 4 | Val loss: 2.0364 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.0510 | Steps: 4 | Val loss: 1.8844 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 18.9713 | Steps: 4 | Val loss: 12.4833 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=20296)[0m top1: 0.1865671641791045
[2m[36m(func pid=20296)[0m top5: 0.6632462686567164
[2m[36m(func pid=20296)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=20296)[0m f1_macro: 0.10313021979157404
[2m[36m(func pid=20296)[0m f1_weighted: 0.13086382128460103
[2m[36m(func pid=20296)[0m f1_per_class: [0.091, 0.059, 0.212, 0.337, 0.0, 0.0, 0.018, 0.313, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
== Status ==
Current time: 2024-01-07 10:43:27 (running for 00:25:57.67)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.737 |      0.103 |                   50 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.439 |      0.161 |                   48 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.735 |      0.238 |                   44 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 28.858 |      0.032 |                    6 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20299)[0m top1: 0.21921641791044777
[2m[36m(func pid=20299)[0m top5: 0.816231343283582
[2m[36m(func pid=20299)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=20299)[0m f1_macro: 0.2142464944120949
[2m[36m(func pid=20299)[0m f1_weighted: 0.14484535389501504
[2m[36m(func pid=20299)[0m f1_per_class: [0.214, 0.379, 0.474, 0.007, 0.073, 0.302, 0.012, 0.435, 0.173, 0.074]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.32136194029850745
[2m[36m(func pid=21639)[0m top5: 0.8614738805970149
[2m[36m(func pid=21639)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=21639)[0m f1_macro: 0.24320839095865104
[2m[36m(func pid=21639)[0m f1_weighted: 0.29393310668004985
[2m[36m(func pid=21639)[0m f1_per_class: [0.183, 0.486, 0.155, 0.138, 0.286, 0.386, 0.366, 0.147, 0.0, 0.286]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m top1: 0.17630597014925373
[2m[36m(func pid=31048)[0m top5: 0.6655783582089553
[2m[36m(func pid=31048)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=31048)[0m f1_macro: 0.06827945906666519
[2m[36m(func pid=31048)[0m f1_weighted: 0.1221501539509224
[2m[36m(func pid=31048)[0m f1_per_class: [0.088, 0.0, 0.0, 0.0, 0.0, 0.238, 0.307, 0.0, 0.05, 0.0]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.7552 | Steps: 4 | Val loss: 2.2261 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.2155 | Steps: 4 | Val loss: 2.0205 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.5970 | Steps: 4 | Val loss: 2.3907 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 27.5862 | Steps: 4 | Val loss: 7.4451 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 10:43:33 (running for 00:26:03.02)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.755 |      0.105 |                   51 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.18  |      0.214 |                   49 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.051 |      0.243 |                   45 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 18.971 |      0.068 |                    7 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.20848880597014927
[2m[36m(func pid=20296)[0m top5: 0.6986940298507462
[2m[36m(func pid=20296)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=20296)[0m f1_macro: 0.10477059052021676
[2m[36m(func pid=20296)[0m f1_weighted: 0.15258504006619633
[2m[36m(func pid=20296)[0m f1_per_class: [0.094, 0.068, 0.158, 0.376, 0.0, 0.0, 0.054, 0.299, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.20988805970149255
[2m[36m(func pid=20299)[0m top5: 0.8022388059701493
[2m[36m(func pid=20299)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=20299)[0m f1_macro: 0.2054939254277004
[2m[36m(func pid=20299)[0m f1_weighted: 0.14296453839905482
[2m[36m(func pid=20299)[0m f1_per_class: [0.231, 0.41, 0.571, 0.057, 0.066, 0.18, 0.0, 0.378, 0.162, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.3064365671641791
[2m[36m(func pid=21639)[0m top5: 0.7425373134328358
[2m[36m(func pid=21639)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=21639)[0m f1_macro: 0.3151810214592644
[2m[36m(func pid=21639)[0m f1_weighted: 0.2725890939257814
[2m[36m(func pid=21639)[0m f1_per_class: [0.385, 0.305, 0.692, 0.48, 0.147, 0.352, 0.0, 0.45, 0.138, 0.204]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m top1: 0.13805970149253732
[2m[36m(func pid=31048)[0m top5: 0.7546641791044776
[2m[36m(func pid=31048)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=31048)[0m f1_macro: 0.06448589077641519
[2m[36m(func pid=31048)[0m f1_weighted: 0.0888021723786085
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.115, 0.0, 0.114, 0.035, 0.281, 0.006, 0.0, 0.094, 0.0]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.6785 | Steps: 4 | Val loss: 2.1975 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.2836 | Steps: 4 | Val loss: 1.9910 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 1.7583 | Steps: 4 | Val loss: 2.4532 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 25.0986 | Steps: 4 | Val loss: 4.7709 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=20296)[0m top1: 0.2462686567164179
[2m[36m(func pid=20296)[0m top5: 0.7434701492537313
[2m[36m(func pid=20296)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=20296)[0m f1_macro: 0.12747273920973404
[2m[36m(func pid=20296)[0m f1_weighted: 0.19114681950645243
[2m[36m(func pid=20296)[0m f1_per_class: [0.131, 0.093, 0.212, 0.427, 0.0, 0.0, 0.117, 0.294, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
== Status ==
Current time: 2024-01-07 10:43:38 (running for 00:26:08.28)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.679 |      0.127 |                   52 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.216 |      0.205 |                   50 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.597 |      0.315 |                   46 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 27.586 |      0.064 |                    8 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20299)[0m top1: 0.20615671641791045
[2m[36m(func pid=20299)[0m top5: 0.7957089552238806
[2m[36m(func pid=20299)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=20299)[0m f1_macro: 0.20254389627831765
[2m[36m(func pid=20299)[0m f1_weighted: 0.18731765268158065
[2m[36m(func pid=20299)[0m f1_per_class: [0.321, 0.341, 0.522, 0.342, 0.043, 0.0, 0.0, 0.323, 0.132, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.30970149253731344
[2m[36m(func pid=21639)[0m top5: 0.7327425373134329
[2m[36m(func pid=21639)[0m f1_micro: 0.30970149253731344
[2m[36m(func pid=21639)[0m f1_macro: 0.20720696818598325
[2m[36m(func pid=21639)[0m f1_weighted: 0.20140853492309552
[2m[36m(func pid=21639)[0m f1_per_class: [0.085, 0.053, 0.621, 0.562, 0.062, 0.024, 0.0, 0.398, 0.028, 0.239]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m top1: 0.353544776119403
[2m[36m(func pid=31048)[0m top5: 0.9496268656716418
[2m[36m(func pid=31048)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=31048)[0m f1_macro: 0.17313755743184628
[2m[36m(func pid=31048)[0m f1_weighted: 0.24670076261004115
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.154, 0.372, 0.56, 0.0, 0.355, 0.018, 0.272, 0.0, 0.0]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.7455 | Steps: 4 | Val loss: 2.1979 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.2160 | Steps: 4 | Val loss: 2.0171 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.9855 | Steps: 4 | Val loss: 2.1913 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 22.8986 | Steps: 4 | Val loss: 16.3023 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 10:43:43 (running for 00:26:13.63)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.746 |      0.136 |                   53 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.284 |      0.203 |                   51 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.758 |      0.207 |                   47 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 25.099 |      0.173 |                    9 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.240205223880597
[2m[36m(func pid=20296)[0m top5: 0.7495335820895522
[2m[36m(func pid=20296)[0m f1_micro: 0.240205223880597
[2m[36m(func pid=20296)[0m f1_macro: 0.1356054170301559
[2m[36m(func pid=20296)[0m f1_weighted: 0.20581693210301538
[2m[36m(func pid=20296)[0m f1_per_class: [0.119, 0.123, 0.229, 0.423, 0.013, 0.0, 0.153, 0.296, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.21641791044776118
[2m[36m(func pid=20299)[0m top5: 0.7789179104477612
[2m[36m(func pid=20299)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=20299)[0m f1_macro: 0.20563759615550786
[2m[36m(func pid=20299)[0m f1_weighted: 0.19742791687257336
[2m[36m(func pid=20299)[0m f1_per_class: [0.303, 0.334, 0.545, 0.386, 0.044, 0.0, 0.0, 0.313, 0.131, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.291044776119403
[2m[36m(func pid=21639)[0m top5: 0.7915111940298507
[2m[36m(func pid=21639)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=21639)[0m f1_macro: 0.21189265687305953
[2m[36m(func pid=21639)[0m f1_weighted: 0.31597158950697063
[2m[36m(func pid=21639)[0m f1_per_class: [0.277, 0.422, 0.24, 0.29, 0.068, 0.0, 0.465, 0.25, 0.023, 0.083]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m top1: 0.03591417910447761
[2m[36m(func pid=31048)[0m top5: 0.4664179104477612
[2m[36m(func pid=31048)[0m f1_micro: 0.03591417910447761
[2m[36m(func pid=31048)[0m f1_macro: 0.1006851866023826
[2m[36m(func pid=31048)[0m f1_weighted: 0.02394300499492284
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.0, 0.4, 0.0, 0.2, 0.0, 0.0, 0.288, 0.093, 0.026]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.6301 | Steps: 4 | Val loss: 2.1926 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.1363 | Steps: 4 | Val loss: 2.0617 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.3318 | Steps: 4 | Val loss: 2.3606 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 18.0029 | Steps: 4 | Val loss: 7.7091 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 10:43:49 (running for 00:26:19.13)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.63  |      0.15  |                   54 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.216 |      0.206 |                   52 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.986 |      0.212 |                   48 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 22.899 |      0.101 |                   10 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.25699626865671643
[2m[36m(func pid=20296)[0m top5: 0.7779850746268657
[2m[36m(func pid=20296)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=20296)[0m f1_macro: 0.15040784783505776
[2m[36m(func pid=20296)[0m f1_weighted: 0.24652617260899712
[2m[36m(func pid=20296)[0m f1_per_class: [0.119, 0.154, 0.234, 0.42, 0.013, 0.0, 0.276, 0.289, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.18330223880597016
[2m[36m(func pid=20299)[0m top5: 0.7896455223880597
[2m[36m(func pid=20299)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=20299)[0m f1_macro: 0.14172364094974457
[2m[36m(func pid=20299)[0m f1_weighted: 0.17412990221883637
[2m[36m(func pid=20299)[0m f1_per_class: [0.139, 0.29, 0.168, 0.352, 0.036, 0.0, 0.0, 0.306, 0.127, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m top1: 0.25886194029850745
[2m[36m(func pid=21639)[0m top5: 0.7943097014925373
[2m[36m(func pid=21639)[0m f1_micro: 0.25886194029850745
[2m[36m(func pid=21639)[0m f1_macro: 0.19348053166928908
[2m[36m(func pid=21639)[0m f1_weighted: 0.2387548547844344
[2m[36m(func pid=21639)[0m f1_per_class: [0.24, 0.459, 0.358, 0.0, 0.056, 0.016, 0.482, 0.0, 0.146, 0.178]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m top1: 0.28125
[2m[36m(func pid=31048)[0m top5: 0.8805970149253731
[2m[36m(func pid=31048)[0m f1_micro: 0.28125
[2m[36m(func pid=31048)[0m f1_macro: 0.19402765703828967
[2m[36m(func pid=31048)[0m f1_weighted: 0.17396188846165336
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.396, 0.438, 0.0, 0.0, 0.427, 0.1, 0.386, 0.0, 0.194]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.6786 | Steps: 4 | Val loss: 2.1808 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.2798 | Steps: 4 | Val loss: 2.1103 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 14.3463 | Steps: 4 | Val loss: 8.0547 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.1748 | Steps: 4 | Val loss: 2.3300 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 10:43:54 (running for 00:26:24.54)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.679 |      0.162 |                   55 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.136 |      0.142 |                   53 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.332 |      0.193 |                   49 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 18.003 |      0.194 |                   11 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.2868470149253731
[2m[36m(func pid=20296)[0m top5: 0.820429104477612
[2m[36m(func pid=20296)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=20296)[0m f1_macro: 0.1620801396827818
[2m[36m(func pid=20296)[0m f1_weighted: 0.29060557656323177
[2m[36m(func pid=20296)[0m f1_per_class: [0.114, 0.199, 0.216, 0.409, 0.015, 0.0, 0.416, 0.252, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.13992537313432835
[2m[36m(func pid=20299)[0m top5: 0.8036380597014925
[2m[36m(func pid=20299)[0m f1_micro: 0.13992537313432835
[2m[36m(func pid=20299)[0m f1_macro: 0.10776693342930446
[2m[36m(func pid=20299)[0m f1_weighted: 0.13285428373566813
[2m[36m(func pid=20299)[0m f1_per_class: [0.119, 0.17, 0.125, 0.29, 0.03, 0.0, 0.0, 0.322, 0.022, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=31048)[0m top1: 0.1478544776119403
[2m[36m(func pid=31048)[0m top5: 0.8344216417910447
[2m[36m(func pid=31048)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=31048)[0m f1_macro: 0.13353725259707247
[2m[36m(func pid=31048)[0m f1_weighted: 0.16615884772342435
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.49, 0.25, 0.068, 0.027]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m top1: 0.22667910447761194
[2m[36m(func pid=21639)[0m top5: 0.8166977611940298
[2m[36m(func pid=21639)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=21639)[0m f1_macro: 0.19268537329580931
[2m[36m(func pid=21639)[0m f1_weighted: 0.1941202611153986
[2m[36m(func pid=21639)[0m f1_per_class: [0.18, 0.272, 0.202, 0.288, 0.154, 0.305, 0.0, 0.431, 0.0, 0.095]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.6349 | Steps: 4 | Val loss: 2.1756 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.2573 | Steps: 4 | Val loss: 2.1394 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 13.0356 | Steps: 4 | Val loss: 10.7801 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.9987 | Steps: 4 | Val loss: 2.1169 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=20296)[0m top1: 0.310634328358209
[2m[36m(func pid=20296)[0m top5: 0.8306902985074627
[2m[36m(func pid=20296)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=20296)[0m f1_macro: 0.15795168721658268
[2m[36m(func pid=20296)[0m f1_weighted: 0.30464242237370437
[2m[36m(func pid=20296)[0m f1_per_class: [0.115, 0.183, 0.192, 0.423, 0.022, 0.0, 0.475, 0.169, 0.0, 0.0]
[2m[36m(func pid=20296)[0m 
== Status ==
Current time: 2024-01-07 10:44:00 (running for 00:26:29.77)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.635 |      0.158 |                   56 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.28  |      0.108 |                   54 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.175 |      0.193 |                   50 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 14.346 |      0.134 |                   12 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20299)[0m top1: 0.13805970149253732
[2m[36m(func pid=20299)[0m top5: 0.7971082089552238
[2m[36m(func pid=20299)[0m f1_micro: 0.13805970149253732
[2m[36m(func pid=20299)[0m f1_macro: 0.1206742363277431
[2m[36m(func pid=20299)[0m f1_weighted: 0.12714604572863117
[2m[36m(func pid=20299)[0m f1_per_class: [0.143, 0.092, 0.222, 0.304, 0.029, 0.0, 0.0, 0.37, 0.0, 0.047]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=31048)[0m top1: 0.22014925373134328
[2m[36m(func pid=31048)[0m top5: 0.8899253731343284
[2m[36m(func pid=31048)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=31048)[0m f1_macro: 0.14998568360241307
[2m[36m(func pid=31048)[0m f1_weighted: 0.09715207391048836
[2m[36m(func pid=31048)[0m f1_per_class: [0.28, 0.33, 0.328, 0.0, 0.0, 0.0, 0.003, 0.537, 0.022, 0.0]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m top1: 0.29850746268656714
[2m[36m(func pid=21639)[0m top5: 0.8330223880597015
[2m[36m(func pid=21639)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=21639)[0m f1_macro: 0.20815248829531452
[2m[36m(func pid=21639)[0m f1_weighted: 0.23231162509738582
[2m[36m(func pid=21639)[0m f1_per_class: [0.295, 0.032, 0.214, 0.545, 0.084, 0.402, 0.0, 0.323, 0.0, 0.185]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.7200 | Steps: 4 | Val loss: 2.1739 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.5773 | Steps: 4 | Val loss: 2.1405 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 20.7673 | Steps: 4 | Val loss: 8.8929 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.6707 | Steps: 4 | Val loss: 1.9677 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 10:44:05 (running for 00:26:35.06)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.72  |      0.159 |                   57 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.257 |      0.121 |                   55 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.999 |      0.208 |                   51 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 13.036 |      0.15  |                   13 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.31576492537313433
[2m[36m(func pid=20296)[0m top5: 0.8334888059701493
[2m[36m(func pid=20296)[0m f1_micro: 0.31576492537313433
[2m[36m(func pid=20296)[0m f1_macro: 0.1590347086941813
[2m[36m(func pid=20296)[0m f1_weighted: 0.3061501131739711
[2m[36m(func pid=20296)[0m f1_per_class: [0.105, 0.179, 0.22, 0.411, 0.0, 0.0, 0.501, 0.126, 0.0, 0.048]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.14692164179104478
[2m[36m(func pid=20299)[0m top5: 0.8097014925373134
[2m[36m(func pid=20299)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=20299)[0m f1_macro: 0.17093583660507877
[2m[36m(func pid=20299)[0m f1_weighted: 0.14537887060436144
[2m[36m(func pid=20299)[0m f1_per_class: [0.132, 0.042, 0.485, 0.313, 0.034, 0.145, 0.0, 0.477, 0.0, 0.082]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=31048)[0m top1: 0.2523320895522388
[2m[36m(func pid=31048)[0m top5: 0.7905783582089553
[2m[36m(func pid=31048)[0m f1_micro: 0.2523320895522388
[2m[36m(func pid=31048)[0m f1_macro: 0.12727789112231763
[2m[36m(func pid=31048)[0m f1_weighted: 0.16135048762199486
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.005, 0.25, 0.51, 0.204, 0.0, 0.0, 0.251, 0.0, 0.053]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m top1: 0.29151119402985076
[2m[36m(func pid=21639)[0m top5: 0.8759328358208955
[2m[36m(func pid=21639)[0m f1_micro: 0.29151119402985076
[2m[36m(func pid=21639)[0m f1_macro: 0.292664550121506
[2m[36m(func pid=21639)[0m f1_weighted: 0.3389491677082301
[2m[36m(func pid=21639)[0m f1_per_class: [0.35, 0.362, 0.588, 0.403, 0.055, 0.0, 0.401, 0.472, 0.132, 0.163]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.6916 | Steps: 4 | Val loss: 2.1591 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.1818 | Steps: 4 | Val loss: 2.1055 | Batch size: 32 | lr: 0.001 | Duration: 2.79s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 15.7826 | Steps: 4 | Val loss: 14.1730 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.4556 | Steps: 4 | Val loss: 2.4117 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 10:44:10 (running for 00:26:40.40)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.72  |      0.159 |                   57 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.182 |      0.175 |                   57 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.671 |      0.293 |                   52 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 20.767 |      0.127 |                   14 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20299)[0m top1: 0.197294776119403
[2m[36m(func pid=20299)[0m top5: 0.8069029850746269
[2m[36m(func pid=20299)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=20299)[0m f1_macro: 0.17475079929855813
[2m[36m(func pid=20299)[0m f1_weighted: 0.1602651095703714
[2m[36m(func pid=20299)[0m f1_per_class: [0.193, 0.037, 0.486, 0.367, 0.095, 0.262, 0.0, 0.229, 0.0, 0.079]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=20296)[0m top1: 0.33908582089552236
[2m[36m(func pid=20296)[0m top5: 0.8390858208955224
[2m[36m(func pid=20296)[0m f1_micro: 0.33908582089552236
[2m[36m(func pid=20296)[0m f1_macro: 0.16589590895948156
[2m[36m(func pid=20296)[0m f1_weighted: 0.30622436879389087
[2m[36m(func pid=20296)[0m f1_per_class: [0.128, 0.105, 0.282, 0.449, 0.029, 0.0, 0.508, 0.106, 0.0, 0.051]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=31048)[0m top1: 0.19029850746268656
[2m[36m(func pid=31048)[0m top5: 0.6431902985074627
[2m[36m(func pid=31048)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=31048)[0m f1_macro: 0.14152008547726536
[2m[36m(func pid=31048)[0m f1_weighted: 0.1645126909821029
[2m[36m(func pid=31048)[0m f1_per_class: [0.278, 0.0, 0.514, 0.0, 0.03, 0.065, 0.495, 0.0, 0.0, 0.033]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m top1: 0.28777985074626866
[2m[36m(func pid=21639)[0m top5: 0.8180970149253731
[2m[36m(func pid=21639)[0m f1_micro: 0.28777985074626866
[2m[36m(func pid=21639)[0m f1_macro: 0.1590526246107055
[2m[36m(func pid=21639)[0m f1_weighted: 0.2435935869274797
[2m[36m(func pid=21639)[0m f1_per_class: [0.167, 0.474, 0.13, 0.0, 0.063, 0.0, 0.488, 0.188, 0.0, 0.081]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.7144 | Steps: 4 | Val loss: 2.1539 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.2224 | Steps: 4 | Val loss: 2.0681 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 19.9694 | Steps: 4 | Val loss: 9.1826 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.6957 | Steps: 4 | Val loss: 2.1904 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 10:44:16 (running for 00:26:45.71)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.714 |      0.157 |                   59 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.182 |      0.175 |                   57 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.456 |      0.159 |                   53 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 15.783 |      0.142 |                   15 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.34654850746268656
[2m[36m(func pid=20296)[0m top5: 0.8376865671641791
[2m[36m(func pid=20296)[0m f1_micro: 0.34654850746268656
[2m[36m(func pid=20296)[0m f1_macro: 0.15659328339806972
[2m[36m(func pid=20296)[0m f1_weighted: 0.30485941603164746
[2m[36m(func pid=20296)[0m f1_per_class: [0.12, 0.095, 0.264, 0.444, 0.0, 0.0, 0.524, 0.071, 0.0, 0.049]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.20242537313432835
[2m[36m(func pid=20299)[0m top5: 0.8274253731343284
[2m[36m(func pid=20299)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=20299)[0m f1_macro: 0.130201894729496
[2m[36m(func pid=20299)[0m f1_weighted: 0.1539372868717826
[2m[36m(func pid=20299)[0m f1_per_class: [0.178, 0.042, 0.175, 0.378, 0.123, 0.254, 0.0, 0.106, 0.0, 0.047]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=31048)[0m top1: 0.23600746268656717
[2m[36m(func pid=31048)[0m top5: 0.7635261194029851
[2m[36m(func pid=31048)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=31048)[0m f1_macro: 0.18613900593813215
[2m[36m(func pid=31048)[0m f1_weighted: 0.13983457343271702
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.431, 0.476, 0.0, 0.0, 0.307, 0.0, 0.376, 0.132, 0.14]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m top1: 0.291044776119403
[2m[36m(func pid=21639)[0m top5: 0.7667910447761194
[2m[36m(func pid=21639)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=21639)[0m f1_macro: 0.240056610697912
[2m[36m(func pid=21639)[0m f1_weighted: 0.26542857048922475
[2m[36m(func pid=21639)[0m f1_per_class: [0.255, 0.487, 0.216, 0.384, 0.151, 0.315, 0.003, 0.5, 0.0, 0.089]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.6185 | Steps: 4 | Val loss: 2.1573 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.2056 | Steps: 4 | Val loss: 2.0101 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 13.4700 | Steps: 4 | Val loss: 14.6554 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.9164 | Steps: 4 | Val loss: 2.0830 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 10:44:21 (running for 00:26:50.92)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.618 |      0.156 |                   60 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.222 |      0.13  |                   58 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.696 |      0.24  |                   54 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 19.969 |      0.186 |                   16 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.3460820895522388
[2m[36m(func pid=20296)[0m top5: 0.8381529850746269
[2m[36m(func pid=20296)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=20296)[0m f1_macro: 0.15642885168807974
[2m[36m(func pid=20296)[0m f1_weighted: 0.3134204145008403
[2m[36m(func pid=20296)[0m f1_per_class: [0.13, 0.151, 0.229, 0.441, 0.0, 0.0, 0.526, 0.058, 0.0, 0.03]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.2150186567164179
[2m[36m(func pid=20299)[0m top5: 0.8507462686567164
[2m[36m(func pid=20299)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=20299)[0m f1_macro: 0.1652141687519206
[2m[36m(func pid=20299)[0m f1_weighted: 0.17242174706232055
[2m[36m(func pid=20299)[0m f1_per_class: [0.259, 0.12, 0.333, 0.365, 0.102, 0.251, 0.016, 0.107, 0.058, 0.042]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=31048)[0m top1: 0.13992537313432835
[2m[36m(func pid=31048)[0m top5: 0.6282649253731343
[2m[36m(func pid=31048)[0m f1_micro: 0.13992537313432835
[2m[36m(func pid=31048)[0m f1_macro: 0.14942955039468422
[2m[36m(func pid=31048)[0m f1_weighted: 0.09808544103345211
[2m[36m(func pid=31048)[0m f1_per_class: [0.22, 0.463, 0.5, 0.0, 0.0, 0.0, 0.0, 0.14, 0.036, 0.136]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m top1: 0.3656716417910448
[2m[36m(func pid=21639)[0m top5: 0.7863805970149254
[2m[36m(func pid=21639)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=21639)[0m f1_macro: 0.32122096257186905
[2m[36m(func pid=21639)[0m f1_weighted: 0.3142787857825142
[2m[36m(func pid=21639)[0m f1_per_class: [0.412, 0.371, 0.375, 0.568, 0.179, 0.361, 0.0, 0.529, 0.149, 0.267]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.6358 | Steps: 4 | Val loss: 2.1559 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.2113 | Steps: 4 | Val loss: 1.9653 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 13.0178 | Steps: 4 | Val loss: 7.9329 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.9572 | Steps: 4 | Val loss: 2.1649 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 10:44:26 (running for 00:26:56.12)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.636 |      0.169 |                   61 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.206 |      0.165 |                   59 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.916 |      0.321 |                   55 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 13.47  |      0.149 |                   17 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.35867537313432835
[2m[36m(func pid=20296)[0m top5: 0.8353544776119403
[2m[36m(func pid=20296)[0m f1_micro: 0.35867537313432835
[2m[36m(func pid=20296)[0m f1_macro: 0.16856771065833281
[2m[36m(func pid=20296)[0m f1_weighted: 0.32884527312234824
[2m[36m(func pid=20296)[0m f1_per_class: [0.145, 0.164, 0.209, 0.461, 0.0, 0.0, 0.539, 0.085, 0.058, 0.025]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.24720149253731344
[2m[36m(func pid=20299)[0m top5: 0.8694029850746269
[2m[36m(func pid=20299)[0m f1_micro: 0.24720149253731344
[2m[36m(func pid=20299)[0m f1_macro: 0.2224525778519248
[2m[36m(func pid=20299)[0m f1_weighted: 0.22005511135676814
[2m[36m(func pid=20299)[0m f1_per_class: [0.26, 0.244, 0.48, 0.392, 0.13, 0.269, 0.037, 0.238, 0.099, 0.077]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=31048)[0m top1: 0.32742537313432835
[2m[36m(func pid=31048)[0m top5: 0.7938432835820896
[2m[36m(func pid=31048)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=31048)[0m f1_macro: 0.21570493215305803
[2m[36m(func pid=31048)[0m f1_weighted: 0.28304262383824835
[2m[36m(func pid=31048)[0m f1_per_class: [0.254, 0.419, 0.349, 0.0, 0.064, 0.252, 0.568, 0.0, 0.106, 0.145]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m top1: 0.22761194029850745
[2m[36m(func pid=21639)[0m top5: 0.9113805970149254
[2m[36m(func pid=21639)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=21639)[0m f1_macro: 0.22712387125920425
[2m[36m(func pid=21639)[0m f1_weighted: 0.24784506357906017
[2m[36m(func pid=21639)[0m f1_per_class: [0.256, 0.174, 0.444, 0.377, 0.045, 0.265, 0.167, 0.395, 0.0, 0.148]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.7638 | Steps: 4 | Val loss: 2.1624 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3246 | Steps: 4 | Val loss: 1.9190 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 10.7750 | Steps: 4 | Val loss: 9.3954 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 10:44:31 (running for 00:27:01.53)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.764 |      0.149 |                   62 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.211 |      0.222 |                   60 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.957 |      0.227 |                   56 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 13.018 |      0.216 |                   18 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.34048507462686567
[2m[36m(func pid=20296)[0m top5: 0.8213619402985075
[2m[36m(func pid=20296)[0m f1_micro: 0.34048507462686567
[2m[36m(func pid=20296)[0m f1_macro: 0.1489532349496513
[2m[36m(func pid=20296)[0m f1_weighted: 0.3180809711105226
[2m[36m(func pid=20296)[0m f1_per_class: [0.071, 0.154, 0.123, 0.451, 0.0, 0.0, 0.527, 0.07, 0.073, 0.022]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.9094 | Steps: 4 | Val loss: 2.9933 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=20299)[0m top1: 0.31343283582089554
[2m[36m(func pid=20299)[0m top5: 0.8903917910447762
[2m[36m(func pid=20299)[0m f1_micro: 0.31343283582089554
[2m[36m(func pid=20299)[0m f1_macro: 0.2739843990428795
[2m[36m(func pid=20299)[0m f1_weighted: 0.2771147024165472
[2m[36m(func pid=20299)[0m f1_per_class: [0.274, 0.31, 0.476, 0.495, 0.117, 0.32, 0.016, 0.504, 0.152, 0.077]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=31048)[0m top1: 0.09188432835820895
[2m[36m(func pid=31048)[0m top5: 0.7672574626865671
[2m[36m(func pid=31048)[0m f1_micro: 0.09188432835820894
[2m[36m(func pid=31048)[0m f1_macro: 0.15075916472601075
[2m[36m(func pid=31048)[0m f1_weighted: 0.0891311714878348
[2m[36m(func pid=31048)[0m f1_per_class: [0.289, 0.162, 0.632, 0.094, 0.059, 0.0, 0.068, 0.0, 0.096, 0.107]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m top1: 0.1828358208955224
[2m[36m(func pid=21639)[0m top5: 0.6702425373134329
[2m[36m(func pid=21639)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=21639)[0m f1_macro: 0.14719869186931267
[2m[36m(func pid=21639)[0m f1_weighted: 0.19812077658505964
[2m[36m(func pid=21639)[0m f1_per_class: [0.044, 0.0, 0.078, 0.0, 0.033, 0.247, 0.482, 0.356, 0.06, 0.171]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.6540 | Steps: 4 | Val loss: 2.1565 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.2224 | Steps: 4 | Val loss: 1.8986 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 7.6225 | Steps: 4 | Val loss: 5.8534 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 10:44:37 (running for 00:27:06.83)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.654 |      0.153 |                   63 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.325 |      0.274 |                   61 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.909 |      0.147 |                   57 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 10.775 |      0.151 |                   19 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.3516791044776119
[2m[36m(func pid=20296)[0m top5: 0.8330223880597015
[2m[36m(func pid=20296)[0m f1_micro: 0.3516791044776119
[2m[36m(func pid=20296)[0m f1_macro: 0.1534463942899303
[2m[36m(func pid=20296)[0m f1_weighted: 0.3275525619465719
[2m[36m(func pid=20296)[0m f1_per_class: [0.107, 0.184, 0.119, 0.445, 0.0, 0.0, 0.548, 0.058, 0.056, 0.018]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.7774 | Steps: 4 | Val loss: 2.0969 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=20299)[0m top1: 0.324160447761194
[2m[36m(func pid=20299)[0m top5: 0.8698694029850746
[2m[36m(func pid=20299)[0m f1_micro: 0.324160447761194
[2m[36m(func pid=20299)[0m f1_macro: 0.2565228630612283
[2m[36m(func pid=20299)[0m f1_weighted: 0.28377684894935534
[2m[36m(func pid=20299)[0m f1_per_class: [0.312, 0.418, 0.324, 0.478, 0.09, 0.377, 0.0, 0.371, 0.118, 0.077]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=31048)[0m top1: 0.33302238805970147
[2m[36m(func pid=31048)[0m top5: 0.8428171641791045
[2m[36m(func pid=31048)[0m f1_micro: 0.33302238805970147
[2m[36m(func pid=31048)[0m f1_macro: 0.22950460198100037
[2m[36m(func pid=31048)[0m f1_weighted: 0.27888873462506175
[2m[36m(func pid=31048)[0m f1_per_class: [0.043, 0.481, 0.124, 0.119, 0.27, 0.414, 0.29, 0.401, 0.028, 0.125]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m top1: 0.23180970149253732
[2m[36m(func pid=21639)[0m top5: 0.8185634328358209
[2m[36m(func pid=21639)[0m f1_micro: 0.23180970149253732
[2m[36m(func pid=21639)[0m f1_macro: 0.2074668287464197
[2m[36m(func pid=21639)[0m f1_weighted: 0.24729223504230977
[2m[36m(func pid=21639)[0m f1_per_class: [0.165, 0.262, 0.22, 0.371, 0.314, 0.024, 0.22, 0.332, 0.1, 0.067]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6734 | Steps: 4 | Val loss: 2.1681 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.1975 | Steps: 4 | Val loss: 1.9449 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 8.9181 | Steps: 4 | Val loss: 4.9617 | Batch size: 32 | lr: 0.1 | Duration: 2.74s
== Status ==
Current time: 2024-01-07 10:44:42 (running for 00:27:12.33)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.673 |      0.143 |                   64 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.222 |      0.257 |                   62 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.777 |      0.207 |                   58 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  7.622 |      0.23  |                   20 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.32322761194029853
[2m[36m(func pid=20296)[0m top5: 0.8222947761194029
[2m[36m(func pid=20296)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=20296)[0m f1_macro: 0.14345740276989097
[2m[36m(func pid=20296)[0m f1_weighted: 0.31707458980804865
[2m[36m(func pid=20296)[0m f1_per_class: [0.039, 0.224, 0.097, 0.409, 0.0, 0.0, 0.532, 0.045, 0.048, 0.041]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.4977 | Steps: 4 | Val loss: 1.7654 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=20299)[0m top1: 0.2933768656716418
[2m[36m(func pid=20299)[0m top5: 0.8451492537313433
[2m[36m(func pid=20299)[0m f1_micro: 0.2933768656716418
[2m[36m(func pid=20299)[0m f1_macro: 0.20875766844504212
[2m[36m(func pid=20299)[0m f1_weighted: 0.2665909833890154
[2m[36m(func pid=20299)[0m f1_per_class: [0.105, 0.432, 0.167, 0.454, 0.068, 0.318, 0.0, 0.367, 0.098, 0.077]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=31048)[0m top1: 0.44449626865671643
[2m[36m(func pid=31048)[0m top5: 0.9309701492537313
[2m[36m(func pid=31048)[0m f1_micro: 0.44449626865671643
[2m[36m(func pid=31048)[0m f1_macro: 0.26367943109397607
[2m[36m(func pid=31048)[0m f1_weighted: 0.42063695058292677
[2m[36m(func pid=31048)[0m f1_per_class: [0.128, 0.361, 0.5, 0.59, 0.123, 0.098, 0.571, 0.0, 0.133, 0.133]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m top1: 0.4146455223880597
[2m[36m(func pid=21639)[0m top5: 0.8843283582089553
[2m[36m(func pid=21639)[0m f1_micro: 0.4146455223880597
[2m[36m(func pid=21639)[0m f1_macro: 0.30373229579983546
[2m[36m(func pid=21639)[0m f1_weighted: 0.38845204747347967
[2m[36m(func pid=21639)[0m f1_per_class: [0.352, 0.525, 0.632, 0.471, 0.0, 0.069, 0.392, 0.51, 0.028, 0.06]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6046 | Steps: 4 | Val loss: 2.1742 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.2861 | Steps: 4 | Val loss: 2.0425 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 6.4780 | Steps: 4 | Val loss: 7.1765 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 10:44:47 (running for 00:27:17.49)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.605 |      0.146 |                   65 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.197 |      0.209 |                   63 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.498 |      0.304 |                   59 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  8.918 |      0.264 |                   21 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.2971082089552239
[2m[36m(func pid=20296)[0m top5: 0.8125
[2m[36m(func pid=20296)[0m f1_micro: 0.2971082089552239
[2m[36m(func pid=20296)[0m f1_macro: 0.14563406568080436
[2m[36m(func pid=20296)[0m f1_weighted: 0.30908080612070316
[2m[36m(func pid=20296)[0m f1_per_class: [0.07, 0.249, 0.106, 0.388, 0.0, 0.0, 0.505, 0.058, 0.055, 0.026]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.22154850746268656
[2m[36m(func pid=20299)[0m top5: 0.808768656716418
[2m[36m(func pid=20299)[0m f1_micro: 0.22154850746268656
[2m[36m(func pid=20299)[0m f1_macro: 0.1824279964532511
[2m[36m(func pid=20299)[0m f1_weighted: 0.21640695129457116
[2m[36m(func pid=20299)[0m f1_per_class: [0.1, 0.359, 0.114, 0.35, 0.049, 0.216, 0.0, 0.392, 0.167, 0.077]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.8708 | Steps: 4 | Val loss: 1.8256 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=31048)[0m top1: 0.17863805970149255
[2m[36m(func pid=31048)[0m top5: 0.8036380597014925
[2m[36m(func pid=31048)[0m f1_micro: 0.17863805970149257
[2m[36m(func pid=31048)[0m f1_macro: 0.20820482311829278
[2m[36m(func pid=31048)[0m f1_weighted: 0.1543193255698885
[2m[36m(func pid=31048)[0m f1_per_class: [0.128, 0.473, 0.5, 0.11, 0.031, 0.031, 0.0, 0.426, 0.169, 0.214]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m top1: 0.34328358208955223
[2m[36m(func pid=21639)[0m top5: 0.8763992537313433
[2m[36m(func pid=21639)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=21639)[0m f1_macro: 0.25736128807209124
[2m[36m(func pid=21639)[0m f1_weighted: 0.35964777188618485
[2m[36m(func pid=21639)[0m f1_per_class: [0.087, 0.496, 0.205, 0.199, 0.0, 0.398, 0.465, 0.444, 0.137, 0.143]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.6217 | Steps: 4 | Val loss: 2.1838 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.0355 | Steps: 4 | Val loss: 2.0899 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 3.3076 | Steps: 4 | Val loss: 7.9423 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 10:44:53 (running for 00:27:22.98)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.622 |      0.142 |                   66 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.286 |      0.182 |                   64 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.871 |      0.257 |                   60 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  6.478 |      0.208 |                   22 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.2579291044776119
[2m[36m(func pid=20296)[0m top5: 0.8031716417910447
[2m[36m(func pid=20296)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=20296)[0m f1_macro: 0.1419212680914565
[2m[36m(func pid=20296)[0m f1_weighted: 0.28657074879418526
[2m[36m(func pid=20296)[0m f1_per_class: [0.062, 0.335, 0.105, 0.284, 0.0, 0.0, 0.472, 0.077, 0.066, 0.017]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.15764925373134328
[2m[36m(func pid=20299)[0m top5: 0.7621268656716418
[2m[36m(func pid=20299)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=20299)[0m f1_macro: 0.15701864131350773
[2m[36m(func pid=20299)[0m f1_weighted: 0.10971279213148712
[2m[36m(func pid=20299)[0m f1_per_class: [0.244, 0.323, 0.222, 0.044, 0.036, 0.05, 0.0, 0.38, 0.201, 0.069]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3197 | Steps: 4 | Val loss: 2.9772 | Batch size: 32 | lr: 0.01 | Duration: 3.23s
[2m[36m(func pid=31048)[0m top1: 0.3493470149253731
[2m[36m(func pid=31048)[0m top5: 0.9113805970149254
[2m[36m(func pid=31048)[0m f1_micro: 0.3493470149253731
[2m[36m(func pid=31048)[0m f1_macro: 0.16181751602025438
[2m[36m(func pid=31048)[0m f1_weighted: 0.25760793463702575
[2m[36m(func pid=31048)[0m f1_per_class: [0.083, 0.15, 0.136, 0.522, 0.0, 0.419, 0.078, 0.203, 0.027, 0.0]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.7323 | Steps: 4 | Val loss: 2.1991 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.2370 | Steps: 4 | Val loss: 2.1534 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=21639)[0m top1: 0.24347014925373134
[2m[36m(func pid=21639)[0m top5: 0.6707089552238806
[2m[36m(func pid=21639)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=21639)[0m f1_macro: 0.17054076011131994
[2m[36m(func pid=21639)[0m f1_weighted: 0.24641571185614256
[2m[36m(func pid=21639)[0m f1_per_class: [0.111, 0.0, 0.049, 0.026, 0.105, 0.371, 0.566, 0.414, 0.0, 0.064]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 10.5784 | Steps: 4 | Val loss: 8.8638 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 10:44:58 (running for 00:27:28.19)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.732 |      0.133 |                   67 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.035 |      0.157 |                   65 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.32  |      0.171 |                   61 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  3.308 |      0.162 |                   23 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.24253731343283583
[2m[36m(func pid=20296)[0m top5: 0.7803171641791045
[2m[36m(func pid=20296)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=20296)[0m f1_macro: 0.13330177761710404
[2m[36m(func pid=20296)[0m f1_weighted: 0.2621266011224997
[2m[36m(func pid=20296)[0m f1_per_class: [0.053, 0.418, 0.098, 0.194, 0.0, 0.0, 0.431, 0.081, 0.036, 0.024]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.13432835820895522
[2m[36m(func pid=20299)[0m top5: 0.7360074626865671
[2m[36m(func pid=20299)[0m f1_micro: 0.13432835820895522
[2m[36m(func pid=20299)[0m f1_macro: 0.1407736734290501
[2m[36m(func pid=20299)[0m f1_weighted: 0.08475926169614703
[2m[36m(func pid=20299)[0m f1_per_class: [0.2, 0.29, 0.301, 0.0, 0.031, 0.008, 0.0, 0.401, 0.116, 0.061]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.4115 | Steps: 4 | Val loss: 3.1806 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
[2m[36m(func pid=31048)[0m top1: 0.30923507462686567
[2m[36m(func pid=31048)[0m top5: 0.6772388059701493
[2m[36m(func pid=31048)[0m f1_micro: 0.30923507462686567
[2m[36m(func pid=31048)[0m f1_macro: 0.27591220141483586
[2m[36m(func pid=31048)[0m f1_weighted: 0.292585982508507
[2m[36m(func pid=31048)[0m f1_per_class: [0.117, 0.482, 0.64, 0.0, 0.111, 0.431, 0.442, 0.318, 0.034, 0.185]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.7017 | Steps: 4 | Val loss: 2.2048 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.2488 | Steps: 4 | Val loss: 2.1725 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=21639)[0m top1: 0.13712686567164178
[2m[36m(func pid=21639)[0m top5: 0.6716417910447762
[2m[36m(func pid=21639)[0m f1_micro: 0.13712686567164178
[2m[36m(func pid=21639)[0m f1_macro: 0.164115297171402
[2m[36m(func pid=21639)[0m f1_weighted: 0.11856342174262785
[2m[36m(func pid=21639)[0m f1_per_class: [0.23, 0.0, 0.375, 0.176, 0.037, 0.23, 0.037, 0.353, 0.104, 0.1]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 9.2656 | Steps: 4 | Val loss: 12.8226 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 10:45:03 (running for 00:27:33.53)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.702 |      0.133 |                   68 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.237 |      0.141 |                   66 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.411 |      0.164 |                   62 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 10.578 |      0.276 |                   24 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.21595149253731344
[2m[36m(func pid=20296)[0m top5: 0.7723880597014925
[2m[36m(func pid=20296)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=20296)[0m f1_macro: 0.13336634923884666
[2m[36m(func pid=20296)[0m f1_weighted: 0.23112299085185276
[2m[36m(func pid=20296)[0m f1_per_class: [0.07, 0.441, 0.111, 0.143, 0.0, 0.0, 0.342, 0.168, 0.037, 0.022]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.1609141791044776
[2m[36m(func pid=20299)[0m top5: 0.6753731343283582
[2m[36m(func pid=20299)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=20299)[0m f1_macro: 0.16399332075480377
[2m[36m(func pid=20299)[0m f1_weighted: 0.10475952211500396
[2m[36m(func pid=20299)[0m f1_per_class: [0.245, 0.313, 0.379, 0.0, 0.039, 0.167, 0.0, 0.364, 0.077, 0.056]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.3860 | Steps: 4 | Val loss: 2.1744 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=31048)[0m top1: 0.08675373134328358
[2m[36m(func pid=31048)[0m top5: 0.6324626865671642
[2m[36m(func pid=31048)[0m f1_micro: 0.08675373134328358
[2m[36m(func pid=31048)[0m f1_macro: 0.13682280063873742
[2m[36m(func pid=31048)[0m f1_weighted: 0.032805945801318515
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.0, 0.692, 0.0, 0.034, 0.016, 0.0, 0.359, 0.124, 0.143]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.7248 | Steps: 4 | Val loss: 2.2054 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.2804 | Steps: 4 | Val loss: 2.1829 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=21639)[0m top1: 0.32649253731343286
[2m[36m(func pid=21639)[0m top5: 0.8334888059701493
[2m[36m(func pid=21639)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=21639)[0m f1_macro: 0.21554619426281785
[2m[36m(func pid=21639)[0m f1_weighted: 0.28938671105765745
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.0, 0.222, 0.546, 0.071, 0.354, 0.207, 0.514, 0.024, 0.217]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 7.8656 | Steps: 4 | Val loss: 11.9872 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 10:45:09 (running for 00:27:38.97)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.702 |      0.133 |                   68 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.28  |      0.184 |                   68 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.386 |      0.216 |                   63 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  9.266 |      0.137 |                   25 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.19776119402985073
[2m[36m(func pid=20296)[0m top5: 0.773320895522388
[2m[36m(func pid=20296)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=20296)[0m f1_macro: 0.12382014738573836
[2m[36m(func pid=20296)[0m f1_weighted: 0.21907856820243618
[2m[36m(func pid=20296)[0m f1_per_class: [0.069, 0.415, 0.114, 0.185, 0.0, 0.0, 0.289, 0.115, 0.021, 0.03]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.1791044776119403
[2m[36m(func pid=20299)[0m top5: 0.6459888059701493
[2m[36m(func pid=20299)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=20299)[0m f1_macro: 0.1835992820985401
[2m[36m(func pid=20299)[0m f1_weighted: 0.12582740630749603
[2m[36m(func pid=20299)[0m f1_per_class: [0.284, 0.344, 0.32, 0.0, 0.058, 0.337, 0.0, 0.247, 0.133, 0.114]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.5388 | Steps: 4 | Val loss: 1.6416 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=31048)[0m top1: 0.08955223880597014
[2m[36m(func pid=31048)[0m top5: 0.7975746268656716
[2m[36m(func pid=31048)[0m f1_micro: 0.08955223880597016
[2m[36m(func pid=31048)[0m f1_macro: 0.12420706824629926
[2m[36m(func pid=31048)[0m f1_weighted: 0.0825593376513698
[2m[36m(func pid=31048)[0m f1_per_class: [0.115, 0.0, 0.196, 0.15, 0.203, 0.0, 0.015, 0.529, 0.0, 0.034]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.1880 | Steps: 4 | Val loss: 2.2229 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.7159 | Steps: 4 | Val loss: 2.2019 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=21639)[0m top1: 0.4454291044776119
[2m[36m(func pid=21639)[0m top5: 0.9155783582089553
[2m[36m(func pid=21639)[0m f1_micro: 0.4454291044776119
[2m[36m(func pid=21639)[0m f1_macro: 0.3115262445340209
[2m[36m(func pid=21639)[0m f1_weighted: 0.4287680635162463
[2m[36m(func pid=21639)[0m f1_per_class: [0.238, 0.522, 0.224, 0.327, 0.245, 0.447, 0.547, 0.412, 0.0, 0.154]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 7.3968 | Steps: 4 | Val loss: 12.5287 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 10:45:14 (running for 00:27:44.31)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.725 |      0.124 |                   69 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.188 |      0.127 |                   69 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.539 |      0.312 |                   64 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  7.866 |      0.124 |                   26 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.2019589552238806
[2m[36m(func pid=20296)[0m top5: 0.7714552238805971
[2m[36m(func pid=20296)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=20296)[0m f1_macro: 0.12388055442841424
[2m[36m(func pid=20296)[0m f1_weighted: 0.22900794227096233
[2m[36m(func pid=20296)[0m f1_per_class: [0.077, 0.344, 0.102, 0.315, 0.011, 0.0, 0.246, 0.091, 0.022, 0.03]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.11940298507462686
[2m[36m(func pid=20299)[0m top5: 0.6977611940298507
[2m[36m(func pid=20299)[0m f1_micro: 0.11940298507462686
[2m[36m(func pid=20299)[0m f1_macro: 0.12746151887380414
[2m[36m(func pid=20299)[0m f1_weighted: 0.07813345555628176
[2m[36m(func pid=20299)[0m f1_per_class: [0.206, 0.098, 0.27, 0.0, 0.1, 0.385, 0.0, 0.144, 0.073, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.8144 | Steps: 4 | Val loss: 2.3882 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=31048)[0m top1: 0.19076492537313433
[2m[36m(func pid=31048)[0m top5: 0.7416044776119403
[2m[36m(func pid=31048)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=31048)[0m f1_macro: 0.1784303269170039
[2m[36m(func pid=31048)[0m f1_weighted: 0.10921568389747383
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.439, 0.72, 0.0, 0.074, 0.037, 0.0, 0.367, 0.07, 0.077]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.6670 | Steps: 4 | Val loss: 2.1927 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.4043 | Steps: 4 | Val loss: 2.1587 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=21639)[0m top1: 0.2621268656716418
[2m[36m(func pid=21639)[0m top5: 0.7719216417910447
[2m[36m(func pid=21639)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=21639)[0m f1_macro: 0.22251672391462302
[2m[36m(func pid=21639)[0m f1_weighted: 0.15591654960671472
[2m[36m(func pid=21639)[0m f1_per_class: [0.269, 0.466, 0.426, 0.0, 0.205, 0.357, 0.0, 0.421, 0.0, 0.082]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 11.0479 | Steps: 4 | Val loss: 8.2087 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 10:45:19 (running for 00:27:49.60)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.667 |      0.127 |                   71 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.188 |      0.127 |                   69 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.814 |      0.223 |                   65 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  7.397 |      0.178 |                   27 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.19916044776119404
[2m[36m(func pid=20296)[0m top5: 0.7761194029850746
[2m[36m(func pid=20296)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=20296)[0m f1_macro: 0.12742428876342898
[2m[36m(func pid=20296)[0m f1_weighted: 0.2316207320375201
[2m[36m(func pid=20296)[0m f1_per_class: [0.1, 0.302, 0.119, 0.362, 0.021, 0.0, 0.235, 0.076, 0.025, 0.034]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.1166044776119403
[2m[36m(func pid=20299)[0m top5: 0.7943097014925373
[2m[36m(func pid=20299)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=20299)[0m f1_macro: 0.10872713094554336
[2m[36m(func pid=20299)[0m f1_weighted: 0.0600691941419794
[2m[36m(func pid=20299)[0m f1_per_class: [0.145, 0.005, 0.282, 0.0, 0.12, 0.401, 0.0, 0.133, 0.0, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=31048)[0m top1: 0.24347014925373134
[2m[36m(func pid=31048)[0m top5: 0.820429104477612
[2m[36m(func pid=31048)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=31048)[0m f1_macro: 0.25451606745590905
[2m[36m(func pid=31048)[0m f1_weighted: 0.22948791567012186
[2m[36m(func pid=31048)[0m f1_per_class: [0.324, 0.503, 0.625, 0.417, 0.24, 0.0, 0.0, 0.182, 0.04, 0.214]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.4838 | Steps: 4 | Val loss: 2.7461 | Batch size: 32 | lr: 0.01 | Duration: 3.28s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.7570 | Steps: 4 | Val loss: 2.1874 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.3467 | Steps: 4 | Val loss: 1.9733 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=21639)[0m top1: 0.1571828358208955
[2m[36m(func pid=21639)[0m top5: 0.7262126865671642
[2m[36m(func pid=21639)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=21639)[0m f1_macro: 0.17754009877301377
[2m[36m(func pid=21639)[0m f1_weighted: 0.14186042517407282
[2m[36m(func pid=21639)[0m f1_per_class: [0.087, 0.037, 0.189, 0.215, 0.049, 0.302, 0.0, 0.535, 0.124, 0.238]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 14.5273 | Steps: 4 | Val loss: 18.3354 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 10:45:25 (running for 00:27:54.81)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.757 |      0.127 |                   72 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.404 |      0.109 |                   70 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.484 |      0.178 |                   66 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 11.048 |      0.255 |                   28 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.2126865671641791
[2m[36m(func pid=20296)[0m top5: 0.7863805970149254
[2m[36m(func pid=20296)[0m f1_micro: 0.2126865671641791
[2m[36m(func pid=20296)[0m f1_macro: 0.126923643528529
[2m[36m(func pid=20296)[0m f1_weighted: 0.2410956271524287
[2m[36m(func pid=20296)[0m f1_per_class: [0.095, 0.249, 0.105, 0.408, 0.031, 0.0, 0.255, 0.09, 0.0, 0.036]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.22761194029850745
[2m[36m(func pid=20299)[0m top5: 0.8652052238805971
[2m[36m(func pid=20299)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=20299)[0m f1_macro: 0.15843921713891193
[2m[36m(func pid=20299)[0m f1_weighted: 0.17871302098430852
[2m[36m(func pid=20299)[0m f1_per_class: [0.162, 0.135, 0.216, 0.334, 0.113, 0.374, 0.0, 0.228, 0.021, 0.0]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=31048)[0m top1: 0.11707089552238806
[2m[36m(func pid=31048)[0m top5: 0.5219216417910447
[2m[36m(func pid=31048)[0m f1_micro: 0.11707089552238806
[2m[36m(func pid=31048)[0m f1_macro: 0.1574091184487517
[2m[36m(func pid=31048)[0m f1_weighted: 0.11839209487620755
[2m[36m(func pid=31048)[0m f1_per_class: [0.105, 0.093, 0.483, 0.233, 0.066, 0.0, 0.0, 0.54, 0.0, 0.054]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.2334 | Steps: 4 | Val loss: 2.1598 | Batch size: 32 | lr: 0.01 | Duration: 3.13s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.6288 | Steps: 4 | Val loss: 2.1612 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.2860 | Steps: 4 | Val loss: 1.8662 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 11.6051 | Steps: 4 | Val loss: 9.4520 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=21639)[0m top1: 0.28404850746268656
[2m[36m(func pid=21639)[0m top5: 0.8586753731343284
[2m[36m(func pid=21639)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=21639)[0m f1_macro: 0.24096375301571987
[2m[36m(func pid=21639)[0m f1_weighted: 0.3043296203127053
[2m[36m(func pid=21639)[0m f1_per_class: [0.201, 0.073, 0.123, 0.431, 0.058, 0.355, 0.301, 0.493, 0.144, 0.231]
[2m[36m(func pid=21639)[0m 
== Status ==
Current time: 2024-01-07 10:45:30 (running for 00:28:00.11)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.629 |      0.138 |                   73 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.347 |      0.158 |                   71 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  2.233 |      0.241 |                   67 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 14.527 |      0.157 |                   29 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.2621268656716418
[2m[36m(func pid=20296)[0m top5: 0.8264925373134329
[2m[36m(func pid=20296)[0m f1_micro: 0.2621268656716418
[2m[36m(func pid=20296)[0m f1_macro: 0.13827861135630007
[2m[36m(func pid=20296)[0m f1_weighted: 0.2831044945096406
[2m[36m(func pid=20296)[0m f1_per_class: [0.062, 0.166, 0.137, 0.468, 0.041, 0.0, 0.391, 0.076, 0.0, 0.041]
[2m[36m(func pid=20296)[0m 
[2m[36m(func pid=20299)[0m top1: 0.2966417910447761
[2m[36m(func pid=20299)[0m top5: 0.9104477611940298
[2m[36m(func pid=20299)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=20299)[0m f1_macro: 0.23602266852335058
[2m[36m(func pid=20299)[0m f1_weighted: 0.24786141367992612
[2m[36m(func pid=20299)[0m f1_per_class: [0.282, 0.404, 0.256, 0.339, 0.14, 0.338, 0.027, 0.449, 0.048, 0.077]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=31048)[0m top1: 0.3987873134328358
[2m[36m(func pid=31048)[0m top5: 0.8390858208955224
[2m[36m(func pid=31048)[0m f1_micro: 0.3987873134328358
[2m[36m(func pid=31048)[0m f1_macro: 0.1754624542426781
[2m[36m(func pid=31048)[0m f1_weighted: 0.33869228658222283
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.432, 0.348, 0.33, 0.0, 0.0, 0.568, 0.0, 0.0, 0.077]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.6431 | Steps: 4 | Val loss: 1.7565 | Batch size: 32 | lr: 0.01 | Duration: 2.94s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.6043 | Steps: 4 | Val loss: 2.1587 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.4206 | Steps: 4 | Val loss: 1.8751 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=21639)[0m top1: 0.4435634328358209
[2m[36m(func pid=21639)[0m top5: 0.9132462686567164
[2m[36m(func pid=21639)[0m f1_micro: 0.4435634328358209
[2m[36m(func pid=21639)[0m f1_macro: 0.31987414167876993
[2m[36m(func pid=21639)[0m f1_weighted: 0.4446574487884786
[2m[36m(func pid=21639)[0m f1_per_class: [0.206, 0.56, 0.208, 0.378, 0.222, 0.429, 0.527, 0.477, 0.0, 0.194]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 11.8490 | Steps: 4 | Val loss: 10.5684 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=20296)[0m top1: 0.28218283582089554
[2m[36m(func pid=20296)[0m top5: 0.8577425373134329
[2m[36m(func pid=20296)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=20296)[0m f1_macro: 0.1437533014505829
[2m[36m(func pid=20296)[0m f1_weighted: 0.30616578266695227
[2m[36m(func pid=20296)[0m f1_per_class: [0.054, 0.189, 0.116, 0.439, 0.027, 0.023, 0.476, 0.072, 0.0, 0.041]
[2m[36m(func pid=20296)[0m 
== Status ==
Current time: 2024-01-07 10:45:35 (running for 00:28:05.37)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=12
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 4 RUNNING, 12 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00012 | RUNNING    | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |  2.604 |      0.144 |                   74 |
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.286 |      0.236 |                   72 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.643 |      0.32  |                   68 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 11.605 |      0.175 |                   30 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20299)[0m top1: 0.3306902985074627
[2m[36m(func pid=20299)[0m top5: 0.8908582089552238
[2m[36m(func pid=20299)[0m f1_micro: 0.3306902985074627
[2m[36m(func pid=20299)[0m f1_macro: 0.23688094604669815
[2m[36m(func pid=20299)[0m f1_weighted: 0.32919365724794936
[2m[36m(func pid=20299)[0m f1_per_class: [0.109, 0.394, 0.185, 0.372, 0.12, 0.369, 0.312, 0.268, 0.0, 0.24]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=31048)[0m top1: 0.23973880597014927
[2m[36m(func pid=31048)[0m top5: 0.8404850746268657
[2m[36m(func pid=31048)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=31048)[0m f1_macro: 0.16658494257603707
[2m[36m(func pid=31048)[0m f1_weighted: 0.20905299257190998
[2m[36m(func pid=31048)[0m f1_per_class: [0.123, 0.194, 0.143, 0.427, 0.0, 0.321, 0.0, 0.214, 0.063, 0.182]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.6435 | Steps: 4 | Val loss: 2.0612 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=20296)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.6556 | Steps: 4 | Val loss: 2.1606 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.1450 | Steps: 4 | Val loss: 1.9917 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 11.3697 | Steps: 4 | Val loss: 18.4389 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=21639)[0m top1: 0.302705223880597
[2m[36m(func pid=21639)[0m top5: 0.8465485074626866
[2m[36m(func pid=21639)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=21639)[0m f1_macro: 0.18023638597115524
[2m[36m(func pid=21639)[0m f1_weighted: 0.21453737124864483
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.445, 0.222, 0.208, 0.0, 0.371, 0.034, 0.433, 0.0, 0.09]
[2m[36m(func pid=21639)[0m 
== Status ==
Current time: 2024-01-07 10:45:41 (running for 00:28:10.75)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=13
Bracket: Iter 75.000: 0.233
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (8 PENDING, 3 RUNNING, 13 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00013 | RUNNING    | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |  2.421 |      0.237 |                   73 |
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.644 |      0.18  |                   69 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 11.849 |      0.167 |                   31 |
| train_952df_00016 | PENDING    |                     | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=20296)[0m top1: 0.2728544776119403
[2m[36m(func pid=20296)[0m top5: 0.8684701492537313
[2m[36m(func pid=20296)[0m f1_micro: 0.2728544776119403
[2m[36m(func pid=20296)[0m f1_macro: 0.14938585361917206
[2m[36m(func pid=20296)[0m f1_weighted: 0.2987848849940887
[2m[36m(func pid=20296)[0m f1_per_class: [0.099, 0.156, 0.14, 0.429, 0.041, 0.043, 0.465, 0.097, 0.0, 0.026]
[2m[36m(func pid=20299)[0m top1: 0.29990671641791045
[2m[36m(func pid=20299)[0m top5: 0.8698694029850746
[2m[36m(func pid=20299)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=20299)[0m f1_macro: 0.18536488853127317
[2m[36m(func pid=20299)[0m f1_weighted: 0.3310016543754187
[2m[36m(func pid=20299)[0m f1_per_class: [0.063, 0.343, 0.094, 0.423, 0.078, 0.348, 0.365, 0.016, 0.028, 0.095]
[2m[36m(func pid=20299)[0m 
[2m[36m(func pid=31048)[0m top1: 0.11847014925373134
[2m[36m(func pid=31048)[0m top5: 0.5499067164179104
[2m[36m(func pid=31048)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=31048)[0m f1_macro: 0.09046343788797555
[2m[36m(func pid=31048)[0m f1_weighted: 0.09263641458518748
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.088, 0.031, 0.216, 0.2, 0.0, 0.0, 0.241, 0.0, 0.128]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 1.8332 | Steps: 4 | Val loss: 2.2252 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=20299)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.4379 | Steps: 4 | Val loss: 2.0740 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 12.0738 | Steps: 4 | Val loss: 10.6905 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=21639)[0m top1: 0.3208955223880597
[2m[36m(func pid=21639)[0m top5: 0.761660447761194
[2m[36m(func pid=21639)[0m f1_micro: 0.3208955223880597
[2m[36m(func pid=21639)[0m f1_macro: 0.27690469307484017
[2m[36m(func pid=21639)[0m f1_weighted: 0.27501762961861004
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.219, 0.783, 0.58, 0.131, 0.305, 0.0, 0.486, 0.171, 0.093]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=20299)[0m top1: 0.19169776119402984
[2m[36m(func pid=20299)[0m top5: 0.8348880597014925
[2m[36m(func pid=20299)[0m f1_micro: 0.19169776119402984
[2m[36m(func pid=20299)[0m f1_macro: 0.1344579191782966
[2m[36m(func pid=20299)[0m f1_weighted: 0.2119356950088204
[2m[36m(func pid=20299)[0m f1_per_class: [0.183, 0.303, 0.134, 0.083, 0.063, 0.143, 0.385, 0.0, 0.0, 0.05]
[2m[36m(func pid=31048)[0m top1: 0.2887126865671642
[2m[36m(func pid=31048)[0m top5: 0.8292910447761194
[2m[36m(func pid=31048)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=31048)[0m f1_macro: 0.2174918569110193
[2m[36m(func pid=31048)[0m f1_weighted: 0.240388927963146
[2m[36m(func pid=31048)[0m f1_per_class: [0.26, 0.438, 0.471, 0.046, 0.066, 0.0, 0.411, 0.338, 0.0, 0.146]
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.7366 | Steps: 4 | Val loss: 2.6283 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 10:45:46 (running for 00:28:16.64)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.2225
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (7 PENDING, 3 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.833 |      0.277 |                   70 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 11.37  |      0.09  |                   32 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | PENDING    |                     | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=38780)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=38780)[0m Configuration completed!
[2m[36m(func pid=38780)[0m New optimizer parameters:
[2m[36m(func pid=38780)[0m SGD (
[2m[36m(func pid=38780)[0m Parameter Group 0
[2m[36m(func pid=38780)[0m     dampening: 0
[2m[36m(func pid=38780)[0m     differentiable: False
[2m[36m(func pid=38780)[0m     foreach: None
[2m[36m(func pid=38780)[0m     lr: 0.0001
[2m[36m(func pid=38780)[0m     maximize: False
[2m[36m(func pid=38780)[0m     momentum: 0.99
[2m[36m(func pid=38780)[0m     nesterov: False
[2m[36m(func pid=38780)[0m     weight_decay: 1e-05
[2m[36m(func pid=38780)[0m )
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=21639)[0m top1: 0.20009328358208955
[2m[36m(func pid=21639)[0m top5: 0.7481343283582089
[2m[36m(func pid=21639)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=21639)[0m f1_macro: 0.2167302621370611
[2m[36m(func pid=21639)[0m f1_weighted: 0.17937111376528211
[2m[36m(func pid=21639)[0m f1_per_class: [0.145, 0.0, 0.444, 0.339, 0.059, 0.302, 0.031, 0.498, 0.104, 0.245]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 20.4628 | Steps: 4 | Val loss: 10.9565 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.6609 | Steps: 4 | Val loss: 1.9519 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1182 | Steps: 4 | Val loss: 2.3287 | Batch size: 32 | lr: 0.0001 | Duration: 4.47s
[2m[36m(func pid=31048)[0m top1: 0.25513059701492535
[2m[36m(func pid=31048)[0m top5: 0.730410447761194
[2m[36m(func pid=31048)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=31048)[0m f1_macro: 0.22536269952749155
[2m[36m(func pid=31048)[0m f1_weighted: 0.2509431457393104
[2m[36m(func pid=31048)[0m f1_per_class: [0.34, 0.254, 0.632, 0.0, 0.04, 0.339, 0.523, 0.0, 0.0, 0.126]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m top1: 0.39505597014925375
[2m[36m(func pid=21639)[0m top5: 0.8796641791044776
[2m[36m(func pid=21639)[0m f1_micro: 0.39505597014925375
[2m[36m(func pid=21639)[0m f1_macro: 0.23868664246924154
[2m[36m(func pid=21639)[0m f1_weighted: 0.36649437917716604
[2m[36m(func pid=21639)[0m f1_per_class: [0.215, 0.005, 0.258, 0.563, 0.067, 0.177, 0.542, 0.274, 0.053, 0.233]
[2m[36m(func pid=38780)[0m top1: 0.14225746268656717
[2m[36m(func pid=38780)[0m top5: 0.5368470149253731
[2m[36m(func pid=38780)[0m f1_micro: 0.14225746268656717
[2m[36m(func pid=38780)[0m f1_macro: 0.042196360160917525
[2m[36m(func pid=38780)[0m f1_weighted: 0.07996003327509599
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.0, 0.0, 0.251, 0.0, 0.0, 0.0, 0.171, 0.0, 0.0]
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 7.0479 | Steps: 4 | Val loss: 30.2723 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 10:45:55 (running for 00:28:25.51)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.2225
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.737 |      0.217 |                   71 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 20.463 |      0.225 |                   34 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |        |            |                      |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=39363)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=39363)[0m Configuration completed!
[2m[36m(func pid=39363)[0m New optimizer parameters:
[2m[36m(func pid=39363)[0m SGD (
[2m[36m(func pid=39363)[0m Parameter Group 0
[2m[36m(func pid=39363)[0m     dampening: 0
[2m[36m(func pid=39363)[0m     differentiable: False
[2m[36m(func pid=39363)[0m     foreach: None
[2m[36m(func pid=39363)[0m     lr: 0.001
[2m[36m(func pid=39363)[0m     maximize: False
[2m[36m(func pid=39363)[0m     momentum: 0.99
[2m[36m(func pid=39363)[0m     nesterov: False
[2m[36m(func pid=39363)[0m     weight_decay: 1e-05
[2m[36m(func pid=39363)[0m )
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:46:01 (running for 00:28:30.88)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.2225
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.661 |      0.239 |                   72 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  7.048 |      0.15  |                   35 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  3.118 |      0.042 |                    1 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |        |            |                      |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.15951492537313433
[2m[36m(func pid=31048)[0m top5: 0.5867537313432836
[2m[36m(func pid=31048)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=31048)[0m f1_macro: 0.15031391839643646
[2m[36m(func pid=31048)[0m f1_weighted: 0.1182144709707606
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.259, 0.045, 0.0, 0.125, 0.366, 0.0, 0.441, 0.093, 0.174]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.4305 | Steps: 4 | Val loss: 1.9033 | Batch size: 32 | lr: 0.01 | Duration: 3.10s
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0041 | Steps: 4 | Val loss: 2.3105 | Batch size: 32 | lr: 0.0001 | Duration: 3.08s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0509 | Steps: 4 | Val loss: 2.3299 | Batch size: 32 | lr: 0.001 | Duration: 4.51s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 16.0472 | Steps: 4 | Val loss: 27.8000 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=38780)[0m top1: 0.19682835820895522
[2m[36m(func pid=38780)[0m top5: 0.5839552238805971
[2m[36m(func pid=38780)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=38780)[0m f1_macro: 0.041732576366722715
[2m[36m(func pid=38780)[0m f1_weighted: 0.10146725619969159
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.0, 0.0, 0.35, 0.0, 0.0, 0.0, 0.068, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=21639)[0m top1: 0.3605410447761194
[2m[36m(func pid=21639)[0m top5: 0.8708022388059702
[2m[36m(func pid=21639)[0m f1_micro: 0.3605410447761194
[2m[36m(func pid=21639)[0m f1_macro: 0.24827071677076643
[2m[36m(func pid=21639)[0m f1_weighted: 0.3464333521589557
[2m[36m(func pid=21639)[0m f1_per_class: [0.0, 0.471, 0.361, 0.258, 0.11, 0.188, 0.466, 0.45, 0.095, 0.084]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=39363)[0m top1: 0.11753731343283583
[2m[36m(func pid=39363)[0m top5: 0.49486940298507465
[2m[36m(func pid=39363)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=39363)[0m f1_macro: 0.059834705225876875
[2m[36m(func pid=39363)[0m f1_weighted: 0.08458393395413773
[2m[36m(func pid=39363)[0m f1_per_class: [0.039, 0.0, 0.01, 0.239, 0.0, 0.0, 0.0, 0.293, 0.0, 0.017]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:46:06 (running for 00:28:36.16)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.2225
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.431 |      0.248 |                   73 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 16.047 |      0.22  |                   36 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  3.004 |      0.042 |                    2 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  3.051 |      0.06  |                    1 |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.25652985074626866
[2m[36m(func pid=31048)[0m top5: 0.5564365671641791
[2m[36m(func pid=31048)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=31048)[0m f1_macro: 0.22010991136718885
[2m[36m(func pid=31048)[0m f1_weighted: 0.15872414304320548
[2m[36m(func pid=31048)[0m f1_per_class: [0.169, 0.506, 0.632, 0.0, 0.111, 0.405, 0.0, 0.279, 0.0, 0.1]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.0507 | Steps: 4 | Val loss: 2.3140 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.4711 | Steps: 4 | Val loss: 2.2011 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9064 | Steps: 4 | Val loss: 2.3386 | Batch size: 32 | lr: 0.001 | Duration: 2.72s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 19.5916 | Steps: 4 | Val loss: 16.1995 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=38780)[0m top1: 0.22014925373134328
[2m[36m(func pid=38780)[0m top5: 0.5792910447761194
[2m[36m(func pid=38780)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=38780)[0m f1_macro: 0.04394396256509434
[2m[36m(func pid=38780)[0m f1_weighted: 0.10779737691732921
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.0, 0.0, 0.379, 0.0, 0.0, 0.0, 0.027, 0.0, 0.033]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=21639)[0m top1: 0.24347014925373134
[2m[36m(func pid=21639)[0m top5: 0.8092350746268657
[2m[36m(func pid=21639)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=21639)[0m f1_macro: 0.24802283095389788
[2m[36m(func pid=21639)[0m f1_weighted: 0.24648715592555703
[2m[36m(func pid=21639)[0m f1_per_class: [0.303, 0.259, 0.56, 0.451, 0.074, 0.299, 0.022, 0.399, 0.049, 0.066]
[2m[36m(func pid=21639)[0m 
[2m[36m(func pid=39363)[0m top1: 0.020988805970149255
[2m[36m(func pid=39363)[0m top5: 0.49533582089552236
[2m[36m(func pid=39363)[0m f1_micro: 0.020988805970149255
[2m[36m(func pid=39363)[0m f1_macro: 0.010345586088985942
[2m[36m(func pid=39363)[0m f1_weighted: 0.006030987546065897
[2m[36m(func pid=39363)[0m f1_per_class: [0.077, 0.0, 0.011, 0.016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:46:11 (running for 00:28:41.55)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=14
Bracket: Iter 75.000: 0.2225
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (6 PENDING, 4 RUNNING, 14 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00014 | RUNNING    | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |  1.471 |      0.248 |                   74 |
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 19.592 |      0.103 |                   37 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  3.051 |      0.044 |                    3 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.906 |      0.01  |                    2 |
| train_952df_00018 | PENDING    |                     | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.20382462686567165
[2m[36m(func pid=31048)[0m top5: 0.6180037313432836
[2m[36m(func pid=31048)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=31048)[0m f1_macro: 0.10255413483013617
[2m[36m(func pid=31048)[0m f1_weighted: 0.19242229388803975
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.198, 0.12, 0.0, 0.042, 0.0, 0.518, 0.0, 0.053, 0.095]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9586 | Steps: 4 | Val loss: 2.3202 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=21639)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.6174 | Steps: 4 | Val loss: 2.5036 | Batch size: 32 | lr: 0.01 | Duration: 3.07s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.7572 | Steps: 4 | Val loss: 2.3534 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 8.6477 | Steps: 4 | Val loss: 16.9181 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=38780)[0m top1: 0.22574626865671643
[2m[36m(func pid=38780)[0m top5: 0.5503731343283582
[2m[36m(func pid=38780)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=38780)[0m f1_macro: 0.05039116885993485
[2m[36m(func pid=38780)[0m f1_weighted: 0.1149013273736682
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.005, 0.04, 0.4, 0.0, 0.0, 0.0, 0.033, 0.0, 0.026]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=21639)[0m top1: 0.19542910447761194
[2m[36m(func pid=21639)[0m top5: 0.7719216417910447
[2m[36m(func pid=21639)[0m f1_micro: 0.19542910447761194
[2m[36m(func pid=21639)[0m f1_macro: 0.19093172500110678
[2m[36m(func pid=21639)[0m f1_weighted: 0.18637561311115147
[2m[36m(func pid=21639)[0m f1_per_class: [0.15, 0.037, 0.378, 0.224, 0.088, 0.302, 0.192, 0.262, 0.066, 0.21]
[2m[36m(func pid=39363)[0m top1: 0.020988805970149255
[2m[36m(func pid=39363)[0m top5: 0.3736007462686567
[2m[36m(func pid=39363)[0m f1_micro: 0.020988805970149255
[2m[36m(func pid=39363)[0m f1_macro: 0.008121198317211071
[2m[36m(func pid=39363)[0m f1_weighted: 0.0015048098573692801
[2m[36m(func pid=39363)[0m f1_per_class: [0.07, 0.0, 0.011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=31048)[0m top1: 0.26492537313432835
[2m[36m(func pid=31048)[0m top5: 0.6319962686567164
[2m[36m(func pid=31048)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=31048)[0m f1_macro: 0.1950031893576208
[2m[36m(func pid=31048)[0m f1_weighted: 0.14190852275557927
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.434, 0.393, 0.0, 0.174, 0.322, 0.0, 0.42, 0.0, 0.207]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9027 | Steps: 4 | Val loss: 2.3304 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.8004 | Steps: 4 | Val loss: 2.3725 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 13.1252 | Steps: 4 | Val loss: 24.8909 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=38780)[0m top1: 0.12546641791044777
[2m[36m(func pid=38780)[0m top5: 0.53125
[2m[36m(func pid=38780)[0m f1_micro: 0.12546641791044777
[2m[36m(func pid=38780)[0m f1_macro: 0.0407301475567588
[2m[36m(func pid=38780)[0m f1_weighted: 0.09763344756778514
[2m[36m(func pid=38780)[0m f1_per_class: [0.026, 0.0, 0.021, 0.344, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=39363)[0m top1: 0.0065298507462686565
[2m[36m(func pid=39363)[0m top5: 0.4085820895522388
[2m[36m(func pid=39363)[0m f1_micro: 0.0065298507462686565
[2m[36m(func pid=39363)[0m f1_macro: 0.004441327723235156
[2m[36m(func pid=39363)[0m f1_weighted: 0.000735714833964818
[2m[36m(func pid=39363)[0m f1_per_class: [0.032, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=31048)[0m top1: 0.13899253731343283
[2m[36m(func pid=31048)[0m top5: 0.5405783582089553
[2m[36m(func pid=31048)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=31048)[0m f1_macro: 0.1710729406634896
[2m[36m(func pid=31048)[0m f1_weighted: 0.12945375935851633
[2m[36m(func pid=31048)[0m f1_per_class: [0.127, 0.356, 0.082, 0.0, 0.049, 0.258, 0.0, 0.502, 0.108, 0.23]
== Status ==
Current time: 2024-01-07 10:46:17 (running for 00:28:46.88)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  8.648 |      0.195 |                   38 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.959 |      0.05  |                    4 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.757 |      0.008 |                    3 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 10:46:24 (running for 00:28:54.20)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  8.648 |      0.195 |                   38 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.959 |      0.05  |                    4 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.8   |      0.004 |                    4 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=40628)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=40628)[0m Configuration completed!
[2m[36m(func pid=40628)[0m New optimizer parameters:
[2m[36m(func pid=40628)[0m SGD (
[2m[36m(func pid=40628)[0m Parameter Group 0
[2m[36m(func pid=40628)[0m     dampening: 0
[2m[36m(func pid=40628)[0m     differentiable: False
[2m[36m(func pid=40628)[0m     foreach: None
[2m[36m(func pid=40628)[0m     lr: 0.01
[2m[36m(func pid=40628)[0m     maximize: False
[2m[36m(func pid=40628)[0m     momentum: 0.99
[2m[36m(func pid=40628)[0m     nesterov: False
[2m[36m(func pid=40628)[0m     weight_decay: 1e-05
[2m[36m(func pid=40628)[0m )
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8570 | Steps: 4 | Val loss: 2.3395 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 15.4356 | Steps: 4 | Val loss: 23.5678 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.1129 | Steps: 4 | Val loss: 2.3424 | Batch size: 32 | lr: 0.001 | Duration: 3.19s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0208 | Steps: 4 | Val loss: 2.3749 | Batch size: 32 | lr: 0.01 | Duration: 4.31s
== Status ==
Current time: 2024-01-07 10:46:29 (running for 00:28:59.22)
Memory usage on this node: 25.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 13.125 |      0.171 |                   39 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.903 |      0.041 |                    5 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.8   |      0.004 |                    4 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |        |            |                      |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.024253731343283583
[2m[36m(func pid=38780)[0m top5: 0.5251865671641791
[2m[36m(func pid=38780)[0m f1_micro: 0.024253731343283583
[2m[36m(func pid=38780)[0m f1_macro: 0.015572471895359458
[2m[36m(func pid=38780)[0m f1_weighted: 0.02769368567175716
[2m[36m(func pid=38780)[0m f1_per_class: [0.044, 0.005, 0.014, 0.092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=31048)[0m top1: 0.3474813432835821
[2m[36m(func pid=31048)[0m top5: 0.6576492537313433
[2m[36m(func pid=31048)[0m f1_micro: 0.3474813432835821
[2m[36m(func pid=31048)[0m f1_macro: 0.14394889116598528
[2m[36m(func pid=31048)[0m f1_weighted: 0.2590132136749813
[2m[36m(func pid=31048)[0m f1_per_class: [0.184, 0.459, 0.119, 0.0, 0.0, 0.0, 0.586, 0.0, 0.0, 0.092]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.027985074626865673
[2m[36m(func pid=39363)[0m top5: 0.3969216417910448
[2m[36m(func pid=39363)[0m f1_micro: 0.027985074626865673
[2m[36m(func pid=39363)[0m f1_macro: 0.01558104080388128
[2m[36m(func pid=39363)[0m f1_weighted: 0.0027759899383431297
[2m[36m(func pid=39363)[0m f1_per_class: [0.0, 0.0, 0.033, 0.0, 0.059, 0.0, 0.0, 0.0, 0.065, 0.0]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.006063432835820896
[2m[36m(func pid=40628)[0m top5: 0.5862873134328358
[2m[36m(func pid=40628)[0m f1_micro: 0.006063432835820896
[2m[36m(func pid=40628)[0m f1_macro: 0.001207617278216442
[2m[36m(func pid=40628)[0m f1_weighted: 7.322306257842233e-05
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.0, 0.012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8778 | Steps: 4 | Val loss: 2.3542 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 18.4245 | Steps: 4 | Val loss: 18.2304 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7699 | Steps: 4 | Val loss: 2.2401 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.8785 | Steps: 4 | Val loss: 2.2367 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 10:46:35 (running for 00:29:04.95)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 15.436 |      0.144 |                   40 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.878 |      0.009 |                    7 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  3.113 |      0.016 |                    5 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  3.021 |      0.001 |                    1 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.00792910447761194
[2m[36m(func pid=38780)[0m top5: 0.49580223880597013
[2m[36m(func pid=38780)[0m f1_micro: 0.00792910447761194
[2m[36m(func pid=38780)[0m f1_macro: 0.008727838593903196
[2m[36m(func pid=38780)[0m f1_weighted: 0.0016102500123848927
[2m[36m(func pid=38780)[0m f1_per_class: [0.075, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=31048)[0m top1: 0.11753731343283583
[2m[36m(func pid=31048)[0m top5: 0.6693097014925373
[2m[36m(func pid=31048)[0m f1_micro: 0.11753731343283581
[2m[36m(func pid=31048)[0m f1_macro: 0.16269877769126478
[2m[36m(func pid=31048)[0m f1_weighted: 0.058881889654221885
[2m[36m(func pid=31048)[0m f1_per_class: [0.352, 0.133, 0.556, 0.0, 0.25, 0.181, 0.0, 0.0, 0.048, 0.108]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.0480410447761194
[2m[36m(func pid=39363)[0m top5: 0.5555037313432836
[2m[36m(func pid=39363)[0m f1_micro: 0.0480410447761194
[2m[36m(func pid=39363)[0m f1_macro: 0.06346856311393881
[2m[36m(func pid=39363)[0m f1_weighted: 0.027597482433477403
[2m[36m(func pid=39363)[0m f1_per_class: [0.0, 0.0, 0.4, 0.0, 0.049, 0.0, 0.073, 0.0, 0.087, 0.027]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.060167910447761194
[2m[36m(func pid=40628)[0m top5: 0.632929104477612
[2m[36m(func pid=40628)[0m f1_micro: 0.060167910447761194
[2m[36m(func pid=40628)[0m f1_macro: 0.021303023132852508
[2m[36m(func pid=40628)[0m f1_weighted: 0.008565967453044918
[2m[36m(func pid=40628)[0m f1_per_class: [0.101, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.112, 0.0, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8168 | Steps: 4 | Val loss: 2.3585 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 11.1058 | Steps: 4 | Val loss: 16.1553 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.6750 | Steps: 4 | Val loss: 2.1352 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.1674 | Steps: 4 | Val loss: 2.1366 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
== Status ==
Current time: 2024-01-07 10:46:40 (running for 00:29:10.28)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 18.424 |      0.163 |                   41 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.817 |      0.004 |                    8 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.77  |      0.063 |                    6 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  2.879 |      0.021 |                    2 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.006996268656716418
[2m[36m(func pid=38780)[0m top5: 0.46548507462686567
[2m[36m(func pid=38780)[0m f1_micro: 0.006996268656716418
[2m[36m(func pid=38780)[0m f1_macro: 0.0038675540385359635
[2m[36m(func pid=38780)[0m f1_weighted: 0.00061006382005806
[2m[36m(func pid=38780)[0m f1_per_class: [0.026, 0.0, 0.013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=31048)[0m top1: 0.12313432835820895
[2m[36m(func pid=31048)[0m top5: 0.6716417910447762
[2m[36m(func pid=31048)[0m f1_micro: 0.12313432835820895
[2m[36m(func pid=31048)[0m f1_macro: 0.10375730868790989
[2m[36m(func pid=31048)[0m f1_weighted: 0.1012432771908438
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.0, 0.229, 0.289, 0.028, 0.0, 0.0, 0.269, 0.047, 0.176]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.06623134328358209
[2m[36m(func pid=39363)[0m top5: 0.7430037313432836
[2m[36m(func pid=39363)[0m f1_micro: 0.06623134328358209
[2m[36m(func pid=39363)[0m f1_macro: 0.06838938240139603
[2m[36m(func pid=39363)[0m f1_weighted: 0.06570185132510659
[2m[36m(func pid=39363)[0m f1_per_class: [0.152, 0.258, 0.0, 0.029, 0.093, 0.0, 0.018, 0.0, 0.11, 0.024]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.17210820895522388
[2m[36m(func pid=40628)[0m top5: 0.6469216417910447
[2m[36m(func pid=40628)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=40628)[0m f1_macro: 0.02939068100358423
[2m[36m(func pid=40628)[0m f1_weighted: 0.05058377467501204
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.7920 | Steps: 4 | Val loss: 2.3642 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 21.2003 | Steps: 4 | Val loss: 17.9119 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8195 | Steps: 4 | Val loss: 2.0860 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.6301 | Steps: 4 | Val loss: 2.3674 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 10:46:45 (running for 00:29:15.60)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 11.106 |      0.104 |                   42 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.792 |      0.008 |                    9 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.675 |      0.068 |                    7 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  3.167 |      0.029 |                    3 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.01166044776119403
[2m[36m(func pid=38780)[0m top5: 0.4701492537313433
[2m[36m(func pid=38780)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=38780)[0m f1_macro: 0.007791486341849318
[2m[36m(func pid=38780)[0m f1_weighted: 0.0013928955342823784
[2m[36m(func pid=38780)[0m f1_per_class: [0.064, 0.0, 0.014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=31048)[0m top1: 0.21921641791044777
[2m[36m(func pid=31048)[0m top5: 0.5834888059701493
[2m[36m(func pid=31048)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=31048)[0m f1_macro: 0.17034216153457715
[2m[36m(func pid=31048)[0m f1_weighted: 0.22875540473378642
[2m[36m(func pid=31048)[0m f1_per_class: [0.07, 0.211, 0.096, 0.0, 0.044, 0.0, 0.53, 0.507, 0.0, 0.246]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.18423507462686567
[2m[36m(func pid=39363)[0m top5: 0.6805037313432836
[2m[36m(func pid=39363)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=39363)[0m f1_macro: 0.06846123804433538
[2m[36m(func pid=39363)[0m f1_weighted: 0.0967718226299418
[2m[36m(func pid=39363)[0m f1_per_class: [0.143, 0.354, 0.0, 0.088, 0.0, 0.071, 0.0, 0.0, 0.0, 0.029]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.0228544776119403
[2m[36m(func pid=40628)[0m top5: 0.33348880597014924
[2m[36m(func pid=40628)[0m f1_micro: 0.0228544776119403
[2m[36m(func pid=40628)[0m f1_macro: 0.028381191273988592
[2m[36m(func pid=40628)[0m f1_weighted: 0.013418962742125102
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.018, 0.0, 0.0, 0.182, 0.084, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8936 | Steps: 4 | Val loss: 2.3740 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 7.9895 | Steps: 4 | Val loss: 15.6609 | Batch size: 32 | lr: 0.1 | Duration: 3.16s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6055 | Steps: 4 | Val loss: 2.0797 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.0273 | Steps: 4 | Val loss: 2.4591 | Batch size: 32 | lr: 0.01 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 10:46:51 (running for 00:29:21.05)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 21.2   |      0.17  |                   43 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.894 |      0.007 |                   10 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.819 |      0.068 |                    8 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  2.63  |      0.028 |                    4 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.01632462686567164
[2m[36m(func pid=38780)[0m top5: 0.5317164179104478
[2m[36m(func pid=38780)[0m f1_micro: 0.01632462686567164
[2m[36m(func pid=38780)[0m f1_macro: 0.007161487115767025
[2m[36m(func pid=38780)[0m f1_weighted: 0.0020158508936757016
[2m[36m(func pid=38780)[0m f1_per_class: [0.047, 0.005, 0.019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=31048)[0m top1: 0.13759328358208955
[2m[36m(func pid=31048)[0m top5: 0.742070895522388
[2m[36m(func pid=31048)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=31048)[0m f1_macro: 0.16866837139584534
[2m[36m(func pid=31048)[0m f1_weighted: 0.06394590951208384
[2m[36m(func pid=31048)[0m f1_per_class: [0.38, 0.082, 0.7, 0.013, 0.0, 0.212, 0.0, 0.12, 0.04, 0.14]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.24673507462686567
[2m[36m(func pid=39363)[0m top5: 0.5960820895522388
[2m[36m(func pid=39363)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=39363)[0m f1_macro: 0.11697158546049065
[2m[36m(func pid=39363)[0m f1_weighted: 0.1339609586837815
[2m[36m(func pid=39363)[0m f1_per_class: [0.0, 0.385, 0.0, 0.023, 0.0, 0.307, 0.0, 0.455, 0.0, 0.0]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.03917910447761194
[2m[36m(func pid=40628)[0m top5: 0.4155783582089552
[2m[36m(func pid=40628)[0m f1_micro: 0.03917910447761194
[2m[36m(func pid=40628)[0m f1_macro: 0.04455141622695112
[2m[36m(func pid=40628)[0m f1_weighted: 0.033465864026477624
[2m[36m(func pid=40628)[0m f1_per_class: [0.035, 0.182, 0.211, 0.0, 0.018, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9590 | Steps: 4 | Val loss: 2.3783 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 14.5720 | Steps: 4 | Val loss: 15.3183 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.7031 | Steps: 4 | Val loss: 2.1530 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.2723 | Steps: 4 | Val loss: 2.0925 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=38780)[0m top1: 0.01958955223880597
[2m[36m(func pid=38780)[0m top5: 0.5368470149253731
[2m[36m(func pid=38780)[0m f1_micro: 0.01958955223880597
[2m[36m(func pid=38780)[0m f1_macro: 0.008892034016739
[2m[36m(func pid=38780)[0m f1_weighted: 0.003707121880872658
[2m[36m(func pid=38780)[0m f1_per_class: [0.034, 0.016, 0.038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
== Status ==
Current time: 2024-01-07 10:46:56 (running for 00:29:26.31)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  7.99  |      0.169 |                   44 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.959 |      0.009 |                   11 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.605 |      0.117 |                    9 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  3.027 |      0.045 |                    5 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.11007462686567164
[2m[36m(func pid=31048)[0m top5: 0.7178171641791045
[2m[36m(func pid=31048)[0m f1_micro: 0.11007462686567164
[2m[36m(func pid=31048)[0m f1_macro: 0.16466290499897673
[2m[36m(func pid=31048)[0m f1_weighted: 0.07564676359234435
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.032, 0.632, 0.129, 0.253, 0.0, 0.006, 0.392, 0.078, 0.125]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.1814365671641791
[2m[36m(func pid=39363)[0m top5: 0.5583022388059702
[2m[36m(func pid=39363)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=39363)[0m f1_macro: 0.12254224636300108
[2m[36m(func pid=39363)[0m f1_weighted: 0.10535449036128831
[2m[36m(func pid=39363)[0m f1_per_class: [0.0, 0.269, 0.377, 0.0, 0.0, 0.413, 0.0, 0.166, 0.0, 0.0]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.23507462686567165
[2m[36m(func pid=40628)[0m top5: 0.7136194029850746
[2m[36m(func pid=40628)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=40628)[0m f1_macro: 0.07686461615555315
[2m[36m(func pid=40628)[0m f1_weighted: 0.1500287359173119
[2m[36m(func pid=40628)[0m f1_per_class: [0.137, 0.326, 0.0, 0.0, 0.0, 0.0, 0.306, 0.0, 0.0, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.7698 | Steps: 4 | Val loss: 2.3572 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 10.2376 | Steps: 4 | Val loss: 21.9277 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.7756 | Steps: 4 | Val loss: 2.2698 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.1127 | Steps: 4 | Val loss: 2.3559 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 10:47:01 (running for 00:29:31.55)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 14.572 |      0.165 |                   45 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.77  |      0.029 |                   12 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.703 |      0.123 |                   10 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  3.272 |      0.077 |                    6 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.043843283582089554
[2m[36m(func pid=38780)[0m top5: 0.5354477611940298
[2m[36m(func pid=38780)[0m f1_micro: 0.043843283582089554
[2m[36m(func pid=38780)[0m f1_macro: 0.02923532237865359
[2m[36m(func pid=38780)[0m f1_weighted: 0.04257149828336017
[2m[36m(func pid=38780)[0m f1_per_class: [0.039, 0.091, 0.076, 0.0, 0.0, 0.0, 0.086, 0.0, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=31048)[0m top1: 0.21828358208955223
[2m[36m(func pid=31048)[0m top5: 0.5027985074626866
[2m[36m(func pid=31048)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=31048)[0m f1_macro: 0.14927134318623683
[2m[36m(func pid=31048)[0m f1_weighted: 0.10166450991822142
[2m[36m(func pid=31048)[0m f1_per_class: [0.13, 0.422, 0.485, 0.0, 0.0, 0.0, 0.0, 0.394, 0.0, 0.062]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.12126865671641791
[2m[36m(func pid=39363)[0m top5: 0.6333955223880597
[2m[36m(func pid=39363)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=39363)[0m f1_macro: 0.08189096148900395
[2m[36m(func pid=39363)[0m f1_weighted: 0.06605331233681269
[2m[36m(func pid=39363)[0m f1_per_class: [0.0, 0.052, 0.167, 0.0, 0.048, 0.422, 0.0, 0.131, 0.0, 0.0]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.036380597014925374
[2m[36m(func pid=40628)[0m top5: 0.8255597014925373
[2m[36m(func pid=40628)[0m f1_micro: 0.036380597014925374
[2m[36m(func pid=40628)[0m f1_macro: 0.0688620165553331
[2m[36m(func pid=40628)[0m f1_weighted: 0.03493012165316254
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.011, 0.312, 0.084, 0.203, 0.051, 0.0, 0.0, 0.0, 0.027]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.9678 | Steps: 4 | Val loss: 2.3371 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 14.7959 | Steps: 4 | Val loss: 15.2517 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8201 | Steps: 4 | Val loss: 2.2859 | Batch size: 32 | lr: 0.001 | Duration: 2.77s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 3.9563 | Steps: 4 | Val loss: 3.3009 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 10:47:07 (running for 00:29:36.80)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 10.238 |      0.149 |                   46 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.968 |      0.079 |                   13 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.776 |      0.082 |                   11 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  3.113 |      0.069 |                    7 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.1226679104477612
[2m[36m(func pid=38780)[0m top5: 0.5419776119402985
[2m[36m(func pid=38780)[0m f1_micro: 0.1226679104477612
[2m[36m(func pid=38780)[0m f1_macro: 0.07918170858684392
[2m[36m(func pid=38780)[0m f1_weighted: 0.1371822704734709
[2m[36m(func pid=38780)[0m f1_per_class: [0.045, 0.25, 0.187, 0.0, 0.0, 0.0, 0.309, 0.0, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=31048)[0m top1: 0.28638059701492535
[2m[36m(func pid=31048)[0m top5: 0.679570895522388
[2m[36m(func pid=31048)[0m f1_micro: 0.28638059701492535
[2m[36m(func pid=31048)[0m f1_macro: 0.20417768376107945
[2m[36m(func pid=31048)[0m f1_weighted: 0.21102398338542339
[2m[36m(func pid=31048)[0m f1_per_class: [0.344, 0.313, 0.571, 0.494, 0.047, 0.0, 0.003, 0.092, 0.0, 0.176]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.1044776119402985
[2m[36m(func pid=39363)[0m top5: 0.5573694029850746
[2m[36m(func pid=39363)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=39363)[0m f1_macro: 0.07984408058719532
[2m[36m(func pid=39363)[0m f1_weighted: 0.06411029611724496
[2m[36m(func pid=39363)[0m f1_per_class: [0.0, 0.0, 0.172, 0.0, 0.103, 0.338, 0.054, 0.131, 0.0, 0.0]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.0625
[2m[36m(func pid=40628)[0m top5: 0.5694962686567164
[2m[36m(func pid=40628)[0m f1_micro: 0.0625
[2m[36m(func pid=40628)[0m f1_macro: 0.09782407020633437
[2m[36m(func pid=40628)[0m f1_weighted: 0.04898244820763697
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.046, 0.368, 0.007, 0.026, 0.155, 0.0, 0.269, 0.108, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.7944 | Steps: 4 | Val loss: 2.3126 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 6.1193 | Steps: 4 | Val loss: 19.1655 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.6553 | Steps: 4 | Val loss: 2.2132 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.3289 | Steps: 4 | Val loss: 2.8201 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 10:47:12 (running for 00:29:42.14)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 14.796 |      0.204 |                   47 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.794 |      0.11  |                   14 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.82  |      0.08  |                   12 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  3.956 |      0.098 |                    8 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.19776119402985073
[2m[36m(func pid=38780)[0m top5: 0.585820895522388
[2m[36m(func pid=38780)[0m f1_micro: 0.19776119402985073
[2m[36m(func pid=38780)[0m f1_macro: 0.10954319597010956
[2m[36m(func pid=38780)[0m f1_weighted: 0.19279930461497935
[2m[36m(func pid=38780)[0m f1_per_class: [0.055, 0.34, 0.259, 0.0, 0.0, 0.0, 0.442, 0.0, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=31048)[0m top1: 0.1646455223880597
[2m[36m(func pid=31048)[0m top5: 0.683768656716418
[2m[36m(func pid=31048)[0m f1_micro: 0.1646455223880597
[2m[36m(func pid=31048)[0m f1_macro: 0.12177701789642217
[2m[36m(func pid=31048)[0m f1_weighted: 0.10579977855493841
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.277, 0.279, 0.0, 0.206, 0.277, 0.066, 0.0, 0.114, 0.0]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.09841417910447761
[2m[36m(func pid=39363)[0m top5: 0.6333955223880597
[2m[36m(func pid=39363)[0m f1_micro: 0.0984141791044776
[2m[36m(func pid=39363)[0m f1_macro: 0.1296534952808873
[2m[36m(func pid=39363)[0m f1_weighted: 0.06469956640312066
[2m[36m(func pid=39363)[0m f1_per_class: [0.226, 0.0, 0.5, 0.01, 0.044, 0.125, 0.075, 0.293, 0.0, 0.024]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.25326492537313433
[2m[36m(func pid=40628)[0m top5: 0.6441231343283582
[2m[36m(func pid=40628)[0m f1_micro: 0.25326492537313433
[2m[36m(func pid=40628)[0m f1_macro: 0.17121930666827537
[2m[36m(func pid=40628)[0m f1_weighted: 0.23520297890519348
[2m[36m(func pid=40628)[0m f1_per_class: [0.29, 0.373, 0.0, 0.518, 0.117, 0.0, 0.0, 0.286, 0.082, 0.046]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.6876 | Steps: 4 | Val loss: 2.2605 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 16.1941 | Steps: 4 | Val loss: 18.2536 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.5372 | Steps: 4 | Val loss: 2.2088 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8782 | Steps: 4 | Val loss: 2.5416 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 10:47:17 (running for 00:29:47.40)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  6.119 |      0.122 |                   48 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.688 |      0.129 |                   15 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.655 |      0.13  |                   13 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  3.329 |      0.171 |                    9 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.271455223880597
[2m[36m(func pid=38780)[0m top5: 0.6142723880597015
[2m[36m(func pid=38780)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=38780)[0m f1_macro: 0.12922830023449436
[2m[36m(func pid=38780)[0m f1_weighted: 0.22906663140185177
[2m[36m(func pid=38780)[0m f1_per_class: [0.086, 0.427, 0.25, 0.0, 0.0, 0.0, 0.509, 0.0, 0.021, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=31048)[0m top1: 0.23600746268656717
[2m[36m(func pid=31048)[0m top5: 0.48927238805970147
[2m[36m(func pid=31048)[0m f1_micro: 0.23600746268656717
[2m[36m(func pid=31048)[0m f1_macro: 0.2113365204317344
[2m[36m(func pid=31048)[0m f1_weighted: 0.13392111261914993
[2m[36m(func pid=31048)[0m f1_per_class: [0.212, 0.482, 0.696, 0.0, 0.154, 0.199, 0.0, 0.307, 0.0, 0.064]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.06529850746268656
[2m[36m(func pid=39363)[0m top5: 0.7276119402985075
[2m[36m(func pid=39363)[0m f1_micro: 0.06529850746268656
[2m[36m(func pid=39363)[0m f1_macro: 0.08100509544048264
[2m[36m(func pid=39363)[0m f1_weighted: 0.07186549686302472
[2m[36m(func pid=39363)[0m f1_per_class: [0.218, 0.0, 0.0, 0.163, 0.037, 0.0, 0.0, 0.367, 0.0, 0.024]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.22761194029850745
[2m[36m(func pid=40628)[0m top5: 0.8871268656716418
[2m[36m(func pid=40628)[0m f1_micro: 0.22761194029850745
[2m[36m(func pid=40628)[0m f1_macro: 0.08680559909821413
[2m[36m(func pid=40628)[0m f1_weighted: 0.19226858237675185
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.454, 0.0, 0.0, 0.0, 0.0, 0.382, 0.0, 0.0, 0.032]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.6665 | Steps: 4 | Val loss: 2.2314 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 8.4623 | Steps: 4 | Val loss: 22.7016 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7171 | Steps: 4 | Val loss: 2.1678 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.8340 | Steps: 4 | Val loss: 2.9069 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 10:47:23 (running for 00:29:52.82)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 16.194 |      0.211 |                   49 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.667 |      0.133 |                   16 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.537 |      0.081 |                   14 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  2.878 |      0.087 |                   10 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.29990671641791045
[2m[36m(func pid=38780)[0m top5: 0.6413246268656716
[2m[36m(func pid=38780)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=38780)[0m f1_macro: 0.13291078892160635
[2m[36m(func pid=38780)[0m f1_weighted: 0.2364270928204578
[2m[36m(func pid=38780)[0m f1_per_class: [0.094, 0.442, 0.267, 0.0, 0.0, 0.0, 0.526, 0.0, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=31048)[0m top1: 0.3283582089552239
[2m[36m(func pid=31048)[0m top5: 0.6236007462686567
[2m[36m(func pid=31048)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=31048)[0m f1_macro: 0.2754308790615352
[2m[36m(func pid=31048)[0m f1_weighted: 0.24209521161443628
[2m[36m(func pid=31048)[0m f1_per_class: [0.389, 0.037, 0.632, 0.565, 0.111, 0.265, 0.0, 0.528, 0.102, 0.127]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.17583955223880596
[2m[36m(func pid=39363)[0m top5: 0.7555970149253731
[2m[36m(func pid=39363)[0m f1_micro: 0.17583955223880596
[2m[36m(func pid=39363)[0m f1_macro: 0.10536413939832312
[2m[36m(func pid=39363)[0m f1_weighted: 0.15131503485790831
[2m[36m(func pid=39363)[0m f1_per_class: [0.296, 0.0, 0.143, 0.509, 0.041, 0.0, 0.0, 0.032, 0.0, 0.033]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.2630597014925373
[2m[36m(func pid=40628)[0m top5: 0.6422574626865671
[2m[36m(func pid=40628)[0m f1_micro: 0.2630597014925373
[2m[36m(func pid=40628)[0m f1_macro: 0.2194834088560879
[2m[36m(func pid=40628)[0m f1_weighted: 0.15250377446613664
[2m[36m(func pid=40628)[0m f1_per_class: [0.201, 0.454, 0.439, 0.0, 0.214, 0.347, 0.0, 0.436, 0.0, 0.105]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6495 | Steps: 4 | Val loss: 2.2010 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 10.3977 | Steps: 4 | Val loss: 29.7687 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.3517 | Steps: 4 | Val loss: 2.1512 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.7183 | Steps: 4 | Val loss: 4.0673 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 10:47:28 (running for 00:29:57.96)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  8.462 |      0.275 |                   50 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.649 |      0.126 |                   17 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.717 |      0.105 |                   15 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  2.834 |      0.219 |                   11 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.30177238805970147
[2m[36m(func pid=38780)[0m top5: 0.6791044776119403
[2m[36m(func pid=38780)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=38780)[0m f1_macro: 0.12631617147930055
[2m[36m(func pid=38780)[0m f1_weighted: 0.22764464472643675
[2m[36m(func pid=38780)[0m f1_per_class: [0.118, 0.409, 0.207, 0.0, 0.0, 0.0, 0.513, 0.0, 0.016, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=31048)[0m top1: 0.10680970149253731
[2m[36m(func pid=31048)[0m top5: 0.590018656716418
[2m[36m(func pid=31048)[0m f1_micro: 0.10680970149253732
[2m[36m(func pid=31048)[0m f1_macro: 0.16037966752593238
[2m[36m(func pid=31048)[0m f1_weighted: 0.09367175819599535
[2m[36m(func pid=31048)[0m f1_per_class: [0.263, 0.178, 0.328, 0.02, 0.026, 0.181, 0.0, 0.483, 0.0, 0.126]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.19869402985074627
[2m[36m(func pid=39363)[0m top5: 0.7280783582089553
[2m[36m(func pid=39363)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=39363)[0m f1_macro: 0.1369312276689489
[2m[36m(func pid=39363)[0m f1_weighted: 0.1589570311485144
[2m[36m(func pid=39363)[0m f1_per_class: [0.14, 0.0, 0.486, 0.532, 0.053, 0.0, 0.0, 0.0, 0.117, 0.04]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.23414179104477612
[2m[36m(func pid=40628)[0m top5: 0.6310634328358209
[2m[36m(func pid=40628)[0m f1_micro: 0.23414179104477612
[2m[36m(func pid=40628)[0m f1_macro: 0.1652247454056042
[2m[36m(func pid=40628)[0m f1_weighted: 0.19242181654138849
[2m[36m(func pid=40628)[0m f1_per_class: [0.171, 0.0, 0.188, 0.467, 0.076, 0.287, 0.0, 0.354, 0.109, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.6320 | Steps: 4 | Val loss: 2.1799 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 10.8045 | Steps: 4 | Val loss: 18.6304 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.3939 | Steps: 4 | Val loss: 2.1990 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 4.5042 | Steps: 4 | Val loss: 4.4458 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 10:47:33 (running for 00:30:03.20)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 10.398 |      0.16  |                   51 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.632 |      0.122 |                   18 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.352 |      0.137 |                   16 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  2.718 |      0.165 |                   12 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.2989738805970149
[2m[36m(func pid=38780)[0m top5: 0.7192164179104478
[2m[36m(func pid=38780)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=38780)[0m f1_macro: 0.12165210531168011
[2m[36m(func pid=38780)[0m f1_weighted: 0.22224743100452976
[2m[36m(func pid=38780)[0m f1_per_class: [0.124, 0.393, 0.154, 0.0, 0.0, 0.0, 0.502, 0.0, 0.043, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=31048)[0m top1: 0.2868470149253731
[2m[36m(func pid=31048)[0m top5: 0.6403917910447762
[2m[36m(func pid=31048)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=31048)[0m f1_macro: 0.2677457498586075
[2m[36m(func pid=31048)[0m f1_weighted: 0.2674637945135991
[2m[36m(func pid=31048)[0m f1_per_class: [0.111, 0.453, 0.351, 0.377, 0.264, 0.398, 0.0, 0.525, 0.0, 0.2]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.09701492537313433
[2m[36m(func pid=39363)[0m top5: 0.7243470149253731
[2m[36m(func pid=39363)[0m f1_micro: 0.09701492537313433
[2m[36m(func pid=39363)[0m f1_macro: 0.0855452075260337
[2m[36m(func pid=39363)[0m f1_weighted: 0.09988456131801836
[2m[36m(func pid=39363)[0m f1_per_class: [0.0, 0.062, 0.289, 0.297, 0.058, 0.0, 0.0, 0.0, 0.114, 0.035]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.17723880597014927
[2m[36m(func pid=40628)[0m top5: 0.6194029850746269
[2m[36m(func pid=40628)[0m f1_micro: 0.17723880597014927
[2m[36m(func pid=40628)[0m f1_macro: 0.20614157825999962
[2m[36m(func pid=40628)[0m f1_weighted: 0.170024517491986
[2m[36m(func pid=40628)[0m f1_per_class: [0.343, 0.0, 0.514, 0.447, 0.025, 0.0, 0.0, 0.498, 0.156, 0.077]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.5896 | Steps: 4 | Val loss: 2.1646 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 16.0596 | Steps: 4 | Val loss: 10.2713 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.5907 | Steps: 4 | Val loss: 2.2775 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 10:47:38 (running for 00:30:08.49)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 10.804 |      0.268 |                   52 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.59  |      0.111 |                   19 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.394 |      0.086 |                   17 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  4.504 |      0.206 |                   13 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.279384328358209
[2m[36m(func pid=38780)[0m top5: 0.7779850746268657
[2m[36m(func pid=38780)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=38780)[0m f1_macro: 0.11074529906453745
[2m[36m(func pid=38780)[0m f1_weighted: 0.2068727929290757
[2m[36m(func pid=38780)[0m f1_per_class: [0.067, 0.375, 0.125, 0.0, 0.0, 0.0, 0.461, 0.0, 0.079, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.6038 | Steps: 4 | Val loss: 2.9764 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=31048)[0m top1: 0.41651119402985076
[2m[36m(func pid=31048)[0m top5: 0.8889925373134329
[2m[36m(func pid=31048)[0m f1_micro: 0.41651119402985076
[2m[36m(func pid=31048)[0m f1_macro: 0.231976565171441
[2m[36m(func pid=31048)[0m f1_weighted: 0.3493699749292613
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.149, 0.431, 0.545, 0.0, 0.161, 0.403, 0.505, 0.0, 0.126]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.09794776119402986
[2m[36m(func pid=39363)[0m top5: 0.7374067164179104
[2m[36m(func pid=39363)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=39363)[0m f1_macro: 0.06755595438708079
[2m[36m(func pid=39363)[0m f1_weighted: 0.05748170051365043
[2m[36m(func pid=39363)[0m f1_per_class: [0.0, 0.286, 0.161, 0.0, 0.059, 0.0, 0.0, 0.078, 0.06, 0.032]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.24766791044776118
[2m[36m(func pid=40628)[0m top5: 0.7966417910447762
[2m[36m(func pid=40628)[0m f1_micro: 0.24766791044776118
[2m[36m(func pid=40628)[0m f1_macro: 0.2204589911750639
[2m[36m(func pid=40628)[0m f1_weighted: 0.2074918686525955
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.374, 0.526, 0.127, 0.138, 0.331, 0.122, 0.488, 0.0, 0.099]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7231 | Steps: 4 | Val loss: 2.1659 | Batch size: 32 | lr: 0.0001 | Duration: 3.09s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 13.1653 | Steps: 4 | Val loss: 15.2918 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.2879 | Steps: 4 | Val loss: 2.3976 | Batch size: 32 | lr: 0.001 | Duration: 3.21s
== Status ==
Current time: 2024-01-07 10:47:44 (running for 00:30:14.02)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 16.06  |      0.232 |                   53 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.723 |      0.108 |                   20 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.591 |      0.068 |                   18 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  2.604 |      0.22  |                   14 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.24347014925373134
[2m[36m(func pid=38780)[0m top5: 0.8022388059701493
[2m[36m(func pid=38780)[0m f1_micro: 0.24347014925373134
[2m[36m(func pid=38780)[0m f1_macro: 0.10815184070716732
[2m[36m(func pid=38780)[0m f1_weighted: 0.19076340025090577
[2m[36m(func pid=38780)[0m f1_per_class: [0.077, 0.36, 0.106, 0.007, 0.0, 0.0, 0.407, 0.0, 0.093, 0.032]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.3093 | Steps: 4 | Val loss: 2.1260 | Batch size: 32 | lr: 0.01 | Duration: 3.12s
[2m[36m(func pid=31048)[0m top1: 0.2775186567164179
[2m[36m(func pid=31048)[0m top5: 0.683768656716418
[2m[36m(func pid=31048)[0m f1_micro: 0.2775186567164179
[2m[36m(func pid=31048)[0m f1_macro: 0.24111658007999198
[2m[36m(func pid=31048)[0m f1_weighted: 0.27208308740923404
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.482, 0.55, 0.0, 0.222, 0.0, 0.53, 0.36, 0.104, 0.162]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.15065298507462688
[2m[36m(func pid=39363)[0m top5: 0.7714552238805971
[2m[36m(func pid=39363)[0m f1_micro: 0.15065298507462688
[2m[36m(func pid=39363)[0m f1_macro: 0.10471225408709868
[2m[36m(func pid=39363)[0m f1_weighted: 0.08775382604594457
[2m[36m(func pid=39363)[0m f1_per_class: [0.0, 0.306, 0.133, 0.0, 0.057, 0.0, 0.019, 0.477, 0.0, 0.054]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.4388992537313433
[2m[36m(func pid=40628)[0m top5: 0.9207089552238806
[2m[36m(func pid=40628)[0m f1_micro: 0.4388992537313433
[2m[36m(func pid=40628)[0m f1_macro: 0.24834290200628284
[2m[36m(func pid=40628)[0m f1_weighted: 0.3907558909978534
[2m[36m(func pid=40628)[0m f1_per_class: [0.208, 0.112, 0.514, 0.529, 0.0, 0.36, 0.582, 0.0, 0.0, 0.179]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.5539 | Steps: 4 | Val loss: 2.1560 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 21.6556 | Steps: 4 | Val loss: 15.0171 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 10:47:49 (running for 00:30:19.38)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 13.165 |      0.241 |                   54 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.554 |      0.108 |                   21 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.288 |      0.105 |                   19 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  2.309 |      0.248 |                   15 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.21128731343283583
[2m[36m(func pid=38780)[0m top5: 0.8031716417910447
[2m[36m(func pid=38780)[0m f1_micro: 0.21128731343283583
[2m[36m(func pid=38780)[0m f1_macro: 0.1083573005321471
[2m[36m(func pid=38780)[0m f1_weighted: 0.1926862564599672
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.375, 0.137, 0.066, 0.01, 0.015, 0.346, 0.0, 0.113, 0.022]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.3990 | Steps: 4 | Val loss: 2.4330 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.9469 | Steps: 4 | Val loss: 2.7220 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=31048)[0m top1: 0.14832089552238806
[2m[36m(func pid=31048)[0m top5: 0.7299440298507462
[2m[36m(func pid=31048)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=31048)[0m f1_macro: 0.1478954264427032
[2m[36m(func pid=31048)[0m f1_weighted: 0.11197389041964521
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.218, 0.373, 0.0, 0.037, 0.314, 0.031, 0.455, 0.0, 0.051]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.18796641791044777
[2m[36m(func pid=39363)[0m top5: 0.6949626865671642
[2m[36m(func pid=39363)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=39363)[0m f1_macro: 0.12601951791696975
[2m[36m(func pid=39363)[0m f1_weighted: 0.10278108684126022
[2m[36m(func pid=39363)[0m f1_per_class: [0.044, 0.332, 0.148, 0.0, 0.086, 0.161, 0.0, 0.414, 0.0, 0.075]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.34794776119402987
[2m[36m(func pid=40628)[0m top5: 0.8166977611940298
[2m[36m(func pid=40628)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=40628)[0m f1_macro: 0.1963410164201329
[2m[36m(func pid=40628)[0m f1_weighted: 0.3324728367298457
[2m[36m(func pid=40628)[0m f1_per_class: [0.152, 0.005, 0.154, 0.399, 0.0, 0.358, 0.543, 0.181, 0.054, 0.118]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5595 | Steps: 4 | Val loss: 2.1540 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 15.0950 | Steps: 4 | Val loss: 11.9755 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 10:47:55 (running for 00:30:24.80)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 21.656 |      0.148 |                   55 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.56  |      0.117 |                   22 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.399 |      0.126 |                   20 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  2.947 |      0.196 |                   16 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.16557835820895522
[2m[36m(func pid=38780)[0m top5: 0.7845149253731343
[2m[36m(func pid=38780)[0m f1_micro: 0.16557835820895522
[2m[36m(func pid=38780)[0m f1_macro: 0.11736515852705824
[2m[36m(func pid=38780)[0m f1_weighted: 0.19244579051520502
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.314, 0.17, 0.18, 0.031, 0.099, 0.24, 0.0, 0.106, 0.033]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.4272 | Steps: 4 | Val loss: 2.3639 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 5.8113 | Steps: 4 | Val loss: 5.0955 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=31048)[0m top1: 0.3763992537313433
[2m[36m(func pid=31048)[0m top5: 0.8246268656716418
[2m[36m(func pid=31048)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=31048)[0m f1_macro: 0.25574343278774997
[2m[36m(func pid=31048)[0m f1_weighted: 0.3539505199877269
[2m[36m(func pid=31048)[0m f1_per_class: [0.255, 0.0, 0.267, 0.523, 0.084, 0.436, 0.431, 0.34, 0.0, 0.222]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.21735074626865672
[2m[36m(func pid=39363)[0m top5: 0.6665111940298507
[2m[36m(func pid=39363)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=39363)[0m f1_macro: 0.19312865143023702
[2m[36m(func pid=39363)[0m f1_weighted: 0.13725087804966588
[2m[36m(func pid=39363)[0m f1_per_class: [0.219, 0.373, 0.407, 0.0, 0.117, 0.221, 0.067, 0.297, 0.0, 0.229]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.15485074626865672
[2m[36m(func pid=40628)[0m top5: 0.6399253731343284
[2m[36m(func pid=40628)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=40628)[0m f1_macro: 0.16918130132273654
[2m[36m(func pid=40628)[0m f1_weighted: 0.08511019227796886
[2m[36m(func pid=40628)[0m f1_per_class: [0.279, 0.027, 0.392, 0.0, 0.0, 0.44, 0.0, 0.301, 0.083, 0.169]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.5334 | Steps: 4 | Val loss: 2.1616 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 17.1126 | Steps: 4 | Val loss: 12.4090 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.6843 | Steps: 4 | Val loss: 5.3687 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 10:48:00 (running for 00:30:30.45)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 15.095 |      0.256 |                   56 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.533 |      0.095 |                   23 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.427 |      0.193 |                   21 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  5.811 |      0.169 |                   17 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.12639925373134328
[2m[36m(func pid=38780)[0m top5: 0.7453358208955224
[2m[36m(func pid=38780)[0m f1_micro: 0.12639925373134328
[2m[36m(func pid=38780)[0m f1_macro: 0.09495111284925363
[2m[36m(func pid=38780)[0m f1_weighted: 0.14456276638885102
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.152, 0.163, 0.282, 0.041, 0.071, 0.085, 0.016, 0.107, 0.031]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.4359 | Steps: 4 | Val loss: 2.3760 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=31048)[0m top1: 0.18936567164179105
[2m[36m(func pid=31048)[0m top5: 0.6618470149253731
[2m[36m(func pid=31048)[0m f1_micro: 0.18936567164179105
[2m[36m(func pid=31048)[0m f1_macro: 0.1613999129991645
[2m[36m(func pid=31048)[0m f1_weighted: 0.1596210026342337
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.505, 0.062, 0.0, 0.256, 0.229, 0.086, 0.234, 0.101, 0.141]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.1865671641791045
[2m[36m(func pid=39363)[0m top5: 0.6725746268656716
[2m[36m(func pid=39363)[0m f1_micro: 0.1865671641791045
[2m[36m(func pid=39363)[0m f1_macro: 0.2133904818356458
[2m[36m(func pid=39363)[0m f1_weighted: 0.1680696969235092
[2m[36m(func pid=39363)[0m f1_per_class: [0.176, 0.325, 0.5, 0.0, 0.12, 0.258, 0.191, 0.221, 0.068, 0.276]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.1767723880597015
[2m[36m(func pid=40628)[0m top5: 0.5970149253731343
[2m[36m(func pid=40628)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=40628)[0m f1_macro: 0.11105234960957608
[2m[36m(func pid=40628)[0m f1_weighted: 0.08978760022925693
[2m[36m(func pid=40628)[0m f1_per_class: [0.08, 0.328, 0.0, 0.0, 0.052, 0.032, 0.0, 0.417, 0.054, 0.148]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.5972 | Steps: 4 | Val loss: 2.1538 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 15.6065 | Steps: 4 | Val loss: 10.5129 | Batch size: 32 | lr: 0.1 | Duration: 2.72s
== Status ==
Current time: 2024-01-07 10:48:06 (running for 00:30:35.96)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 17.113 |      0.161 |                   57 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.597 |      0.111 |                   24 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.436 |      0.213 |                   22 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  2.684 |      0.111 |                   18 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.15578358208955223
[2m[36m(func pid=38780)[0m top5: 0.769589552238806
[2m[36m(func pid=38780)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=38780)[0m f1_macro: 0.11096001608192878
[2m[36m(func pid=38780)[0m f1_weighted: 0.16514885494906373
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.056, 0.153, 0.376, 0.051, 0.128, 0.079, 0.121, 0.111, 0.035]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.2943 | Steps: 4 | Val loss: 2.3651 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 3.0570 | Steps: 4 | Val loss: 5.6101 | Batch size: 32 | lr: 0.01 | Duration: 3.15s
[2m[36m(func pid=31048)[0m top1: 0.3628731343283582
[2m[36m(func pid=31048)[0m top5: 0.808768656716418
[2m[36m(func pid=31048)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=31048)[0m f1_macro: 0.19769986677634868
[2m[36m(func pid=31048)[0m f1_weighted: 0.2622594228752674
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.511, 0.609, 0.003, 0.084, 0.024, 0.549, 0.0, 0.026, 0.172]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.11847014925373134
[2m[36m(func pid=39363)[0m top5: 0.6627798507462687
[2m[36m(func pid=39363)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=39363)[0m f1_macro: 0.15931564668019954
[2m[36m(func pid=39363)[0m f1_weighted: 0.07915808929942234
[2m[36m(func pid=39363)[0m f1_per_class: [0.274, 0.037, 0.5, 0.0, 0.119, 0.248, 0.068, 0.189, 0.087, 0.071]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.20708955223880596
[2m[36m(func pid=40628)[0m top5: 0.6590485074626866
[2m[36m(func pid=40628)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=40628)[0m f1_macro: 0.16823811188814236
[2m[36m(func pid=40628)[0m f1_weighted: 0.17693405426413
[2m[36m(func pid=40628)[0m f1_per_class: [0.125, 0.037, 0.571, 0.521, 0.028, 0.0, 0.0, 0.309, 0.0, 0.091]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.6728 | Steps: 4 | Val loss: 2.1642 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 10.7716 | Steps: 4 | Val loss: 20.5983 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 10:48:11 (running for 00:30:41.56)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 15.607 |      0.198 |                   58 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.673 |      0.126 |                   25 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.294 |      0.159 |                   23 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  3.057 |      0.168 |                   19 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.20009328358208955
[2m[36m(func pid=38780)[0m top5: 0.7173507462686567
[2m[36m(func pid=38780)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=38780)[0m f1_macro: 0.12623383128404558
[2m[36m(func pid=38780)[0m f1_weighted: 0.17180882555423502
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.005, 0.111, 0.44, 0.052, 0.186, 0.012, 0.323, 0.087, 0.045]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.4423 | Steps: 4 | Val loss: 2.1906 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 4.3612 | Steps: 4 | Val loss: 4.8977 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=31048)[0m top1: 0.2751865671641791
[2m[36m(func pid=31048)[0m top5: 0.5452425373134329
[2m[36m(func pid=31048)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=31048)[0m f1_macro: 0.24263992577184212
[2m[36m(func pid=31048)[0m f1_weighted: 0.2174318800838252
[2m[36m(func pid=31048)[0m f1_per_class: [0.274, 0.0, 0.556, 0.509, 0.074, 0.308, 0.0, 0.488, 0.0, 0.219]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.14365671641791045
[2m[36m(func pid=39363)[0m top5: 0.7378731343283582
[2m[36m(func pid=39363)[0m f1_micro: 0.14365671641791045
[2m[36m(func pid=39363)[0m f1_macro: 0.1864594225628842
[2m[36m(func pid=39363)[0m f1_weighted: 0.11629288667614554
[2m[36m(func pid=39363)[0m f1_per_class: [0.333, 0.005, 0.545, 0.0, 0.127, 0.306, 0.181, 0.198, 0.098, 0.071]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.6069 | Steps: 4 | Val loss: 2.1532 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=40628)[0m top1: 0.13759328358208955
[2m[36m(func pid=40628)[0m top5: 0.6590485074626866
[2m[36m(func pid=40628)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=40628)[0m f1_macro: 0.11354211767636557
[2m[36m(func pid=40628)[0m f1_weighted: 0.1499051604463616
[2m[36m(func pid=40628)[0m f1_per_class: [0.131, 0.0, 0.137, 0.384, 0.044, 0.061, 0.056, 0.248, 0.0, 0.074]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 12.4925 | Steps: 4 | Val loss: 22.8142 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:48:17 (running for 00:30:46.78)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 10.772 |      0.243 |                   59 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.607 |      0.151 |                   26 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.442 |      0.186 |                   24 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  4.361 |      0.114 |                   20 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.25652985074626866
[2m[36m(func pid=38780)[0m top5: 0.6609141791044776
[2m[36m(func pid=38780)[0m f1_micro: 0.25652985074626866
[2m[36m(func pid=38780)[0m f1_macro: 0.15050154315711717
[2m[36m(func pid=38780)[0m f1_weighted: 0.19610643673941233
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.0, 0.111, 0.492, 0.046, 0.252, 0.0, 0.417, 0.126, 0.061]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.9591 | Steps: 4 | Val loss: 5.1816 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.0587 | Steps: 4 | Val loss: 1.9916 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=31048)[0m top1: 0.15951492537313433
[2m[36m(func pid=31048)[0m top5: 0.33955223880597013
[2m[36m(func pid=31048)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=31048)[0m f1_macro: 0.16138533016159073
[2m[36m(func pid=31048)[0m f1_weighted: 0.11654004019841938
[2m[36m(func pid=31048)[0m f1_per_class: [0.106, 0.233, 0.14, 0.0, 0.079, 0.406, 0.0, 0.355, 0.115, 0.181]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=40628)[0m top1: 0.19263059701492538
[2m[36m(func pid=40628)[0m top5: 0.7434701492537313
[2m[36m(func pid=40628)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=40628)[0m f1_macro: 0.15141681253016606
[2m[36m(func pid=40628)[0m f1_weighted: 0.17212004684659685
[2m[36m(func pid=40628)[0m f1_per_class: [0.05, 0.385, 0.049, 0.0, 0.2, 0.372, 0.183, 0.0, 0.121, 0.154]
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.7132 | Steps: 4 | Val loss: 2.1535 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=39363)[0m top1: 0.24067164179104478
[2m[36m(func pid=39363)[0m top5: 0.8190298507462687
[2m[36m(func pid=39363)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=39363)[0m f1_macro: 0.2028642734300595
[2m[36m(func pid=39363)[0m f1_weighted: 0.22864663093178225
[2m[36m(func pid=39363)[0m f1_per_class: [0.303, 0.037, 0.282, 0.25, 0.109, 0.424, 0.257, 0.261, 0.104, 0.0]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 12.9684 | Steps: 4 | Val loss: 18.1073 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
== Status ==
Current time: 2024-01-07 10:48:22 (running for 00:30:52.27)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 12.493 |      0.161 |                   60 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.713 |      0.14  |                   27 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  3.059 |      0.203 |                   25 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  2.959 |      0.151 |                   21 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.27425373134328357
[2m[36m(func pid=38780)[0m top5: 0.617070895522388
[2m[36m(func pid=38780)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=38780)[0m f1_macro: 0.14024202279404285
[2m[36m(func pid=38780)[0m f1_weighted: 0.1955988391723523
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.0, 0.103, 0.521, 0.086, 0.22, 0.0, 0.384, 0.035, 0.054]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 4.5390 | Steps: 4 | Val loss: 4.2783 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=31048)[0m top1: 0.25
[2m[36m(func pid=31048)[0m top5: 0.5895522388059702
[2m[36m(func pid=31048)[0m f1_micro: 0.25
[2m[36m(func pid=31048)[0m f1_macro: 0.1979846515950892
[2m[36m(func pid=31048)[0m f1_weighted: 0.16497319364514176
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.509, 0.458, 0.0, 0.0, 0.401, 0.0, 0.411, 0.124, 0.077]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.2572 | Steps: 4 | Val loss: 1.9322 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.5253 | Steps: 4 | Val loss: 2.1527 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=40628)[0m top1: 0.3512126865671642
[2m[36m(func pid=40628)[0m top5: 0.7901119402985075
[2m[36m(func pid=40628)[0m f1_micro: 0.3512126865671642
[2m[36m(func pid=40628)[0m f1_macro: 0.22133983045828504
[2m[36m(func pid=40628)[0m f1_weighted: 0.3056894407137229
[2m[36m(func pid=40628)[0m f1_per_class: [0.179, 0.531, 0.5, 0.0, 0.0, 0.313, 0.562, 0.0, 0.129, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=39363)[0m top1: 0.33115671641791045
[2m[36m(func pid=39363)[0m top5: 0.8362873134328358
[2m[36m(func pid=39363)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=39363)[0m f1_macro: 0.19365345132994286
[2m[36m(func pid=39363)[0m f1_weighted: 0.2786532954617111
[2m[36m(func pid=39363)[0m f1_per_class: [0.038, 0.005, 0.153, 0.522, 0.123, 0.35, 0.207, 0.467, 0.0, 0.071]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 16.8295 | Steps: 4 | Val loss: 8.8350 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 10:48:27 (running for 00:30:57.54)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 12.968 |      0.198 |                   61 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.525 |      0.124 |                   28 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.257 |      0.194 |                   26 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  4.539 |      0.221 |                   22 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.26026119402985076
[2m[36m(func pid=38780)[0m top5: 0.6077425373134329
[2m[36m(func pid=38780)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=38780)[0m f1_macro: 0.12375671934466555
[2m[36m(func pid=38780)[0m f1_weighted: 0.1841371447683863
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.0, 0.108, 0.538, 0.07, 0.12, 0.0, 0.302, 0.025, 0.075]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 3.3369 | Steps: 4 | Val loss: 3.3192 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=31048)[0m top1: 0.4361007462686567
[2m[36m(func pid=31048)[0m top5: 0.8833955223880597
[2m[36m(func pid=31048)[0m f1_micro: 0.4361007462686567
[2m[36m(func pid=31048)[0m f1_macro: 0.26907271831386487
[2m[36m(func pid=31048)[0m f1_weighted: 0.40400471465441956
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.349, 0.692, 0.537, 0.101, 0.11, 0.575, 0.0, 0.067, 0.259]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.3485 | Steps: 4 | Val loss: 2.0625 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=40628)[0m top1: 0.3927238805970149
[2m[36m(func pid=40628)[0m top5: 0.8805970149253731
[2m[36m(func pid=40628)[0m f1_micro: 0.39272388059701496
[2m[36m(func pid=40628)[0m f1_macro: 0.1828523661436517
[2m[36m(func pid=40628)[0m f1_weighted: 0.2737327970303524
[2m[36m(func pid=40628)[0m f1_per_class: [0.353, 0.483, 0.267, 0.0, 0.0, 0.0, 0.595, 0.0, 0.13, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.5471 | Steps: 4 | Val loss: 2.1482 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 15.2555 | Steps: 4 | Val loss: 10.1069 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=39363)[0m top1: 0.29990671641791045
[2m[36m(func pid=39363)[0m top5: 0.8656716417910447
[2m[36m(func pid=39363)[0m f1_micro: 0.29990671641791045
[2m[36m(func pid=39363)[0m f1_macro: 0.17245010934719723
[2m[36m(func pid=39363)[0m f1_weighted: 0.21632088528821836
[2m[36m(func pid=39363)[0m f1_per_class: [0.0, 0.0, 0.178, 0.533, 0.143, 0.31, 0.012, 0.434, 0.0, 0.114]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:48:33 (running for 00:31:03.21)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 16.829 |      0.269 |                   62 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.547 |      0.117 |                   29 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.349 |      0.172 |                   27 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  3.337 |      0.183 |                   23 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.22994402985074627
[2m[36m(func pid=38780)[0m top5: 0.6035447761194029
[2m[36m(func pid=38780)[0m f1_micro: 0.22994402985074627
[2m[36m(func pid=38780)[0m f1_macro: 0.11730373032209282
[2m[36m(func pid=38780)[0m f1_weighted: 0.16749117107852987
[2m[36m(func pid=38780)[0m f1_per_class: [0.117, 0.0, 0.162, 0.526, 0.066, 0.023, 0.0, 0.238, 0.0, 0.041]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 5.2204 | Steps: 4 | Val loss: 2.9927 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=31048)[0m top1: 0.23787313432835822
[2m[36m(func pid=31048)[0m top5: 0.7999067164179104
[2m[36m(func pid=31048)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=31048)[0m f1_macro: 0.2770579661246522
[2m[36m(func pid=31048)[0m f1_weighted: 0.2754319027247082
[2m[36m(func pid=31048)[0m f1_per_class: [0.303, 0.381, 0.632, 0.45, 0.028, 0.077, 0.101, 0.486, 0.149, 0.163]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.3053 | Steps: 4 | Val loss: 2.2740 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=40628)[0m top1: 0.2635261194029851
[2m[36m(func pid=40628)[0m top5: 0.902518656716418
[2m[36m(func pid=40628)[0m f1_micro: 0.2635261194029851
[2m[36m(func pid=40628)[0m f1_macro: 0.19684334040831591
[2m[36m(func pid=40628)[0m f1_weighted: 0.24153256326891204
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.53, 0.375, 0.321, 0.205, 0.276, 0.046, 0.188, 0.028, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.5013 | Steps: 4 | Val loss: 2.1415 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 11.7432 | Steps: 4 | Val loss: 11.5211 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=39363)[0m top1: 0.2537313432835821
[2m[36m(func pid=39363)[0m top5: 0.8773320895522388
[2m[36m(func pid=39363)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=39363)[0m f1_macro: 0.14078187350925028
[2m[36m(func pid=39363)[0m f1_weighted: 0.19723435202970852
[2m[36m(func pid=39363)[0m f1_per_class: [0.043, 0.0, 0.164, 0.524, 0.121, 0.319, 0.006, 0.162, 0.0, 0.07]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:48:38 (running for 00:31:08.66)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 15.255 |      0.277 |                   63 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.501 |      0.139 |                   30 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.305 |      0.141 |                   28 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  5.22  |      0.197 |                   24 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.20569029850746268
[2m[36m(func pid=38780)[0m top5: 0.6096082089552238
[2m[36m(func pid=38780)[0m f1_micro: 0.20569029850746268
[2m[36m(func pid=38780)[0m f1_macro: 0.13937369713454978
[2m[36m(func pid=38780)[0m f1_weighted: 0.15828323300076175
[2m[36m(func pid=38780)[0m f1_per_class: [0.2, 0.0, 0.417, 0.493, 0.055, 0.008, 0.0, 0.221, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 3.4644 | Steps: 4 | Val loss: 3.4077 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=31048)[0m top1: 0.2835820895522388
[2m[36m(func pid=31048)[0m top5: 0.75
[2m[36m(func pid=31048)[0m f1_micro: 0.2835820895522388
[2m[36m(func pid=31048)[0m f1_macro: 0.24947076641149635
[2m[36m(func pid=31048)[0m f1_weighted: 0.20882957700703722
[2m[36m(func pid=31048)[0m f1_per_class: [0.188, 0.518, 0.387, 0.121, 0.197, 0.373, 0.019, 0.435, 0.096, 0.162]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.0114 | Steps: 4 | Val loss: 2.3423 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.4797 | Steps: 4 | Val loss: 2.1447 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=40628)[0m top1: 0.2947761194029851
[2m[36m(func pid=40628)[0m top5: 0.8684701492537313
[2m[36m(func pid=40628)[0m f1_micro: 0.2947761194029851
[2m[36m(func pid=40628)[0m f1_macro: 0.19916142637656511
[2m[36m(func pid=40628)[0m f1_weighted: 0.34156597565371116
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.271, 0.069, 0.392, 0.055, 0.307, 0.444, 0.263, 0.0, 0.19]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 12.7204 | Steps: 4 | Val loss: 13.8292 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=39363)[0m top1: 0.09188432835820895
[2m[36m(func pid=39363)[0m top5: 0.8628731343283582
[2m[36m(func pid=39363)[0m f1_micro: 0.09188432835820894
[2m[36m(func pid=39363)[0m f1_macro: 0.11044984662269938
[2m[36m(func pid=39363)[0m f1_weighted: 0.06634131939250712
[2m[36m(func pid=39363)[0m f1_per_class: [0.288, 0.042, 0.314, 0.026, 0.07, 0.286, 0.032, 0.016, 0.0, 0.031]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:48:44 (running for 00:31:13.91)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 11.743 |      0.249 |                   64 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.48  |      0.137 |                   31 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.011 |      0.11  |                   29 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  3.464 |      0.199 |                   25 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.18050373134328357
[2m[36m(func pid=38780)[0m top5: 0.6497201492537313
[2m[36m(func pid=38780)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=38780)[0m f1_macro: 0.13653495891354733
[2m[36m(func pid=38780)[0m f1_weighted: 0.14243703444854713
[2m[36m(func pid=38780)[0m f1_per_class: [0.203, 0.0, 0.455, 0.439, 0.05, 0.0, 0.0, 0.22, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=31048)[0m top1: 0.26119402985074625
[2m[36m(func pid=31048)[0m top5: 0.8083022388059702
[2m[36m(func pid=31048)[0m f1_micro: 0.26119402985074625
[2m[36m(func pid=31048)[0m f1_macro: 0.1465284705615958
[2m[36m(func pid=31048)[0m f1_weighted: 0.2870165484108653
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.151, 0.075, 0.47, 0.3, 0.0, 0.425, 0.0, 0.0, 0.044]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 3.3026 | Steps: 4 | Val loss: 5.3647 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.1157 | Steps: 4 | Val loss: 2.4268 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.5186 | Steps: 4 | Val loss: 2.1487 | Batch size: 32 | lr: 0.0001 | Duration: 3.13s
[2m[36m(func pid=40628)[0m top1: 0.12826492537313433
[2m[36m(func pid=40628)[0m top5: 0.8213619402985075
[2m[36m(func pid=40628)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=40628)[0m f1_macro: 0.12471953035922674
[2m[36m(func pid=40628)[0m f1_weighted: 0.1503960804921792
[2m[36m(func pid=40628)[0m f1_per_class: [0.163, 0.0, 0.389, 0.042, 0.045, 0.205, 0.365, 0.0, 0.0, 0.038]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 14.5941 | Steps: 4 | Val loss: 13.5680 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=39363)[0m top1: 0.10494402985074627
[2m[36m(func pid=39363)[0m top5: 0.8540111940298507
[2m[36m(func pid=39363)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=39363)[0m f1_macro: 0.13437127726213577
[2m[36m(func pid=39363)[0m f1_weighted: 0.11750243726318008
[2m[36m(func pid=39363)[0m f1_per_class: [0.253, 0.047, 0.5, 0.0, 0.044, 0.226, 0.251, 0.0, 0.0, 0.023]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:48:49 (running for 00:31:19.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 12.72  |      0.147 |                   65 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.519 |      0.122 |                   32 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.116 |      0.134 |                   30 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  3.303 |      0.125 |                   26 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.14692164179104478
[2m[36m(func pid=38780)[0m top5: 0.695429104477612
[2m[36m(func pid=38780)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=38780)[0m f1_macro: 0.12181631187581102
[2m[36m(func pid=38780)[0m f1_weighted: 0.11300588647169633
[2m[36m(func pid=38780)[0m f1_per_class: [0.18, 0.0, 0.444, 0.339, 0.053, 0.0, 0.0, 0.201, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=31048)[0m top1: 0.21548507462686567
[2m[36m(func pid=31048)[0m top5: 0.7416044776119403
[2m[36m(func pid=31048)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=31048)[0m f1_macro: 0.1612673779708276
[2m[36m(func pid=31048)[0m f1_weighted: 0.18819366049717884
[2m[36m(func pid=31048)[0m f1_per_class: [0.365, 0.413, 0.0, 0.0, 0.062, 0.0, 0.312, 0.195, 0.089, 0.176]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 4.1463 | Steps: 4 | Val loss: 5.3351 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.0963 | Steps: 4 | Val loss: 2.4400 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.4952 | Steps: 4 | Val loss: 2.1393 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 18.7700 | Steps: 4 | Val loss: 38.4698 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=40628)[0m top1: 0.13899253731343283
[2m[36m(func pid=40628)[0m top5: 0.7336753731343284
[2m[36m(func pid=40628)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=40628)[0m f1_macro: 0.15027959050201084
[2m[36m(func pid=40628)[0m f1_weighted: 0.14813069006586144
[2m[36m(func pid=40628)[0m f1_per_class: [0.253, 0.073, 0.267, 0.29, 0.044, 0.172, 0.073, 0.0, 0.107, 0.225]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=39363)[0m top1: 0.1609141791044776
[2m[36m(func pid=39363)[0m top5: 0.7560634328358209
[2m[36m(func pid=39363)[0m f1_micro: 0.1609141791044776
[2m[36m(func pid=39363)[0m f1_macro: 0.14336243004268043
[2m[36m(func pid=39363)[0m f1_weighted: 0.17728859146426415
[2m[36m(func pid=39363)[0m f1_per_class: [0.253, 0.131, 0.5, 0.0, 0.032, 0.008, 0.487, 0.0, 0.0, 0.023]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:48:55 (running for 00:31:25.17)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 18.77  |      0.095 |                   67 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.519 |      0.122 |                   32 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.096 |      0.143 |                   31 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  4.146 |      0.15  |                   27 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.18236940298507462
[2m[36m(func pid=31048)[0m top5: 0.6609141791044776
[2m[36m(func pid=31048)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=31048)[0m f1_macro: 0.09516692225908477
[2m[36m(func pid=31048)[0m f1_weighted: 0.1140944569799696
[2m[36m(func pid=31048)[0m f1_per_class: [0.34, 0.0, 0.0, 0.0, 0.105, 0.243, 0.264, 0.0, 0.0, 0.0]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.11986940298507463
[2m[36m(func pid=38780)[0m top5: 0.7206156716417911
[2m[36m(func pid=38780)[0m f1_micro: 0.11986940298507463
[2m[36m(func pid=38780)[0m f1_macro: 0.10991544747530395
[2m[36m(func pid=38780)[0m f1_weighted: 0.08461528673143257
[2m[36m(func pid=38780)[0m f1_per_class: [0.177, 0.0, 0.444, 0.241, 0.051, 0.0, 0.0, 0.187, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 3.2599 | Steps: 4 | Val loss: 3.8547 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.9856 | Steps: 4 | Val loss: 2.3471 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 23.4571 | Steps: 4 | Val loss: 39.4782 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=40628)[0m top1: 0.35401119402985076
[2m[36m(func pid=40628)[0m top5: 0.8115671641791045
[2m[36m(func pid=40628)[0m f1_micro: 0.35401119402985076
[2m[36m(func pid=40628)[0m f1_macro: 0.2569733659984804
[2m[36m(func pid=40628)[0m f1_weighted: 0.3181994335311786
[2m[36m(func pid=40628)[0m f1_per_class: [0.383, 0.466, 0.364, 0.554, 0.1, 0.289, 0.112, 0.0, 0.137, 0.167]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.4655 | Steps: 4 | Val loss: 2.1306 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=39363)[0m top1: 0.2593283582089552
[2m[36m(func pid=39363)[0m top5: 0.7075559701492538
[2m[36m(func pid=39363)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=39363)[0m f1_macro: 0.1717214425265105
[2m[36m(func pid=39363)[0m f1_weighted: 0.21374270601700895
[2m[36m(func pid=39363)[0m f1_per_class: [0.34, 0.211, 0.457, 0.0, 0.035, 0.0, 0.554, 0.0, 0.047, 0.074]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:49:00 (running for 00:31:30.56)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 23.457 |      0.158 |                   68 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.495 |      0.11  |                   33 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.986 |      0.172 |                   32 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  3.26  |      0.257 |                   28 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.20242537313432835
[2m[36m(func pid=31048)[0m top5: 0.5331156716417911
[2m[36m(func pid=31048)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=31048)[0m f1_macro: 0.1581148471071049
[2m[36m(func pid=31048)[0m f1_weighted: 0.12424655334656862
[2m[36m(func pid=31048)[0m f1_per_class: [0.274, 0.479, 0.314, 0.0, 0.115, 0.189, 0.027, 0.0, 0.077, 0.105]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.10587686567164178
[2m[36m(func pid=38780)[0m top5: 0.730410447761194
[2m[36m(func pid=38780)[0m f1_micro: 0.10587686567164178
[2m[36m(func pid=38780)[0m f1_macro: 0.08373560557041451
[2m[36m(func pid=38780)[0m f1_weighted: 0.06705227042050949
[2m[36m(func pid=38780)[0m f1_per_class: [0.169, 0.0, 0.25, 0.181, 0.044, 0.0, 0.0, 0.193, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 4.1765 | Steps: 4 | Val loss: 4.4398 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.2274 | Steps: 4 | Val loss: 2.2292 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=40628)[0m top1: 0.3003731343283582
[2m[36m(func pid=40628)[0m top5: 0.8348880597014925
[2m[36m(func pid=40628)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=40628)[0m f1_macro: 0.17388781148082216
[2m[36m(func pid=40628)[0m f1_weighted: 0.2792477153816516
[2m[36m(func pid=40628)[0m f1_per_class: [0.043, 0.54, 0.051, 0.325, 0.077, 0.373, 0.149, 0.104, 0.0, 0.077]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 25.7739 | Steps: 4 | Val loss: 39.0575 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5775 | Steps: 4 | Val loss: 2.1427 | Batch size: 32 | lr: 0.0001 | Duration: 3.04s
[2m[36m(func pid=39363)[0m top1: 0.26725746268656714
[2m[36m(func pid=39363)[0m top5: 0.7434701492537313
[2m[36m(func pid=39363)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=39363)[0m f1_macro: 0.13937203261499675
[2m[36m(func pid=39363)[0m f1_weighted: 0.21648409712485653
[2m[36m(func pid=39363)[0m f1_per_class: [0.0, 0.246, 0.44, 0.0, 0.042, 0.0, 0.563, 0.0, 0.104, 0.0]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:49:06 (running for 00:31:35.98)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 25.774 |      0.094 |                   69 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.466 |      0.084 |                   34 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.227 |      0.139 |                   33 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  4.177 |      0.174 |                   29 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.0960820895522388
[2m[36m(func pid=31048)[0m top5: 0.45475746268656714
[2m[36m(func pid=31048)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=31048)[0m f1_macro: 0.09412547803426863
[2m[36m(func pid=31048)[0m f1_weighted: 0.09538224883173788
[2m[36m(func pid=31048)[0m f1_per_class: [0.072, 0.44, 0.053, 0.0, 0.029, 0.076, 0.018, 0.0, 0.03, 0.222]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.09654850746268656
[2m[36m(func pid=38780)[0m top5: 0.757929104477612
[2m[36m(func pid=38780)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=38780)[0m f1_macro: 0.10640016913127806
[2m[36m(func pid=38780)[0m f1_weighted: 0.05611984546903373
[2m[36m(func pid=38780)[0m f1_per_class: [0.144, 0.0, 0.526, 0.131, 0.035, 0.0, 0.0, 0.227, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 7.0103 | Steps: 4 | Val loss: 4.3203 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.6538 | Steps: 4 | Val loss: 2.1984 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 18.0022 | Steps: 4 | Val loss: 15.2160 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=40628)[0m top1: 0.2989738805970149
[2m[36m(func pid=40628)[0m top5: 0.8334888059701493
[2m[36m(func pid=40628)[0m f1_micro: 0.2989738805970149
[2m[36m(func pid=40628)[0m f1_macro: 0.24556847011303834
[2m[36m(func pid=40628)[0m f1_weighted: 0.22784202436066733
[2m[36m(func pid=40628)[0m f1_per_class: [0.296, 0.425, 0.56, 0.219, 0.0, 0.301, 0.084, 0.394, 0.0, 0.176]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.5689 | Steps: 4 | Val loss: 2.1579 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=39363)[0m top1: 0.271455223880597
[2m[36m(func pid=39363)[0m top5: 0.8456156716417911
[2m[36m(func pid=39363)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=39363)[0m f1_macro: 0.14861483268881875
[2m[36m(func pid=39363)[0m f1_weighted: 0.24212016873805145
[2m[36m(func pid=39363)[0m f1_per_class: [0.0, 0.326, 0.189, 0.0, 0.048, 0.055, 0.565, 0.062, 0.164, 0.077]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:49:11 (running for 00:31:41.27)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 18.002 |      0.221 |                   70 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.577 |      0.106 |                   35 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.654 |      0.149 |                   34 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  7.01  |      0.246 |                   30 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.29197761194029853
[2m[36m(func pid=31048)[0m top5: 0.7294776119402985
[2m[36m(func pid=31048)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=31048)[0m f1_macro: 0.22096375336856103
[2m[36m(func pid=31048)[0m f1_weighted: 0.25167485762860364
[2m[36m(func pid=31048)[0m f1_per_class: [0.151, 0.053, 0.308, 0.466, 0.333, 0.449, 0.164, 0.0, 0.078, 0.209]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.08861940298507463
[2m[36m(func pid=38780)[0m top5: 0.7868470149253731
[2m[36m(func pid=38780)[0m f1_micro: 0.08861940298507463
[2m[36m(func pid=38780)[0m f1_macro: 0.10637441668483634
[2m[36m(func pid=38780)[0m f1_weighted: 0.055670535171894464
[2m[36m(func pid=38780)[0m f1_per_class: [0.144, 0.005, 0.483, 0.091, 0.026, 0.0, 0.022, 0.294, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 4.0681 | Steps: 4 | Val loss: 4.0024 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.1840 | Steps: 4 | Val loss: 2.0991 | Batch size: 32 | lr: 0.001 | Duration: 2.70s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 12.1651 | Steps: 4 | Val loss: 6.8124 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=40628)[0m top1: 0.291044776119403
[2m[36m(func pid=40628)[0m top5: 0.7961753731343284
[2m[36m(func pid=40628)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=40628)[0m f1_macro: 0.23445524786775698
[2m[36m(func pid=40628)[0m f1_weighted: 0.2608872713243123
[2m[36m(func pid=40628)[0m f1_per_class: [0.225, 0.263, 0.4, 0.547, 0.302, 0.0, 0.106, 0.293, 0.133, 0.075]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.6490 | Steps: 4 | Val loss: 2.1314 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=39363)[0m top1: 0.19263059701492538
[2m[36m(func pid=39363)[0m top5: 0.8502798507462687
[2m[36m(func pid=39363)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=39363)[0m f1_macro: 0.14911324795807362
[2m[36m(func pid=39363)[0m f1_weighted: 0.12927871653655182
[2m[36m(func pid=39363)[0m f1_per_class: [0.0, 0.341, 0.15, 0.007, 0.076, 0.372, 0.003, 0.301, 0.164, 0.077]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:49:17 (running for 00:31:46.77)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 12.165 |      0.261 |                   71 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.569 |      0.106 |                   36 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.184 |      0.149 |                   35 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  4.068 |      0.234 |                   31 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.37779850746268656
[2m[36m(func pid=31048)[0m top5: 0.9090485074626866
[2m[36m(func pid=31048)[0m f1_micro: 0.3777985074626865
[2m[36m(func pid=31048)[0m f1_macro: 0.26088292906714294
[2m[36m(func pid=31048)[0m f1_weighted: 0.35676834256291007
[2m[36m(func pid=31048)[0m f1_per_class: [0.222, 0.527, 0.2, 0.511, 0.2, 0.385, 0.182, 0.282, 0.026, 0.074]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.09654850746268656
[2m[36m(func pid=38780)[0m top5: 0.7784514925373134
[2m[36m(func pid=38780)[0m f1_micro: 0.09654850746268658
[2m[36m(func pid=38780)[0m f1_macro: 0.11793205211432922
[2m[36m(func pid=38780)[0m f1_weighted: 0.07231016782840219
[2m[36m(func pid=38780)[0m f1_per_class: [0.169, 0.046, 0.5, 0.07, 0.026, 0.0, 0.07, 0.298, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 5.1657 | Steps: 4 | Val loss: 8.1173 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 1.8221 | Steps: 4 | Val loss: 2.2011 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 11.9345 | Steps: 4 | Val loss: 13.3618 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=40628)[0m top1: 0.08815298507462686
[2m[36m(func pid=40628)[0m top5: 0.7005597014925373
[2m[36m(func pid=40628)[0m f1_micro: 0.08815298507462686
[2m[36m(func pid=40628)[0m f1_macro: 0.13908478422826137
[2m[36m(func pid=40628)[0m f1_weighted: 0.046720110927834624
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.0, 0.353, 0.0, 0.32, 0.0, 0.027, 0.524, 0.086, 0.081]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.5277 | Steps: 4 | Val loss: 2.1405 | Batch size: 32 | lr: 0.0001 | Duration: 3.02s
[2m[36m(func pid=39363)[0m top1: 0.3050373134328358
[2m[36m(func pid=39363)[0m top5: 0.808768656716418
[2m[36m(func pid=39363)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=39363)[0m f1_macro: 0.22375724182392612
[2m[36m(func pid=39363)[0m f1_weighted: 0.27550756366031387
[2m[36m(func pid=39363)[0m f1_per_class: [0.115, 0.479, 0.114, 0.439, 0.185, 0.414, 0.0, 0.271, 0.026, 0.195]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:49:22 (running for 00:31:52.33)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 11.934 |      0.175 |                   72 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.649 |      0.118 |                   37 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.822 |      0.224 |                   36 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  5.166 |      0.139 |                   32 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.2019589552238806
[2m[36m(func pid=31048)[0m top5: 0.8120335820895522
[2m[36m(func pid=31048)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=31048)[0m f1_macro: 0.1745946268234964
[2m[36m(func pid=31048)[0m f1_weighted: 0.1507519678608779
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.227, 0.373, 0.0, 0.233, 0.245, 0.182, 0.425, 0.0, 0.06]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.10960820895522388
[2m[36m(func pid=38780)[0m top5: 0.7933768656716418
[2m[36m(func pid=38780)[0m f1_micro: 0.10960820895522388
[2m[36m(func pid=38780)[0m f1_macro: 0.11241427906509945
[2m[36m(func pid=38780)[0m f1_weighted: 0.1106411745595253
[2m[36m(func pid=38780)[0m f1_per_class: [0.137, 0.092, 0.256, 0.045, 0.024, 0.0, 0.186, 0.384, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 7.5714 | Steps: 4 | Val loss: 8.6060 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.2802 | Steps: 4 | Val loss: 2.4297 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 17.2310 | Steps: 4 | Val loss: 11.5223 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=40628)[0m top1: 0.14458955223880596
[2m[36m(func pid=40628)[0m top5: 0.5559701492537313
[2m[36m(func pid=40628)[0m f1_micro: 0.14458955223880596
[2m[36m(func pid=40628)[0m f1_macro: 0.12351811005812738
[2m[36m(func pid=40628)[0m f1_weighted: 0.11357934860357753
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.0, 0.045, 0.0, 0.132, 0.281, 0.164, 0.512, 0.028, 0.073]
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.4751 | Steps: 4 | Val loss: 2.1335 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=39363)[0m top1: 0.34794776119402987
[2m[36m(func pid=39363)[0m top5: 0.7238805970149254
[2m[36m(func pid=39363)[0m f1_micro: 0.34794776119402987
[2m[36m(func pid=39363)[0m f1_macro: 0.2439156839521118
[2m[36m(func pid=39363)[0m f1_weighted: 0.2915393438348257
[2m[36m(func pid=39363)[0m f1_per_class: [0.123, 0.328, 0.127, 0.549, 0.294, 0.49, 0.0, 0.309, 0.0, 0.219]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:49:27 (running for 00:31:57.64)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 17.231 |      0.221 |                   73 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.528 |      0.112 |                   38 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.28  |      0.244 |                   37 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  7.571 |      0.124 |                   33 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.3069029850746269
[2m[36m(func pid=31048)[0m top5: 0.8628731343283582
[2m[36m(func pid=31048)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=31048)[0m f1_macro: 0.22144136919082547
[2m[36m(func pid=31048)[0m f1_weighted: 0.32455531105007024
[2m[36m(func pid=31048)[0m f1_per_class: [0.273, 0.068, 0.226, 0.414, 0.036, 0.0, 0.547, 0.422, 0.0, 0.229]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.13759328358208955
[2m[36m(func pid=38780)[0m top5: 0.8013059701492538
[2m[36m(func pid=38780)[0m f1_micro: 0.13759328358208955
[2m[36m(func pid=38780)[0m f1_macro: 0.11233334405891357
[2m[36m(func pid=38780)[0m f1_weighted: 0.15358524036232984
[2m[36m(func pid=38780)[0m f1_per_class: [0.113, 0.139, 0.156, 0.023, 0.026, 0.0, 0.338, 0.328, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 6.2049 | Steps: 4 | Val loss: 10.1635 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.1396 | Steps: 4 | Val loss: 2.5238 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 10.9554 | Steps: 4 | Val loss: 14.7475 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=40628)[0m top1: 0.1394589552238806
[2m[36m(func pid=40628)[0m top5: 0.6254664179104478
[2m[36m(func pid=40628)[0m f1_micro: 0.1394589552238806
[2m[36m(func pid=40628)[0m f1_macro: 0.13856111591614917
[2m[36m(func pid=40628)[0m f1_weighted: 0.07978250421013558
[2m[36m(func pid=40628)[0m f1_per_class: [0.16, 0.0, 0.11, 0.026, 0.082, 0.203, 0.032, 0.569, 0.0, 0.203]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.3326 | Steps: 4 | Val loss: 2.1105 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=39363)[0m top1: 0.35027985074626866
[2m[36m(func pid=39363)[0m top5: 0.6767723880597015
[2m[36m(func pid=39363)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=39363)[0m f1_macro: 0.25845676120532673
[2m[36m(func pid=39363)[0m f1_weighted: 0.27453418418332937
[2m[36m(func pid=39363)[0m f1_per_class: [0.207, 0.203, 0.344, 0.55, 0.2, 0.499, 0.0, 0.317, 0.0, 0.265]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:49:33 (running for 00:32:02.96)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.21200000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 10.955 |      0.195 |                   74 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.475 |      0.112 |                   39 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.14  |      0.258 |                   38 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  6.205 |      0.139 |                   34 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.1571828358208955
[2m[36m(func pid=31048)[0m top5: 0.8274253731343284
[2m[36m(func pid=31048)[0m f1_micro: 0.1571828358208955
[2m[36m(func pid=31048)[0m f1_macro: 0.19479159002351204
[2m[36m(func pid=31048)[0m f1_weighted: 0.14528219061982078
[2m[36m(func pid=31048)[0m f1_per_class: [0.277, 0.486, 0.268, 0.032, 0.222, 0.13, 0.0, 0.451, 0.081, 0.0]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.15578358208955223
[2m[36m(func pid=38780)[0m top5: 0.7933768656716418
[2m[36m(func pid=38780)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=38780)[0m f1_macro: 0.10977706041173466
[2m[36m(func pid=38780)[0m f1_weighted: 0.17097016601864654
[2m[36m(func pid=38780)[0m f1_per_class: [0.109, 0.17, 0.122, 0.013, 0.026, 0.0, 0.403, 0.255, 0.0, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 4.7729 | Steps: 4 | Val loss: 11.1953 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.9521 | Steps: 4 | Val loss: 2.5684 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 9.8309 | Steps: 4 | Val loss: 8.4588 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=40628)[0m top1: 0.17537313432835822
[2m[36m(func pid=40628)[0m top5: 0.507929104477612
[2m[36m(func pid=40628)[0m f1_micro: 0.17537313432835822
[2m[36m(func pid=40628)[0m f1_macro: 0.16604309395787795
[2m[36m(func pid=40628)[0m f1_weighted: 0.14847013843484216
[2m[36m(func pid=40628)[0m f1_per_class: [0.204, 0.0, 0.471, 0.429, 0.033, 0.0, 0.0, 0.334, 0.0, 0.19]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.4841 | Steps: 4 | Val loss: 2.0991 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=39363)[0m top1: 0.341884328358209
[2m[36m(func pid=39363)[0m top5: 0.6585820895522388
[2m[36m(func pid=39363)[0m f1_micro: 0.341884328358209
[2m[36m(func pid=39363)[0m f1_macro: 0.26526343369165384
[2m[36m(func pid=39363)[0m f1_weighted: 0.27480074462855875
[2m[36m(func pid=39363)[0m f1_per_class: [0.184, 0.206, 0.444, 0.541, 0.2, 0.51, 0.0, 0.34, 0.0, 0.227]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:49:38 (running for 00:32:08.23)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  9.831 |      0.259 |                   75 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.333 |      0.11  |                   40 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.952 |      0.265 |                   39 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  4.773 |      0.166 |                   35 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.427705223880597
[2m[36m(func pid=31048)[0m top5: 0.8871268656716418
[2m[36m(func pid=31048)[0m f1_micro: 0.427705223880597
[2m[36m(func pid=31048)[0m f1_macro: 0.2588008441661401
[2m[36m(func pid=31048)[0m f1_weighted: 0.3840447123311294
[2m[36m(func pid=31048)[0m f1_per_class: [0.044, 0.506, 0.159, 0.187, 0.182, 0.426, 0.576, 0.331, 0.0, 0.176]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.16371268656716417
[2m[36m(func pid=38780)[0m top5: 0.7966417910447762
[2m[36m(func pid=38780)[0m f1_micro: 0.16371268656716417
[2m[36m(func pid=38780)[0m f1_macro: 0.10269196221717185
[2m[36m(func pid=38780)[0m f1_weighted: 0.17549501175771098
[2m[36m(func pid=38780)[0m f1_per_class: [0.038, 0.188, 0.109, 0.003, 0.027, 0.0, 0.428, 0.216, 0.018, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 11.5645 | Steps: 4 | Val loss: 10.4969 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.0193 | Steps: 4 | Val loss: 2.5509 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 8.6571 | Steps: 4 | Val loss: 18.6412 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
[2m[36m(func pid=40628)[0m top1: 0.2234141791044776
[2m[36m(func pid=40628)[0m top5: 0.5778917910447762
[2m[36m(func pid=40628)[0m f1_micro: 0.2234141791044776
[2m[36m(func pid=40628)[0m f1_macro: 0.14051832342201037
[2m[36m(func pid=40628)[0m f1_weighted: 0.18053760650928236
[2m[36m(func pid=40628)[0m f1_per_class: [0.317, 0.03, 0.0, 0.53, 0.038, 0.0, 0.0, 0.324, 0.0, 0.165]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.4103 | Steps: 4 | Val loss: 2.0736 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=39363)[0m top1: 0.345615671641791
[2m[36m(func pid=39363)[0m top5: 0.667910447761194
[2m[36m(func pid=39363)[0m f1_micro: 0.345615671641791
[2m[36m(func pid=39363)[0m f1_macro: 0.2919340777556522
[2m[36m(func pid=39363)[0m f1_weighted: 0.30137113612498156
[2m[36m(func pid=39363)[0m f1_per_class: [0.178, 0.361, 0.462, 0.538, 0.273, 0.468, 0.0, 0.375, 0.109, 0.155]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:49:43 (running for 00:32:13.49)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  8.657 |      0.116 |                   76 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.484 |      0.103 |                   41 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.019 |      0.292 |                   40 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  | 11.565 |      0.141 |                   36 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.10727611940298508
[2m[36m(func pid=31048)[0m top5: 0.7262126865671642
[2m[36m(func pid=31048)[0m f1_micro: 0.10727611940298508
[2m[36m(func pid=31048)[0m f1_macro: 0.11579938346321021
[2m[36m(func pid=31048)[0m f1_weighted: 0.07620661196572999
[2m[36m(func pid=31048)[0m f1_per_class: [0.224, 0.047, 0.211, 0.117, 0.031, 0.0, 0.015, 0.397, 0.027, 0.09]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.177705223880597
[2m[36m(func pid=38780)[0m top5: 0.7756529850746269
[2m[36m(func pid=38780)[0m f1_micro: 0.177705223880597
[2m[36m(func pid=38780)[0m f1_macro: 0.11338931632639533
[2m[36m(func pid=38780)[0m f1_weighted: 0.18708374166137873
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.206, 0.148, 0.0, 0.027, 0.0, 0.454, 0.212, 0.086, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 8.6945 | Steps: 4 | Val loss: 8.5834 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.8229 | Steps: 4 | Val loss: 2.5210 | Batch size: 32 | lr: 0.001 | Duration: 2.86s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 9.1743 | Steps: 4 | Val loss: 20.9545 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=40628)[0m top1: 0.1884328358208955
[2m[36m(func pid=40628)[0m top5: 0.6324626865671642
[2m[36m(func pid=40628)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=40628)[0m f1_macro: 0.1311238452027826
[2m[36m(func pid=40628)[0m f1_weighted: 0.1635454408561876
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.296, 0.0, 0.29, 0.1, 0.0, 0.0, 0.474, 0.089, 0.063]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.3299 | Steps: 4 | Val loss: 2.0626 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=39363)[0m top1: 0.31902985074626866
[2m[36m(func pid=39363)[0m top5: 0.6805037313432836
[2m[36m(func pid=39363)[0m f1_micro: 0.31902985074626866
[2m[36m(func pid=39363)[0m f1_macro: 0.3329183880449657
[2m[36m(func pid=39363)[0m f1_weighted: 0.2903861816417428
[2m[36m(func pid=39363)[0m f1_per_class: [0.271, 0.533, 0.667, 0.369, 0.286, 0.454, 0.0, 0.451, 0.121, 0.177]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:49:48 (running for 00:32:18.68)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  9.174 |      0.17  |                   77 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.41  |      0.113 |                   42 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.823 |      0.333 |                   41 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  8.695 |      0.131 |                   37 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.22527985074626866
[2m[36m(func pid=31048)[0m top5: 0.6464552238805971
[2m[36m(func pid=31048)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=31048)[0m f1_macro: 0.17037388291600855
[2m[36m(func pid=31048)[0m f1_weighted: 0.18807558506381977
[2m[36m(func pid=31048)[0m f1_per_class: [0.269, 0.0, 0.379, 0.0, 0.047, 0.032, 0.568, 0.0, 0.101, 0.308]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.20942164179104478
[2m[36m(func pid=38780)[0m top5: 0.7798507462686567
[2m[36m(func pid=38780)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=38780)[0m f1_macro: 0.11724810530280452
[2m[36m(func pid=38780)[0m f1_weighted: 0.20354383551031377
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.207, 0.162, 0.0, 0.031, 0.0, 0.52, 0.137, 0.115, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 7.5850 | Steps: 4 | Val loss: 7.6980 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.0196 | Steps: 4 | Val loss: 2.2933 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 13.2674 | Steps: 4 | Val loss: 18.6312 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=40628)[0m top1: 0.19029850746268656
[2m[36m(func pid=40628)[0m top5: 0.6944962686567164
[2m[36m(func pid=40628)[0m f1_micro: 0.19029850746268656
[2m[36m(func pid=40628)[0m f1_macro: 0.15741193621234237
[2m[36m(func pid=40628)[0m f1_weighted: 0.19779878305549475
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.246, 0.1, 0.481, 0.245, 0.0, 0.0, 0.25, 0.072, 0.18]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3888 | Steps: 4 | Val loss: 2.0439 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=39363)[0m top1: 0.28544776119402987
[2m[36m(func pid=39363)[0m top5: 0.7094216417910447
[2m[36m(func pid=39363)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=39363)[0m f1_macro: 0.26756188444904566
[2m[36m(func pid=39363)[0m f1_weighted: 0.20231319851451743
[2m[36m(func pid=39363)[0m f1_per_class: [0.0, 0.502, 0.667, 0.107, 0.229, 0.448, 0.0, 0.381, 0.143, 0.2]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:49:54 (running for 00:32:23.87)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 13.267 |      0.221 |                   78 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.33  |      0.117 |                   43 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.02  |      0.268 |                   42 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  7.585 |      0.157 |                   38 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.24860074626865672
[2m[36m(func pid=31048)[0m top5: 0.6613805970149254
[2m[36m(func pid=31048)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=31048)[0m f1_macro: 0.22115154822835362
[2m[36m(func pid=31048)[0m f1_weighted: 0.1945297866069722
[2m[36m(func pid=31048)[0m f1_per_class: [0.179, 0.0, 0.4, 0.454, 0.204, 0.286, 0.0, 0.412, 0.027, 0.25]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.24440298507462688
[2m[36m(func pid=38780)[0m top5: 0.7966417910447762
[2m[36m(func pid=38780)[0m f1_micro: 0.24440298507462688
[2m[36m(func pid=38780)[0m f1_macro: 0.1237978989076014
[2m[36m(func pid=38780)[0m f1_weighted: 0.218977752832964
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.235, 0.176, 0.0, 0.036, 0.0, 0.56, 0.101, 0.129, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 9.8028 | Steps: 4 | Val loss: 7.1028 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 1.9193 | Steps: 4 | Val loss: 1.9996 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 16.2635 | Steps: 4 | Val loss: 27.6291 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=40628)[0m top1: 0.20848880597014927
[2m[36m(func pid=40628)[0m top5: 0.7873134328358209
[2m[36m(func pid=40628)[0m f1_micro: 0.20848880597014927
[2m[36m(func pid=40628)[0m f1_macro: 0.14942102693555948
[2m[36m(func pid=40628)[0m f1_weighted: 0.20330371715000087
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.387, 0.039, 0.323, 0.189, 0.293, 0.029, 0.0, 0.0, 0.234]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.4685 | Steps: 4 | Val loss: 2.0391 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=39363)[0m top1: 0.3045708955223881
[2m[36m(func pid=39363)[0m top5: 0.7947761194029851
[2m[36m(func pid=39363)[0m f1_micro: 0.3045708955223881
[2m[36m(func pid=39363)[0m f1_macro: 0.2691089088220985
[2m[36m(func pid=39363)[0m f1_weighted: 0.21225758918466664
[2m[36m(func pid=39363)[0m f1_per_class: [0.264, 0.491, 0.449, 0.139, 0.237, 0.451, 0.0, 0.346, 0.178, 0.136]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:49:59 (running for 00:32:29.23)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 16.264 |      0.178 |                   79 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.389 |      0.124 |                   44 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.919 |      0.269 |                   43 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  9.803 |      0.149 |                   39 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.18610074626865672
[2m[36m(func pid=31048)[0m top5: 0.5536380597014925
[2m[36m(func pid=31048)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=31048)[0m f1_macro: 0.17797352722369003
[2m[36m(func pid=31048)[0m f1_weighted: 0.16540989041324747
[2m[36m(func pid=31048)[0m f1_per_class: [0.23, 0.063, 0.129, 0.406, 0.324, 0.047, 0.0, 0.402, 0.126, 0.052]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.24953358208955223
[2m[36m(func pid=38780)[0m top5: 0.8199626865671642
[2m[36m(func pid=38780)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=38780)[0m f1_macro: 0.1242213400968187
[2m[36m(func pid=38780)[0m f1_weighted: 0.2201697099339694
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.268, 0.185, 0.0, 0.041, 0.0, 0.552, 0.06, 0.136, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 6.6763 | Steps: 4 | Val loss: 4.7330 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 1.9728 | Steps: 4 | Val loss: 1.8080 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 14.7393 | Steps: 4 | Val loss: 38.8339 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=40628)[0m top1: 0.32742537313432835
[2m[36m(func pid=40628)[0m top5: 0.8624067164179104
[2m[36m(func pid=40628)[0m f1_micro: 0.32742537313432835
[2m[36m(func pid=40628)[0m f1_macro: 0.21765134446927736
[2m[36m(func pid=40628)[0m f1_weighted: 0.35612771093714934
[2m[36m(func pid=40628)[0m f1_per_class: [0.085, 0.488, 0.211, 0.261, 0.144, 0.257, 0.55, 0.0, 0.0, 0.182]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.4623 | Steps: 4 | Val loss: 2.0252 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=39363)[0m top1: 0.28591417910447764
[2m[36m(func pid=39363)[0m top5: 0.8717350746268657
[2m[36m(func pid=39363)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=39363)[0m f1_macro: 0.21096649019
[2m[36m(func pid=39363)[0m f1_weighted: 0.20618547340521903
[2m[36m(func pid=39363)[0m f1_per_class: [0.229, 0.45, 0.216, 0.104, 0.143, 0.376, 0.084, 0.406, 0.0, 0.103]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:50:04 (running for 00:32:34.68)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 14.739 |      0.181 |                   80 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.468 |      0.124 |                   45 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.973 |      0.211 |                   44 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  6.676 |      0.218 |                   40 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.26725746268656714
[2m[36m(func pid=31048)[0m top5: 0.40671641791044777
[2m[36m(func pid=31048)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=31048)[0m f1_macro: 0.18061220333733768
[2m[36m(func pid=31048)[0m f1_weighted: 0.15173635149783143
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.46, 0.202, 0.0, 0.138, 0.364, 0.0, 0.366, 0.199, 0.077]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.26399253731343286
[2m[36m(func pid=38780)[0m top5: 0.832089552238806
[2m[36m(func pid=38780)[0m f1_micro: 0.26399253731343286
[2m[36m(func pid=38780)[0m f1_macro: 0.12891544272147137
[2m[36m(func pid=38780)[0m f1_weighted: 0.22801422004464295
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.295, 0.194, 0.0, 0.047, 0.0, 0.565, 0.047, 0.142, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 7.7766 | Steps: 4 | Val loss: 7.2468 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 1.5759 | Steps: 4 | Val loss: 1.6860 | Batch size: 32 | lr: 0.001 | Duration: 3.23s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 18.8080 | Steps: 4 | Val loss: 37.7186 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=40628)[0m top1: 0.20708955223880596
[2m[36m(func pid=40628)[0m top5: 0.8810634328358209
[2m[36m(func pid=40628)[0m f1_micro: 0.20708955223880596
[2m[36m(func pid=40628)[0m f1_macro: 0.15951003774103717
[2m[36m(func pid=40628)[0m f1_weighted: 0.2224611696225531
[2m[36m(func pid=40628)[0m f1_per_class: [0.064, 0.478, 0.267, 0.01, 0.174, 0.0, 0.441, 0.0, 0.0, 0.162]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.5136 | Steps: 4 | Val loss: 2.0091 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=39363)[0m top1: 0.36007462686567165
[2m[36m(func pid=39363)[0m top5: 0.9253731343283582
[2m[36m(func pid=39363)[0m f1_micro: 0.3600746268656716
[2m[36m(func pid=39363)[0m f1_macro: 0.2530760688688694
[2m[36m(func pid=39363)[0m f1_weighted: 0.3792325781809532
[2m[36m(func pid=39363)[0m f1_per_class: [0.268, 0.414, 0.24, 0.351, 0.072, 0.348, 0.491, 0.27, 0.0, 0.077]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:50:10 (running for 00:32:39.97)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 18.808 |      0.156 |                   81 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.462 |      0.129 |                   46 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.576 |      0.253 |                   45 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  7.777 |      0.16  |                   41 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.10914179104477612
[2m[36m(func pid=31048)[0m top5: 0.4216417910447761
[2m[36m(func pid=31048)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=31048)[0m f1_macro: 0.1560146732060934
[2m[36m(func pid=31048)[0m f1_weighted: 0.06992264317026439
[2m[36m(func pid=31048)[0m f1_per_class: [0.265, 0.089, 0.545, 0.0, 0.041, 0.191, 0.0, 0.388, 0.041, 0.0]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.2751865671641791
[2m[36m(func pid=38780)[0m top5: 0.8563432835820896
[2m[36m(func pid=38780)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=38780)[0m f1_macro: 0.13333169950672832
[2m[36m(func pid=38780)[0m f1_weighted: 0.233922495334964
[2m[36m(func pid=38780)[0m f1_per_class: [0.0, 0.324, 0.19, 0.0, 0.06, 0.0, 0.567, 0.047, 0.145, 0.0]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 6.1106 | Steps: 4 | Val loss: 4.3167 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 1.8688 | Steps: 4 | Val loss: 1.7390 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 17.5935 | Steps: 4 | Val loss: 18.8448 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=40628)[0m top1: 0.41324626865671643
[2m[36m(func pid=40628)[0m top5: 0.878731343283582
[2m[36m(func pid=40628)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=40628)[0m f1_macro: 0.2243388556216172
[2m[36m(func pid=40628)[0m f1_weighted: 0.3938488774865952
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.553, 0.471, 0.493, 0.075, 0.0, 0.522, 0.0, 0.027, 0.103]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.3174 | Steps: 4 | Val loss: 1.9747 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=39363)[0m top1: 0.36240671641791045
[2m[36m(func pid=39363)[0m top5: 0.9342350746268657
[2m[36m(func pid=39363)[0m f1_micro: 0.36240671641791045
[2m[36m(func pid=39363)[0m f1_macro: 0.2223010875747252
[2m[36m(func pid=39363)[0m f1_weighted: 0.3794902491903116
[2m[36m(func pid=39363)[0m f1_per_class: [0.3, 0.264, 0.333, 0.493, 0.047, 0.252, 0.534, 0.0, 0.0, 0.0]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=31048)[0m top1: 0.13526119402985073
[2m[36m(func pid=31048)[0m top5: 0.7607276119402985
[2m[36m(func pid=31048)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=31048)[0m f1_macro: 0.18310314235622716
[2m[36m(func pid=31048)[0m f1_weighted: 0.15391320262368954
[2m[36m(func pid=31048)[0m f1_per_class: [0.355, 0.263, 0.583, 0.3, 0.145, 0.077, 0.0, 0.032, 0.077, 0.0]
== Status ==
Current time: 2024-01-07 10:50:15 (running for 00:32:45.34)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 17.593 |      0.183 |                   82 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.514 |      0.133 |                   47 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.869 |      0.222 |                   46 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  6.111 |      0.224 |                   42 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.28125
[2m[36m(func pid=38780)[0m top5: 0.8666044776119403
[2m[36m(func pid=38780)[0m f1_micro: 0.28125
[2m[36m(func pid=38780)[0m f1_macro: 0.16704138474559269
[2m[36m(func pid=38780)[0m f1_weighted: 0.24683450694038164
[2m[36m(func pid=38780)[0m f1_per_class: [0.148, 0.332, 0.262, 0.02, 0.06, 0.03, 0.563, 0.047, 0.132, 0.077]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 4.3617 | Steps: 4 | Val loss: 6.2093 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.0695 | Steps: 4 | Val loss: 1.8545 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 21.4746 | Steps: 4 | Val loss: 13.0895 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=40628)[0m top1: 0.2490671641791045
[2m[36m(func pid=40628)[0m top5: 0.7663246268656716
[2m[36m(func pid=40628)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=40628)[0m f1_macro: 0.23881032833257781
[2m[36m(func pid=40628)[0m f1_weighted: 0.22550295591715308
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.509, 0.556, 0.282, 0.062, 0.217, 0.0, 0.407, 0.148, 0.209]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.3326 | Steps: 4 | Val loss: 1.9568 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
== Status ==
Current time: 2024-01-07 10:50:20 (running for 00:32:50.66)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 17.593 |      0.183 |                   82 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.317 |      0.167 |                   48 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.069 |      0.218 |                   47 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  4.362 |      0.239 |                   43 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.27238805970149255
[2m[36m(func pid=31048)[0m top5: 0.8404850746268657
[2m[36m(func pid=31048)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=31048)[0m f1_macro: 0.16165583985611393
[2m[36m(func pid=31048)[0m f1_weighted: 0.27997349896886414
[2m[36m(func pid=31048)[0m f1_per_class: [0.044, 0.475, 0.145, 0.054, 0.0, 0.394, 0.456, 0.0, 0.0, 0.048]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.3400186567164179
[2m[36m(func pid=39363)[0m top5: 0.9356343283582089
[2m[36m(func pid=39363)[0m f1_micro: 0.3400186567164179
[2m[36m(func pid=39363)[0m f1_macro: 0.2179209192470582
[2m[36m(func pid=39363)[0m f1_weighted: 0.35004008146366317
[2m[36m(func pid=39363)[0m f1_per_class: [0.294, 0.086, 0.444, 0.52, 0.038, 0.187, 0.533, 0.0, 0.0, 0.077]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=38780)[0m top1: 0.3069029850746269
[2m[36m(func pid=38780)[0m top5: 0.878731343283582
[2m[36m(func pid=38780)[0m f1_micro: 0.3069029850746269
[2m[36m(func pid=38780)[0m f1_macro: 0.20880163936014823
[2m[36m(func pid=38780)[0m f1_weighted: 0.2786175738555007
[2m[36m(func pid=38780)[0m f1_per_class: [0.292, 0.365, 0.344, 0.076, 0.078, 0.161, 0.538, 0.047, 0.114, 0.074]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 5.3152 | Steps: 4 | Val loss: 10.1528 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 5.0534 | Steps: 4 | Val loss: 21.5233 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 1.8573 | Steps: 4 | Val loss: 1.9011 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=40628)[0m top1: 0.17397388059701493
[2m[36m(func pid=40628)[0m top5: 0.5503731343283582
[2m[36m(func pid=40628)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=40628)[0m f1_macro: 0.14021191463447003
[2m[36m(func pid=40628)[0m f1_weighted: 0.12521318109334276
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.426, 0.103, 0.0, 0.115, 0.255, 0.0, 0.265, 0.144, 0.094]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.3066 | Steps: 4 | Val loss: 1.9515 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:50:26 (running for 00:32:56.07)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  5.053 |      0.189 |                   84 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.333 |      0.209 |                   49 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.069 |      0.218 |                   47 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  5.315 |      0.14  |                   44 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.20382462686567165
[2m[36m(func pid=31048)[0m top5: 0.6291977611940298
[2m[36m(func pid=31048)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=31048)[0m f1_macro: 0.1886328618616301
[2m[36m(func pid=31048)[0m f1_weighted: 0.20514460083586328
[2m[36m(func pid=31048)[0m f1_per_class: [0.109, 0.353, 0.063, 0.26, 0.083, 0.349, 0.0, 0.457, 0.0, 0.214]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.32649253731343286
[2m[36m(func pid=39363)[0m top5: 0.9263059701492538
[2m[36m(func pid=39363)[0m f1_micro: 0.32649253731343286
[2m[36m(func pid=39363)[0m f1_macro: 0.22785659830209
[2m[36m(func pid=39363)[0m f1_weighted: 0.3501058888199247
[2m[36m(func pid=39363)[0m f1_per_class: [0.305, 0.15, 0.513, 0.521, 0.036, 0.181, 0.496, 0.0, 0.0, 0.077]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=38780)[0m top1: 0.32322761194029853
[2m[36m(func pid=38780)[0m top5: 0.882929104477612
[2m[36m(func pid=38780)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=38780)[0m f1_macro: 0.22269346033526416
[2m[36m(func pid=38780)[0m f1_weighted: 0.3165805895231618
[2m[36m(func pid=38780)[0m f1_per_class: [0.263, 0.388, 0.367, 0.224, 0.084, 0.267, 0.481, 0.047, 0.055, 0.051]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 10.8388 | Steps: 4 | Val loss: 14.7837 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 12.9977 | Steps: 4 | Val loss: 18.7370 | Batch size: 32 | lr: 0.1 | Duration: 2.77s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.0280 | Steps: 4 | Val loss: 1.9515 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=40628)[0m top1: 0.13992537313432835
[2m[36m(func pid=40628)[0m top5: 0.5214552238805971
[2m[36m(func pid=40628)[0m f1_micro: 0.13992537313432835
[2m[36m(func pid=40628)[0m f1_macro: 0.1245405712696176
[2m[36m(func pid=40628)[0m f1_weighted: 0.10439882912700185
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.283, 0.058, 0.0, 0.235, 0.31, 0.0, 0.303, 0.0, 0.057]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.3052 | Steps: 4 | Val loss: 1.9307 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=31048)[0m top1: 0.2537313432835821
[2m[36m(func pid=31048)[0m top5: 0.707089552238806
[2m[36m(func pid=31048)[0m f1_micro: 0.2537313432835821
[2m[36m(func pid=31048)[0m f1_macro: 0.17800541640095033
[2m[36m(func pid=31048)[0m f1_weighted: 0.22147382568928684
[2m[36m(func pid=31048)[0m f1_per_class: [0.122, 0.18, 0.117, 0.473, 0.052, 0.182, 0.0, 0.506, 0.147, 0.0]
[2m[36m(func pid=31048)[0m 
== Status ==
Current time: 2024-01-07 10:50:31 (running for 00:33:01.26)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 12.998 |      0.178 |                   85 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.307 |      0.223 |                   50 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.857 |      0.228 |                   48 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  | 10.839 |      0.125 |                   45 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=39363)[0m top1: 0.30177238805970147
[2m[36m(func pid=39363)[0m top5: 0.8973880597014925
[2m[36m(func pid=39363)[0m f1_micro: 0.30177238805970147
[2m[36m(func pid=39363)[0m f1_macro: 0.25048821140097005
[2m[36m(func pid=39363)[0m f1_weighted: 0.3399181164836123
[2m[36m(func pid=39363)[0m f1_per_class: [0.324, 0.2, 0.5, 0.485, 0.042, 0.263, 0.39, 0.229, 0.0, 0.072]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=38780)[0m top1: 0.363339552238806
[2m[36m(func pid=38780)[0m top5: 0.8847947761194029
[2m[36m(func pid=38780)[0m f1_micro: 0.363339552238806
[2m[36m(func pid=38780)[0m f1_macro: 0.2503465139541773
[2m[36m(func pid=38780)[0m f1_weighted: 0.35981164049287045
[2m[36m(func pid=38780)[0m f1_per_class: [0.255, 0.425, 0.435, 0.366, 0.117, 0.335, 0.452, 0.032, 0.0, 0.087]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 14.2058 | Steps: 4 | Val loss: 11.2540 | Batch size: 32 | lr: 0.01 | Duration: 2.75s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 15.6813 | Steps: 4 | Val loss: 10.1013 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.6489 | Steps: 4 | Val loss: 2.0355 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=40628)[0m top1: 0.18236940298507462
[2m[36m(func pid=40628)[0m top5: 0.7178171641791045
[2m[36m(func pid=40628)[0m f1_micro: 0.18236940298507462
[2m[36m(func pid=40628)[0m f1_macro: 0.14566683567044447
[2m[36m(func pid=40628)[0m f1_weighted: 0.1275028687547977
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.405, 0.066, 0.0, 0.202, 0.262, 0.0, 0.432, 0.0, 0.091]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.4264 | Steps: 4 | Val loss: 1.9085 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 10:50:36 (running for 00:33:06.29)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 15.681 |      0.235 |                   86 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.305 |      0.25  |                   51 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.028 |      0.25  |                   49 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  | 14.206 |      0.146 |                   46 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.40158582089552236
[2m[36m(func pid=31048)[0m top5: 0.9169776119402985
[2m[36m(func pid=31048)[0m f1_micro: 0.40158582089552236
[2m[36m(func pid=31048)[0m f1_macro: 0.23484916262512426
[2m[36m(func pid=31048)[0m f1_weighted: 0.3644476210285765
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.498, 0.455, 0.318, 0.105, 0.152, 0.554, 0.0, 0.054, 0.214]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.2453358208955224
[2m[36m(func pid=39363)[0m top5: 0.8498134328358209
[2m[36m(func pid=39363)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=39363)[0m f1_macro: 0.26655032702130194
[2m[36m(func pid=39363)[0m f1_weighted: 0.2701410475186379
[2m[36m(func pid=39363)[0m f1_per_class: [0.256, 0.284, 0.696, 0.395, 0.058, 0.264, 0.142, 0.456, 0.047, 0.067]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=38780)[0m top1: 0.3969216417910448
[2m[36m(func pid=38780)[0m top5: 0.8880597014925373
[2m[36m(func pid=38780)[0m f1_micro: 0.3969216417910448
[2m[36m(func pid=38780)[0m f1_macro: 0.27781512372276473
[2m[36m(func pid=38780)[0m f1_weighted: 0.39137061733002737
[2m[36m(func pid=38780)[0m f1_per_class: [0.289, 0.453, 0.462, 0.448, 0.161, 0.417, 0.424, 0.062, 0.0, 0.062]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 9.6592 | Steps: 4 | Val loss: 5.5943 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 18.2179 | Steps: 4 | Val loss: 17.8348 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.1898 | Steps: 4 | Val loss: 2.2223 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.2863 | Steps: 4 | Val loss: 1.8950 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=40628)[0m top1: 0.386660447761194
[2m[36m(func pid=40628)[0m top5: 0.8731343283582089
[2m[36m(func pid=40628)[0m f1_micro: 0.386660447761194
[2m[36m(func pid=40628)[0m f1_macro: 0.26223299023789953
[2m[36m(func pid=40628)[0m f1_weighted: 0.3560845990691508
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.512, 0.306, 0.551, 0.107, 0.293, 0.157, 0.504, 0.0, 0.194]
[2m[36m(func pid=40628)[0m 
== Status ==
Current time: 2024-01-07 10:50:41 (running for 00:33:11.67)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 18.218 |      0.197 |                   87 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.426 |      0.278 |                   52 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.649 |      0.267 |                   50 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  9.659 |      0.262 |                   47 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.20149253731343283
[2m[36m(func pid=31048)[0m top5: 0.8311567164179104
[2m[36m(func pid=31048)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=31048)[0m f1_macro: 0.19715113502114698
[2m[36m(func pid=31048)[0m f1_weighted: 0.16798940934811943
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.379, 0.545, 0.122, 0.118, 0.466, 0.0, 0.157, 0.0, 0.184]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.19916044776119404
[2m[36m(func pid=39363)[0m top5: 0.7667910447761194
[2m[36m(func pid=39363)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=39363)[0m f1_macro: 0.25165908020123817
[2m[36m(func pid=39363)[0m f1_weighted: 0.1874387537782954
[2m[36m(func pid=39363)[0m f1_per_class: [0.241, 0.347, 0.696, 0.197, 0.066, 0.265, 0.0, 0.476, 0.128, 0.101]
[2m[36m(func pid=38780)[0m top1: 0.40671641791044777
[2m[36m(func pid=38780)[0m top5: 0.8922574626865671
[2m[36m(func pid=38780)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=38780)[0m f1_macro: 0.2912476948408959
[2m[36m(func pid=38780)[0m f1_weighted: 0.40136792345268374
[2m[36m(func pid=38780)[0m f1_per_class: [0.301, 0.437, 0.48, 0.484, 0.112, 0.437, 0.407, 0.159, 0.0, 0.096]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 4.8254 | Steps: 4 | Val loss: 4.8236 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 26.9966 | Steps: 4 | Val loss: 19.0054 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.5483 | Steps: 4 | Val loss: 1.8888 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.7723 | Steps: 4 | Val loss: 2.3195 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=40628)[0m top1: 0.39972014925373134
[2m[36m(func pid=40628)[0m top5: 0.9416977611940298
[2m[36m(func pid=40628)[0m f1_micro: 0.39972014925373134
[2m[36m(func pid=40628)[0m f1_macro: 0.25999683539159923
[2m[36m(func pid=40628)[0m f1_weighted: 0.3809865555841169
[2m[36m(func pid=40628)[0m f1_per_class: [0.347, 0.503, 0.6, 0.378, 0.07, 0.0, 0.583, 0.0, 0.119, 0.0]
[2m[36m(func pid=40628)[0m 
== Status ==
Current time: 2024-01-07 10:50:47 (running for 00:33:16.94)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 26.997 |      0.237 |                   88 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.286 |      0.291 |                   53 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.19  |      0.252 |                   51 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  4.825 |      0.26  |                   48 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.30597014925373134
[2m[36m(func pid=31048)[0m top5: 0.7294776119402985
[2m[36m(func pid=31048)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=31048)[0m f1_macro: 0.2374620511597068
[2m[36m(func pid=31048)[0m f1_weighted: 0.24702107117726652
[2m[36m(func pid=31048)[0m f1_per_class: [0.167, 0.186, 0.09, 0.45, 0.312, 0.453, 0.0, 0.5, 0.0, 0.216]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.40671641791044777
[2m[36m(func pid=38780)[0m top5: 0.8964552238805971
[2m[36m(func pid=38780)[0m f1_micro: 0.40671641791044777
[2m[36m(func pid=38780)[0m f1_macro: 0.2878282449395328
[2m[36m(func pid=38780)[0m f1_weighted: 0.3953597317923766
[2m[36m(func pid=38780)[0m f1_per_class: [0.313, 0.407, 0.462, 0.514, 0.063, 0.439, 0.363, 0.218, 0.0, 0.099]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.21595149253731344
[2m[36m(func pid=39363)[0m top5: 0.7411380597014925
[2m[36m(func pid=39363)[0m f1_micro: 0.21595149253731344
[2m[36m(func pid=39363)[0m f1_macro: 0.24305696494267975
[2m[36m(func pid=39363)[0m f1_weighted: 0.17194064568034584
[2m[36m(func pid=39363)[0m f1_per_class: [0.269, 0.446, 0.369, 0.042, 0.097, 0.331, 0.0, 0.528, 0.124, 0.225]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 6.2746 | Steps: 4 | Val loss: 10.6901 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 8.5897 | Steps: 4 | Val loss: 16.2349 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.7330 | Steps: 4 | Val loss: 2.3090 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=40628)[0m top1: 0.2490671641791045
[2m[36m(func pid=40628)[0m top5: 0.7112873134328358
[2m[36m(func pid=40628)[0m f1_micro: 0.2490671641791045
[2m[36m(func pid=40628)[0m f1_macro: 0.15560381618758942
[2m[36m(func pid=40628)[0m f1_weighted: 0.24360766796095834
[2m[36m(func pid=40628)[0m f1_per_class: [0.162, 0.441, 0.267, 0.0, 0.041, 0.0, 0.532, 0.0, 0.113, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.2529 | Steps: 4 | Val loss: 1.8968 | Batch size: 32 | lr: 0.0001 | Duration: 3.16s
== Status ==
Current time: 2024-01-07 10:50:52 (running for 00:33:22.24)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  8.59  |      0.203 |                   89 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.548 |      0.288 |                   54 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.772 |      0.243 |                   52 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  6.275 |      0.156 |                   49 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.197294776119403
[2m[36m(func pid=31048)[0m top5: 0.761660447761194
[2m[36m(func pid=31048)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=31048)[0m f1_macro: 0.20257525068042112
[2m[36m(func pid=31048)[0m f1_weighted: 0.18294889364630235
[2m[36m(func pid=31048)[0m f1_per_class: [0.37, 0.54, 0.556, 0.137, 0.102, 0.0, 0.12, 0.0, 0.095, 0.107]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.37173507462686567
[2m[36m(func pid=38780)[0m top5: 0.8973880597014925
[2m[36m(func pid=38780)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=38780)[0m f1_macro: 0.2680991720251888
[2m[36m(func pid=38780)[0m f1_weighted: 0.3367483191806737
[2m[36m(func pid=38780)[0m f1_per_class: [0.324, 0.299, 0.444, 0.529, 0.098, 0.397, 0.218, 0.28, 0.0, 0.091]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.2462686567164179
[2m[36m(func pid=39363)[0m top5: 0.7402052238805971
[2m[36m(func pid=39363)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=39363)[0m f1_macro: 0.2071482430123844
[2m[36m(func pid=39363)[0m f1_weighted: 0.18448851763504315
[2m[36m(func pid=39363)[0m f1_per_class: [0.12, 0.49, 0.124, 0.075, 0.124, 0.342, 0.0, 0.517, 0.137, 0.143]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 10.6508 | Steps: 4 | Val loss: 14.0896 | Batch size: 32 | lr: 0.01 | Duration: 2.98s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 10.2184 | Steps: 4 | Val loss: 15.1289 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.3902 | Steps: 4 | Val loss: 1.9037 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=40628)[0m top1: 0.14925373134328357
[2m[36m(func pid=40628)[0m top5: 0.6166044776119403
[2m[36m(func pid=40628)[0m f1_micro: 0.14925373134328357
[2m[36m(func pid=40628)[0m f1_macro: 0.12142267037843046
[2m[36m(func pid=40628)[0m f1_weighted: 0.14484448225054103
[2m[36m(func pid=40628)[0m f1_per_class: [0.286, 0.453, 0.143, 0.0, 0.04, 0.0, 0.189, 0.0, 0.103, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.9381 | Steps: 4 | Val loss: 2.2607 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 10:50:57 (running for 00:33:27.67)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 10.218 |      0.165 |                   90 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.253 |      0.268 |                   55 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.733 |      0.207 |                   53 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  | 10.651 |      0.121 |                   50 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.2579291044776119
[2m[36m(func pid=31048)[0m top5: 0.808768656716418
[2m[36m(func pid=31048)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=31048)[0m f1_macro: 0.16536511528885908
[2m[36m(func pid=31048)[0m f1_weighted: 0.2935423445820833
[2m[36m(func pid=31048)[0m f1_per_class: [0.214, 0.27, 0.13, 0.321, 0.044, 0.0, 0.498, 0.0, 0.073, 0.103]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.3521455223880597
[2m[36m(func pid=38780)[0m top5: 0.8950559701492538
[2m[36m(func pid=38780)[0m f1_micro: 0.3521455223880597
[2m[36m(func pid=38780)[0m f1_macro: 0.24796978594523256
[2m[36m(func pid=38780)[0m f1_weighted: 0.2995856468934585
[2m[36m(func pid=38780)[0m f1_per_class: [0.298, 0.209, 0.408, 0.536, 0.082, 0.368, 0.147, 0.305, 0.0, 0.127]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.271455223880597
[2m[36m(func pid=39363)[0m top5: 0.761660447761194
[2m[36m(func pid=39363)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=39363)[0m f1_macro: 0.217979504772272
[2m[36m(func pid=39363)[0m f1_weighted: 0.2174814404724622
[2m[36m(func pid=39363)[0m f1_per_class: [0.168, 0.49, 0.081, 0.195, 0.135, 0.356, 0.0, 0.447, 0.165, 0.143]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 15.7363 | Steps: 4 | Val loss: 14.0360 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 12.9843 | Steps: 4 | Val loss: 18.2699 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=40628)[0m top1: 0.21641791044776118
[2m[36m(func pid=40628)[0m top5: 0.6305970149253731
[2m[36m(func pid=40628)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=40628)[0m f1_macro: 0.20955437717979092
[2m[36m(func pid=40628)[0m f1_weighted: 0.14291844197757025
[2m[36m(func pid=40628)[0m f1_per_class: [0.364, 0.509, 0.541, 0.0, 0.102, 0.245, 0.0, 0.19, 0.145, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.2683 | Steps: 4 | Val loss: 1.9395 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 1.4548 | Steps: 4 | Val loss: 2.1390 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 10:51:03 (running for 00:33:33.06)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 12.984 |      0.164 |                   91 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.39  |      0.248 |                   56 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.938 |      0.218 |                   54 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  | 15.736 |      0.21  |                   51 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.15578358208955223
[2m[36m(func pid=31048)[0m top5: 0.7098880597014925
[2m[36m(func pid=31048)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=31048)[0m f1_macro: 0.1643494498475266
[2m[36m(func pid=31048)[0m f1_weighted: 0.1552182381719834
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.278, 0.355, 0.228, 0.058, 0.066, 0.0, 0.526, 0.075, 0.057]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.32882462686567165
[2m[36m(func pid=38780)[0m top5: 0.8838619402985075
[2m[36m(func pid=38780)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=38780)[0m f1_macro: 0.20671371890502513
[2m[36m(func pid=38780)[0m f1_weighted: 0.2594571051828176
[2m[36m(func pid=38780)[0m f1_per_class: [0.231, 0.166, 0.268, 0.534, 0.056, 0.345, 0.062, 0.275, 0.0, 0.132]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.3138992537313433
[2m[36m(func pid=39363)[0m top5: 0.789179104477612
[2m[36m(func pid=39363)[0m f1_micro: 0.3138992537313433
[2m[36m(func pid=39363)[0m f1_macro: 0.24416743498872603
[2m[36m(func pid=39363)[0m f1_weighted: 0.27944591083220843
[2m[36m(func pid=39363)[0m f1_per_class: [0.182, 0.527, 0.113, 0.387, 0.14, 0.359, 0.009, 0.439, 0.132, 0.154]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 11.4244 | Steps: 4 | Val loss: 13.8410 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 5.6330 | Steps: 4 | Val loss: 17.2216 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=40628)[0m top1: 0.22807835820895522
[2m[36m(func pid=40628)[0m top5: 0.6902985074626866
[2m[36m(func pid=40628)[0m f1_micro: 0.22807835820895522
[2m[36m(func pid=40628)[0m f1_macro: 0.1338101162084993
[2m[36m(func pid=40628)[0m f1_weighted: 0.13524977068427874
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.512, 0.082, 0.0, 0.213, 0.255, 0.0, 0.276, 0.0, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.4784 | Steps: 4 | Val loss: 1.9831 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.1106 | Steps: 4 | Val loss: 2.1464 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 10:51:08 (running for 00:33:38.36)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  5.633 |      0.206 |                   92 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.268 |      0.207 |                   57 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.455 |      0.244 |                   55 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  | 11.424 |      0.134 |                   52 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.18097014925373134
[2m[36m(func pid=31048)[0m top5: 0.6753731343283582
[2m[36m(func pid=31048)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=31048)[0m f1_macro: 0.20635394633187204
[2m[36m(func pid=31048)[0m f1_weighted: 0.10703648525366066
[2m[36m(func pid=31048)[0m f1_per_class: [0.156, 0.175, 0.6, 0.003, 0.137, 0.284, 0.016, 0.477, 0.046, 0.17]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.30783582089552236
[2m[36m(func pid=38780)[0m top5: 0.8745335820895522
[2m[36m(func pid=38780)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=38780)[0m f1_macro: 0.1786672282467762
[2m[36m(func pid=38780)[0m f1_weighted: 0.23468260911943345
[2m[36m(func pid=38780)[0m f1_per_class: [0.154, 0.138, 0.189, 0.519, 0.0, 0.332, 0.018, 0.289, 0.0, 0.147]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.35261194029850745
[2m[36m(func pid=39363)[0m top5: 0.7915111940298507
[2m[36m(func pid=39363)[0m f1_micro: 0.35261194029850745
[2m[36m(func pid=39363)[0m f1_macro: 0.2689425980099195
[2m[36m(func pid=39363)[0m f1_weighted: 0.3442207445929547
[2m[36m(func pid=39363)[0m f1_per_class: [0.189, 0.547, 0.166, 0.49, 0.123, 0.327, 0.122, 0.5, 0.106, 0.12]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 8.9996 | Steps: 4 | Val loss: 8.1708 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 13.4044 | Steps: 4 | Val loss: 12.5414 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=40628)[0m top1: 0.25046641791044777
[2m[36m(func pid=40628)[0m top5: 0.8134328358208955
[2m[36m(func pid=40628)[0m f1_micro: 0.25046641791044777
[2m[36m(func pid=40628)[0m f1_macro: 0.15465289147644096
[2m[36m(func pid=40628)[0m f1_weighted: 0.16464116369542683
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.495, 0.074, 0.023, 0.1, 0.328, 0.04, 0.364, 0.0, 0.122]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.4558 | Steps: 4 | Val loss: 1.9950 | Batch size: 32 | lr: 0.0001 | Duration: 2.98s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.5166 | Steps: 4 | Val loss: 1.8795 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=31048)[0m top1: 0.16977611940298507
[2m[36m(func pid=31048)[0m top5: 0.7896455223880597
[2m[36m(func pid=31048)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=31048)[0m f1_macro: 0.2158604203554737
[2m[36m(func pid=31048)[0m f1_weighted: 0.13918232489743318
[2m[36m(func pid=31048)[0m f1_per_class: [0.317, 0.13, 0.415, 0.122, 0.063, 0.416, 0.003, 0.303, 0.131, 0.258]
[2m[36m(func pid=31048)[0m 
== Status ==
Current time: 2024-01-07 10:51:13 (running for 00:33:43.59)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 13.404 |      0.216 |                   93 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.478 |      0.179 |                   58 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  2.111 |      0.269 |                   56 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  9     |      0.155 |                   53 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.29757462686567165
[2m[36m(func pid=38780)[0m top5: 0.8656716417910447
[2m[36m(func pid=38780)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=38780)[0m f1_macro: 0.17538372699530555
[2m[36m(func pid=38780)[0m f1_weighted: 0.2237698301328724
[2m[36m(func pid=38780)[0m f1_per_class: [0.143, 0.077, 0.159, 0.512, 0.0, 0.321, 0.012, 0.376, 0.0, 0.154]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.41324626865671643
[2m[36m(func pid=39363)[0m top5: 0.8502798507462687
[2m[36m(func pid=39363)[0m f1_micro: 0.4132462686567165
[2m[36m(func pid=39363)[0m f1_macro: 0.3083469197538678
[2m[36m(func pid=39363)[0m f1_weighted: 0.4259762257071916
[2m[36m(func pid=39363)[0m f1_per_class: [0.237, 0.499, 0.338, 0.537, 0.113, 0.33, 0.392, 0.4, 0.096, 0.14]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 4.3722 | Steps: 4 | Val loss: 7.1356 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 10.5973 | Steps: 4 | Val loss: 17.0680 | Batch size: 32 | lr: 0.1 | Duration: 2.76s
[2m[36m(func pid=40628)[0m top1: 0.3246268656716418
[2m[36m(func pid=40628)[0m top5: 0.8325559701492538
[2m[36m(func pid=40628)[0m f1_micro: 0.3246268656716418
[2m[36m(func pid=40628)[0m f1_macro: 0.22654079776070293
[2m[36m(func pid=40628)[0m f1_weighted: 0.2595404508813166
[2m[36m(func pid=40628)[0m f1_per_class: [0.286, 0.102, 0.262, 0.558, 0.39, 0.0, 0.169, 0.428, 0.0, 0.07]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.2996 | Steps: 4 | Val loss: 1.9973 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.8603 | Steps: 4 | Val loss: 1.6719 | Batch size: 32 | lr: 0.001 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 10:51:19 (running for 00:33:48.77)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 10.597 |      0.232 |                   94 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.456 |      0.175 |                   59 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.517 |      0.308 |                   57 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  4.372 |      0.227 |                   54 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.2891791044776119
[2m[36m(func pid=31048)[0m top5: 0.6609141791044776
[2m[36m(func pid=31048)[0m f1_micro: 0.2891791044776119
[2m[36m(func pid=31048)[0m f1_macro: 0.23210255016404902
[2m[36m(func pid=31048)[0m f1_weighted: 0.2689785757482294
[2m[36m(func pid=31048)[0m f1_per_class: [0.189, 0.525, 0.27, 0.428, 0.076, 0.221, 0.0, 0.431, 0.04, 0.141]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m top1: 0.457089552238806
[2m[36m(func pid=39363)[0m top5: 0.8973880597014925
[2m[36m(func pid=39363)[0m f1_micro: 0.457089552238806
[2m[36m(func pid=39363)[0m f1_macro: 0.3158108983316503
[2m[36m(func pid=39363)[0m f1_weighted: 0.4522419862474376
[2m[36m(func pid=39363)[0m f1_per_class: [0.299, 0.497, 0.5, 0.558, 0.126, 0.251, 0.549, 0.061, 0.074, 0.244]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=38780)[0m top1: 0.2868470149253731
[2m[36m(func pid=38780)[0m top5: 0.8512126865671642
[2m[36m(func pid=38780)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=38780)[0m f1_macro: 0.17316136614818944
[2m[36m(func pid=38780)[0m f1_weighted: 0.21585800032357352
[2m[36m(func pid=38780)[0m f1_per_class: [0.095, 0.057, 0.171, 0.51, 0.056, 0.304, 0.003, 0.402, 0.0, 0.133]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 9.7176 | Steps: 4 | Val loss: 9.9244 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 8.8823 | Steps: 4 | Val loss: 12.3796 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 10:51:24 (running for 00:33:53.82)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 10.597 |      0.232 |                   94 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.3   |      0.173 |                   60 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.86  |      0.316 |                   58 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  9.718 |      0.207 |                   55 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.3162313432835821
[2m[36m(func pid=40628)[0m top5: 0.8027052238805971
[2m[36m(func pid=40628)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=40628)[0m f1_macro: 0.20699132478967855
[2m[36m(func pid=40628)[0m f1_weighted: 0.2702696039014121
[2m[36m(func pid=40628)[0m f1_per_class: [0.249, 0.0, 0.462, 0.559, 0.0, 0.0, 0.262, 0.483, 0.0, 0.057]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.3872 | Steps: 4 | Val loss: 2.0085 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.6155 | Steps: 4 | Val loss: 1.6156 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=31048)[0m top1: 0.29617537313432835
[2m[36m(func pid=31048)[0m top5: 0.804570895522388
[2m[36m(func pid=31048)[0m f1_micro: 0.29617537313432835
[2m[36m(func pid=31048)[0m f1_macro: 0.22776696427397236
[2m[36m(func pid=31048)[0m f1_weighted: 0.296385104057341
[2m[36m(func pid=31048)[0m f1_per_class: [0.271, 0.446, 0.462, 0.273, 0.28, 0.016, 0.434, 0.0, 0.037, 0.06]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.27425373134328357
[2m[36m(func pid=38780)[0m top5: 0.8348880597014925
[2m[36m(func pid=38780)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=38780)[0m f1_macro: 0.16792013170705689
[2m[36m(func pid=38780)[0m f1_weighted: 0.2115824797918397
[2m[36m(func pid=38780)[0m f1_per_class: [0.096, 0.072, 0.17, 0.493, 0.051, 0.295, 0.0, 0.407, 0.0, 0.095]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.4435634328358209
[2m[36m(func pid=39363)[0m top5: 0.9006529850746269
[2m[36m(func pid=39363)[0m f1_micro: 0.4435634328358209
[2m[36m(func pid=39363)[0m f1_macro: 0.2897567706515742
[2m[36m(func pid=39363)[0m f1_weighted: 0.42773131425219785
[2m[36m(func pid=39363)[0m f1_per_class: [0.326, 0.443, 0.513, 0.525, 0.125, 0.144, 0.584, 0.0, 0.043, 0.194]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 10.8300 | Steps: 4 | Val loss: 9.0875 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 8.9877 | Steps: 4 | Val loss: 15.8164 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 10:51:29 (running for 00:33:58.99)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  8.882 |      0.228 |                   95 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.387 |      0.168 |                   61 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.615 |      0.29  |                   59 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  | 10.83  |      0.238 |                   56 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.37919776119402987
[2m[36m(func pid=40628)[0m top5: 0.8227611940298507
[2m[36m(func pid=40628)[0m f1_micro: 0.37919776119402987
[2m[36m(func pid=40628)[0m f1_macro: 0.23835081387476692
[2m[36m(func pid=40628)[0m f1_weighted: 0.3403103716085439
[2m[36m(func pid=40628)[0m f1_per_class: [0.351, 0.0, 0.6, 0.576, 0.0, 0.0, 0.508, 0.286, 0.0, 0.063]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.4055 | Steps: 4 | Val loss: 2.0463 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.5744 | Steps: 4 | Val loss: 1.7653 | Batch size: 32 | lr: 0.001 | Duration: 2.82s
[2m[36m(func pid=31048)[0m top1: 0.16371268656716417
[2m[36m(func pid=31048)[0m top5: 0.7901119402985075
[2m[36m(func pid=31048)[0m f1_micro: 0.16371268656716417
[2m[36m(func pid=31048)[0m f1_macro: 0.13551373353567203
[2m[36m(func pid=31048)[0m f1_weighted: 0.15461276477731326
[2m[36m(func pid=31048)[0m f1_per_class: [0.0, 0.058, 0.147, 0.122, 0.086, 0.402, 0.191, 0.0, 0.103, 0.247]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m top1: 0.27705223880597013
[2m[36m(func pid=38780)[0m top5: 0.8260261194029851
[2m[36m(func pid=38780)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=38780)[0m f1_macro: 0.17740772042850178
[2m[36m(func pid=38780)[0m f1_weighted: 0.2248523422736238
[2m[36m(func pid=38780)[0m f1_per_class: [0.114, 0.14, 0.134, 0.493, 0.103, 0.31, 0.0, 0.4, 0.0, 0.081]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.37220149253731344
[2m[36m(func pid=39363)[0m top5: 0.8763992537313433
[2m[36m(func pid=39363)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=39363)[0m f1_macro: 0.28974239460631546
[2m[36m(func pid=39363)[0m f1_weighted: 0.3810567503612706
[2m[36m(func pid=39363)[0m f1_per_class: [0.33, 0.366, 0.621, 0.357, 0.105, 0.26, 0.572, 0.031, 0.104, 0.152]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 12.0858 | Steps: 4 | Val loss: 7.1802 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 7.6520 | Steps: 4 | Val loss: 19.8663 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 10:51:34 (running for 00:34:04.32)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  8.988 |      0.136 |                   96 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.405 |      0.177 |                   62 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.574 |      0.29  |                   60 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  | 12.086 |      0.247 |                   57 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.37173507462686567
[2m[36m(func pid=40628)[0m top5: 0.8278917910447762
[2m[36m(func pid=40628)[0m f1_micro: 0.37173507462686567
[2m[36m(func pid=40628)[0m f1_macro: 0.24745315834546644
[2m[36m(func pid=40628)[0m f1_weighted: 0.33335133040047604
[2m[36m(func pid=40628)[0m f1_per_class: [0.163, 0.0, 0.72, 0.512, 0.165, 0.0, 0.555, 0.248, 0.028, 0.084]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=31048)[0m top1: 0.15298507462686567
[2m[36m(func pid=31048)[0m top5: 0.6716417910447762
[2m[36m(func pid=31048)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=31048)[0m f1_macro: 0.14928973387711372
[2m[36m(func pid=31048)[0m f1_weighted: 0.13112109089833948
[2m[36m(func pid=31048)[0m f1_per_class: [0.222, 0.0, 0.113, 0.16, 0.05, 0.383, 0.071, 0.207, 0.024, 0.262]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.5033 | Steps: 4 | Val loss: 2.0658 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.3176 | Steps: 4 | Val loss: 2.0680 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=38780)[0m top1: 0.2593283582089552
[2m[36m(func pid=38780)[0m top5: 0.8180970149253731
[2m[36m(func pid=38780)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=38780)[0m f1_macro: 0.1764076516051865
[2m[36m(func pid=38780)[0m f1_weighted: 0.21645232582169072
[2m[36m(func pid=38780)[0m f1_per_class: [0.126, 0.121, 0.13, 0.464, 0.116, 0.322, 0.0, 0.423, 0.0, 0.061]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.271455223880597
[2m[36m(func pid=39363)[0m top5: 0.8120335820895522
[2m[36m(func pid=39363)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=39363)[0m f1_macro: 0.27402594174140205
[2m[36m(func pid=39363)[0m f1_weighted: 0.29365746513075386
[2m[36m(func pid=39363)[0m f1_per_class: [0.369, 0.337, 0.513, 0.071, 0.091, 0.306, 0.488, 0.311, 0.144, 0.111]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 10.5925 | Steps: 4 | Val loss: 8.3863 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 23.4283 | Steps: 4 | Val loss: 13.0715 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 10:51:39 (running for 00:34:09.56)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |  7.652 |      0.149 |                   97 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.503 |      0.176 |                   63 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.318 |      0.274 |                   61 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  | 10.592 |      0.188 |                   58 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.1287313432835821
[2m[36m(func pid=40628)[0m top5: 0.7513992537313433
[2m[36m(func pid=40628)[0m f1_micro: 0.1287313432835821
[2m[36m(func pid=40628)[0m f1_macro: 0.18809799841031896
[2m[36m(func pid=40628)[0m f1_weighted: 0.09544709322195766
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.011, 0.643, 0.063, 0.035, 0.3, 0.022, 0.411, 0.128, 0.268]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.1601 | Steps: 4 | Val loss: 2.0723 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=31048)[0m top1: 0.37593283582089554
[2m[36m(func pid=31048)[0m top5: 0.7486007462686567
[2m[36m(func pid=31048)[0m f1_micro: 0.37593283582089554
[2m[36m(func pid=31048)[0m f1_macro: 0.25705630878060554
[2m[36m(func pid=31048)[0m f1_weighted: 0.36419501703282164
[2m[36m(func pid=31048)[0m f1_per_class: [0.352, 0.0, 0.24, 0.504, 0.104, 0.381, 0.506, 0.303, 0.0, 0.18]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.5447 | Steps: 4 | Val loss: 2.3112 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=38780)[0m top1: 0.22294776119402984
[2m[36m(func pid=38780)[0m top5: 0.8069029850746269
[2m[36m(func pid=38780)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=38780)[0m f1_macro: 0.16851798189888662
[2m[36m(func pid=38780)[0m f1_weighted: 0.18948355621622057
[2m[36m(func pid=38780)[0m f1_per_class: [0.143, 0.083, 0.174, 0.387, 0.104, 0.329, 0.0, 0.424, 0.0, 0.042]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 7.3189 | Steps: 4 | Val loss: 15.0744 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=39363)[0m top1: 0.208955223880597
[2m[36m(func pid=39363)[0m top5: 0.7430037313432836
[2m[36m(func pid=39363)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=39363)[0m f1_macro: 0.2329544387813835
[2m[36m(func pid=39363)[0m f1_weighted: 0.2100257129937359
[2m[36m(func pid=39363)[0m f1_per_class: [0.296, 0.272, 0.44, 0.0, 0.086, 0.29, 0.306, 0.406, 0.155, 0.078]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 5.8831 | Steps: 4 | Val loss: 17.6748 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
== Status ==
Current time: 2024-01-07 10:51:45 (running for 00:34:15.17)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=15
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 4 RUNNING, 15 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |   loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------|
| train_952df_00015 | RUNNING    | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 | 23.428 |      0.257 |                   98 |
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |  2.16  |      0.169 |                   64 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |  1.545 |      0.233 |                   62 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  7.319 |      0.16  |                   59 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |        |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |        |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |        |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |        |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |        |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |  2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |  1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |  2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      | 31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |  2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |  2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |  1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      | 11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |  2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |  1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |  0.97  |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+--------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.3568 | Steps: 4 | Val loss: 2.0963 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=40628)[0m top1: 0.15764925373134328
[2m[36m(func pid=40628)[0m top5: 0.722481343283582
[2m[36m(func pid=40628)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=40628)[0m f1_macro: 0.16002050598063206
[2m[36m(func pid=40628)[0m f1_weighted: 0.10581177545023843
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.338, 0.468, 0.0, 0.089, 0.229, 0.0, 0.201, 0.141, 0.133]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=31048)[0m top1: 0.20335820895522388
[2m[36m(func pid=31048)[0m top5: 0.726679104477612
[2m[36m(func pid=31048)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=31048)[0m f1_macro: 0.21592370340332553
[2m[36m(func pid=31048)[0m f1_weighted: 0.14402774383576625
[2m[36m(func pid=31048)[0m f1_per_class: [0.163, 0.401, 0.5, 0.01, 0.041, 0.28, 0.0, 0.494, 0.089, 0.182]
[2m[36m(func pid=31048)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.7530 | Steps: 4 | Val loss: 2.5490 | Batch size: 32 | lr: 0.001 | Duration: 2.83s
[2m[36m(func pid=38780)[0m top1: 0.18050373134328357
[2m[36m(func pid=38780)[0m top5: 0.8003731343283582
[2m[36m(func pid=38780)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=38780)[0m f1_macro: 0.16910219940366591
[2m[36m(func pid=38780)[0m f1_weighted: 0.1461294989362245
[2m[36m(func pid=38780)[0m f1_per_class: [0.205, 0.061, 0.211, 0.22, 0.097, 0.341, 0.0, 0.457, 0.063, 0.035]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.14972014925373134
[2m[36m(func pid=39363)[0m top5: 0.7000932835820896
[2m[36m(func pid=39363)[0m f1_micro: 0.14972014925373134
[2m[36m(func pid=39363)[0m f1_macro: 0.1842714004554531
[2m[36m(func pid=39363)[0m f1_weighted: 0.11512668967455002
[2m[36m(func pid=39363)[0m f1_per_class: [0.378, 0.252, 0.299, 0.0, 0.07, 0.228, 0.031, 0.356, 0.15, 0.079]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=31048)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 5.8021 | Steps: 4 | Val loss: 14.9020 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 10.1887 | Steps: 4 | Val loss: 17.3117 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 10:51:50 (running for 00:34:20.61)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (5 PENDING, 3 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.357 |      0.169 |                   65 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.753 |      0.184 |                   63 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   7.319 |      0.16  |                   59 |
| train_952df_00019 | PENDING    |                     | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=31048)[0m top1: 0.1912313432835821
[2m[36m(func pid=31048)[0m top5: 0.8143656716417911
[2m[36m(func pid=31048)[0m f1_micro: 0.19123134328358207
[2m[36m(func pid=31048)[0m f1_macro: 0.16185890194877778
[2m[36m(func pid=31048)[0m f1_weighted: 0.1776705016189112
[2m[36m(func pid=31048)[0m f1_per_class: [0.195, 0.537, 0.196, 0.113, 0.142, 0.0, 0.1, 0.264, 0.072, 0.0]
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.2678 | Steps: 4 | Val loss: 2.1069 | Batch size: 32 | lr: 0.0001 | Duration: 3.03s
[2m[36m(func pid=40628)[0m top1: 0.20522388059701493
[2m[36m(func pid=40628)[0m top5: 0.6198694029850746
[2m[36m(func pid=40628)[0m f1_micro: 0.20522388059701493
[2m[36m(func pid=40628)[0m f1_macro: 0.13328501377853766
[2m[36m(func pid=40628)[0m f1_weighted: 0.11492549505939433
[2m[36m(func pid=40628)[0m f1_per_class: [0.043, 0.412, 0.333, 0.0, 0.086, 0.271, 0.0, 0.134, 0.053, 0.0]
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.4689 | Steps: 4 | Val loss: 2.6642 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m top1: 0.1553171641791045
[2m[36m(func pid=38780)[0m top5: 0.7747201492537313
[2m[36m(func pid=38780)[0m f1_micro: 0.1553171641791045
[2m[36m(func pid=38780)[0m f1_macro: 0.17428931392584235
[2m[36m(func pid=38780)[0m f1_weighted: 0.09935655958376882
[2m[36m(func pid=38780)[0m f1_per_class: [0.269, 0.046, 0.338, 0.048, 0.093, 0.341, 0.0, 0.463, 0.114, 0.031]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.15111940298507462
[2m[36m(func pid=39363)[0m top5: 0.6823694029850746
[2m[36m(func pid=39363)[0m f1_micro: 0.15111940298507462
[2m[36m(func pid=39363)[0m f1_macro: 0.1635569251551199
[2m[36m(func pid=39363)[0m f1_weighted: 0.10406722617240002
[2m[36m(func pid=39363)[0m f1_per_class: [0.317, 0.247, 0.226, 0.007, 0.05, 0.245, 0.0, 0.33, 0.09, 0.124]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 9.5041 | Steps: 4 | Val loss: 15.0252 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.1950 | Steps: 4 | Val loss: 2.1351 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.7247 | Steps: 4 | Val loss: 2.7259 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=40628)[0m top1: 0.1623134328358209
[2m[36m(func pid=40628)[0m top5: 0.6464552238805971
[2m[36m(func pid=40628)[0m f1_micro: 0.1623134328358209
[2m[36m(func pid=40628)[0m f1_macro: 0.15944041623748945
[2m[36m(func pid=40628)[0m f1_weighted: 0.127138015126541
[2m[36m(func pid=40628)[0m f1_per_class: [0.189, 0.416, 0.202, 0.0, 0.035, 0.164, 0.0, 0.481, 0.107, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m top1: 0.14878731343283583
[2m[36m(func pid=38780)[0m top5: 0.7588619402985075
[2m[36m(func pid=38780)[0m f1_micro: 0.14878731343283583
[2m[36m(func pid=38780)[0m f1_macro: 0.17739091673180848
[2m[36m(func pid=38780)[0m f1_weighted: 0.09184445242295747
[2m[36m(func pid=38780)[0m f1_per_class: [0.29, 0.047, 0.345, 0.007, 0.092, 0.356, 0.0, 0.485, 0.124, 0.029]
== Status ==
Current time: 2024-01-07 10:51:56 (running for 00:34:26.56)
Memory usage on this node: 22.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.268 |      0.174 |                   66 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.469 |      0.164 |                   64 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   9.504 |      0.159 |                   61 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=55167)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=55167)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=55167)[0m Configuration completed!
[2m[36m(func pid=55167)[0m New optimizer parameters:
[2m[36m(func pid=55167)[0m SGD (
[2m[36m(func pid=55167)[0m Parameter Group 0
[2m[36m(func pid=55167)[0m     dampening: 0
[2m[36m(func pid=55167)[0m     differentiable: False
[2m[36m(func pid=55167)[0m     foreach: None
[2m[36m(func pid=55167)[0m     lr: 0.1
[2m[36m(func pid=55167)[0m     maximize: False
[2m[36m(func pid=55167)[0m     momentum: 0.99
[2m[36m(func pid=55167)[0m     nesterov: False
[2m[36m(func pid=55167)[0m     weight_decay: 1e-05
[2m[36m(func pid=55167)[0m )
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=39363)[0m top1: 0.15298507462686567
[2m[36m(func pid=39363)[0m top5: 0.7173507462686567
[2m[36m(func pid=39363)[0m f1_micro: 0.15298507462686567
[2m[36m(func pid=39363)[0m f1_macro: 0.16877003235001004
[2m[36m(func pid=39363)[0m f1_weighted: 0.11511168232463727
[2m[36m(func pid=39363)[0m f1_per_class: [0.225, 0.266, 0.182, 0.039, 0.041, 0.22, 0.0, 0.359, 0.111, 0.244]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 6.2738 | Steps: 4 | Val loss: 13.3473 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.4184 | Steps: 4 | Val loss: 2.1573 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.4691 | Steps: 4 | Val loss: 2.5687 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 10:52:02 (running for 00:34:31.94)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.195 |      0.177 |                   67 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.725 |      0.169 |                   65 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.274 |      0.176 |                   62 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |         |            |                      |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.16138059701492538
[2m[36m(func pid=40628)[0m top5: 0.6282649253731343
[2m[36m(func pid=40628)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=40628)[0m f1_macro: 0.17639608210341928
[2m[36m(func pid=40628)[0m f1_weighted: 0.1835447052582886
[2m[36m(func pid=40628)[0m f1_per_class: [0.081, 0.467, 0.209, 0.251, 0.051, 0.0, 0.0, 0.447, 0.044, 0.214]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 8.1764 | Steps: 4 | Val loss: 2.9323 | Batch size: 32 | lr: 0.1 | Duration: 4.49s
[2m[36m(func pid=38780)[0m top1: 0.14085820895522388
[2m[36m(func pid=38780)[0m top5: 0.7444029850746269
[2m[36m(func pid=38780)[0m f1_micro: 0.14085820895522388
[2m[36m(func pid=38780)[0m f1_macro: 0.17563603647347942
[2m[36m(func pid=38780)[0m f1_weighted: 0.08581044816961336
[2m[36m(func pid=38780)[0m f1_per_class: [0.292, 0.026, 0.367, 0.0, 0.095, 0.357, 0.0, 0.471, 0.116, 0.031]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.2042910447761194
[2m[36m(func pid=39363)[0m top5: 0.7854477611940298
[2m[36m(func pid=39363)[0m f1_micro: 0.20429104477611942
[2m[36m(func pid=39363)[0m f1_macro: 0.21470703015501127
[2m[36m(func pid=39363)[0m f1_weighted: 0.20531853453892462
[2m[36m(func pid=39363)[0m f1_per_class: [0.209, 0.233, 0.339, 0.376, 0.039, 0.208, 0.0, 0.418, 0.099, 0.227]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 6.6647 | Steps: 4 | Val loss: 13.2255 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=55167)[0m top1: 0.01166044776119403
[2m[36m(func pid=55167)[0m top5: 0.6539179104477612
[2m[36m(func pid=55167)[0m f1_micro: 0.01166044776119403
[2m[36m(func pid=55167)[0m f1_macro: 0.0023052097740894418
[2m[36m(func pid=55167)[0m f1_weighted: 0.0002687977814936383
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.1922 | Steps: 4 | Val loss: 2.1667 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.7453 | Steps: 4 | Val loss: 2.4553 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 10:52:07 (running for 00:34:37.51)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.418 |      0.176 |                   68 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.469 |      0.215 |                   66 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.665 |      0.194 |                   63 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |   8.176 |      0.002 |                    1 |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.26492537313432835
[2m[36m(func pid=40628)[0m top5: 0.6781716417910447
[2m[36m(func pid=40628)[0m f1_micro: 0.26492537313432835
[2m[36m(func pid=40628)[0m f1_macro: 0.1941089759962507
[2m[36m(func pid=40628)[0m f1_weighted: 0.1923331217945214
[2m[36m(func pid=40628)[0m f1_per_class: [0.134, 0.172, 0.218, 0.461, 0.343, 0.0, 0.0, 0.419, 0.039, 0.154]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 20.1375 | Steps: 4 | Val loss: 11.1412 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=38780)[0m top1: 0.11893656716417911
[2m[36m(func pid=38780)[0m top5: 0.7327425373134329
[2m[36m(func pid=38780)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=38780)[0m f1_macro: 0.17141612346387436
[2m[36m(func pid=38780)[0m f1_weighted: 0.08018031476496454
[2m[36m(func pid=38780)[0m f1_per_class: [0.306, 0.032, 0.4, 0.0, 0.066, 0.301, 0.0, 0.47, 0.11, 0.029]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.23880597014925373
[2m[36m(func pid=39363)[0m top5: 0.8036380597014925
[2m[36m(func pid=39363)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=39363)[0m f1_macro: 0.24122158255915208
[2m[36m(func pid=39363)[0m f1_weighted: 0.218298539958961
[2m[36m(func pid=39363)[0m f1_per_class: [0.312, 0.072, 0.545, 0.533, 0.033, 0.104, 0.006, 0.466, 0.125, 0.216]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 10.1302 | Steps: 4 | Val loss: 11.2857 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=55167)[0m top1: 0.1166044776119403
[2m[36m(func pid=55167)[0m top5: 0.48507462686567165
[2m[36m(func pid=55167)[0m f1_micro: 0.1166044776119403
[2m[36m(func pid=55167)[0m f1_macro: 0.046214904275562116
[2m[36m(func pid=55167)[0m f1_weighted: 0.02526677706183849
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.0, 0.255, 0.0, 0.0, 0.208, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.3456 | Steps: 4 | Val loss: 2.1822 | Batch size: 32 | lr: 0.0001 | Duration: 2.62s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.1146 | Steps: 4 | Val loss: 2.2252 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 10:52:13 (running for 00:34:42.99)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.192 |      0.171 |                   69 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.745 |      0.241 |                   67 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  10.13  |      0.198 |                   64 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  20.138 |      0.046 |                    2 |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.28451492537313433
[2m[36m(func pid=40628)[0m top5: 0.75
[2m[36m(func pid=40628)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=40628)[0m f1_macro: 0.19819908480332618
[2m[36m(func pid=40628)[0m f1_weighted: 0.21116482528341396
[2m[36m(func pid=40628)[0m f1_per_class: [0.157, 0.141, 0.134, 0.517, 0.211, 0.0, 0.006, 0.541, 0.035, 0.24]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 47.4623 | Steps: 4 | Val loss: 29.3456 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=38780)[0m top1: 0.10960820895522388
[2m[36m(func pid=38780)[0m top5: 0.7364738805970149
[2m[36m(func pid=38780)[0m f1_micro: 0.10960820895522388
[2m[36m(func pid=38780)[0m f1_macro: 0.17020438235589735
[2m[36m(func pid=38780)[0m f1_weighted: 0.07651247428871637
[2m[36m(func pid=38780)[0m f1_per_class: [0.286, 0.011, 0.417, 0.0, 0.06, 0.281, 0.003, 0.49, 0.121, 0.033]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.29757462686567165
[2m[36m(func pid=39363)[0m top5: 0.832089552238806
[2m[36m(func pid=39363)[0m f1_micro: 0.29757462686567165
[2m[36m(func pid=39363)[0m f1_macro: 0.2638346226940719
[2m[36m(func pid=39363)[0m f1_weighted: 0.28548936812411824
[2m[36m(func pid=39363)[0m f1_per_class: [0.415, 0.063, 0.564, 0.557, 0.037, 0.101, 0.223, 0.448, 0.026, 0.205]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 6.5635 | Steps: 4 | Val loss: 8.8227 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=55167)[0m top1: 0.009328358208955223
[2m[36m(func pid=55167)[0m top5: 0.20382462686567165
[2m[36m(func pid=55167)[0m f1_micro: 0.009328358208955223
[2m[36m(func pid=55167)[0m f1_macro: 0.029078744209984558
[2m[36m(func pid=55167)[0m f1_weighted: 0.001784054508722471
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.0, 0.276, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.4741 | Steps: 4 | Val loss: 2.1950 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 1.5766 | Steps: 4 | Val loss: 1.9554 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
== Status ==
Current time: 2024-01-07 10:52:18 (running for 00:34:48.63)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.346 |      0.17  |                   70 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   2.115 |      0.264 |                   68 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.563 |      0.203 |                   65 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  47.462 |      0.029 |                    3 |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.28218283582089554
[2m[36m(func pid=40628)[0m top5: 0.8278917910447762
[2m[36m(func pid=40628)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=40628)[0m f1_macro: 0.20251193021605532
[2m[36m(func pid=40628)[0m f1_weighted: 0.32399347133229733
[2m[36m(func pid=40628)[0m f1_per_class: [0.043, 0.162, 0.117, 0.511, 0.174, 0.0, 0.4, 0.491, 0.074, 0.052]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 69.6648 | Steps: 4 | Val loss: 44.2677 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=38780)[0m top1: 0.09841417910447761
[2m[36m(func pid=38780)[0m top5: 0.75
[2m[36m(func pid=38780)[0m f1_micro: 0.0984141791044776
[2m[36m(func pid=38780)[0m f1_macro: 0.15683611245228643
[2m[36m(func pid=38780)[0m f1_weighted: 0.0681824936774739
[2m[36m(func pid=38780)[0m f1_per_class: [0.241, 0.005, 0.415, 0.0, 0.05, 0.233, 0.003, 0.484, 0.105, 0.031]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.3628731343283582
[2m[36m(func pid=39363)[0m top5: 0.8661380597014925
[2m[36m(func pid=39363)[0m f1_micro: 0.3628731343283582
[2m[36m(func pid=39363)[0m f1_macro: 0.22269775824931837
[2m[36m(func pid=39363)[0m f1_weighted: 0.3591890287560733
[2m[36m(func pid=39363)[0m f1_per_class: [0.269, 0.181, 0.273, 0.561, 0.048, 0.105, 0.485, 0.062, 0.052, 0.192]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m top1: 0.017723880597014924
[2m[36m(func pid=55167)[0m top5: 0.4361007462686567
[2m[36m(func pid=55167)[0m f1_micro: 0.017723880597014924
[2m[36m(func pid=55167)[0m f1_macro: 0.01899241616892612
[2m[36m(func pid=55167)[0m f1_weighted: 0.0036821832357827214
[2m[36m(func pid=55167)[0m f1_per_class: [0.166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 7.3748 | Steps: 4 | Val loss: 9.6707 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.6161 | Steps: 4 | Val loss: 2.1601 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.1827 | Steps: 4 | Val loss: 1.9207 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 10:52:24 (running for 00:34:54.04)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.474 |      0.157 |                   71 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.577 |      0.223 |                   69 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   7.375 |      0.139 |                   66 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  69.665 |      0.019 |                    4 |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.25699626865671643
[2m[36m(func pid=40628)[0m top5: 0.8731343283582089
[2m[36m(func pid=40628)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=40628)[0m f1_macro: 0.13898770035837374
[2m[36m(func pid=40628)[0m f1_weighted: 0.2320651934828975
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.232, 0.206, 0.023, 0.111, 0.0, 0.575, 0.201, 0.0, 0.041]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 78.4133 | Steps: 4 | Val loss: 38.6054 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
[2m[36m(func pid=38780)[0m top1: 0.10401119402985075
[2m[36m(func pid=38780)[0m top5: 0.7332089552238806
[2m[36m(func pid=38780)[0m f1_micro: 0.10401119402985075
[2m[36m(func pid=38780)[0m f1_macro: 0.1529764164854714
[2m[36m(func pid=38780)[0m f1_weighted: 0.07012228616662157
[2m[36m(func pid=38780)[0m f1_per_class: [0.212, 0.016, 0.429, 0.0, 0.043, 0.262, 0.006, 0.417, 0.118, 0.028]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m top1: 0.3619402985074627
[2m[36m(func pid=39363)[0m top5: 0.8544776119402985
[2m[36m(func pid=39363)[0m f1_micro: 0.3619402985074627
[2m[36m(func pid=39363)[0m f1_macro: 0.2162731636483599
[2m[36m(func pid=39363)[0m f1_weighted: 0.3785637241351263
[2m[36m(func pid=39363)[0m f1_per_class: [0.288, 0.426, 0.107, 0.456, 0.068, 0.032, 0.547, 0.0, 0.071, 0.167]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 8.3163 | Steps: 4 | Val loss: 7.8764 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
[2m[36m(func pid=55167)[0m top1: 0.017257462686567165
[2m[36m(func pid=55167)[0m top5: 0.4939365671641791
[2m[36m(func pid=55167)[0m f1_micro: 0.017257462686567165
[2m[36m(func pid=55167)[0m f1_macro: 0.010510446546060415
[2m[36m(func pid=55167)[0m f1_weighted: 0.0030104471642243185
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.018, 0.0, 0.0, 0.0, 0.087, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.2492 | Steps: 4 | Val loss: 2.1523 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=40628)[0m top1: 0.38526119402985076
[2m[36m(func pid=40628)[0m top5: 0.8218283582089553
[2m[36m(func pid=40628)[0m f1_micro: 0.38526119402985076
[2m[36m(func pid=40628)[0m f1_macro: 0.253570887655642
[2m[36m(func pid=40628)[0m f1_weighted: 0.2877892662505965
[2m[36m(func pid=40628)[0m f1_per_class: [0.286, 0.502, 0.429, 0.054, 0.222, 0.0, 0.544, 0.163, 0.028, 0.308]
== Status ==
Current time: 2024-01-07 10:52:29 (running for 00:34:59.39)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.616 |      0.153 |                   72 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   2.183 |      0.216 |                   70 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   8.316 |      0.254 |                   67 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  78.413 |      0.011 |                    5 |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.2603 | Steps: 4 | Val loss: 1.9503 | Batch size: 32 | lr: 0.001 | Duration: 3.20s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 59.2184 | Steps: 4 | Val loss: 58.9826 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=38780)[0m top1: 0.09468283582089553
[2m[36m(func pid=38780)[0m top5: 0.7430037313432836
[2m[36m(func pid=38780)[0m f1_micro: 0.09468283582089553
[2m[36m(func pid=38780)[0m f1_macro: 0.13670596049744382
[2m[36m(func pid=38780)[0m f1_weighted: 0.05461619455912198
[2m[36m(func pid=38780)[0m f1_per_class: [0.206, 0.026, 0.435, 0.0, 0.041, 0.145, 0.003, 0.368, 0.113, 0.029]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 6.3980 | Steps: 4 | Val loss: 7.3383 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=39363)[0m top1: 0.32882462686567165
[2m[36m(func pid=39363)[0m top5: 0.8456156716417911
[2m[36m(func pid=39363)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=39363)[0m f1_macro: 0.19057512579381636
[2m[36m(func pid=39363)[0m f1_weighted: 0.3161649474735717
[2m[36m(func pid=39363)[0m f1_per_class: [0.236, 0.477, 0.103, 0.204, 0.092, 0.016, 0.554, 0.0, 0.077, 0.147]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m top1: 0.18330223880597016
[2m[36m(func pid=55167)[0m top5: 0.3894589552238806
[2m[36m(func pid=55167)[0m f1_micro: 0.18330223880597016
[2m[36m(func pid=55167)[0m f1_macro: 0.0612771365032299
[2m[36m(func pid=55167)[0m f1_weighted: 0.0693769648527457
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.316, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.4319 | Steps: 4 | Val loss: 2.1487 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
== Status ==
Current time: 2024-01-07 10:52:35 (running for 00:35:04.81)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.249 |      0.137 |                   73 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.26  |      0.191 |                   71 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.398 |      0.246 |                   68 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  59.218 |      0.061 |                    6 |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.23740671641791045
[2m[36m(func pid=40628)[0m top5: 0.8041044776119403
[2m[36m(func pid=40628)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=40628)[0m f1_macro: 0.2455384144998765
[2m[36m(func pid=40628)[0m f1_weighted: 0.20234933268433825
[2m[36m(func pid=40628)[0m f1_per_class: [0.302, 0.495, 0.6, 0.089, 0.24, 0.0, 0.184, 0.387, 0.082, 0.077]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=38780)[0m top1: 0.09001865671641791
[2m[36m(func pid=38780)[0m top5: 0.75
[2m[36m(func pid=38780)[0m f1_micro: 0.0900186567164179
[2m[36m(func pid=38780)[0m f1_macro: 0.1273459651031076
[2m[36m(func pid=38780)[0m f1_weighted: 0.04496393027183769
[2m[36m(func pid=38780)[0m f1_per_class: [0.26, 0.032, 0.393, 0.0, 0.037, 0.045, 0.006, 0.352, 0.114, 0.034]
[2m[36m(func pid=38780)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 1.2880 | Steps: 4 | Val loss: 1.9717 | Batch size: 32 | lr: 0.001 | Duration: 2.85s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 76.0557 | Steps: 4 | Val loss: 68.0712 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 7.1467 | Steps: 4 | Val loss: 9.6386 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=39363)[0m top1: 0.3283582089552239
[2m[36m(func pid=39363)[0m top5: 0.8288246268656716
[2m[36m(func pid=39363)[0m f1_micro: 0.3283582089552239
[2m[36m(func pid=39363)[0m f1_macro: 0.22343945685650796
[2m[36m(func pid=39363)[0m f1_weighted: 0.30875726088488326
[2m[36m(func pid=39363)[0m f1_per_class: [0.215, 0.436, 0.208, 0.117, 0.119, 0.031, 0.564, 0.316, 0.095, 0.134]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m top1: 0.11800373134328358
[2m[36m(func pid=55167)[0m top5: 0.36007462686567165
[2m[36m(func pid=55167)[0m f1_micro: 0.11800373134328358
[2m[36m(func pid=55167)[0m f1_macro: 0.059543839319508005
[2m[36m(func pid=55167)[0m f1_weighted: 0.02655098941816777
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.0, 0.232, 0.0, 0.154, 0.21, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=38780)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.3810 | Steps: 4 | Val loss: 2.1187 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=40628)[0m top1: 0.2150186567164179
[2m[36m(func pid=40628)[0m top5: 0.715018656716418
[2m[36m(func pid=40628)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=40628)[0m f1_macro: 0.1656527554052698
[2m[36m(func pid=40628)[0m f1_weighted: 0.14407710238218488
[2m[36m(func pid=40628)[0m f1_per_class: [0.283, 0.496, 0.143, 0.077, 0.187, 0.082, 0.0, 0.278, 0.11, 0.0]
[2m[36m(func pid=40628)[0m 
== Status ==
Current time: 2024-01-07 10:52:40 (running for 00:35:10.10)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=16
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 PENDING, 4 RUNNING, 16 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00016 | RUNNING    | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.432 |      0.127 |                   74 |
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.288 |      0.223 |                   72 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   7.147 |      0.166 |                   69 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  76.056 |      0.06  |                    7 |
| train_952df_00020 | PENDING    |                     | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=38780)[0m top1: 0.09468283582089553
[2m[36m(func pid=38780)[0m top5: 0.7649253731343284
[2m[36m(func pid=38780)[0m f1_micro: 0.09468283582089553
[2m[36m(func pid=38780)[0m f1_macro: 0.13525411076550034
[2m[36m(func pid=38780)[0m f1_weighted: 0.05562061651867055
[2m[36m(func pid=38780)[0m f1_per_class: [0.297, 0.075, 0.423, 0.029, 0.033, 0.0, 0.006, 0.338, 0.114, 0.037]
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 1.6181 | Steps: 4 | Val loss: 1.9977 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 83.1104 | Steps: 4 | Val loss: 36.8092 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 5.7549 | Steps: 4 | Val loss: 11.9088 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=55167)[0m top1: 0.09421641791044776
[2m[36m(func pid=55167)[0m top5: 0.777518656716418
[2m[36m(func pid=55167)[0m f1_micro: 0.09421641791044776
[2m[36m(func pid=55167)[0m f1_macro: 0.09076534988202442
[2m[36m(func pid=55167)[0m f1_weighted: 0.07546968390061175
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.0, 0.455, 0.216, 0.026, 0.0, 0.0, 0.211, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=39363)[0m top1: 0.2868470149253731
[2m[36m(func pid=39363)[0m top5: 0.8236940298507462
[2m[36m(func pid=39363)[0m f1_micro: 0.2868470149253731
[2m[36m(func pid=39363)[0m f1_macro: 0.2631203545457217
[2m[36m(func pid=39363)[0m f1_weighted: 0.29386525711472095
[2m[36m(func pid=39363)[0m f1_per_class: [0.23, 0.416, 0.5, 0.196, 0.151, 0.087, 0.4, 0.399, 0.16, 0.091]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=40628)[0m top1: 0.22434701492537312
[2m[36m(func pid=40628)[0m top5: 0.6343283582089553
[2m[36m(func pid=40628)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=40628)[0m f1_macro: 0.19955382858921344
[2m[36m(func pid=40628)[0m f1_weighted: 0.17424967008651424
[2m[36m(func pid=40628)[0m f1_per_class: [0.221, 0.438, 0.267, 0.057, 0.05, 0.439, 0.0, 0.362, 0.162, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 1.5574 | Steps: 4 | Val loss: 2.1001 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 116.6117 | Steps: 4 | Val loss: 33.1599 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 8.7094 | Steps: 4 | Val loss: 14.4432 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 10:52:45 (running for 00:35:15.32)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.233
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.618 |      0.263 |                   73 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   5.755 |      0.2   |                   70 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  83.11  |      0.091 |                    8 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=57543)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=57543)[0m Configuration completed!
[2m[36m(func pid=57543)[0m New optimizer parameters:
[2m[36m(func pid=57543)[0m SGD (
[2m[36m(func pid=57543)[0m Parameter Group 0
[2m[36m(func pid=57543)[0m     dampening: 0
[2m[36m(func pid=57543)[0m     differentiable: False
[2m[36m(func pid=57543)[0m     foreach: None
[2m[36m(func pid=57543)[0m     lr: 0.0001
[2m[36m(func pid=57543)[0m     maximize: False
[2m[36m(func pid=57543)[0m     momentum: 0.9
[2m[36m(func pid=57543)[0m     nesterov: False
[2m[36m(func pid=57543)[0m     weight_decay: 1e-05
[2m[36m(func pid=57543)[0m )
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.3003731343283582
[2m[36m(func pid=55167)[0m top5: 0.7336753731343284
[2m[36m(func pid=55167)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=55167)[0m f1_macro: 0.07872988499056827
[2m[36m(func pid=55167)[0m f1_weighted: 0.21758093671935397
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.0, 0.0, 0.185, 0.0, 0.0, 0.555, 0.0, 0.0, 0.047]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=39363)[0m top1: 0.26632462686567165
[2m[36m(func pid=39363)[0m top5: 0.8134328358208955
[2m[36m(func pid=39363)[0m f1_micro: 0.26632462686567165
[2m[36m(func pid=39363)[0m f1_macro: 0.251680511228077
[2m[36m(func pid=39363)[0m f1_weighted: 0.2588593132254336
[2m[36m(func pid=39363)[0m f1_per_class: [0.293, 0.411, 0.432, 0.422, 0.159, 0.207, 0.04, 0.342, 0.136, 0.074]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:52:50 (running for 00:35:20.58)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.233
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.557 |      0.252 |                   74 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   8.709 |      0.226 |                   71 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 116.612 |      0.079 |                    9 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |         |            |                      |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.1669776119402985
[2m[36m(func pid=40628)[0m top5: 0.6581156716417911
[2m[36m(func pid=40628)[0m f1_micro: 0.1669776119402985
[2m[36m(func pid=40628)[0m f1_macro: 0.2255897162049462
[2m[36m(func pid=40628)[0m f1_weighted: 0.1419247565973997
[2m[36m(func pid=40628)[0m f1_per_class: [0.292, 0.189, 0.632, 0.08, 0.031, 0.374, 0.0, 0.5, 0.158, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 92.4047 | Steps: 4 | Val loss: 51.5921 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.4138 | Steps: 4 | Val loss: 2.1772 | Batch size: 32 | lr: 0.001 | Duration: 3.17s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1417 | Steps: 4 | Val loss: 2.3277 | Batch size: 32 | lr: 0.0001 | Duration: 4.34s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 11.8753 | Steps: 4 | Val loss: 14.3826 | Batch size: 32 | lr: 0.01 | Duration: 3.03s
[2m[36m(func pid=55167)[0m top1: 0.20335820895522388
[2m[36m(func pid=55167)[0m top5: 0.5345149253731343
[2m[36m(func pid=55167)[0m f1_micro: 0.20335820895522388
[2m[36m(func pid=55167)[0m f1_macro: 0.11165436024287381
[2m[36m(func pid=55167)[0m f1_weighted: 0.1035408527239423
[2m[36m(func pid=55167)[0m f1_per_class: [0.153, 0.344, 0.0, 0.0, 0.0, 0.0, 0.034, 0.528, 0.0, 0.058]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=39363)[0m top1: 0.28125
[2m[36m(func pid=39363)[0m top5: 0.8013059701492538
[2m[36m(func pid=39363)[0m f1_micro: 0.28125
[2m[36m(func pid=39363)[0m f1_macro: 0.26893837343488286
[2m[36m(func pid=39363)[0m f1_weighted: 0.26413159032592926
[2m[36m(func pid=39363)[0m f1_per_class: [0.381, 0.317, 0.579, 0.526, 0.13, 0.233, 0.0, 0.336, 0.111, 0.077]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=57543)[0m top1: 0.14412313432835822
[2m[36m(func pid=57543)[0m top5: 0.5340485074626866
[2m[36m(func pid=57543)[0m f1_micro: 0.14412313432835822
[2m[36m(func pid=57543)[0m f1_macro: 0.04262512686330063
[2m[36m(func pid=57543)[0m f1_weighted: 0.08015660485578102
[2m[36m(func pid=57543)[0m f1_per_class: [0.0, 0.0, 0.0, 0.251, 0.0, 0.0, 0.0, 0.175, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
== Status ==
Current time: 2024-01-07 10:52:56 (running for 00:35:26.06)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.2465
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.414 |      0.269 |                   75 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |  11.875 |      0.227 |                   72 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  92.405 |      0.112 |                   10 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   3.142 |      0.043 |                    1 |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.16324626865671643
[2m[36m(func pid=40628)[0m top5: 0.722481343283582
[2m[36m(func pid=40628)[0m f1_micro: 0.16324626865671643
[2m[36m(func pid=40628)[0m f1_macro: 0.22745445647835147
[2m[36m(func pid=40628)[0m f1_weighted: 0.13945607783690678
[2m[36m(func pid=40628)[0m f1_per_class: [0.275, 0.216, 0.692, 0.074, 0.034, 0.341, 0.0, 0.459, 0.183, 0.0]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 61.3442 | Steps: 4 | Val loss: 69.9708 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 75] | Train loss: 1.4817 | Steps: 4 | Val loss: 2.2222 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 3.0381 | Steps: 4 | Val loss: 2.3123 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 7.0160 | Steps: 4 | Val loss: 12.2070 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=55167)[0m top1: 0.06576492537313433
[2m[36m(func pid=55167)[0m top5: 0.6236007462686567
[2m[36m(func pid=55167)[0m f1_micro: 0.06576492537313433
[2m[36m(func pid=55167)[0m f1_macro: 0.05885875762454351
[2m[36m(func pid=55167)[0m f1_weighted: 0.048345517611090384
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.193, 0.129, 0.0, 0.022, 0.0, 0.0, 0.244, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=39363)[0m top1: 0.3087686567164179
[2m[36m(func pid=39363)[0m top5: 0.7882462686567164
[2m[36m(func pid=39363)[0m f1_micro: 0.3087686567164179
[2m[36m(func pid=39363)[0m f1_macro: 0.2646161419655789
[2m[36m(func pid=39363)[0m f1_weighted: 0.2719081349619557
[2m[36m(func pid=39363)[0m f1_per_class: [0.3, 0.242, 0.512, 0.557, 0.112, 0.355, 0.0, 0.325, 0.119, 0.125]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=57543)[0m top1: 0.197294776119403
[2m[36m(func pid=57543)[0m top5: 0.5811567164179104
[2m[36m(func pid=57543)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=57543)[0m f1_macro: 0.040076287705619204
[2m[36m(func pid=57543)[0m f1_weighted: 0.100907054603436
[2m[36m(func pid=57543)[0m f1_per_class: [0.0, 0.0, 0.0, 0.352, 0.0, 0.0, 0.0, 0.049, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
== Status ==
Current time: 2024-01-07 10:53:01 (running for 00:35:31.56)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.2465
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.482 |      0.265 |                   76 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   7.016 |      0.193 |                   73 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  61.344 |      0.059 |                   11 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   3.038 |      0.04  |                    2 |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.20242537313432835
[2m[36m(func pid=40628)[0m top5: 0.7565298507462687
[2m[36m(func pid=40628)[0m f1_micro: 0.20242537313432832
[2m[36m(func pid=40628)[0m f1_macro: 0.19300286863118837
[2m[36m(func pid=40628)[0m f1_weighted: 0.14155489308030592
[2m[36m(func pid=40628)[0m f1_per_class: [0.218, 0.362, 0.329, 0.01, 0.067, 0.325, 0.018, 0.348, 0.179, 0.074]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 100.9108 | Steps: 4 | Val loss: 82.6564 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 76] | Train loss: 1.2913 | Steps: 4 | Val loss: 2.2727 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.9922 | Steps: 4 | Val loss: 2.3186 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 4.1863 | Steps: 4 | Val loss: 9.0571 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=55167)[0m top1: 0.06436567164179105
[2m[36m(func pid=55167)[0m top5: 0.5769589552238806
[2m[36m(func pid=55167)[0m f1_micro: 0.06436567164179105
[2m[36m(func pid=55167)[0m f1_macro: 0.11586111707907394
[2m[36m(func pid=55167)[0m f1_weighted: 0.044963176311478566
[2m[36m(func pid=55167)[0m f1_per_class: [0.154, 0.0, 0.344, 0.0, 0.286, 0.31, 0.0, 0.0, 0.065, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=39363)[0m top1: 0.302705223880597
[2m[36m(func pid=39363)[0m top5: 0.78125
[2m[36m(func pid=39363)[0m f1_micro: 0.302705223880597
[2m[36m(func pid=39363)[0m f1_macro: 0.23784514184812305
[2m[36m(func pid=39363)[0m f1_weighted: 0.25422711268339676
[2m[36m(func pid=39363)[0m f1_per_class: [0.303, 0.173, 0.209, 0.529, 0.099, 0.346, 0.0, 0.396, 0.12, 0.205]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=57543)[0m top1: 0.21548507462686567
[2m[36m(func pid=57543)[0m top5: 0.5774253731343284
[2m[36m(func pid=57543)[0m f1_micro: 0.21548507462686567
[2m[36m(func pid=57543)[0m f1_macro: 0.04012689463764664
[2m[36m(func pid=57543)[0m f1_weighted: 0.10605684964367756
[2m[36m(func pid=57543)[0m f1_per_class: [0.0, 0.0, 0.0, 0.375, 0.0, 0.0, 0.0, 0.027, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
== Status ==
Current time: 2024-01-07 10:53:07 (running for 00:35:36.82)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=17
Bracket: Iter 75.000: 0.2465
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 4 RUNNING, 17 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.291 |      0.238 |                   77 |
| train_952df_00018 | RUNNING    | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   4.186 |      0.217 |                   74 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 100.911 |      0.116 |                   12 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.992 |      0.04  |                    3 |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.3003731343283582
[2m[36m(func pid=40628)[0m top5: 0.7630597014925373
[2m[36m(func pid=40628)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=40628)[0m f1_macro: 0.21748130672484475
[2m[36m(func pid=40628)[0m f1_weighted: 0.26433412168806386
[2m[36m(func pid=40628)[0m f1_per_class: [0.043, 0.455, 0.314, 0.0, 0.138, 0.436, 0.39, 0.188, 0.129, 0.081]
[2m[36m(func pid=40628)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 65.4441 | Steps: 4 | Val loss: 56.2988 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.9563 | Steps: 4 | Val loss: 2.3187 | Batch size: 32 | lr: 0.0001 | Duration: 2.71s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 77] | Train loss: 1.3351 | Steps: 4 | Val loss: 2.3491 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=40628)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 6.7192 | Steps: 4 | Val loss: 9.1164 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=55167)[0m top1: 0.24253731343283583
[2m[36m(func pid=55167)[0m top5: 0.6646455223880597
[2m[36m(func pid=55167)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=55167)[0m f1_macro: 0.14423788677096672
[2m[36m(func pid=55167)[0m f1_weighted: 0.15608806458580604
[2m[36m(func pid=55167)[0m f1_per_class: [0.244, 0.0, 0.514, 0.0, 0.0, 0.304, 0.38, 0.0, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.23087686567164178
[2m[36m(func pid=57543)[0m top5: 0.5727611940298507
[2m[36m(func pid=57543)[0m f1_micro: 0.23087686567164178
[2m[36m(func pid=57543)[0m f1_macro: 0.04541670020533815
[2m[36m(func pid=57543)[0m f1_weighted: 0.11105783407657657
[2m[36m(func pid=57543)[0m f1_per_class: [0.0, 0.0, 0.0, 0.39, 0.0, 0.0, 0.0, 0.032, 0.0, 0.032]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=39363)[0m top1: 0.2943097014925373
[2m[36m(func pid=39363)[0m top5: 0.7840485074626866
[2m[36m(func pid=39363)[0m f1_micro: 0.2943097014925373
[2m[36m(func pid=39363)[0m f1_macro: 0.24879838734593437
[2m[36m(func pid=39363)[0m f1_weighted: 0.25534986361094
[2m[36m(func pid=39363)[0m f1_per_class: [0.286, 0.176, 0.173, 0.526, 0.082, 0.296, 0.0, 0.46, 0.22, 0.27]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:53:12 (running for 00:35:42.05)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 PENDING, 3 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.335 |      0.249 |                   78 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  65.444 |      0.144 |                   13 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.956 |      0.045 |                    4 |
| train_952df_00021 | PENDING    |                     | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=40628)[0m top1: 0.22294776119402984
[2m[36m(func pid=40628)[0m top5: 0.8227611940298507
[2m[36m(func pid=40628)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=40628)[0m f1_macro: 0.17199431206524485
[2m[36m(func pid=40628)[0m f1_weighted: 0.2256271488834995
[2m[36m(func pid=40628)[0m f1_per_class: [0.0, 0.438, 0.444, 0.036, 0.245, 0.024, 0.439, 0.0, 0.055, 0.039]
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 51.7483 | Steps: 4 | Val loss: 38.2348 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9680 | Steps: 4 | Val loss: 2.3244 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 78] | Train loss: 1.3000 | Steps: 4 | Val loss: 2.1869 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=55167)[0m top1: 0.28544776119402987
[2m[36m(func pid=55167)[0m top5: 0.7933768656716418
[2m[36m(func pid=55167)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=55167)[0m f1_macro: 0.18404010009619923
[2m[36m(func pid=55167)[0m f1_weighted: 0.17809426530954806
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.391, 0.488, 0.0, 0.0, 0.433, 0.116, 0.413, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.23367537313432835
[2m[36m(func pid=57543)[0m top5: 0.5522388059701493
[2m[36m(func pid=57543)[0m f1_micro: 0.23367537313432835
[2m[36m(func pid=57543)[0m f1_macro: 0.052213084286470327
[2m[36m(func pid=57543)[0m f1_weighted: 0.1116305117761654
[2m[36m(func pid=57543)[0m f1_per_class: [0.0, 0.0, 0.08, 0.393, 0.0, 0.0, 0.0, 0.022, 0.0, 0.027]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=39363)[0m top1: 0.30597014925373134
[2m[36m(func pid=39363)[0m top5: 0.8227611940298507
[2m[36m(func pid=39363)[0m f1_micro: 0.30597014925373134
[2m[36m(func pid=39363)[0m f1_macro: 0.26976537713691096
[2m[36m(func pid=39363)[0m f1_weighted: 0.25708607528261734
[2m[36m(func pid=39363)[0m f1_per_class: [0.33, 0.126, 0.387, 0.546, 0.071, 0.303, 0.0, 0.505, 0.217, 0.213]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 34.2348 | Steps: 4 | Val loss: 38.3365 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.8875 | Steps: 4 | Val loss: 2.3253 | Batch size: 32 | lr: 0.0001 | Duration: 3.05s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 79] | Train loss: 1.3480 | Steps: 4 | Val loss: 2.0790 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=59162)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=59162)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=59162)[0m Configuration completed!
[2m[36m(func pid=59162)[0m New optimizer parameters:
[2m[36m(func pid=59162)[0m SGD (
[2m[36m(func pid=59162)[0m Parameter Group 0
[2m[36m(func pid=59162)[0m     dampening: 0
[2m[36m(func pid=59162)[0m     differentiable: False
[2m[36m(func pid=59162)[0m     foreach: None
[2m[36m(func pid=59162)[0m     lr: 0.001
[2m[36m(func pid=59162)[0m     maximize: False
[2m[36m(func pid=59162)[0m     momentum: 0.9
[2m[36m(func pid=59162)[0m     nesterov: False
[2m[36m(func pid=59162)[0m     weight_decay: 1e-05
[2m[36m(func pid=59162)[0m )
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 10:53:21 (running for 00:35:51.30)
Memory usage on this node: 24.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.3   |      0.27  |                   79 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  34.235 |      0.133 |                   15 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.968 |      0.052 |                    5 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=55167)[0m top1: 0.2733208955223881
[2m[36m(func pid=55167)[0m top5: 0.7868470149253731
[2m[36m(func pid=55167)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=55167)[0m f1_macro: 0.1332220473456531
[2m[36m(func pid=55167)[0m f1_weighted: 0.16146852680921417
[2m[36m(func pid=55167)[0m f1_per_class: [0.255, 0.0, 0.314, 0.484, 0.053, 0.0, 0.024, 0.201, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.2462686567164179
[2m[36m(func pid=57543)[0m top5: 0.542910447761194
[2m[36m(func pid=57543)[0m f1_micro: 0.2462686567164179
[2m[36m(func pid=57543)[0m f1_macro: 0.06054838827394733
[2m[36m(func pid=57543)[0m f1_weighted: 0.1164076921197569
[2m[36m(func pid=57543)[0m f1_per_class: [0.0, 0.005, 0.174, 0.406, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=39363)[0m top1: 0.3125
[2m[36m(func pid=39363)[0m top5: 0.8446828358208955
[2m[36m(func pid=39363)[0m f1_micro: 0.3125
[2m[36m(func pid=39363)[0m f1_macro: 0.282901914254151
[2m[36m(func pid=39363)[0m f1_weighted: 0.2913889818975074
[2m[36m(func pid=39363)[0m f1_per_class: [0.352, 0.233, 0.444, 0.552, 0.064, 0.276, 0.068, 0.473, 0.178, 0.19]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 41.0745 | Steps: 4 | Val loss: 67.8279 | Batch size: 32 | lr: 0.1 | Duration: 3.13s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.8498 | Steps: 4 | Val loss: 2.3225 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.0481 | Steps: 4 | Val loss: 2.3172 | Batch size: 32 | lr: 0.001 | Duration: 4.52s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 80] | Train loss: 1.2809 | Steps: 4 | Val loss: 1.9587 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 10:53:27 (running for 00:35:56.83)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.348 |      0.283 |                   80 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  41.075 |      0.063 |                   16 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.888 |      0.061 |                    6 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |         |            |                      |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=55167)[0m top1: 0.04710820895522388
[2m[36m(func pid=55167)[0m top5: 0.5988805970149254
[2m[36m(func pid=55167)[0m f1_micro: 0.04710820895522388
[2m[36m(func pid=55167)[0m f1_macro: 0.06256838785653618
[2m[36m(func pid=55167)[0m f1_weighted: 0.033116221467981315
[2m[36m(func pid=55167)[0m f1_per_class: [0.052, 0.0, 0.297, 0.089, 0.045, 0.0, 0.012, 0.0, 0.0, 0.13]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.23880597014925373
[2m[36m(func pid=57543)[0m top5: 0.5480410447761194
[2m[36m(func pid=57543)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=57543)[0m f1_macro: 0.05403420957932662
[2m[36m(func pid=57543)[0m f1_weighted: 0.11563633684354856
[2m[36m(func pid=57543)[0m f1_per_class: [0.035, 0.005, 0.083, 0.404, 0.0, 0.0, 0.0, 0.012, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.1791044776119403
[2m[36m(func pid=59162)[0m top5: 0.5331156716417911
[2m[36m(func pid=59162)[0m f1_micro: 0.17910447761194032
[2m[36m(func pid=59162)[0m f1_macro: 0.05810198186900865
[2m[36m(func pid=59162)[0m f1_weighted: 0.10233770407595973
[2m[36m(func pid=59162)[0m f1_per_class: [0.034, 0.0, 0.0, 0.317, 0.0, 0.0, 0.0, 0.231, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=39363)[0m top1: 0.3460820895522388
[2m[36m(func pid=39363)[0m top5: 0.8614738805970149
[2m[36m(func pid=39363)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=39363)[0m f1_macro: 0.3094534838608256
[2m[36m(func pid=39363)[0m f1_weighted: 0.3602640455081051
[2m[36m(func pid=39363)[0m f1_per_class: [0.36, 0.322, 0.522, 0.55, 0.06, 0.293, 0.258, 0.381, 0.187, 0.162]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 49.0379 | Steps: 4 | Val loss: 66.9639 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.8976 | Steps: 4 | Val loss: 2.3239 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9126 | Steps: 4 | Val loss: 2.2906 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 81] | Train loss: 1.1243 | Steps: 4 | Val loss: 1.8314 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:53:32 (running for 00:36:02.06)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.281 |      0.309 |                   81 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  49.038 |      0.122 |                   17 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.85  |      0.054 |                    7 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   3.048 |      0.058 |                    1 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=55167)[0m top1: 0.20942164179104478
[2m[36m(func pid=55167)[0m top5: 0.5298507462686567
[2m[36m(func pid=55167)[0m f1_micro: 0.20942164179104478
[2m[36m(func pid=55167)[0m f1_macro: 0.12152108298814249
[2m[36m(func pid=55167)[0m f1_weighted: 0.17341111039438006
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.0, 0.31, 0.013, 0.166, 0.0, 0.545, 0.0, 0.094, 0.087]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.20382462686567165
[2m[36m(func pid=57543)[0m top5: 0.5466417910447762
[2m[36m(func pid=57543)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=57543)[0m f1_macro: 0.042557471264367805
[2m[36m(func pid=57543)[0m f1_weighted: 0.10490087279121632
[2m[36m(func pid=57543)[0m f1_per_class: [0.0, 0.0, 0.051, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.22388059701492538
[2m[36m(func pid=59162)[0m top5: 0.5699626865671642
[2m[36m(func pid=59162)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=59162)[0m f1_macro: 0.06883921346944658
[2m[36m(func pid=59162)[0m f1_weighted: 0.10762535458907373
[2m[36m(func pid=59162)[0m f1_per_class: [0.132, 0.0, 0.184, 0.372, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=39363)[0m top1: 0.3763992537313433
[2m[36m(func pid=39363)[0m top5: 0.8768656716417911
[2m[36m(func pid=39363)[0m f1_micro: 0.3763992537313433
[2m[36m(func pid=39363)[0m f1_macro: 0.328551023683199
[2m[36m(func pid=39363)[0m f1_weighted: 0.416440207463854
[2m[36m(func pid=39363)[0m f1_per_class: [0.368, 0.432, 0.632, 0.506, 0.052, 0.293, 0.463, 0.163, 0.176, 0.2]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 88.8270 | Steps: 4 | Val loss: 65.4704 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.8245 | Steps: 4 | Val loss: 2.3226 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 2.8864 | Steps: 4 | Val loss: 2.2711 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 82] | Train loss: 1.7482 | Steps: 4 | Val loss: 1.8339 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 10:53:37 (running for 00:36:07.26)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.124 |      0.329 |                   82 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  88.827 |      0.129 |                   18 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.898 |      0.043 |                    8 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.913 |      0.069 |                    2 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=55167)[0m top1: 0.2555970149253731
[2m[36m(func pid=55167)[0m top5: 0.5839552238805971
[2m[36m(func pid=55167)[0m f1_micro: 0.2555970149253731
[2m[36m(func pid=55167)[0m f1_macro: 0.1294536853840534
[2m[36m(func pid=55167)[0m f1_weighted: 0.20490574731103522
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.0, 0.074, 0.088, 0.324, 0.0, 0.571, 0.057, 0.097, 0.084]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.1669776119402985
[2m[36m(func pid=57543)[0m top5: 0.5443097014925373
[2m[36m(func pid=57543)[0m f1_micro: 0.1669776119402985
[2m[36m(func pid=57543)[0m f1_macro: 0.04096645509945346
[2m[36m(func pid=57543)[0m f1_weighted: 0.09747631451904895
[2m[36m(func pid=57543)[0m f1_per_class: [0.0, 0.005, 0.034, 0.339, 0.0, 0.0, 0.0, 0.031, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.12453358208955224
[2m[36m(func pid=59162)[0m top5: 0.5620335820895522
[2m[36m(func pid=59162)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=59162)[0m f1_macro: 0.0369862258953168
[2m[36m(func pid=59162)[0m f1_weighted: 0.09770411886846755
[2m[36m(func pid=59162)[0m f1_per_class: [0.0, 0.0, 0.02, 0.35, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=39363)[0m top1: 0.3736007462686567
[2m[36m(func pid=39363)[0m top5: 0.8708022388059702
[2m[36m(func pid=39363)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=39363)[0m f1_macro: 0.2995650412804677
[2m[36m(func pid=39363)[0m f1_weighted: 0.388962377924323
[2m[36m(func pid=39363)[0m f1_per_class: [0.381, 0.481, 0.524, 0.308, 0.062, 0.247, 0.561, 0.135, 0.105, 0.192]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 54.0898 | Steps: 4 | Val loss: 37.3474 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.8538 | Steps: 4 | Val loss: 2.3250 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 2.7883 | Steps: 4 | Val loss: 2.2466 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 83] | Train loss: 1.2826 | Steps: 4 | Val loss: 1.9411 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 10:53:42 (running for 00:36:12.62)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.748 |      0.3   |                   83 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  54.09  |      0.114 |                   19 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.824 |      0.041 |                    9 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.886 |      0.037 |                    3 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=55167)[0m top1: 0.2140858208955224
[2m[36m(func pid=55167)[0m top5: 0.8143656716417911
[2m[36m(func pid=55167)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=55167)[0m f1_macro: 0.11416595545462563
[2m[36m(func pid=55167)[0m f1_weighted: 0.10778462939652846
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.46, 0.17, 0.003, 0.0, 0.0, 0.025, 0.23, 0.144, 0.11]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.11240671641791045
[2m[36m(func pid=57543)[0m top5: 0.539179104477612
[2m[36m(func pid=57543)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=57543)[0m f1_macro: 0.03270317423632114
[2m[36m(func pid=57543)[0m f1_weighted: 0.0801981626470505
[2m[36m(func pid=57543)[0m f1_per_class: [0.0, 0.011, 0.023, 0.277, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.18703358208955223
[2m[36m(func pid=59162)[0m top5: 0.6627798507462687
[2m[36m(func pid=59162)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=59162)[0m f1_macro: 0.051144868124299615
[2m[36m(func pid=59162)[0m f1_weighted: 0.10595643130985034
[2m[36m(func pid=59162)[0m f1_per_class: [0.102, 0.0, 0.038, 0.372, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=39363)[0m top1: 0.34468283582089554
[2m[36m(func pid=39363)[0m top5: 0.8572761194029851
[2m[36m(func pid=39363)[0m f1_micro: 0.34468283582089554
[2m[36m(func pid=39363)[0m f1_macro: 0.2644485842889371
[2m[36m(func pid=39363)[0m f1_weighted: 0.3497352187008496
[2m[36m(func pid=39363)[0m f1_per_class: [0.361, 0.472, 0.393, 0.238, 0.055, 0.091, 0.552, 0.188, 0.129, 0.167]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 30.0313 | Steps: 4 | Val loss: 31.3003 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9124 | Steps: 4 | Val loss: 2.3343 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 2.9372 | Steps: 4 | Val loss: 2.2128 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 84] | Train loss: 1.2130 | Steps: 4 | Val loss: 2.0131 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 10:53:48 (running for 00:36:17.82)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.283 |      0.264 |                   84 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  54.09  |      0.114 |                   19 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.912 |      0.02  |                   11 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.788 |      0.051 |                    4 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.05970149253731343
[2m[36m(func pid=57543)[0m top5: 0.5214552238805971
[2m[36m(func pid=57543)[0m f1_micro: 0.05970149253731343
[2m[36m(func pid=57543)[0m f1_macro: 0.020106254558794046
[2m[36m(func pid=57543)[0m f1_weighted: 0.051344153083420746
[2m[36m(func pid=57543)[0m f1_per_class: [0.0, 0.0, 0.017, 0.184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.376865671641791
[2m[36m(func pid=55167)[0m top5: 0.8913246268656716
[2m[36m(func pid=55167)[0m f1_micro: 0.376865671641791
[2m[36m(func pid=55167)[0m f1_macro: 0.2228956844470822
[2m[36m(func pid=55167)[0m f1_weighted: 0.32910355035856814
[2m[36m(func pid=55167)[0m f1_per_class: [0.255, 0.291, 0.267, 0.503, 0.0, 0.49, 0.248, 0.0, 0.0, 0.175]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m top1: 0.22201492537313433
[2m[36m(func pid=59162)[0m top5: 0.7705223880597015
[2m[36m(func pid=59162)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=59162)[0m f1_macro: 0.057103099844326474
[2m[36m(func pid=59162)[0m f1_weighted: 0.10711960512949954
[2m[36m(func pid=59162)[0m f1_per_class: [0.084, 0.0, 0.112, 0.375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=39363)[0m top1: 0.3269589552238806
[2m[36m(func pid=39363)[0m top5: 0.835820895522388
[2m[36m(func pid=39363)[0m f1_micro: 0.3269589552238806
[2m[36m(func pid=39363)[0m f1_macro: 0.26491070956916274
[2m[36m(func pid=39363)[0m f1_weighted: 0.3475084922929716
[2m[36m(func pid=39363)[0m f1_per_class: [0.297, 0.464, 0.316, 0.228, 0.051, 0.085, 0.529, 0.365, 0.161, 0.154]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.9020 | Steps: 4 | Val loss: 2.3350 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 63.3978 | Steps: 4 | Val loss: 45.4929 | Batch size: 32 | lr: 0.1 | Duration: 2.78s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 2.7213 | Steps: 4 | Val loss: 2.1894 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 10:53:53 (running for 00:36:23.06)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.213 |      0.265 |                   85 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  30.031 |      0.223 |                   20 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.902 |      0.017 |                   12 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.937 |      0.057 |                    5 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.0457089552238806
[2m[36m(func pid=57543)[0m top5: 0.5069962686567164
[2m[36m(func pid=57543)[0m f1_micro: 0.0457089552238806
[2m[36m(func pid=57543)[0m f1_macro: 0.01661683798003483
[2m[36m(func pid=57543)[0m f1_weighted: 0.041278929950145885
[2m[36m(func pid=57543)[0m f1_per_class: [0.0, 0.005, 0.016, 0.144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 85] | Train loss: 1.0196 | Steps: 4 | Val loss: 2.1855 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=55167)[0m top1: 0.29850746268656714
[2m[36m(func pid=55167)[0m top5: 0.8222947761194029
[2m[36m(func pid=55167)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=55167)[0m f1_macro: 0.15063400815715716
[2m[36m(func pid=55167)[0m f1_weighted: 0.2477018325017446
[2m[36m(func pid=55167)[0m f1_per_class: [0.256, 0.351, 0.0, 0.478, 0.0, 0.311, 0.041, 0.0, 0.0, 0.069]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m top1: 0.16837686567164178
[2m[36m(func pid=59162)[0m top5: 0.7971082089552238
[2m[36m(func pid=59162)[0m f1_micro: 0.16837686567164178
[2m[36m(func pid=59162)[0m f1_macro: 0.06907287464297449
[2m[36m(func pid=59162)[0m f1_weighted: 0.05614612581499591
[2m[36m(func pid=59162)[0m f1_per_class: [0.043, 0.293, 0.345, 0.003, 0.0, 0.0, 0.006, 0.0, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=39363)[0m top1: 0.26725746268656714
[2m[36m(func pid=39363)[0m top5: 0.7971082089552238
[2m[36m(func pid=39363)[0m f1_micro: 0.26725746268656714
[2m[36m(func pid=39363)[0m f1_macro: 0.22055052163719635
[2m[36m(func pid=39363)[0m f1_weighted: 0.2902362637709425
[2m[36m(func pid=39363)[0m f1_per_class: [0.202, 0.45, 0.159, 0.203, 0.058, 0.053, 0.382, 0.412, 0.167, 0.119]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.7878 | Steps: 4 | Val loss: 2.3184 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 55.0167 | Steps: 4 | Val loss: 54.1347 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 2.6864 | Steps: 4 | Val loss: 2.2193 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=57543)[0m top1: 0.0457089552238806
[2m[36m(func pid=57543)[0m top5: 0.5205223880597015
[2m[36m(func pid=57543)[0m f1_micro: 0.0457089552238806
[2m[36m(func pid=57543)[0m f1_macro: 0.01654164442703103
[2m[36m(func pid=57543)[0m f1_weighted: 0.04097460751424761
[2m[36m(func pid=57543)[0m f1_per_class: [0.0, 0.005, 0.017, 0.143, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
== Status ==
Current time: 2024-01-07 10:53:58 (running for 00:36:28.48)
Memory usage on this node: 25.0/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.02  |      0.221 |                   86 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  63.398 |      0.151 |                   21 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.788 |      0.017 |                   13 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.721 |      0.069 |                    6 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.11100746268656717
[2m[36m(func pid=55167)[0m top5: 0.8344216417910447
[2m[36m(func pid=55167)[0m f1_micro: 0.11100746268656717
[2m[36m(func pid=55167)[0m f1_macro: 0.11369371557145487
[2m[36m(func pid=55167)[0m f1_weighted: 0.12210616191064172
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.241, 0.0, 0.0, 0.289, 0.382, 0.106, 0.0, 0.088, 0.031]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 86] | Train loss: 0.8539 | Steps: 4 | Val loss: 2.3829 | Batch size: 32 | lr: 0.001 | Duration: 2.93s
[2m[36m(func pid=59162)[0m top1: 0.1828358208955224
[2m[36m(func pid=59162)[0m top5: 0.5783582089552238
[2m[36m(func pid=59162)[0m f1_micro: 0.1828358208955224
[2m[36m(func pid=59162)[0m f1_macro: 0.07722038365481694
[2m[36m(func pid=59162)[0m f1_weighted: 0.07100159260146092
[2m[36m(func pid=59162)[0m f1_per_class: [0.0, 0.327, 0.303, 0.0, 0.0, 0.096, 0.0, 0.016, 0.03, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=39363)[0m top1: 0.19496268656716417
[2m[36m(func pid=39363)[0m top5: 0.7686567164179104
[2m[36m(func pid=39363)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=39363)[0m f1_macro: 0.17357373492123546
[2m[36m(func pid=39363)[0m f1_weighted: 0.17482094813325802
[2m[36m(func pid=39363)[0m f1_per_class: [0.183, 0.44, 0.125, 0.167, 0.068, 0.127, 0.022, 0.356, 0.142, 0.107]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.8802 | Steps: 4 | Val loss: 2.3212 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 69.7577 | Steps: 4 | Val loss: 56.7027 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.6225 | Steps: 4 | Val loss: 2.2590 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 10:54:04 (running for 00:36:33.83)
Memory usage on this node: 25.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.854 |      0.174 |                   87 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  55.017 |      0.114 |                   22 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.88  |      0.018 |                   14 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.686 |      0.077 |                    7 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=55167)[0m top1: 0.16371268656716417
[2m[36m(func pid=55167)[0m top5: 0.8484141791044776
[2m[36m(func pid=55167)[0m f1_micro: 0.16371268656716417
[2m[36m(func pid=55167)[0m f1_macro: 0.15457680204587634
[2m[36m(func pid=55167)[0m f1_weighted: 0.19331413978461495
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.197, 0.6, 0.121, 0.025, 0.0, 0.392, 0.0, 0.104, 0.105]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.05317164179104478
[2m[36m(func pid=57543)[0m top5: 0.5074626865671642
[2m[36m(func pid=57543)[0m f1_micro: 0.05317164179104478
[2m[36m(func pid=57543)[0m f1_macro: 0.018051840195821314
[2m[36m(func pid=57543)[0m f1_weighted: 0.04554343578462265
[2m[36m(func pid=57543)[0m f1_per_class: [0.0, 0.0, 0.018, 0.163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 87] | Train loss: 1.8206 | Steps: 4 | Val loss: 2.5223 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=59162)[0m top1: 0.18703358208955223
[2m[36m(func pid=59162)[0m top5: 0.46175373134328357
[2m[36m(func pid=59162)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=59162)[0m f1_macro: 0.1337551575470474
[2m[36m(func pid=59162)[0m f1_weighted: 0.09828685462936329
[2m[36m(func pid=59162)[0m f1_per_class: [0.169, 0.367, 0.387, 0.0, 0.0, 0.143, 0.0, 0.163, 0.109, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.7617 | Steps: 4 | Val loss: 2.3169 | Batch size: 32 | lr: 0.0001 | Duration: 2.83s
[2m[36m(func pid=39363)[0m top1: 0.20102611940298507
[2m[36m(func pid=39363)[0m top5: 0.7360074626865671
[2m[36m(func pid=39363)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=39363)[0m f1_macro: 0.1732676212317138
[2m[36m(func pid=39363)[0m f1_weighted: 0.1818455780142977
[2m[36m(func pid=39363)[0m f1_per_class: [0.162, 0.46, 0.106, 0.208, 0.079, 0.129, 0.0, 0.337, 0.131, 0.12]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 58.7865 | Steps: 4 | Val loss: 55.8363 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 10:54:09 (running for 00:36:39.14)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.821 |      0.173 |                   88 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  69.758 |      0.155 |                   23 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.762 |      0.022 |                   15 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.623 |      0.134 |                    8 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=55167)[0m top1: 0.20662313432835822
[2m[36m(func pid=55167)[0m top5: 0.8446828358208955
[2m[36m(func pid=55167)[0m f1_micro: 0.20662313432835824
[2m[36m(func pid=55167)[0m f1_macro: 0.13149097259508533
[2m[36m(func pid=55167)[0m f1_weighted: 0.20075059112283505
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.027, 0.144, 0.499, 0.026, 0.0, 0.084, 0.536, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.0648320895522388
[2m[36m(func pid=57543)[0m top5: 0.5032649253731343
[2m[36m(func pid=57543)[0m f1_micro: 0.0648320895522388
[2m[36m(func pid=57543)[0m f1_macro: 0.02156629287033157
[2m[36m(func pid=57543)[0m f1_weighted: 0.0537949760219035
[2m[36m(func pid=57543)[0m f1_per_class: [0.0, 0.011, 0.019, 0.186, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 2.6435 | Steps: 4 | Val loss: 2.2647 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 88] | Train loss: 1.5293 | Steps: 4 | Val loss: 2.3753 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=59162)[0m top1: 0.18003731343283583
[2m[36m(func pid=59162)[0m top5: 0.5438432835820896
[2m[36m(func pid=59162)[0m f1_micro: 0.1800373134328358
[2m[36m(func pid=59162)[0m f1_macro: 0.16932373997041666
[2m[36m(func pid=59162)[0m f1_weighted: 0.1649242205473001
[2m[36m(func pid=59162)[0m f1_per_class: [0.046, 0.448, 0.247, 0.0, 0.0, 0.301, 0.071, 0.459, 0.085, 0.035]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.8274 | Steps: 4 | Val loss: 2.3076 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 61.1274 | Steps: 4 | Val loss: 68.4527 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=39363)[0m top1: 0.26026119402985076
[2m[36m(func pid=39363)[0m top5: 0.7658582089552238
[2m[36m(func pid=39363)[0m f1_micro: 0.26026119402985076
[2m[36m(func pid=39363)[0m f1_macro: 0.22285867567767986
[2m[36m(func pid=39363)[0m f1_weighted: 0.2526557857567803
[2m[36m(func pid=39363)[0m f1_per_class: [0.184, 0.462, 0.19, 0.399, 0.095, 0.268, 0.0, 0.315, 0.176, 0.139]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:54:14 (running for 00:36:44.47)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.529 |      0.223 |                   89 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  58.786 |      0.131 |                   24 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.827 |      0.025 |                   16 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.643 |      0.169 |                    9 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.06996268656716417
[2m[36m(func pid=57543)[0m top5: 0.5135261194029851
[2m[36m(func pid=57543)[0m f1_micro: 0.06996268656716417
[2m[36m(func pid=57543)[0m f1_macro: 0.02506435646048376
[2m[36m(func pid=57543)[0m f1_weighted: 0.05564214141457068
[2m[36m(func pid=57543)[0m f1_per_class: [0.029, 0.011, 0.02, 0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.35447761194029853
[2m[36m(func pid=55167)[0m top5: 0.6282649253731343
[2m[36m(func pid=55167)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=55167)[0m f1_macro: 0.22785618865090496
[2m[36m(func pid=55167)[0m f1_weighted: 0.31010102986076205
[2m[36m(func pid=55167)[0m f1_per_class: [0.131, 0.54, 0.109, 0.539, 0.185, 0.37, 0.0, 0.328, 0.0, 0.077]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6624 | Steps: 4 | Val loss: 2.2452 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 89] | Train loss: 1.1136 | Steps: 4 | Val loss: 2.1588 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.8327 | Steps: 4 | Val loss: 2.2958 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 57.0192 | Steps: 4 | Val loss: 98.1130 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=59162)[0m top1: 0.25513059701492535
[2m[36m(func pid=59162)[0m top5: 0.6408582089552238
[2m[36m(func pid=59162)[0m f1_micro: 0.25513059701492535
[2m[36m(func pid=59162)[0m f1_macro: 0.12025049240132683
[2m[36m(func pid=59162)[0m f1_weighted: 0.20600436006863235
[2m[36m(func pid=59162)[0m f1_per_class: [0.059, 0.214, 0.128, 0.0, 0.022, 0.046, 0.504, 0.167, 0.064, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=39363)[0m top1: 0.3204291044776119
[2m[36m(func pid=39363)[0m top5: 0.7966417910447762
[2m[36m(func pid=39363)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=39363)[0m f1_macro: 0.27562089454103783
[2m[36m(func pid=39363)[0m f1_weighted: 0.29772244949551213
[2m[36m(func pid=39363)[0m f1_per_class: [0.261, 0.486, 0.415, 0.511, 0.124, 0.307, 0.0, 0.364, 0.155, 0.133]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:54:19 (running for 00:36:49.53)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.114 |      0.276 |                   90 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  61.127 |      0.228 |                   25 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.833 |      0.032 |                   17 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.662 |      0.12  |                   10 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.10307835820895522
[2m[36m(func pid=57543)[0m top5: 0.5265858208955224
[2m[36m(func pid=57543)[0m f1_micro: 0.10307835820895522
[2m[36m(func pid=57543)[0m f1_macro: 0.03174531063317072
[2m[36m(func pid=57543)[0m f1_weighted: 0.07293707400925234
[2m[36m(func pid=57543)[0m f1_per_class: [0.022, 0.025, 0.026, 0.244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.24860074626865672
[2m[36m(func pid=55167)[0m top5: 0.6208022388059702
[2m[36m(func pid=55167)[0m f1_micro: 0.24860074626865672
[2m[36m(func pid=55167)[0m f1_macro: 0.2069437321111672
[2m[36m(func pid=55167)[0m f1_weighted: 0.1994902834948844
[2m[36m(func pid=55167)[0m f1_per_class: [0.085, 0.527, 0.426, 0.092, 0.0, 0.497, 0.0, 0.366, 0.0, 0.077]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.5203 | Steps: 4 | Val loss: 2.2065 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 90] | Train loss: 1.2836 | Steps: 4 | Val loss: 1.9259 | Batch size: 32 | lr: 0.001 | Duration: 2.84s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.8633 | Steps: 4 | Val loss: 2.2875 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 79.3142 | Steps: 4 | Val loss: 89.9552 | Batch size: 32 | lr: 0.1 | Duration: 2.79s
[2m[36m(func pid=59162)[0m top1: 0.279384328358209
[2m[36m(func pid=59162)[0m top5: 0.605410447761194
[2m[36m(func pid=59162)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=59162)[0m f1_macro: 0.09895590918257754
[2m[36m(func pid=59162)[0m f1_weighted: 0.15327768247250234
[2m[36m(func pid=59162)[0m f1_per_class: [0.121, 0.0, 0.37, 0.0, 0.0, 0.0, 0.498, 0.0, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=39363)[0m top1: 0.3736007462686567
[2m[36m(func pid=39363)[0m top5: 0.8376865671641791
[2m[36m(func pid=39363)[0m f1_micro: 0.3736007462686567
[2m[36m(func pid=39363)[0m f1_macro: 0.3308980072285722
[2m[36m(func pid=39363)[0m f1_weighted: 0.33394765371750756
[2m[36m(func pid=39363)[0m f1_per_class: [0.34, 0.5, 0.537, 0.554, 0.211, 0.388, 0.016, 0.45, 0.138, 0.175]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:54:25 (running for 00:36:54.86)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.284 |      0.331 |                   91 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  57.019 |      0.207 |                   26 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.863 |      0.039 |                   18 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.52  |      0.099 |                   11 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.12126865671641791
[2m[36m(func pid=57543)[0m top5: 0.534981343283582
[2m[36m(func pid=57543)[0m f1_micro: 0.12126865671641791
[2m[36m(func pid=57543)[0m f1_macro: 0.03901575549057453
[2m[36m(func pid=57543)[0m f1_weighted: 0.08048187282599918
[2m[36m(func pid=57543)[0m f1_per_class: [0.047, 0.032, 0.034, 0.261, 0.0, 0.0, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.33115671641791045
[2m[36m(func pid=55167)[0m top5: 0.6226679104477612
[2m[36m(func pid=55167)[0m f1_micro: 0.33115671641791045
[2m[36m(func pid=55167)[0m f1_macro: 0.24171662121784446
[2m[36m(func pid=55167)[0m f1_weighted: 0.2786768946725127
[2m[36m(func pid=55167)[0m f1_per_class: [0.195, 0.53, 0.146, 0.349, 0.0, 0.452, 0.0, 0.479, 0.124, 0.141]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 2.8270 | Steps: 4 | Val loss: 2.1605 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 91] | Train loss: 1.3512 | Steps: 4 | Val loss: 1.7475 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7789 | Steps: 4 | Val loss: 2.2704 | Batch size: 32 | lr: 0.0001 | Duration: 2.73s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 111.9638 | Steps: 4 | Val loss: 109.6643 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=59162)[0m top1: 0.2873134328358209
[2m[36m(func pid=59162)[0m top5: 0.6949626865671642
[2m[36m(func pid=59162)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=59162)[0m f1_macro: 0.09794641355374908
[2m[36m(func pid=59162)[0m f1_weighted: 0.1516594683725436
[2m[36m(func pid=59162)[0m f1_per_class: [0.136, 0.0, 0.351, 0.0, 0.0, 0.0, 0.492, 0.0, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=39363)[0m top1: 0.43423507462686567
[2m[36m(func pid=39363)[0m top5: 0.8698694029850746
[2m[36m(func pid=39363)[0m f1_micro: 0.43423507462686567
[2m[36m(func pid=39363)[0m f1_macro: 0.3862887173636214
[2m[36m(func pid=39363)[0m f1_weighted: 0.4232820622760931
[2m[36m(func pid=39363)[0m f1_per_class: [0.353, 0.537, 0.69, 0.569, 0.289, 0.397, 0.265, 0.468, 0.168, 0.127]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:54:30 (running for 00:36:59.98)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.351 |      0.386 |                   92 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  79.314 |      0.242 |                   27 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.779 |      0.05  |                   19 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.827 |      0.098 |                   12 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.13386194029850745
[2m[36m(func pid=57543)[0m top5: 0.5909514925373134
[2m[36m(func pid=57543)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=57543)[0m f1_macro: 0.049891180214447356
[2m[36m(func pid=57543)[0m f1_weighted: 0.08983272695719133
[2m[36m(func pid=57543)[0m f1_per_class: [0.053, 0.04, 0.043, 0.278, 0.0, 0.0, 0.003, 0.047, 0.0, 0.035]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.19869402985074627
[2m[36m(func pid=55167)[0m top5: 0.6417910447761194
[2m[36m(func pid=55167)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=55167)[0m f1_macro: 0.16908810670425617
[2m[36m(func pid=55167)[0m f1_weighted: 0.21884002874184108
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.311, 0.126, 0.345, 0.0, 0.371, 0.0, 0.388, 0.088, 0.063]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 2.5095 | Steps: 4 | Val loss: 2.1331 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 92] | Train loss: 1.1725 | Steps: 4 | Val loss: 1.5866 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.7484 | Steps: 4 | Val loss: 2.2581 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 134.0247 | Steps: 4 | Val loss: 102.4618 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=59162)[0m top1: 0.251865671641791
[2m[36m(func pid=59162)[0m top5: 0.7756529850746269
[2m[36m(func pid=59162)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=59162)[0m f1_macro: 0.10258592997191467
[2m[36m(func pid=59162)[0m f1_weighted: 0.16092191759592483
[2m[36m(func pid=59162)[0m f1_per_class: [0.169, 0.0, 0.275, 0.0, 0.029, 0.008, 0.518, 0.0, 0.0, 0.027]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=39363)[0m top1: 0.47901119402985076
[2m[36m(func pid=39363)[0m top5: 0.9001865671641791
[2m[36m(func pid=39363)[0m f1_micro: 0.47901119402985076
[2m[36m(func pid=39363)[0m f1_macro: 0.37131610799372117
[2m[36m(func pid=39363)[0m f1_weighted: 0.4789497568943936
[2m[36m(func pid=39363)[0m f1_per_class: [0.262, 0.556, 0.444, 0.53, 0.357, 0.41, 0.516, 0.284, 0.16, 0.194]
[2m[36m(func pid=39363)[0m 
== Status ==
Current time: 2024-01-07 10:54:35 (running for 00:37:05.19)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.172 |      0.371 |                   93 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 111.964 |      0.169 |                   28 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.748 |      0.057 |                   20 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.51  |      0.103 |                   13 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.1478544776119403
[2m[36m(func pid=57543)[0m top5: 0.6231343283582089
[2m[36m(func pid=57543)[0m f1_micro: 0.1478544776119403
[2m[36m(func pid=57543)[0m f1_macro: 0.05698305503416732
[2m[36m(func pid=57543)[0m f1_weighted: 0.09992608305410457
[2m[36m(func pid=57543)[0m f1_per_class: [0.067, 0.06, 0.058, 0.294, 0.0, 0.0, 0.009, 0.047, 0.0, 0.035]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.2751865671641791
[2m[36m(func pid=55167)[0m top5: 0.6534514925373134
[2m[36m(func pid=55167)[0m f1_micro: 0.2751865671641791
[2m[36m(func pid=55167)[0m f1_macro: 0.189052016483164
[2m[36m(func pid=55167)[0m f1_weighted: 0.24608761761143227
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.427, 0.128, 0.38, 0.0, 0.309, 0.0, 0.47, 0.051, 0.125]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 2.6276 | Steps: 4 | Val loss: 2.1447 | Batch size: 32 | lr: 0.001 | Duration: 2.94s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 93] | Train loss: 1.2424 | Steps: 4 | Val loss: 1.6334 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.7535 | Steps: 4 | Val loss: 2.2570 | Batch size: 32 | lr: 0.0001 | Duration: 2.77s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 57.6519 | Steps: 4 | Val loss: 85.9732 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=59162)[0m top1: 0.1044776119402985
[2m[36m(func pid=59162)[0m top5: 0.78125
[2m[36m(func pid=59162)[0m f1_micro: 0.1044776119402985
[2m[36m(func pid=59162)[0m f1_macro: 0.09882562337778149
[2m[36m(func pid=59162)[0m f1_weighted: 0.1323213574321261
[2m[36m(func pid=59162)[0m f1_per_class: [0.0, 0.0, 0.168, 0.0, 0.065, 0.252, 0.308, 0.173, 0.0, 0.023]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=57543)[0m top1: 0.14505597014925373
[2m[36m(func pid=57543)[0m top5: 0.6343283582089553
[2m[36m(func pid=57543)[0m f1_micro: 0.14505597014925373
[2m[36m(func pid=57543)[0m f1_macro: 0.05724560825964723
[2m[36m(func pid=57543)[0m f1_weighted: 0.1079046186686471
[2m[36m(func pid=57543)[0m f1_per_class: [0.061, 0.092, 0.06, 0.287, 0.0, 0.0, 0.031, 0.016, 0.0, 0.026]
[2m[36m(func pid=57543)[0m 
== Status ==
Current time: 2024-01-07 10:54:40 (running for 00:37:10.34)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.242 |      0.317 |                   94 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 134.025 |      0.189 |                   29 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.753 |      0.057 |                   21 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.628 |      0.099 |                   14 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=39363)[0m top1: 0.4398320895522388
[2m[36m(func pid=39363)[0m top5: 0.9029850746268657
[2m[36m(func pid=39363)[0m f1_micro: 0.4398320895522388
[2m[36m(func pid=39363)[0m f1_macro: 0.31704540915577917
[2m[36m(func pid=39363)[0m f1_weighted: 0.4308995680695087
[2m[36m(func pid=39363)[0m f1_per_class: [0.293, 0.548, 0.282, 0.47, 0.4, 0.397, 0.472, 0.078, 0.07, 0.16]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m top1: 0.28451492537313433
[2m[36m(func pid=55167)[0m top5: 0.6585820895522388
[2m[36m(func pid=55167)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=55167)[0m f1_macro: 0.19905310259891384
[2m[36m(func pid=55167)[0m f1_weighted: 0.15245141837171527
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.425, 0.489, 0.01, 0.2, 0.389, 0.0, 0.478, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.6128 | Steps: 4 | Val loss: 2.1850 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.7918 | Steps: 4 | Val loss: 2.2515 | Batch size: 32 | lr: 0.0001 | Duration: 2.74s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 94] | Train loss: 1.4986 | Steps: 4 | Val loss: 1.7441 | Batch size: 32 | lr: 0.001 | Duration: 2.87s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 44.9844 | Steps: 4 | Val loss: 74.6164 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=59162)[0m top1: 0.09235074626865672
[2m[36m(func pid=59162)[0m top5: 0.7551305970149254
[2m[36m(func pid=59162)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=59162)[0m f1_macro: 0.08807163919728354
[2m[36m(func pid=59162)[0m f1_weighted: 0.058631158673014216
[2m[36m(func pid=59162)[0m f1_per_class: [0.0, 0.0, 0.171, 0.0, 0.047, 0.351, 0.0, 0.291, 0.0, 0.02]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=57543)[0m top1: 0.14692164179104478
[2m[36m(func pid=57543)[0m top5: 0.6361940298507462
[2m[36m(func pid=57543)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=57543)[0m f1_macro: 0.059145303331865996
[2m[36m(func pid=57543)[0m f1_weighted: 0.11418314746269652
[2m[36m(func pid=57543)[0m f1_per_class: [0.059, 0.106, 0.081, 0.286, 0.0, 0.0, 0.048, 0.0, 0.0, 0.011]
[2m[36m(func pid=57543)[0m 
== Status ==
Current time: 2024-01-07 10:54:45 (running for 00:37:15.60)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.499 |      0.281 |                   95 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  57.652 |      0.199 |                   30 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.792 |      0.059 |                   22 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.613 |      0.088 |                   15 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=39363)[0m top1: 0.384794776119403
[2m[36m(func pid=39363)[0m top5: 0.8927238805970149
[2m[36m(func pid=39363)[0m f1_micro: 0.384794776119403
[2m[36m(func pid=39363)[0m f1_macro: 0.28056848193301465
[2m[36m(func pid=39363)[0m f1_weighted: 0.37865420477842804
[2m[36m(func pid=39363)[0m f1_per_class: [0.255, 0.53, 0.179, 0.331, 0.32, 0.373, 0.435, 0.176, 0.07, 0.136]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m top1: 0.18796641791044777
[2m[36m(func pid=55167)[0m top5: 0.7028917910447762
[2m[36m(func pid=55167)[0m f1_micro: 0.18796641791044777
[2m[36m(func pid=55167)[0m f1_macro: 0.2075616201783975
[2m[36m(func pid=55167)[0m f1_weighted: 0.18266431159298624
[2m[36m(func pid=55167)[0m f1_per_class: [0.348, 0.257, 0.545, 0.363, 0.029, 0.0, 0.0, 0.362, 0.171, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 2.6210 | Steps: 4 | Val loss: 2.1502 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.7500 | Steps: 4 | Val loss: 2.2510 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 95] | Train loss: 0.9731 | Steps: 4 | Val loss: 1.8432 | Batch size: 32 | lr: 0.001 | Duration: 2.89s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 60.1952 | Steps: 4 | Val loss: 98.6427 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
[2m[36m(func pid=59162)[0m top1: 0.17957089552238806
[2m[36m(func pid=59162)[0m top5: 0.7658582089552238
[2m[36m(func pid=59162)[0m f1_micro: 0.17957089552238806
[2m[36m(func pid=59162)[0m f1_macro: 0.1527758650490993
[2m[36m(func pid=59162)[0m f1_weighted: 0.15567711058451927
[2m[36m(func pid=59162)[0m f1_per_class: [0.189, 0.016, 0.244, 0.352, 0.032, 0.25, 0.0, 0.276, 0.127, 0.042]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=57543)[0m top1: 0.14692164179104478
[2m[36m(func pid=57543)[0m top5: 0.6254664179104478
[2m[36m(func pid=57543)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=57543)[0m f1_macro: 0.06187617268683462
[2m[36m(func pid=57543)[0m f1_weighted: 0.11252633291402866
[2m[36m(func pid=57543)[0m f1_per_class: [0.061, 0.13, 0.087, 0.277, 0.0, 0.0, 0.036, 0.0, 0.0, 0.027]
[2m[36m(func pid=57543)[0m 
== Status ==
Current time: 2024-01-07 10:54:51 (running for 00:37:21.02)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.973 |      0.261 |                   96 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  44.984 |      0.208 |                   31 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.75  |      0.062 |                   23 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.621 |      0.153 |                   16 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=39363)[0m top1: 0.35447761194029853
[2m[36m(func pid=39363)[0m top5: 0.8652052238805971
[2m[36m(func pid=39363)[0m f1_micro: 0.35447761194029853
[2m[36m(func pid=39363)[0m f1_macro: 0.26119199357678136
[2m[36m(func pid=39363)[0m f1_weighted: 0.34057367845477987
[2m[36m(func pid=39363)[0m f1_per_class: [0.208, 0.494, 0.17, 0.181, 0.211, 0.346, 0.473, 0.239, 0.026, 0.265]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m top1: 0.13059701492537312
[2m[36m(func pid=55167)[0m top5: 0.6833022388059702
[2m[36m(func pid=55167)[0m f1_micro: 0.13059701492537312
[2m[36m(func pid=55167)[0m f1_macro: 0.18473975909929893
[2m[36m(func pid=55167)[0m f1_weighted: 0.15197181570210305
[2m[36m(func pid=55167)[0m f1_per_class: [0.279, 0.0, 0.6, 0.208, 0.022, 0.0, 0.179, 0.517, 0.043, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.5385 | Steps: 4 | Val loss: 2.1296 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.7274 | Steps: 4 | Val loss: 2.2542 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 96] | Train loss: 1.5351 | Steps: 4 | Val loss: 2.0065 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 103.3572 | Steps: 4 | Val loss: 96.5992 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
[2m[36m(func pid=59162)[0m top1: 0.17397388059701493
[2m[36m(func pid=59162)[0m top5: 0.7672574626865671
[2m[36m(func pid=59162)[0m f1_micro: 0.17397388059701493
[2m[36m(func pid=59162)[0m f1_macro: 0.1606826747812436
[2m[36m(func pid=59162)[0m f1_weighted: 0.11818089471331683
[2m[36m(func pid=59162)[0m f1_per_class: [0.225, 0.294, 0.485, 0.067, 0.046, 0.266, 0.0, 0.121, 0.103, 0.0]
[2m[36m(func pid=57543)[0m top1: 0.1417910447761194
[2m[36m(func pid=57543)[0m top5: 0.6152052238805971
[2m[36m(func pid=57543)[0m f1_micro: 0.1417910447761194
[2m[36m(func pid=57543)[0m f1_macro: 0.06167294154283569
[2m[36m(func pid=57543)[0m f1_weighted: 0.10831056694448386
[2m[36m(func pid=57543)[0m f1_per_class: [0.062, 0.14, 0.092, 0.268, 0.0, 0.0, 0.025, 0.0, 0.0, 0.03]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 10:54:56 (running for 00:37:26.61)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.535 |      0.246 |                   97 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  60.195 |      0.185 |                   32 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.727 |      0.062 |                   24 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.539 |      0.161 |                   17 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=39363)[0m top1: 0.31296641791044777
[2m[36m(func pid=39363)[0m top5: 0.8367537313432836
[2m[36m(func pid=39363)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=39363)[0m f1_macro: 0.2461426575672642
[2m[36m(func pid=39363)[0m f1_weighted: 0.30551827200218934
[2m[36m(func pid=39363)[0m f1_per_class: [0.186, 0.442, 0.175, 0.112, 0.141, 0.328, 0.438, 0.354, 0.023, 0.262]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m top1: 0.22388059701492538
[2m[36m(func pid=55167)[0m top5: 0.625
[2m[36m(func pid=55167)[0m f1_micro: 0.22388059701492538
[2m[36m(func pid=55167)[0m f1_macro: 0.166703180800626
[2m[36m(func pid=55167)[0m f1_weighted: 0.17939499087659735
[2m[36m(func pid=55167)[0m f1_per_class: [0.314, 0.0, 0.545, 0.0, 0.036, 0.0, 0.524, 0.201, 0.045, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.7450 | Steps: 4 | Val loss: 2.2609 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.4810 | Steps: 4 | Val loss: 2.1416 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 97] | Train loss: 0.8302 | Steps: 4 | Val loss: 2.1237 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 109.0029 | Steps: 4 | Val loss: 87.1971 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=57543)[0m top1: 0.12453358208955224
[2m[36m(func pid=57543)[0m top5: 0.601679104477612
[2m[36m(func pid=57543)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=57543)[0m f1_macro: 0.05690739849302958
[2m[36m(func pid=57543)[0m f1_weighted: 0.10252982230824183
[2m[36m(func pid=57543)[0m f1_per_class: [0.048, 0.153, 0.075, 0.235, 0.0, 0.0, 0.03, 0.0, 0.0, 0.028]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.22294776119402984
[2m[36m(func pid=59162)[0m top5: 0.7248134328358209
[2m[36m(func pid=59162)[0m f1_micro: 0.22294776119402981
[2m[36m(func pid=59162)[0m f1_macro: 0.19577146274780335
[2m[36m(func pid=59162)[0m f1_weighted: 0.1364149362603755
[2m[36m(func pid=59162)[0m f1_per_class: [0.185, 0.363, 0.5, 0.0, 0.068, 0.329, 0.0, 0.49, 0.023, 0.0]
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 10:55:02 (running for 00:37:32.03)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.83  |      0.252 |                   98 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 103.357 |      0.167 |                   33 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.745 |      0.057 |                   25 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.481 |      0.196 |                   18 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=39363)[0m top1: 0.28171641791044777
[2m[36m(func pid=39363)[0m top5: 0.8260261194029851
[2m[36m(func pid=39363)[0m f1_micro: 0.28171641791044777
[2m[36m(func pid=39363)[0m f1_macro: 0.2524522955593568
[2m[36m(func pid=39363)[0m f1_weighted: 0.29145861607739726
[2m[36m(func pid=39363)[0m f1_per_class: [0.216, 0.421, 0.258, 0.258, 0.096, 0.303, 0.248, 0.474, 0.058, 0.193]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m top1: 0.09794776119402986
[2m[36m(func pid=55167)[0m top5: 0.5984141791044776
[2m[36m(func pid=55167)[0m f1_micro: 0.09794776119402987
[2m[36m(func pid=55167)[0m f1_macro: 0.12820438370133608
[2m[36m(func pid=55167)[0m f1_weighted: 0.045386885705384496
[2m[36m(func pid=55167)[0m f1_per_class: [0.312, 0.0, 0.282, 0.0, 0.204, 0.0, 0.039, 0.375, 0.07, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.7157 | Steps: 4 | Val loss: 2.2558 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.6310 | Steps: 4 | Val loss: 2.1118 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 98] | Train loss: 1.1077 | Steps: 4 | Val loss: 2.3735 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 66.2505 | Steps: 4 | Val loss: 63.9132 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=57543)[0m top1: 0.13199626865671643
[2m[36m(func pid=57543)[0m top5: 0.6245335820895522
[2m[36m(func pid=57543)[0m f1_micro: 0.13199626865671643
[2m[36m(func pid=57543)[0m f1_macro: 0.0659835105871265
[2m[36m(func pid=57543)[0m f1_weighted: 0.1171970653468915
[2m[36m(func pid=57543)[0m f1_per_class: [0.056, 0.211, 0.091, 0.216, 0.0, 0.0, 0.062, 0.0, 0.0, 0.023]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.23507462686567165
[2m[36m(func pid=59162)[0m top5: 0.7770522388059702
[2m[36m(func pid=59162)[0m f1_micro: 0.23507462686567163
[2m[36m(func pid=59162)[0m f1_macro: 0.1658785891779425
[2m[36m(func pid=59162)[0m f1_weighted: 0.12322111508798643
[2m[36m(func pid=59162)[0m f1_per_class: [0.212, 0.37, 0.385, 0.0, 0.0, 0.228, 0.0, 0.465, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 10:55:07 (running for 00:37:37.45)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.108 |      0.249 |                   99 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 109.003 |      0.128 |                   34 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.716 |      0.066 |                   26 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.631 |      0.166 |                   19 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=39363)[0m top1: 0.23227611940298507
[2m[36m(func pid=39363)[0m top5: 0.8022388059701493
[2m[36m(func pid=39363)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=39363)[0m f1_macro: 0.2494438881054417
[2m[36m(func pid=39363)[0m f1_weighted: 0.23479094281569904
[2m[36m(func pid=39363)[0m f1_per_class: [0.269, 0.315, 0.449, 0.342, 0.062, 0.243, 0.049, 0.486, 0.139, 0.141]
[2m[36m(func pid=39363)[0m 
[2m[36m(func pid=55167)[0m top1: 0.2019589552238806
[2m[36m(func pid=55167)[0m top5: 0.7131529850746269
[2m[36m(func pid=55167)[0m f1_micro: 0.2019589552238806
[2m[36m(func pid=55167)[0m f1_macro: 0.1537053275472534
[2m[36m(func pid=55167)[0m f1_weighted: 0.12600506773223008
[2m[36m(func pid=55167)[0m f1_per_class: [0.283, 0.323, 0.087, 0.0, 0.0, 0.279, 0.031, 0.364, 0.0, 0.17]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.8208 | Steps: 4 | Val loss: 2.2636 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 2.6389 | Steps: 4 | Val loss: 2.0962 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=39363)[0m [N0-GPU0] | [Epoch: 99] | Train loss: 0.9815 | Steps: 4 | Val loss: 2.4865 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 36.5834 | Steps: 4 | Val loss: 60.0309 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=57543)[0m top1: 0.1142723880597015
[2m[36m(func pid=57543)[0m top5: 0.6128731343283582
[2m[36m(func pid=57543)[0m f1_micro: 0.1142723880597015
[2m[36m(func pid=57543)[0m f1_macro: 0.05845441348009437
[2m[36m(func pid=57543)[0m f1_weighted: 0.10427930843922398
[2m[36m(func pid=57543)[0m f1_per_class: [0.05, 0.168, 0.078, 0.203, 0.0, 0.0, 0.057, 0.0, 0.0, 0.029]
[2m[36m(func pid=57543)[0m 
== Status ==
Current time: 2024-01-07 10:55:12 (running for 00:37:42.48)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=18
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 PENDING, 4 RUNNING, 18 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00017 | RUNNING    | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   1.108 |      0.249 |                   99 |
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  66.25  |      0.154 |                   35 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.821 |      0.058 |                   27 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.639 |      0.147 |                   20 |
| train_952df_00022 | PENDING    |                     | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.24300373134328357
[2m[36m(func pid=59162)[0m top5: 0.8558768656716418
[2m[36m(func pid=59162)[0m f1_micro: 0.24300373134328357
[2m[36m(func pid=59162)[0m f1_macro: 0.14729631169806887
[2m[36m(func pid=59162)[0m f1_weighted: 0.17396365256924698
[2m[36m(func pid=59162)[0m f1_per_class: [0.126, 0.397, 0.119, 0.016, 0.016, 0.223, 0.162, 0.412, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m top1: 0.2579291044776119
[2m[36m(func pid=55167)[0m top5: 0.7779850746268657
[2m[36m(func pid=55167)[0m f1_micro: 0.2579291044776119
[2m[36m(func pid=55167)[0m f1_macro: 0.2313030955865952
[2m[36m(func pid=55167)[0m f1_weighted: 0.20839825034329854
[2m[36m(func pid=55167)[0m f1_per_class: [0.33, 0.412, 0.5, 0.0, 0.0, 0.262, 0.234, 0.46, 0.0, 0.115]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=39363)[0m top1: 0.23740671641791045
[2m[36m(func pid=39363)[0m top5: 0.777518656716418
[2m[36m(func pid=39363)[0m f1_micro: 0.23740671641791045
[2m[36m(func pid=39363)[0m f1_macro: 0.2586220442678348
[2m[36m(func pid=39363)[0m f1_weighted: 0.23649745511328166
[2m[36m(func pid=39363)[0m f1_per_class: [0.317, 0.267, 0.512, 0.419, 0.055, 0.251, 0.0, 0.48, 0.174, 0.111]
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.8542 | Steps: 4 | Val loss: 2.2758 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.4960 | Steps: 4 | Val loss: 2.0259 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 67.7439 | Steps: 4 | Val loss: 64.3545 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=57543)[0m top1: 0.10727611940298508
[2m[36m(func pid=57543)[0m top5: 0.6077425373134329
[2m[36m(func pid=57543)[0m f1_micro: 0.10727611940298508
[2m[36m(func pid=57543)[0m f1_macro: 0.05689424111709661
[2m[36m(func pid=57543)[0m f1_weighted: 0.1062096925176594
[2m[36m(func pid=57543)[0m f1_per_class: [0.046, 0.171, 0.065, 0.191, 0.0, 0.0, 0.074, 0.0, 0.0, 0.023]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.3675373134328358
[2m[36m(func pid=59162)[0m top5: 0.9207089552238806
[2m[36m(func pid=59162)[0m f1_micro: 0.36753731343283574
[2m[36m(func pid=59162)[0m f1_macro: 0.15597336964762634
[2m[36m(func pid=59162)[0m f1_weighted: 0.33949475500275456
[2m[36m(func pid=59162)[0m f1_per_class: [0.103, 0.248, 0.102, 0.42, 0.072, 0.016, 0.582, 0.016, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m top1: 0.197294776119403
[2m[36m(func pid=55167)[0m top5: 0.8344216417910447
[2m[36m(func pid=55167)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=55167)[0m f1_macro: 0.20976077005235413
[2m[36m(func pid=55167)[0m f1_weighted: 0.24039927976262476
[2m[36m(func pid=55167)[0m f1_per_class: [0.185, 0.309, 0.72, 0.321, 0.0, 0.0, 0.244, 0.284, 0.0, 0.034]
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.7067 | Steps: 4 | Val loss: 2.2701 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.4317 | Steps: 4 | Val loss: 2.0047 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 10:55:18 (running for 00:37:48.12)
Memory usage on this node: 22.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  36.583 |      0.231 |                   36 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.854 |      0.057 |                   28 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.496 |      0.156 |                   21 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=64536)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=64536)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=64536)[0m Configuration completed!
[2m[36m(func pid=64536)[0m New optimizer parameters:
[2m[36m(func pid=64536)[0m SGD (
[2m[36m(func pid=64536)[0m Parameter Group 0
[2m[36m(func pid=64536)[0m     dampening: 0
[2m[36m(func pid=64536)[0m     differentiable: False
[2m[36m(func pid=64536)[0m     foreach: None
[2m[36m(func pid=64536)[0m     lr: 0.01
[2m[36m(func pid=64536)[0m     maximize: False
[2m[36m(func pid=64536)[0m     momentum: 0.9
[2m[36m(func pid=64536)[0m     nesterov: False
[2m[36m(func pid=64536)[0m     weight_decay: 1e-05
[2m[36m(func pid=64536)[0m )
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.11567164179104478
[2m[36m(func pid=57543)[0m top5: 0.6166044776119403
[2m[36m(func pid=57543)[0m f1_micro: 0.11567164179104478
[2m[36m(func pid=57543)[0m f1_macro: 0.05645765086535638
[2m[36m(func pid=57543)[0m f1_weighted: 0.11385324437166074
[2m[36m(func pid=57543)[0m f1_per_class: [0.038, 0.135, 0.06, 0.216, 0.0, 0.0, 0.098, 0.0, 0.0, 0.018]
[2m[36m(func pid=57543)[0m 
== Status ==
Current time: 2024-01-07 10:55:23 (running for 00:37:53.64)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  67.744 |      0.21  |                   37 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.707 |      0.056 |                   29 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.432 |      0.142 |                   22 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |         |            |                      |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.32136194029850745
[2m[36m(func pid=59162)[0m top5: 0.9165111940298507
[2m[36m(func pid=59162)[0m f1_micro: 0.32136194029850745
[2m[36m(func pid=59162)[0m f1_macro: 0.14220199436913056
[2m[36m(func pid=59162)[0m f1_weighted: 0.2350016201205453
[2m[36m(func pid=59162)[0m f1_per_class: [0.173, 0.0, 0.4, 0.234, 0.067, 0.0, 0.548, 0.0, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 90.8523 | Steps: 4 | Val loss: 57.6347 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.7232 | Steps: 4 | Val loss: 2.2704 | Batch size: 32 | lr: 0.0001 | Duration: 2.81s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 3.1599 | Steps: 4 | Val loss: 2.3416 | Batch size: 32 | lr: 0.01 | Duration: 4.19s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 2.4501 | Steps: 4 | Val loss: 2.0233 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=55167)[0m top1: 0.3568097014925373
[2m[36m(func pid=55167)[0m top5: 0.855410447761194
[2m[36m(func pid=55167)[0m f1_micro: 0.3568097014925374
[2m[36m(func pid=55167)[0m f1_macro: 0.2083516368690177
[2m[36m(func pid=55167)[0m f1_weighted: 0.3412130074703766
[2m[36m(func pid=55167)[0m f1_per_class: [0.043, 0.196, 0.537, 0.577, 0.0, 0.063, 0.412, 0.201, 0.0, 0.054]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.11893656716417911
[2m[36m(func pid=57543)[0m top5: 0.617070895522388
[2m[36m(func pid=57543)[0m f1_micro: 0.11893656716417911
[2m[36m(func pid=57543)[0m f1_macro: 0.05552290689203302
[2m[36m(func pid=57543)[0m f1_weighted: 0.11943418591717368
[2m[36m(func pid=57543)[0m f1_per_class: [0.029, 0.125, 0.042, 0.22, 0.0, 0.0, 0.119, 0.0, 0.0, 0.02]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.01958955223880597
[2m[36m(func pid=64536)[0m top5: 0.4612873134328358
[2m[36m(func pid=64536)[0m f1_micro: 0.01958955223880597
[2m[36m(func pid=64536)[0m f1_macro: 0.025480682943187886
[2m[36m(func pid=64536)[0m f1_weighted: 0.002559575729910702
[2m[36m(func pid=64536)[0m f1_per_class: [0.06, 0.0, 0.169, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025]
[2m[36m(func pid=64536)[0m 
== Status ==
Current time: 2024-01-07 10:55:29 (running for 00:37:59.06)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  90.852 |      0.208 |                   38 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.723 |      0.056 |                   30 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.45  |      0.156 |                   23 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   3.16  |      0.025 |                    1 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.32882462686567165
[2m[36m(func pid=59162)[0m top5: 0.8549440298507462
[2m[36m(func pid=59162)[0m f1_micro: 0.32882462686567165
[2m[36m(func pid=59162)[0m f1_macro: 0.15580223472800675
[2m[36m(func pid=59162)[0m f1_weighted: 0.3033346471096763
[2m[36m(func pid=59162)[0m f1_per_class: [0.18, 0.0, 0.306, 0.457, 0.045, 0.0, 0.57, 0.0, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 91.1338 | Steps: 4 | Val loss: 55.2090 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.7791 | Steps: 4 | Val loss: 2.2778 | Batch size: 32 | lr: 0.0001 | Duration: 2.78s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 2.9499 | Steps: 4 | Val loss: 2.2267 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.3853 | Steps: 4 | Val loss: 2.0003 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=55167)[0m top1: 0.31763059701492535
[2m[36m(func pid=55167)[0m top5: 0.8218283582089553
[2m[36m(func pid=55167)[0m f1_micro: 0.31763059701492535
[2m[36m(func pid=55167)[0m f1_macro: 0.23265434699129
[2m[36m(func pid=55167)[0m f1_weighted: 0.33913991320140674
[2m[36m(func pid=55167)[0m f1_per_class: [0.085, 0.467, 0.052, 0.386, 0.0, 0.336, 0.265, 0.503, 0.0, 0.233]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.10960820895522388
[2m[36m(func pid=57543)[0m top5: 0.6161380597014925
[2m[36m(func pid=57543)[0m f1_micro: 0.10960820895522388
[2m[36m(func pid=57543)[0m f1_macro: 0.055035057097599746
[2m[36m(func pid=57543)[0m f1_weighted: 0.11717789392068605
[2m[36m(func pid=57543)[0m f1_per_class: [0.053, 0.111, 0.031, 0.209, 0.0, 0.0, 0.128, 0.0, 0.0, 0.018]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.29197761194029853
[2m[36m(func pid=64536)[0m top5: 0.6058768656716418
[2m[36m(func pid=64536)[0m f1_micro: 0.29197761194029853
[2m[36m(func pid=64536)[0m f1_macro: 0.06291231992873224
[2m[36m(func pid=64536)[0m f1_weighted: 0.14839621542080886
[2m[36m(func pid=64536)[0m f1_per_class: [0.141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.488, 0.0, 0.0, 0.0]
[2m[36m(func pid=64536)[0m 
== Status ==
Current time: 2024-01-07 10:55:34 (running for 00:38:04.56)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  91.134 |      0.233 |                   39 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.779 |      0.055 |                   31 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.385 |      0.131 |                   24 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.95  |      0.063 |                    2 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.28218283582089554
[2m[36m(func pid=59162)[0m top5: 0.8246268656716418
[2m[36m(func pid=59162)[0m f1_micro: 0.28218283582089554
[2m[36m(func pid=59162)[0m f1_macro: 0.13148403369146147
[2m[36m(func pid=59162)[0m f1_weighted: 0.26396562050831657
[2m[36m(func pid=59162)[0m f1_per_class: [0.136, 0.0, 0.242, 0.523, 0.033, 0.0, 0.381, 0.0, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 86.1511 | Steps: 4 | Val loss: 47.1425 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.6996 | Steps: 4 | Val loss: 2.2693 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 3.1678 | Steps: 4 | Val loss: 2.2649 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.5965 | Steps: 4 | Val loss: 1.9818 | Batch size: 32 | lr: 0.001 | Duration: 3.13s
[2m[36m(func pid=55167)[0m top1: 0.37220149253731344
[2m[36m(func pid=55167)[0m top5: 0.8278917910447762
[2m[36m(func pid=55167)[0m f1_micro: 0.3722014925373134
[2m[36m(func pid=55167)[0m f1_macro: 0.2490015574952218
[2m[36m(func pid=55167)[0m f1_weighted: 0.3468252106792811
[2m[36m(func pid=55167)[0m f1_per_class: [0.256, 0.523, 0.23, 0.334, 0.0, 0.316, 0.318, 0.437, 0.0, 0.077]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.12406716417910447
[2m[36m(func pid=57543)[0m top5: 0.6394589552238806
[2m[36m(func pid=57543)[0m f1_micro: 0.12406716417910447
[2m[36m(func pid=57543)[0m f1_macro: 0.061242721551460724
[2m[36m(func pid=57543)[0m f1_weighted: 0.1360036917870574
[2m[36m(func pid=57543)[0m f1_per_class: [0.045, 0.106, 0.032, 0.227, 0.0, 0.0, 0.178, 0.0, 0.0, 0.025]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.010727611940298507
[2m[36m(func pid=64536)[0m top5: 0.6119402985074627
[2m[36m(func pid=64536)[0m f1_micro: 0.010727611940298507
[2m[36m(func pid=64536)[0m f1_macro: 0.007098809007794466
[2m[36m(func pid=64536)[0m f1_weighted: 0.004156114422399099
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.016, 0.0, 0.0, 0.016, 0.0, 0.0, 0.0, 0.039, 0.0]
[2m[36m(func pid=64536)[0m 
== Status ==
Current time: 2024-01-07 10:55:40 (running for 00:38:10.35)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  86.151 |      0.249 |                   40 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.7   |      0.061 |                   32 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.596 |      0.144 |                   25 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   3.168 |      0.007 |                    3 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.271455223880597
[2m[36m(func pid=59162)[0m top5: 0.8194962686567164
[2m[36m(func pid=59162)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=59162)[0m f1_macro: 0.1442461155783962
[2m[36m(func pid=59162)[0m f1_weighted: 0.22719322328344405
[2m[36m(func pid=59162)[0m f1_per_class: [0.044, 0.0, 0.308, 0.538, 0.037, 0.09, 0.163, 0.262, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 105.2531 | Steps: 4 | Val loss: 64.2857 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.6849 | Steps: 4 | Val loss: 2.2731 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 3.1803 | Steps: 4 | Val loss: 2.2663 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=55167)[0m top1: 0.25279850746268656
[2m[36m(func pid=55167)[0m top5: 0.8115671641791045
[2m[36m(func pid=55167)[0m f1_micro: 0.25279850746268656
[2m[36m(func pid=55167)[0m f1_macro: 0.15357186020499541
[2m[36m(func pid=55167)[0m f1_weighted: 0.20748859968331937
[2m[36m(func pid=55167)[0m f1_per_class: [0.205, 0.541, 0.143, 0.291, 0.0, 0.0, 0.041, 0.221, 0.094, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.13152985074626866
[2m[36m(func pid=57543)[0m top5: 0.6305970149253731
[2m[36m(func pid=57543)[0m f1_micro: 0.13152985074626866
[2m[36m(func pid=57543)[0m f1_macro: 0.06600606400921284
[2m[36m(func pid=57543)[0m f1_weighted: 0.1396539260931364
[2m[36m(func pid=57543)[0m f1_per_class: [0.039, 0.124, 0.035, 0.241, 0.0, 0.0, 0.163, 0.016, 0.0, 0.041]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 2.4213 | Steps: 4 | Val loss: 2.0801 | Batch size: 32 | lr: 0.001 | Duration: 3.15s
[2m[36m(func pid=64536)[0m top1: 0.02845149253731343
[2m[36m(func pid=64536)[0m top5: 0.6175373134328358
[2m[36m(func pid=64536)[0m f1_micro: 0.02845149253731343
[2m[36m(func pid=64536)[0m f1_macro: 0.027870277658340437
[2m[36m(func pid=64536)[0m f1_weighted: 0.026652812410165237
[2m[36m(func pid=64536)[0m f1_per_class: [0.122, 0.14, 0.0, 0.0, 0.017, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=64536)[0m 
== Status ==
Current time: 2024-01-07 10:55:46 (running for 00:38:16.07)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 105.253 |      0.154 |                   41 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.685 |      0.066 |                   33 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.421 |      0.137 |                   26 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   3.18  |      0.028 |                    4 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.20149253731343283
[2m[36m(func pid=59162)[0m top5: 0.7807835820895522
[2m[36m(func pid=59162)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=59162)[0m f1_macro: 0.13714646047089138
[2m[36m(func pid=59162)[0m f1_weighted: 0.1692519038774912
[2m[36m(func pid=59162)[0m f1_per_class: [0.169, 0.0, 0.259, 0.486, 0.052, 0.119, 0.0, 0.214, 0.072, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 94.5119 | Steps: 4 | Val loss: 91.0235 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.6855 | Steps: 4 | Val loss: 2.2685 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 3.4652 | Steps: 4 | Val loss: 2.4574 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=55167)[0m top1: 0.14272388059701493
[2m[36m(func pid=55167)[0m top5: 0.7882462686567164
[2m[36m(func pid=55167)[0m f1_micro: 0.14272388059701493
[2m[36m(func pid=55167)[0m f1_macro: 0.15081986233406286
[2m[36m(func pid=55167)[0m f1_weighted: 0.13535489705864387
[2m[36m(func pid=55167)[0m f1_per_class: [0.379, 0.435, 0.0, 0.045, 0.0, 0.0, 0.019, 0.554, 0.076, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.14039179104477612
[2m[36m(func pid=57543)[0m top5: 0.6497201492537313
[2m[36m(func pid=57543)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=57543)[0m f1_macro: 0.06925152591232689
[2m[36m(func pid=57543)[0m f1_weighted: 0.15425940938143717
[2m[36m(func pid=57543)[0m f1_per_class: [0.029, 0.1, 0.04, 0.238, 0.0, 0.0, 0.227, 0.032, 0.0, 0.027]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 2.4165 | Steps: 4 | Val loss: 2.1438 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=64536)[0m top1: 0.016791044776119403
[2m[36m(func pid=64536)[0m top5: 0.6501865671641791
[2m[36m(func pid=64536)[0m f1_micro: 0.016791044776119403
[2m[36m(func pid=64536)[0m f1_macro: 0.035684284773088396
[2m[36m(func pid=64536)[0m f1_weighted: 0.0031815819171784375
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.005, 0.328, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.024]
[2m[36m(func pid=64536)[0m 
== Status ==
Current time: 2024-01-07 10:55:52 (running for 00:38:21.73)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  94.512 |      0.151 |                   42 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.686 |      0.069 |                   34 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.417 |      0.105 |                   27 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   3.465 |      0.036 |                    5 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.10914179104477612
[2m[36m(func pid=59162)[0m top5: 0.7705223880597015
[2m[36m(func pid=59162)[0m f1_micro: 0.10914179104477612
[2m[36m(func pid=59162)[0m f1_macro: 0.10540971818324998
[2m[36m(func pid=59162)[0m f1_weighted: 0.07092508518573845
[2m[36m(func pid=59162)[0m f1_per_class: [0.238, 0.0, 0.386, 0.193, 0.067, 0.0, 0.0, 0.15, 0.019, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 111.5172 | Steps: 4 | Val loss: 39.4597 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.8551 | Steps: 4 | Val loss: 2.2790 | Batch size: 32 | lr: 0.0001 | Duration: 2.95s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 3.7023 | Steps: 4 | Val loss: 2.5235 | Batch size: 32 | lr: 0.01 | Duration: 2.80s
[2m[36m(func pid=55167)[0m top1: 0.42723880597014924
[2m[36m(func pid=55167)[0m top5: 0.8722014925373134
[2m[36m(func pid=55167)[0m f1_micro: 0.4272388059701493
[2m[36m(func pid=55167)[0m f1_macro: 0.31092166895972817
[2m[36m(func pid=55167)[0m f1_weighted: 0.42209882851116337
[2m[36m(func pid=55167)[0m f1_per_class: [0.259, 0.44, 0.471, 0.525, 0.205, 0.0, 0.544, 0.37, 0.158, 0.138]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m top1: 0.14039179104477612
[2m[36m(func pid=57543)[0m top5: 0.644589552238806
[2m[36m(func pid=57543)[0m f1_micro: 0.14039179104477612
[2m[36m(func pid=57543)[0m f1_macro: 0.08069668553237122
[2m[36m(func pid=57543)[0m f1_weighted: 0.1620270757670194
[2m[36m(func pid=57543)[0m f1_per_class: [0.043, 0.113, 0.039, 0.22, 0.0, 0.0, 0.253, 0.047, 0.041, 0.05]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.013992537313432836
[2m[36m(func pid=64536)[0m top5: 0.6240671641791045
[2m[36m(func pid=64536)[0m f1_micro: 0.013992537313432836
[2m[36m(func pid=64536)[0m f1_macro: 0.016565087488546508
[2m[36m(func pid=64536)[0m f1_weighted: 0.010007290660204124
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.011, 0.0, 0.0, 0.016, 0.0, 0.0, 0.14, 0.0, 0.0]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.4787 | Steps: 4 | Val loss: 2.1095 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 42.9610 | Steps: 4 | Val loss: 51.8686 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.7320 | Steps: 4 | Val loss: 2.2750 | Batch size: 32 | lr: 0.0001 | Duration: 2.99s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 3.5135 | Steps: 4 | Val loss: 2.6042 | Batch size: 32 | lr: 0.01 | Duration: 2.92s
== Status ==
Current time: 2024-01-07 10:55:57 (running for 00:38:27.65)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 111.517 |      0.311 |                   43 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.855 |      0.081 |                   35 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.479 |      0.131 |                   28 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   3.702 |      0.017 |                    6 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.17817164179104478
[2m[36m(func pid=59162)[0m top5: 0.7933768656716418
[2m[36m(func pid=59162)[0m f1_micro: 0.17817164179104475
[2m[36m(func pid=59162)[0m f1_macro: 0.13129766292973338
[2m[36m(func pid=59162)[0m f1_weighted: 0.14053573725256352
[2m[36m(func pid=59162)[0m f1_per_class: [0.222, 0.0, 0.4, 0.435, 0.05, 0.0, 0.0, 0.206, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=57543)[0m top1: 0.16184701492537312
[2m[36m(func pid=57543)[0m top5: 0.6609141791044776
[2m[36m(func pid=57543)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=57543)[0m f1_macro: 0.09345007257460101
[2m[36m(func pid=57543)[0m f1_weighted: 0.19299435344449986
[2m[36m(func pid=57543)[0m f1_per_class: [0.039, 0.152, 0.048, 0.22, 0.0, 0.0, 0.33, 0.074, 0.043, 0.029]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.3344216417910448
[2m[36m(func pid=55167)[0m top5: 0.8218283582089553
[2m[36m(func pid=55167)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=55167)[0m f1_macro: 0.23238663829816258
[2m[36m(func pid=55167)[0m f1_weighted: 0.3526214761055174
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.126, 0.393, 0.518, 0.049, 0.083, 0.508, 0.333, 0.0, 0.314]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=64536)[0m top1: 0.03311567164179104
[2m[36m(func pid=64536)[0m top5: 0.4962686567164179
[2m[36m(func pid=64536)[0m f1_micro: 0.03311567164179104
[2m[36m(func pid=64536)[0m f1_macro: 0.0064108352144469525
[2m[36m(func pid=64536)[0m f1_weighted: 0.0021229911391125638
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.064, 0.0]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.4000 | Steps: 4 | Val loss: 2.0804 | Batch size: 32 | lr: 0.001 | Duration: 2.90s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.7376 | Steps: 4 | Val loss: 2.2679 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 61.5912 | Steps: 4 | Val loss: 110.4288 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 2.7664 | Steps: 4 | Val loss: 2.5746 | Batch size: 32 | lr: 0.01 | Duration: 2.73s
== Status ==
Current time: 2024-01-07 10:56:03 (running for 00:38:33.03)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  42.961 |      0.232 |                   44 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.732 |      0.093 |                   36 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.4   |      0.131 |                   29 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   3.513 |      0.006 |                    7 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.1884328358208955
[2m[36m(func pid=59162)[0m top5: 0.7751865671641791
[2m[36m(func pid=59162)[0m f1_micro: 0.1884328358208955
[2m[36m(func pid=59162)[0m f1_macro: 0.1313944799694388
[2m[36m(func pid=59162)[0m f1_weighted: 0.1644113497822927
[2m[36m(func pid=59162)[0m f1_per_class: [0.198, 0.108, 0.253, 0.438, 0.04, 0.031, 0.0, 0.247, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=57543)[0m top1: 0.16930970149253732
[2m[36m(func pid=57543)[0m top5: 0.6856343283582089
[2m[36m(func pid=57543)[0m f1_micro: 0.16930970149253732
[2m[36m(func pid=57543)[0m f1_macro: 0.09500535994655471
[2m[36m(func pid=57543)[0m f1_weighted: 0.20139607500885628
[2m[36m(func pid=57543)[0m f1_per_class: [0.04, 0.133, 0.057, 0.211, 0.0, 0.0, 0.381, 0.059, 0.048, 0.023]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.09048507462686567
[2m[36m(func pid=55167)[0m top5: 0.6231343283582089
[2m[36m(func pid=55167)[0m f1_micro: 0.09048507462686567
[2m[36m(func pid=55167)[0m f1_macro: 0.10104983420049354
[2m[36m(func pid=55167)[0m f1_weighted: 0.10953342842360528
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.005, 0.035, 0.007, 0.029, 0.02, 0.252, 0.461, 0.0, 0.202]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=64536)[0m top1: 0.013526119402985074
[2m[36m(func pid=64536)[0m top5: 0.7262126865671642
[2m[36m(func pid=64536)[0m f1_micro: 0.013526119402985074
[2m[36m(func pid=64536)[0m f1_macro: 0.01684040625963614
[2m[36m(func pid=64536)[0m f1_weighted: 0.0019562196980399643
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.0, 0.149, 0.0, 0.016, 0.0, 0.003, 0.0, 0.0, 0.0]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.3478 | Steps: 4 | Val loss: 2.0814 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.7155 | Steps: 4 | Val loss: 2.2682 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 3.0373 | Steps: 4 | Val loss: 2.7432 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 183.9619 | Steps: 4 | Val loss: 132.4287 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 10:56:09 (running for 00:38:38.71)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  61.591 |      0.101 |                   45 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.738 |      0.095 |                   37 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.348 |      0.119 |                   30 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.766 |      0.017 |                    8 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.15904850746268656
[2m[36m(func pid=59162)[0m top5: 0.8097014925373134
[2m[36m(func pid=59162)[0m f1_micro: 0.15904850746268656
[2m[36m(func pid=59162)[0m f1_macro: 0.11894107212708319
[2m[36m(func pid=59162)[0m f1_weighted: 0.15654279904742674
[2m[36m(func pid=59162)[0m f1_per_class: [0.0, 0.282, 0.103, 0.13, 0.034, 0.05, 0.134, 0.425, 0.0, 0.031]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=57543)[0m top1: 0.16651119402985073
[2m[36m(func pid=57543)[0m top5: 0.6842350746268657
[2m[36m(func pid=57543)[0m f1_micro: 0.16651119402985073
[2m[36m(func pid=57543)[0m f1_macro: 0.09955956039613324
[2m[36m(func pid=57543)[0m f1_weighted: 0.2023715179323816
[2m[36m(func pid=57543)[0m f1_per_class: [0.041, 0.2, 0.068, 0.159, 0.0, 0.0, 0.39, 0.068, 0.059, 0.01]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.13992537313432835
[2m[36m(func pid=64536)[0m top5: 0.48274253731343286
[2m[36m(func pid=64536)[0m f1_micro: 0.13992537313432835
[2m[36m(func pid=64536)[0m f1_macro: 0.11805300124799556
[2m[36m(func pid=64536)[0m f1_weighted: 0.0864802195218558
[2m[36m(func pid=64536)[0m f1_per_class: [0.129, 0.391, 0.4, 0.0, 0.098, 0.112, 0.0, 0.0, 0.0, 0.05]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=55167)[0m top1: 0.048507462686567165
[2m[36m(func pid=55167)[0m top5: 0.5503731343283582
[2m[36m(func pid=55167)[0m f1_micro: 0.048507462686567165
[2m[36m(func pid=55167)[0m f1_macro: 0.0699865817972174
[2m[36m(func pid=55167)[0m f1_weighted: 0.03902413003013309
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.042, 0.13, 0.003, 0.043, 0.059, 0.003, 0.373, 0.0, 0.047]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.3652 | Steps: 4 | Val loss: 2.0943 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.7859 | Steps: 4 | Val loss: 2.2717 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 2.6003 | Steps: 4 | Val loss: 1.8066 | Batch size: 32 | lr: 0.01 | Duration: 2.76s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 137.1653 | Steps: 4 | Val loss: 94.1744 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 10:56:14 (running for 00:38:44.32)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 183.962 |      0.07  |                   46 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.715 |      0.1   |                   38 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.365 |      0.111 |                   31 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   3.037 |      0.118 |                    9 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.14692164179104478
[2m[36m(func pid=59162)[0m top5: 0.8330223880597015
[2m[36m(func pid=59162)[0m f1_micro: 0.14692164179104478
[2m[36m(func pid=59162)[0m f1_macro: 0.11067265623482862
[2m[36m(func pid=59162)[0m f1_weighted: 0.17164959317781675
[2m[36m(func pid=59162)[0m f1_per_class: [0.043, 0.286, 0.098, 0.032, 0.039, 0.056, 0.308, 0.228, 0.0, 0.017]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=57543)[0m top1: 0.16884328358208955
[2m[36m(func pid=57543)[0m top5: 0.6767723880597015
[2m[36m(func pid=57543)[0m f1_micro: 0.16884328358208955
[2m[36m(func pid=57543)[0m f1_macro: 0.1106828205694604
[2m[36m(func pid=57543)[0m f1_weighted: 0.1985328200988306
[2m[36m(func pid=57543)[0m f1_per_class: [0.061, 0.271, 0.088, 0.118, 0.029, 0.0, 0.367, 0.094, 0.058, 0.021]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.373134328358209
[2m[36m(func pid=64536)[0m top5: 0.9071828358208955
[2m[36m(func pid=64536)[0m f1_micro: 0.373134328358209
[2m[36m(func pid=64536)[0m f1_macro: 0.15278324506183066
[2m[36m(func pid=64536)[0m f1_weighted: 0.30005772496442246
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.0, 0.432, 0.563, 0.0, 0.0, 0.468, 0.0, 0.0, 0.064]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=55167)[0m top1: 0.2224813432835821
[2m[36m(func pid=55167)[0m top5: 0.6861007462686567
[2m[36m(func pid=55167)[0m f1_micro: 0.2224813432835821
[2m[36m(func pid=55167)[0m f1_macro: 0.1886040229856129
[2m[36m(func pid=55167)[0m f1_weighted: 0.15147512921058223
[2m[36m(func pid=55167)[0m f1_per_class: [0.237, 0.48, 0.145, 0.013, 0.123, 0.261, 0.0, 0.463, 0.0, 0.163]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 2.3153 | Steps: 4 | Val loss: 2.1277 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.6896 | Steps: 4 | Val loss: 2.2523 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 2.9268 | Steps: 4 | Val loss: 2.4522 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 74.6902 | Steps: 4 | Val loss: 80.8634 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:56:20 (running for 00:38:49.93)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 137.165 |      0.189 |                   47 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.786 |      0.111 |                   39 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.315 |      0.139 |                   32 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.6   |      0.153 |                   10 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.13386194029850745
[2m[36m(func pid=59162)[0m top5: 0.792910447761194
[2m[36m(func pid=59162)[0m f1_micro: 0.13386194029850745
[2m[36m(func pid=59162)[0m f1_macro: 0.13907100728882268
[2m[36m(func pid=59162)[0m f1_weighted: 0.14383248229449616
[2m[36m(func pid=59162)[0m f1_per_class: [0.069, 0.311, 0.182, 0.0, 0.058, 0.079, 0.18, 0.339, 0.14, 0.032]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=57543)[0m top1: 0.1921641791044776
[2m[36m(func pid=57543)[0m top5: 0.6996268656716418
[2m[36m(func pid=57543)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=57543)[0m f1_macro: 0.12460120280441758
[2m[36m(func pid=57543)[0m f1_weighted: 0.21468460066194914
[2m[36m(func pid=57543)[0m f1_per_class: [0.076, 0.306, 0.117, 0.136, 0.0, 0.0, 0.374, 0.126, 0.078, 0.032]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.07835820895522388
[2m[36m(func pid=64536)[0m top5: 0.7486007462686567
[2m[36m(func pid=64536)[0m f1_micro: 0.07835820895522388
[2m[36m(func pid=64536)[0m f1_macro: 0.041907281425305025
[2m[36m(func pid=64536)[0m f1_weighted: 0.014963513264440488
[2m[36m(func pid=64536)[0m f1_per_class: [0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.182, 0.0, 0.047]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=55167)[0m top1: 0.28544776119402987
[2m[36m(func pid=55167)[0m top5: 0.7602611940298507
[2m[36m(func pid=55167)[0m f1_micro: 0.28544776119402987
[2m[36m(func pid=55167)[0m f1_macro: 0.26071491001117164
[2m[36m(func pid=55167)[0m f1_weighted: 0.17639103239353676
[2m[36m(func pid=55167)[0m f1_per_class: [0.215, 0.471, 0.667, 0.047, 0.277, 0.338, 0.009, 0.506, 0.0, 0.077]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.4529 | Steps: 4 | Val loss: 2.1584 | Batch size: 32 | lr: 0.001 | Duration: 3.10s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.6876 | Steps: 4 | Val loss: 2.2536 | Batch size: 32 | lr: 0.0001 | Duration: 2.90s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 3.3322 | Steps: 4 | Val loss: 3.1568 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 67.2627 | Steps: 4 | Val loss: 61.8729 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=59162)[0m top1: 0.12080223880597014
[2m[36m(func pid=59162)[0m top5: 0.7238805970149254
[2m[36m(func pid=59162)[0m f1_micro: 0.12080223880597014
[2m[36m(func pid=59162)[0m f1_macro: 0.15561519412141647
[2m[36m(func pid=59162)[0m f1_weighted: 0.10221161344330444
[2m[36m(func pid=59162)[0m f1_per_class: [0.179, 0.272, 0.275, 0.0, 0.078, 0.074, 0.025, 0.509, 0.116, 0.029]
== Status ==
Current time: 2024-01-07 10:56:25 (running for 00:38:55.61)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  74.69  |      0.261 |                   48 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.69  |      0.125 |                   40 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.453 |      0.156 |                   33 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.927 |      0.042 |                   11 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=57543)[0m top1: 0.18190298507462688
[2m[36m(func pid=57543)[0m top5: 0.679570895522388
[2m[36m(func pid=57543)[0m f1_micro: 0.1819029850746269
[2m[36m(func pid=57543)[0m f1_macro: 0.12206382962929323
[2m[36m(func pid=57543)[0m f1_weighted: 0.20363520371428948
[2m[36m(func pid=57543)[0m f1_per_class: [0.076, 0.329, 0.119, 0.173, 0.0, 0.0, 0.287, 0.143, 0.068, 0.025]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.17210820895522388
[2m[36m(func pid=64536)[0m top5: 0.5890858208955224
[2m[36m(func pid=64536)[0m f1_micro: 0.17210820895522388
[2m[36m(func pid=64536)[0m f1_macro: 0.15812346185605003
[2m[36m(func pid=64536)[0m f1_weighted: 0.15768360695077616
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.353, 0.19, 0.0, 0.097, 0.322, 0.128, 0.262, 0.106, 0.124]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=55167)[0m top1: 0.4449626865671642
[2m[36m(func pid=55167)[0m top5: 0.8250932835820896
[2m[36m(func pid=55167)[0m f1_micro: 0.4449626865671642
[2m[36m(func pid=55167)[0m f1_macro: 0.30998344815355167
[2m[36m(func pid=55167)[0m f1_weighted: 0.402244809071114
[2m[36m(func pid=55167)[0m f1_per_class: [0.272, 0.489, 0.643, 0.546, 0.276, 0.032, 0.424, 0.419, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.5014 | Steps: 4 | Val loss: 2.1486 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.7126 | Steps: 4 | Val loss: 2.2619 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 4.4770 | Steps: 4 | Val loss: 3.4872 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 88.4008 | Steps: 4 | Val loss: 63.9671 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 10:56:31 (running for 00:39:01.15)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  67.263 |      0.31  |                   49 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.688 |      0.122 |                   41 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.501 |      0.177 |                   34 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   3.332 |      0.158 |                   12 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.17024253731343283
[2m[36m(func pid=59162)[0m top5: 0.695429104477612
[2m[36m(func pid=59162)[0m f1_micro: 0.17024253731343283
[2m[36m(func pid=59162)[0m f1_macro: 0.17728362406185352
[2m[36m(func pid=59162)[0m f1_weighted: 0.10411142308621231
[2m[36m(func pid=59162)[0m f1_per_class: [0.239, 0.379, 0.516, 0.0, 0.085, 0.059, 0.0, 0.319, 0.136, 0.039]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=64536)[0m top1: 0.17490671641791045
[2m[36m(func pid=64536)[0m top5: 0.6296641791044776
[2m[36m(func pid=64536)[0m f1_micro: 0.17490671641791045
[2m[36m(func pid=64536)[0m f1_macro: 0.08415985145262601
[2m[36m(func pid=64536)[0m f1_weighted: 0.09190909864455543
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.347, 0.128, 0.0, 0.082, 0.263, 0.0, 0.0, 0.022, 0.0]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.15764925373134328
[2m[36m(func pid=57543)[0m top5: 0.6585820895522388
[2m[36m(func pid=57543)[0m f1_micro: 0.15764925373134328
[2m[36m(func pid=57543)[0m f1_macro: 0.1131114890703693
[2m[36m(func pid=57543)[0m f1_weighted: 0.17426364395931018
[2m[36m(func pid=57543)[0m f1_per_class: [0.067, 0.319, 0.115, 0.138, 0.0, 0.0, 0.22, 0.187, 0.063, 0.022]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.44776119402985076
[2m[36m(func pid=55167)[0m top5: 0.7989738805970149
[2m[36m(func pid=55167)[0m f1_micro: 0.44776119402985076
[2m[36m(func pid=55167)[0m f1_macro: 0.23831757891842803
[2m[36m(func pid=55167)[0m f1_weighted: 0.3939201844388827
[2m[36m(func pid=55167)[0m f1_per_class: [0.264, 0.324, 0.24, 0.554, 0.219, 0.0, 0.541, 0.242, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.7430 | Steps: 4 | Val loss: 2.2523 | Batch size: 32 | lr: 0.0001 | Duration: 2.94s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.3882 | Steps: 4 | Val loss: 2.1101 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 3.1461 | Steps: 4 | Val loss: 2.3631 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 118.7074 | Steps: 4 | Val loss: 70.2754 | Batch size: 32 | lr: 0.1 | Duration: 2.81s
== Status ==
Current time: 2024-01-07 10:56:36 (running for 00:39:06.60)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  88.401 |      0.238 |                   50 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.713 |      0.113 |                   42 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.501 |      0.177 |                   34 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   3.146 |      0.138 |                   14 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.1837686567164179
[2m[36m(func pid=57543)[0m top5: 0.6823694029850746
[2m[36m(func pid=57543)[0m f1_micro: 0.18376865671641787
[2m[36m(func pid=57543)[0m f1_macro: 0.1314304477142851
[2m[36m(func pid=57543)[0m f1_weighted: 0.19195913274638488
[2m[36m(func pid=57543)[0m f1_per_class: [0.075, 0.378, 0.127, 0.158, 0.0, 0.0, 0.208, 0.28, 0.072, 0.017]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.14738805970149255
[2m[36m(func pid=64536)[0m top5: 0.6674440298507462
[2m[36m(func pid=64536)[0m f1_micro: 0.14738805970149255
[2m[36m(func pid=64536)[0m f1_macro: 0.1384912300847183
[2m[36m(func pid=64536)[0m f1_weighted: 0.1571358333202272
[2m[36m(func pid=64536)[0m f1_per_class: [0.155, 0.031, 0.56, 0.0, 0.026, 0.085, 0.445, 0.0, 0.083, 0.0]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=59162)[0m top1: 0.16184701492537312
[2m[36m(func pid=59162)[0m top5: 0.7896455223880597
[2m[36m(func pid=59162)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=59162)[0m f1_macro: 0.15106496429593164
[2m[36m(func pid=59162)[0m f1_weighted: 0.10036589082916957
[2m[36m(func pid=59162)[0m f1_per_class: [0.135, 0.4, 0.522, 0.007, 0.087, 0.073, 0.0, 0.247, 0.0, 0.04]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m top1: 0.35027985074626866
[2m[36m(func pid=55167)[0m top5: 0.7747201492537313
[2m[36m(func pid=55167)[0m f1_micro: 0.35027985074626866
[2m[36m(func pid=55167)[0m f1_macro: 0.21632122608610072
[2m[36m(func pid=55167)[0m f1_weighted: 0.36513354937297576
[2m[36m(func pid=55167)[0m f1_per_class: [0.091, 0.368, 0.075, 0.405, 0.136, 0.0, 0.526, 0.404, 0.158, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 2.3702 | Steps: 4 | Val loss: 2.3369 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.6703 | Steps: 4 | Val loss: 2.2404 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.3449 | Steps: 4 | Val loss: 2.0395 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 102.3115 | Steps: 4 | Val loss: 138.1230 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 10:56:42 (running for 00:39:11.92)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 118.707 |      0.216 |                   51 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.67  |      0.137 |                   44 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.388 |      0.151 |                   35 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   3.146 |      0.138 |                   14 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.20382462686567165
[2m[36m(func pid=57543)[0m top5: 0.6986940298507462
[2m[36m(func pid=57543)[0m f1_micro: 0.20382462686567165
[2m[36m(func pid=57543)[0m f1_macro: 0.13737655210255917
[2m[36m(func pid=57543)[0m f1_weighted: 0.19480789078557764
[2m[36m(func pid=57543)[0m f1_per_class: [0.078, 0.394, 0.133, 0.152, 0.0, 0.008, 0.203, 0.311, 0.08, 0.014]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.353544776119403
[2m[36m(func pid=64536)[0m top5: 0.7621268656716418
[2m[36m(func pid=64536)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=64536)[0m f1_macro: 0.19622734409077625
[2m[36m(func pid=64536)[0m f1_weighted: 0.22504222353807826
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.0, 0.48, 0.546, 0.069, 0.425, 0.0, 0.337, 0.0, 0.105]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=59162)[0m top1: 0.19869402985074627
[2m[36m(func pid=59162)[0m top5: 0.8708022388059702
[2m[36m(func pid=59162)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=59162)[0m f1_macro: 0.14146120095456408
[2m[36m(func pid=59162)[0m f1_weighted: 0.19806553912284042
[2m[36m(func pid=59162)[0m f1_per_class: [0.13, 0.325, 0.159, 0.353, 0.086, 0.11, 0.051, 0.2, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m top1: 0.125
[2m[36m(func pid=55167)[0m top5: 0.5648320895522388
[2m[36m(func pid=55167)[0m f1_micro: 0.125
[2m[36m(func pid=55167)[0m f1_macro: 0.15489917586475274
[2m[36m(func pid=55167)[0m f1_weighted: 0.09904144089859783
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.342, 0.465, 0.0, 0.141, 0.0, 0.015, 0.496, 0.089, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.7866 | Steps: 4 | Val loss: 2.2392 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 3.4040 | Steps: 4 | Val loss: 2.7058 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 2.5463 | Steps: 4 | Val loss: 2.0587 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 108.7294 | Steps: 4 | Val loss: 120.8826 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 10:56:47 (running for 00:39:17.07)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 102.312 |      0.155 |                   52 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.67  |      0.137 |                   44 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.345 |      0.141 |                   36 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   3.404 |      0.112 |                   16 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.09281716417910447
[2m[36m(func pid=64536)[0m top5: 0.6856343283582089
[2m[36m(func pid=64536)[0m f1_micro: 0.09281716417910447
[2m[36m(func pid=64536)[0m f1_macro: 0.11169556516439269
[2m[36m(func pid=64536)[0m f1_weighted: 0.049521526274198605
[2m[36m(func pid=64536)[0m f1_per_class: [0.15, 0.0, 0.227, 0.016, 0.255, 0.238, 0.0, 0.188, 0.0, 0.044]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.197294776119403
[2m[36m(func pid=57543)[0m top5: 0.6907649253731343
[2m[36m(func pid=57543)[0m f1_micro: 0.197294776119403
[2m[36m(func pid=57543)[0m f1_macro: 0.11945247787744744
[2m[36m(func pid=57543)[0m f1_weighted: 0.15844018902377463
[2m[36m(func pid=57543)[0m f1_per_class: [0.096, 0.37, 0.111, 0.096, 0.0, 0.0, 0.155, 0.292, 0.06, 0.016]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.25699626865671643
[2m[36m(func pid=59162)[0m top5: 0.8638059701492538
[2m[36m(func pid=59162)[0m f1_micro: 0.25699626865671643
[2m[36m(func pid=59162)[0m f1_macro: 0.1576419547344732
[2m[36m(func pid=59162)[0m f1_weighted: 0.2183407995504052
[2m[36m(func pid=59162)[0m f1_per_class: [0.0, 0.063, 0.071, 0.508, 0.108, 0.385, 0.012, 0.258, 0.0, 0.17]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m top1: 0.15578358208955223
[2m[36m(func pid=55167)[0m top5: 0.5377798507462687
[2m[36m(func pid=55167)[0m f1_micro: 0.15578358208955223
[2m[36m(func pid=55167)[0m f1_macro: 0.12417401585639602
[2m[36m(func pid=55167)[0m f1_weighted: 0.13950689054047768
[2m[36m(func pid=55167)[0m f1_per_class: [0.026, 0.363, 0.163, 0.0, 0.096, 0.0, 0.182, 0.283, 0.129, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.7503 | Steps: 4 | Val loss: 2.2325 | Batch size: 32 | lr: 0.0001 | Duration: 2.86s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 2.6183 | Steps: 4 | Val loss: 2.9679 | Batch size: 32 | lr: 0.01 | Duration: 3.11s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 2.4415 | Steps: 4 | Val loss: 2.0409 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 86.8836 | Steps: 4 | Val loss: 102.9114 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 10:56:52 (running for 00:39:22.60)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 108.729 |      0.124 |                   53 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.75  |      0.104 |                   46 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.546 |      0.158 |                   37 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   3.404 |      0.112 |                   16 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.19076492537313433
[2m[36m(func pid=57543)[0m top5: 0.7066231343283582
[2m[36m(func pid=57543)[0m f1_micro: 0.19076492537313436
[2m[36m(func pid=57543)[0m f1_macro: 0.10409621614388269
[2m[36m(func pid=57543)[0m f1_weighted: 0.1453208265915073
[2m[36m(func pid=57543)[0m f1_per_class: [0.083, 0.35, 0.105, 0.094, 0.0, 0.0, 0.148, 0.182, 0.044, 0.035]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.08815298507462686
[2m[36m(func pid=64536)[0m top5: 0.5918843283582089
[2m[36m(func pid=64536)[0m f1_micro: 0.08815298507462686
[2m[36m(func pid=64536)[0m f1_macro: 0.06422134302411722
[2m[36m(func pid=64536)[0m f1_weighted: 0.09427209892628269
[2m[36m(func pid=64536)[0m f1_per_class: [0.079, 0.0, 0.171, 0.0, 0.027, 0.0, 0.304, 0.0, 0.0, 0.06]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=59162)[0m top1: 0.24673507462686567
[2m[36m(func pid=59162)[0m top5: 0.8129664179104478
[2m[36m(func pid=59162)[0m f1_micro: 0.24673507462686567
[2m[36m(func pid=59162)[0m f1_macro: 0.1447873252580599
[2m[36m(func pid=59162)[0m f1_weighted: 0.1962733179593619
[2m[36m(func pid=59162)[0m f1_per_class: [0.0, 0.0, 0.114, 0.461, 0.087, 0.417, 0.0, 0.31, 0.0, 0.058]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m top1: 0.21361940298507462
[2m[36m(func pid=55167)[0m top5: 0.6324626865671642
[2m[36m(func pid=55167)[0m f1_micro: 0.21361940298507465
[2m[36m(func pid=55167)[0m f1_macro: 0.1518123969844692
[2m[36m(func pid=55167)[0m f1_weighted: 0.22593610121625607
[2m[36m(func pid=55167)[0m f1_per_class: [0.049, 0.396, 0.09, 0.0, 0.117, 0.13, 0.408, 0.329, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.7621 | Steps: 4 | Val loss: 2.2301 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 2.7011 | Steps: 4 | Val loss: 3.0135 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 2.3121 | Steps: 4 | Val loss: 2.0637 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 128.6799 | Steps: 4 | Val loss: 77.4584 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
== Status ==
Current time: 2024-01-07 10:56:58 (running for 00:39:27.87)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  86.884 |      0.152 |                   54 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.762 |      0.079 |                   47 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.442 |      0.145 |                   38 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.618 |      0.064 |                   17 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.17257462686567165
[2m[36m(func pid=57543)[0m top5: 0.7145522388059702
[2m[36m(func pid=57543)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=57543)[0m f1_macro: 0.07883751999420344
[2m[36m(func pid=57543)[0m f1_weighted: 0.10612439040803869
[2m[36m(func pid=57543)[0m f1_per_class: [0.08, 0.323, 0.085, 0.034, 0.0, 0.0, 0.1, 0.139, 0.027, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.08488805970149253
[2m[36m(func pid=64536)[0m top5: 0.7238805970149254
[2m[36m(func pid=64536)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=64536)[0m f1_macro: 0.04776483553166159
[2m[36m(func pid=64536)[0m f1_weighted: 0.04855553018220428
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.259, 0.185, 0.0, 0.025, 0.0, 0.009, 0.0, 0.0, 0.0]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=59162)[0m top1: 0.15485074626865672
[2m[36m(func pid=59162)[0m top5: 0.8213619402985075
[2m[36m(func pid=59162)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=59162)[0m f1_macro: 0.1850495597553783
[2m[36m(func pid=59162)[0m f1_weighted: 0.12333875176120464
[2m[36m(func pid=59162)[0m f1_per_class: [0.234, 0.0, 0.474, 0.145, 0.093, 0.419, 0.0, 0.454, 0.0, 0.032]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m top1: 0.3050373134328358
[2m[36m(func pid=55167)[0m top5: 0.7173507462686567
[2m[36m(func pid=55167)[0m f1_micro: 0.3050373134328358
[2m[36m(func pid=55167)[0m f1_macro: 0.2021373414048789
[2m[36m(func pid=55167)[0m f1_weighted: 0.29134417509414495
[2m[36m(func pid=55167)[0m f1_per_class: [0.068, 0.421, 0.087, 0.007, 0.179, 0.418, 0.488, 0.353, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.6652 | Steps: 4 | Val loss: 2.2160 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 2.7523 | Steps: 4 | Val loss: 2.2076 | Batch size: 32 | lr: 0.01 | Duration: 3.01s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 69.4577 | Steps: 4 | Val loss: 67.5348 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.3848 | Steps: 4 | Val loss: 2.1024 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 10:57:03 (running for 00:39:33.10)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 128.68  |      0.202 |                   55 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.665 |      0.077 |                   48 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.312 |      0.185 |                   39 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.701 |      0.048 |                   18 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.1814365671641791
[2m[36m(func pid=57543)[0m top5: 0.753731343283582
[2m[36m(func pid=57543)[0m f1_micro: 0.1814365671641791
[2m[36m(func pid=57543)[0m f1_macro: 0.07696163241536484
[2m[36m(func pid=57543)[0m f1_weighted: 0.10371311884271824
[2m[36m(func pid=57543)[0m f1_per_class: [0.09, 0.324, 0.1, 0.003, 0.0, 0.0, 0.125, 0.128, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.2294776119402985
[2m[36m(func pid=64536)[0m top5: 0.8171641791044776
[2m[36m(func pid=64536)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=64536)[0m f1_macro: 0.2369532882916833
[2m[36m(func pid=64536)[0m f1_weighted: 0.2028755612126292
[2m[36m(func pid=64536)[0m f1_per_class: [0.329, 0.0, 0.615, 0.483, 0.039, 0.2, 0.0, 0.511, 0.123, 0.069]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=55167)[0m top1: 0.314365671641791
[2m[36m(func pid=55167)[0m top5: 0.8125
[2m[36m(func pid=55167)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=55167)[0m f1_macro: 0.2266833669061179
[2m[36m(func pid=55167)[0m f1_weighted: 0.30687998327470306
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.3, 0.468, 0.362, 0.235, 0.316, 0.332, 0.253, 0.0, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m top1: 0.13292910447761194
[2m[36m(func pid=59162)[0m top5: 0.8138992537313433
[2m[36m(func pid=59162)[0m f1_micro: 0.13292910447761194
[2m[36m(func pid=59162)[0m f1_macro: 0.18596977177041818
[2m[36m(func pid=59162)[0m f1_weighted: 0.09145882619622131
[2m[36m(func pid=59162)[0m f1_per_class: [0.24, 0.0, 0.556, 0.029, 0.108, 0.392, 0.0, 0.502, 0.0, 0.032]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.6762 | Steps: 4 | Val loss: 2.2048 | Batch size: 32 | lr: 0.0001 | Duration: 3.10s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 1.9653 | Steps: 4 | Val loss: 2.1180 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 82.4413 | Steps: 4 | Val loss: 71.3894 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.5319 | Steps: 4 | Val loss: 2.1217 | Batch size: 32 | lr: 0.001 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 10:57:08 (running for 00:39:38.54)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  69.458 |      0.227 |                   56 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.665 |      0.077 |                   48 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.385 |      0.186 |                   40 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.965 |      0.177 |                   20 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.21921641791044777
[2m[36m(func pid=64536)[0m top5: 0.8768656716417911
[2m[36m(func pid=64536)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=64536)[0m f1_macro: 0.1774121894360951
[2m[36m(func pid=64536)[0m f1_weighted: 0.1442429219080132
[2m[36m(func pid=64536)[0m f1_per_class: [0.126, 0.394, 0.522, 0.032, 0.0, 0.377, 0.0, 0.323, 0.0, 0.0]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.19496268656716417
[2m[36m(func pid=57543)[0m top5: 0.7784514925373134
[2m[36m(func pid=57543)[0m f1_micro: 0.19496268656716417
[2m[36m(func pid=57543)[0m f1_macro: 0.08726731043537271
[2m[36m(func pid=57543)[0m f1_weighted: 0.11379057043195706
[2m[36m(func pid=57543)[0m f1_per_class: [0.122, 0.322, 0.129, 0.0, 0.0, 0.0, 0.157, 0.143, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.3558768656716418
[2m[36m(func pid=55167)[0m top5: 0.8348880597014925
[2m[36m(func pid=55167)[0m f1_micro: 0.3558768656716418
[2m[36m(func pid=55167)[0m f1_macro: 0.2871700339930057
[2m[36m(func pid=55167)[0m f1_weighted: 0.3123165001484132
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.226, 0.769, 0.556, 0.206, 0.336, 0.15, 0.465, 0.0, 0.162]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m top1: 0.12826492537313433
[2m[36m(func pid=59162)[0m top5: 0.7989738805970149
[2m[36m(func pid=59162)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=59162)[0m f1_macro: 0.15316238822158845
[2m[36m(func pid=59162)[0m f1_weighted: 0.08952918939804155
[2m[36m(func pid=59162)[0m f1_per_class: [0.228, 0.0, 0.303, 0.056, 0.112, 0.349, 0.0, 0.453, 0.0, 0.03]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 2.3849 | Steps: 4 | Val loss: 2.0859 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.6660 | Steps: 4 | Val loss: 2.2006 | Batch size: 32 | lr: 0.0001 | Duration: 2.91s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 64.5221 | Steps: 4 | Val loss: 78.0865 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 2.6261 | Steps: 4 | Val loss: 2.1817 | Batch size: 32 | lr: 0.001 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 10:57:13 (running for 00:39:43.65)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  82.441 |      0.287 |                   57 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.676 |      0.087 |                   49 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.532 |      0.153 |                   41 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.385 |      0.194 |                   21 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.394589552238806
[2m[36m(func pid=64536)[0m top5: 0.8936567164179104
[2m[36m(func pid=64536)[0m f1_micro: 0.394589552238806
[2m[36m(func pid=64536)[0m f1_macro: 0.1938996218405131
[2m[36m(func pid=64536)[0m f1_weighted: 0.2905342874740692
[2m[36m(func pid=64536)[0m f1_per_class: [0.222, 0.504, 0.179, 0.0, 0.0, 0.0, 0.591, 0.366, 0.0, 0.077]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.19309701492537312
[2m[36m(func pid=57543)[0m top5: 0.7989738805970149
[2m[36m(func pid=57543)[0m f1_micro: 0.19309701492537315
[2m[36m(func pid=57543)[0m f1_macro: 0.07950950113912993
[2m[36m(func pid=57543)[0m f1_weighted: 0.10664194504377211
[2m[36m(func pid=57543)[0m f1_per_class: [0.106, 0.318, 0.122, 0.0, 0.0, 0.0, 0.144, 0.105, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.27705223880597013
[2m[36m(func pid=55167)[0m top5: 0.7751865671641791
[2m[36m(func pid=55167)[0m f1_micro: 0.27705223880597013
[2m[36m(func pid=55167)[0m f1_macro: 0.2499702898138147
[2m[36m(func pid=55167)[0m f1_weighted: 0.266142116999523
[2m[36m(func pid=55167)[0m f1_per_class: [0.179, 0.098, 0.741, 0.554, 0.122, 0.169, 0.133, 0.448, 0.0, 0.056]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m top1: 0.134794776119403
[2m[36m(func pid=59162)[0m top5: 0.7369402985074627
[2m[36m(func pid=59162)[0m f1_micro: 0.134794776119403
[2m[36m(func pid=59162)[0m f1_macro: 0.12644944365911215
[2m[36m(func pid=59162)[0m f1_weighted: 0.10683858798254318
[2m[36m(func pid=59162)[0m f1_per_class: [0.041, 0.06, 0.111, 0.117, 0.062, 0.282, 0.0, 0.439, 0.111, 0.042]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 2.5461 | Steps: 4 | Val loss: 1.8493 | Batch size: 32 | lr: 0.01 | Duration: 2.84s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.8069 | Steps: 4 | Val loss: 2.2096 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 66.8829 | Steps: 4 | Val loss: 154.7037 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.3415 | Steps: 4 | Val loss: 2.2104 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
== Status ==
Current time: 2024-01-07 10:57:19 (running for 00:39:48.89)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  64.522 |      0.25  |                   58 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.666 |      0.08  |                   50 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.626 |      0.126 |                   42 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.546 |      0.268 |                   22 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.36847014925373134
[2m[36m(func pid=64536)[0m top5: 0.8362873134328358
[2m[36m(func pid=64536)[0m f1_micro: 0.3684701492537314
[2m[36m(func pid=64536)[0m f1_macro: 0.26823109965002556
[2m[36m(func pid=64536)[0m f1_weighted: 0.31259202354619164
[2m[36m(func pid=64536)[0m f1_per_class: [0.29, 0.499, 0.367, 0.507, 0.0, 0.367, 0.022, 0.465, 0.0, 0.167]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.18610074626865672
[2m[36m(func pid=57543)[0m top5: 0.7971082089552238
[2m[36m(func pid=57543)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=57543)[0m f1_macro: 0.0721450262339117
[2m[36m(func pid=57543)[0m f1_weighted: 0.10622322067784748
[2m[36m(func pid=57543)[0m f1_per_class: [0.065, 0.318, 0.077, 0.0, 0.0, 0.0, 0.144, 0.118, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.09235074626865672
[2m[36m(func pid=55167)[0m top5: 0.6301305970149254
[2m[36m(func pid=55167)[0m f1_micro: 0.09235074626865672
[2m[36m(func pid=55167)[0m f1_macro: 0.15411341763336212
[2m[36m(func pid=55167)[0m f1_weighted: 0.08925340731615057
[2m[36m(func pid=55167)[0m f1_per_class: [0.17, 0.159, 0.426, 0.072, 0.187, 0.0, 0.022, 0.475, 0.0, 0.03]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m top1: 0.11240671641791045
[2m[36m(func pid=59162)[0m top5: 0.7028917910447762
[2m[36m(func pid=59162)[0m f1_micro: 0.11240671641791045
[2m[36m(func pid=59162)[0m f1_macro: 0.11313148561039535
[2m[36m(func pid=59162)[0m f1_weighted: 0.0659053926674896
[2m[36m(func pid=59162)[0m f1_per_class: [0.19, 0.057, 0.163, 0.02, 0.053, 0.24, 0.0, 0.238, 0.103, 0.067]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 1.9570 | Steps: 4 | Val loss: 2.5298 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.7169 | Steps: 4 | Val loss: 2.2193 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 121.1840 | Steps: 4 | Val loss: 178.7362 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3021 | Steps: 4 | Val loss: 2.2254 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=64536)[0m top1: 0.2583955223880597
[2m[36m(func pid=64536)[0m top5: 0.6450559701492538
[2m[36m(func pid=64536)[0m f1_micro: 0.2583955223880597
[2m[36m(func pid=64536)[0m f1_macro: 0.20949804262492452
[2m[36m(func pid=64536)[0m f1_weighted: 0.2320380787728906
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.157, 0.253, 0.487, 0.073, 0.267, 0.0, 0.51, 0.138, 0.211]
[2m[36m(func pid=64536)[0m 
== Status ==
Current time: 2024-01-07 10:57:24 (running for 00:39:54.15)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  66.883 |      0.154 |                   59 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.807 |      0.072 |                   51 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.341 |      0.113 |                   43 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.957 |      0.209 |                   23 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.18050373134328357
[2m[36m(func pid=57543)[0m top5: 0.7868470149253731
[2m[36m(func pid=57543)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=57543)[0m f1_macro: 0.07676349927055871
[2m[36m(func pid=57543)[0m f1_weighted: 0.10940501699876162
[2m[36m(func pid=57543)[0m f1_per_class: [0.065, 0.317, 0.061, 0.013, 0.0, 0.0, 0.131, 0.181, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m top1: 0.08255597014925373
[2m[36m(func pid=55167)[0m top5: 0.5438432835820896
[2m[36m(func pid=55167)[0m f1_micro: 0.08255597014925373
[2m[36m(func pid=55167)[0m f1_macro: 0.13393808796653855
[2m[36m(func pid=55167)[0m f1_weighted: 0.07672690119970212
[2m[36m(func pid=55167)[0m f1_per_class: [0.134, 0.115, 0.4, 0.0, 0.122, 0.0, 0.082, 0.454, 0.0, 0.033]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m top1: 0.10494402985074627
[2m[36m(func pid=59162)[0m top5: 0.7672574626865671
[2m[36m(func pid=59162)[0m f1_micro: 0.10494402985074627
[2m[36m(func pid=59162)[0m f1_macro: 0.12497909300385508
[2m[36m(func pid=59162)[0m f1_weighted: 0.06678910074258526
[2m[36m(func pid=59162)[0m f1_per_class: [0.134, 0.118, 0.48, 0.003, 0.045, 0.22, 0.0, 0.25, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 2.3390 | Steps: 4 | Val loss: 3.1835 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.6521 | Steps: 4 | Val loss: 2.2149 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 78.5816 | Steps: 4 | Val loss: 140.5678 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 10:57:29 (running for 00:39:59.19)
Memory usage on this node: 25.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  | 121.184 |      0.134 |                   60 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.717 |      0.077 |                   52 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.302 |      0.125 |                   44 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.339 |      0.123 |                   24 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.08488805970149253
[2m[36m(func pid=64536)[0m top5: 0.707089552238806
[2m[36m(func pid=64536)[0m f1_micro: 0.08488805970149253
[2m[36m(func pid=64536)[0m f1_macro: 0.12268693673186411
[2m[36m(func pid=64536)[0m f1_weighted: 0.08443459227965622
[2m[36m(func pid=64536)[0m f1_per_class: [0.324, 0.073, 0.2, 0.129, 0.02, 0.0, 0.0, 0.481, 0.0, 0.0]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.18889925373134328
[2m[36m(func pid=57543)[0m top5: 0.7887126865671642
[2m[36m(func pid=57543)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=57543)[0m f1_macro: 0.08644737391170543
[2m[36m(func pid=57543)[0m f1_weighted: 0.12504855929608713
[2m[36m(func pid=57543)[0m f1_per_class: [0.09, 0.323, 0.063, 0.032, 0.0, 0.0, 0.157, 0.2, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.3649 | Steps: 4 | Val loss: 2.1409 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=55167)[0m top1: 0.13899253731343283
[2m[36m(func pid=55167)[0m top5: 0.5750932835820896
[2m[36m(func pid=55167)[0m f1_micro: 0.13899253731343283
[2m[36m(func pid=55167)[0m f1_macro: 0.14236855817174593
[2m[36m(func pid=55167)[0m f1_weighted: 0.1536952274446172
[2m[36m(func pid=55167)[0m f1_per_class: [0.159, 0.123, 0.256, 0.0, 0.028, 0.0, 0.362, 0.275, 0.05, 0.17]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 2.1073 | Steps: 4 | Val loss: 1.7749 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=59162)[0m top1: 0.12826492537313433
[2m[36m(func pid=59162)[0m top5: 0.8302238805970149
[2m[36m(func pid=59162)[0m f1_micro: 0.12826492537313433
[2m[36m(func pid=59162)[0m f1_macro: 0.13360889552296604
[2m[36m(func pid=59162)[0m f1_weighted: 0.1048352434769973
[2m[36m(func pid=59162)[0m f1_per_class: [0.145, 0.088, 0.5, 0.174, 0.044, 0.221, 0.0, 0.163, 0.0, 0.0]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.7850 | Steps: 4 | Val loss: 2.2326 | Batch size: 32 | lr: 0.0001 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 10:57:34 (running for 00:40:04.36)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  78.582 |      0.142 |                   61 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.652 |      0.086 |                   53 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.365 |      0.134 |                   45 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.107 |      0.241 |                   25 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.32509328358208955
[2m[36m(func pid=64536)[0m top5: 0.8964552238805971
[2m[36m(func pid=64536)[0m f1_micro: 0.32509328358208955
[2m[36m(func pid=64536)[0m f1_macro: 0.24103112992130357
[2m[36m(func pid=64536)[0m f1_weighted: 0.29843633546670023
[2m[36m(func pid=64536)[0m f1_per_class: [0.329, 0.13, 0.375, 0.559, 0.044, 0.0, 0.274, 0.453, 0.0, 0.246]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 68.6721 | Steps: 4 | Val loss: 127.2866 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=57543)[0m top1: 0.17630597014925373
[2m[36m(func pid=57543)[0m top5: 0.7639925373134329
[2m[36m(func pid=57543)[0m f1_micro: 0.17630597014925373
[2m[36m(func pid=57543)[0m f1_macro: 0.09372924609899022
[2m[36m(func pid=57543)[0m f1_weighted: 0.11874534772324408
[2m[36m(func pid=57543)[0m f1_per_class: [0.105, 0.328, 0.052, 0.038, 0.0, 0.0, 0.106, 0.309, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.2954 | Steps: 4 | Val loss: 2.0367 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=55167)[0m top1: 0.15391791044776118
[2m[36m(func pid=55167)[0m top5: 0.6231343283582089
[2m[36m(func pid=55167)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=55167)[0m f1_macro: 0.18908507891874757
[2m[36m(func pid=55167)[0m f1_weighted: 0.172232473414342
[2m[36m(func pid=55167)[0m f1_per_class: [0.088, 0.054, 0.645, 0.02, 0.025, 0.0, 0.447, 0.188, 0.123, 0.302]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 1.9798 | Steps: 4 | Val loss: 1.9378 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.6717 | Steps: 4 | Val loss: 2.2303 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=59162)[0m top1: 0.22014925373134328
[2m[36m(func pid=59162)[0m top5: 0.867070895522388
[2m[36m(func pid=59162)[0m f1_micro: 0.22014925373134328
[2m[36m(func pid=59162)[0m f1_macro: 0.17445541500645068
[2m[36m(func pid=59162)[0m f1_weighted: 0.20644369935274873
[2m[36m(func pid=59162)[0m f1_per_class: [0.195, 0.099, 0.198, 0.424, 0.057, 0.251, 0.048, 0.342, 0.053, 0.077]
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 10:57:39 (running for 00:40:09.52)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  68.672 |      0.189 |                   62 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.785 |      0.094 |                   54 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.295 |      0.174 |                   46 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.98  |      0.231 |                   26 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.2196828358208955
[2m[36m(func pid=64536)[0m top5: 0.8269589552238806
[2m[36m(func pid=64536)[0m f1_micro: 0.2196828358208955
[2m[36m(func pid=64536)[0m f1_macro: 0.23139046313836106
[2m[36m(func pid=64536)[0m f1_weighted: 0.18019040928304628
[2m[36m(func pid=64536)[0m f1_per_class: [0.333, 0.482, 0.279, 0.003, 0.37, 0.068, 0.168, 0.386, 0.104, 0.119]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 68.5109 | Steps: 4 | Val loss: 78.1353 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=57543)[0m top1: 0.18889925373134328
[2m[36m(func pid=57543)[0m top5: 0.7789179104477612
[2m[36m(func pid=57543)[0m f1_micro: 0.18889925373134325
[2m[36m(func pid=57543)[0m f1_macro: 0.09730384702611236
[2m[36m(func pid=57543)[0m f1_weighted: 0.14572968444737577
[2m[36m(func pid=57543)[0m f1_per_class: [0.051, 0.341, 0.05, 0.053, 0.0, 0.0, 0.18, 0.297, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.1662 | Steps: 4 | Val loss: 2.0070 | Batch size: 32 | lr: 0.001 | Duration: 2.88s
[2m[36m(func pid=55167)[0m top1: 0.261660447761194
[2m[36m(func pid=55167)[0m top5: 0.7481343283582089
[2m[36m(func pid=55167)[0m f1_micro: 0.261660447761194
[2m[36m(func pid=55167)[0m f1_macro: 0.2785304294519463
[2m[36m(func pid=55167)[0m f1_weighted: 0.2905852835651397
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.337, 0.667, 0.048, 0.047, 0.323, 0.486, 0.426, 0.146, 0.305]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 1.8309 | Steps: 4 | Val loss: 2.3665 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.7478 | Steps: 4 | Val loss: 2.2318 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=59162)[0m top1: 0.24953358208955223
[2m[36m(func pid=59162)[0m top5: 0.8656716417910447
[2m[36m(func pid=59162)[0m f1_micro: 0.24953358208955223
[2m[36m(func pid=59162)[0m f1_macro: 0.19398613998810046
[2m[36m(func pid=59162)[0m f1_weighted: 0.2546815486382444
[2m[36m(func pid=59162)[0m f1_per_class: [0.192, 0.282, 0.203, 0.41, 0.064, 0.268, 0.118, 0.341, 0.0, 0.062]
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 10:57:45 (running for 00:40:14.74)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  68.511 |      0.279 |                   63 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.672 |      0.097 |                   55 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.166 |      0.194 |                   47 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.831 |      0.174 |                   27 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.1707089552238806
[2m[36m(func pid=64536)[0m top5: 0.6926305970149254
[2m[36m(func pid=64536)[0m f1_micro: 0.1707089552238806
[2m[36m(func pid=64536)[0m f1_macro: 0.17380679689016512
[2m[36m(func pid=64536)[0m f1_weighted: 0.11666529503457505
[2m[36m(func pid=64536)[0m f1_per_class: [0.123, 0.297, 0.333, 0.0, 0.08, 0.314, 0.0, 0.37, 0.025, 0.197]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 54.0381 | Steps: 4 | Val loss: 60.6023 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=57543)[0m top1: 0.19869402985074627
[2m[36m(func pid=57543)[0m top5: 0.7765858208955224
[2m[36m(func pid=57543)[0m f1_micro: 0.19869402985074627
[2m[36m(func pid=57543)[0m f1_macro: 0.1110910716208934
[2m[36m(func pid=57543)[0m f1_weighted: 0.17994246166791697
[2m[36m(func pid=57543)[0m f1_per_class: [0.058, 0.358, 0.051, 0.084, 0.01, 0.0, 0.258, 0.27, 0.022, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.2813 | Steps: 4 | Val loss: 2.0099 | Batch size: 32 | lr: 0.001 | Duration: 2.98s
[2m[36m(func pid=55167)[0m top1: 0.3344216417910448
[2m[36m(func pid=55167)[0m top5: 0.8269589552238806
[2m[36m(func pid=55167)[0m f1_micro: 0.3344216417910448
[2m[36m(func pid=55167)[0m f1_macro: 0.2558313101090477
[2m[36m(func pid=55167)[0m f1_weighted: 0.2636568645002813
[2m[36m(func pid=55167)[0m f1_per_class: [0.0, 0.474, 0.153, 0.057, 0.359, 0.438, 0.26, 0.468, 0.178, 0.171]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 2.5447 | Steps: 4 | Val loss: 1.7170 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.6995 | Steps: 4 | Val loss: 2.2357 | Batch size: 32 | lr: 0.0001 | Duration: 2.76s
[2m[36m(func pid=59162)[0m top1: 0.22201492537313433
[2m[36m(func pid=59162)[0m top5: 0.8582089552238806
[2m[36m(func pid=59162)[0m f1_micro: 0.22201492537313433
[2m[36m(func pid=59162)[0m f1_macro: 0.19721532699280425
[2m[36m(func pid=59162)[0m f1_weighted: 0.1916076637713428
[2m[36m(func pid=59162)[0m f1_per_class: [0.207, 0.336, 0.349, 0.029, 0.07, 0.295, 0.217, 0.327, 0.0, 0.143]
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 10:57:50 (running for 00:40:19.94)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  54.038 |      0.256 |                   64 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.748 |      0.111 |                   56 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.281 |      0.197 |                   48 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.545 |      0.27  |                   28 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.3941231343283582
[2m[36m(func pid=64536)[0m top5: 0.8959888059701493
[2m[36m(func pid=64536)[0m f1_micro: 0.3941231343283582
[2m[36m(func pid=64536)[0m f1_macro: 0.2695950295185954
[2m[36m(func pid=64536)[0m f1_weighted: 0.39611163352665535
[2m[36m(func pid=64536)[0m f1_per_class: [0.118, 0.097, 0.393, 0.572, 0.054, 0.286, 0.514, 0.457, 0.0, 0.205]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 40.9992 | Steps: 4 | Val loss: 64.8436 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
[2m[36m(func pid=57543)[0m top1: 0.18097014925373134
[2m[36m(func pid=57543)[0m top5: 0.7677238805970149
[2m[36m(func pid=57543)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=57543)[0m f1_macro: 0.10960384913586967
[2m[36m(func pid=57543)[0m f1_weighted: 0.18133633947200548
[2m[36m(func pid=57543)[0m f1_per_class: [0.06, 0.345, 0.054, 0.078, 0.006, 0.008, 0.278, 0.249, 0.019, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 2.2758 | Steps: 4 | Val loss: 1.9867 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 2.4984 | Steps: 4 | Val loss: 2.2932 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=55167)[0m top1: 0.3204291044776119
[2m[36m(func pid=55167)[0m top5: 0.8507462686567164
[2m[36m(func pid=55167)[0m f1_micro: 0.3204291044776119
[2m[36m(func pid=55167)[0m f1_macro: 0.22318371736526865
[2m[36m(func pid=55167)[0m f1_weighted: 0.23079648311294895
[2m[36m(func pid=55167)[0m f1_per_class: [0.189, 0.459, 0.289, 0.225, 0.0, 0.418, 0.027, 0.426, 0.027, 0.171]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.6608 | Steps: 4 | Val loss: 2.2337 | Batch size: 32 | lr: 0.0001 | Duration: 3.00s
[2m[36m(func pid=59162)[0m top1: 0.23787313432835822
[2m[36m(func pid=59162)[0m top5: 0.8619402985074627
[2m[36m(func pid=59162)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=59162)[0m f1_macro: 0.19962010323180984
[2m[36m(func pid=59162)[0m f1_weighted: 0.20776658613330826
[2m[36m(func pid=59162)[0m f1_per_class: [0.282, 0.319, 0.4, 0.0, 0.068, 0.3, 0.321, 0.23, 0.0, 0.077]
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 10:57:55 (running for 00:40:25.37)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  40.999 |      0.223 |                   65 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.699 |      0.11  |                   57 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.276 |      0.2   |                   49 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.498 |      0.202 |                   29 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.22574626865671643
[2m[36m(func pid=64536)[0m top5: 0.730410447761194
[2m[36m(func pid=64536)[0m f1_micro: 0.22574626865671643
[2m[36m(func pid=64536)[0m f1_macro: 0.20209227050491582
[2m[36m(func pid=64536)[0m f1_weighted: 0.14567083009644283
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.432, 0.333, 0.0, 0.203, 0.28, 0.0, 0.505, 0.158, 0.109]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 88.2660 | Steps: 4 | Val loss: 65.2428 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=57543)[0m top1: 0.18516791044776118
[2m[36m(func pid=57543)[0m top5: 0.7649253731343284
[2m[36m(func pid=57543)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=57543)[0m f1_macro: 0.11259746987282596
[2m[36m(func pid=57543)[0m f1_weighted: 0.19464515990850537
[2m[36m(func pid=57543)[0m f1_per_class: [0.064, 0.339, 0.059, 0.061, 0.005, 0.008, 0.346, 0.216, 0.028, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 2.3709 | Steps: 4 | Val loss: 1.9736 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 2.7367 | Steps: 4 | Val loss: 2.5400 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=55167)[0m top1: 0.3894589552238806
[2m[36m(func pid=55167)[0m top5: 0.8432835820895522
[2m[36m(func pid=55167)[0m f1_micro: 0.3894589552238806
[2m[36m(func pid=55167)[0m f1_macro: 0.28326186715789514
[2m[36m(func pid=55167)[0m f1_weighted: 0.324173963048641
[2m[36m(func pid=55167)[0m f1_per_class: [0.304, 0.53, 0.348, 0.517, 0.0, 0.396, 0.022, 0.429, 0.028, 0.259]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.6698 | Steps: 4 | Val loss: 2.2240 | Batch size: 32 | lr: 0.0001 | Duration: 2.84s
[2m[36m(func pid=59162)[0m top1: 0.23880597014925373
[2m[36m(func pid=59162)[0m top5: 0.8614738805970149
[2m[36m(func pid=59162)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=59162)[0m f1_macro: 0.17935261652764128
[2m[36m(func pid=59162)[0m f1_weighted: 0.21653391410319972
[2m[36m(func pid=59162)[0m f1_per_class: [0.069, 0.315, 0.192, 0.007, 0.06, 0.284, 0.328, 0.454, 0.0, 0.085]
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 10:58:01 (running for 00:40:30.74)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  88.266 |      0.283 |                   66 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.661 |      0.113 |                   58 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.371 |      0.179 |                   50 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.737 |      0.177 |                   30 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.23274253731343283
[2m[36m(func pid=64536)[0m top5: 0.5932835820895522
[2m[36m(func pid=64536)[0m f1_micro: 0.23274253731343286
[2m[36m(func pid=64536)[0m f1_macro: 0.17660893828344787
[2m[36m(func pid=64536)[0m f1_weighted: 0.1505356961991121
[2m[36m(func pid=64536)[0m f1_per_class: [0.184, 0.513, 0.121, 0.0, 0.127, 0.335, 0.0, 0.253, 0.051, 0.182]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.21082089552238806
[2m[36m(func pid=57543)[0m top5: 0.7411380597014925
[2m[36m(func pid=57543)[0m f1_micro: 0.21082089552238809
[2m[36m(func pid=57543)[0m f1_macro: 0.1303658485010377
[2m[36m(func pid=57543)[0m f1_weighted: 0.21141130845688388
[2m[36m(func pid=57543)[0m f1_per_class: [0.069, 0.36, 0.076, 0.032, 0.016, 0.0, 0.408, 0.248, 0.065, 0.03]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 47.5968 | Steps: 4 | Val loss: 76.7252 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 2.2903 | Steps: 4 | Val loss: 1.9814 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 2.1518 | Steps: 4 | Val loss: 2.3108 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=55167)[0m top1: 0.34328358208955223
[2m[36m(func pid=55167)[0m top5: 0.8055037313432836
[2m[36m(func pid=55167)[0m f1_micro: 0.34328358208955223
[2m[36m(func pid=55167)[0m f1_macro: 0.2618681847972408
[2m[36m(func pid=55167)[0m f1_weighted: 0.2854869240250733
[2m[36m(func pid=55167)[0m f1_per_class: [0.245, 0.299, 0.56, 0.564, 0.0, 0.354, 0.003, 0.418, 0.027, 0.148]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.7050 | Steps: 4 | Val loss: 2.2261 | Batch size: 32 | lr: 0.0001 | Duration: 2.96s
[2m[36m(func pid=59162)[0m top1: 0.20055970149253732
[2m[36m(func pid=59162)[0m top5: 0.8540111940298507
[2m[36m(func pid=59162)[0m f1_micro: 0.20055970149253732
[2m[36m(func pid=59162)[0m f1_macro: 0.14912815684386155
[2m[36m(func pid=59162)[0m f1_weighted: 0.18642433949164594
[2m[36m(func pid=59162)[0m f1_per_class: [0.042, 0.322, 0.168, 0.265, 0.042, 0.132, 0.052, 0.402, 0.027, 0.042]
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 10:58:06 (running for 00:40:35.88)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  47.597 |      0.262 |                   67 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.67  |      0.13  |                   59 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.29  |      0.149 |                   51 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.152 |      0.147 |                   31 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.2103544776119403
[2m[36m(func pid=64536)[0m top5: 0.8157649253731343
[2m[36m(func pid=64536)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=64536)[0m f1_macro: 0.1469404071673299
[2m[36m(func pid=64536)[0m f1_weighted: 0.17426152658448213
[2m[36m(func pid=64536)[0m f1_per_class: [0.162, 0.0, 0.171, 0.462, 0.082, 0.083, 0.051, 0.231, 0.0, 0.227]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.19916044776119404
[2m[36m(func pid=57543)[0m top5: 0.7346082089552238
[2m[36m(func pid=57543)[0m f1_micro: 0.19916044776119404
[2m[36m(func pid=57543)[0m f1_macro: 0.12462661204400904
[2m[36m(func pid=57543)[0m f1_weighted: 0.20567557382016172
[2m[36m(func pid=57543)[0m f1_per_class: [0.073, 0.352, 0.076, 0.016, 0.018, 0.008, 0.41, 0.227, 0.066, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 68.8831 | Steps: 4 | Val loss: 100.7584 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 2.1950 | Steps: 4 | Val loss: 1.9600 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 3.3074 | Steps: 4 | Val loss: 2.1717 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=55167)[0m top1: 0.19682835820895522
[2m[36m(func pid=55167)[0m top5: 0.7252798507462687
[2m[36m(func pid=55167)[0m f1_micro: 0.1968283582089552
[2m[36m(func pid=55167)[0m f1_macro: 0.1911186751052921
[2m[36m(func pid=55167)[0m f1_weighted: 0.18912588933666508
[2m[36m(func pid=55167)[0m f1_per_class: [0.173, 0.005, 0.571, 0.454, 0.0, 0.195, 0.022, 0.422, 0.026, 0.043]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.6887 | Steps: 4 | Val loss: 2.2202 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 10:58:11 (running for 00:40:41.09)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  68.883 |      0.191 |                   68 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.705 |      0.125 |                   60 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.195 |      0.16  |                   52 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.152 |      0.147 |                   31 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.271455223880597
[2m[36m(func pid=59162)[0m top5: 0.8404850746268657
[2m[36m(func pid=59162)[0m f1_micro: 0.271455223880597
[2m[36m(func pid=59162)[0m f1_macro: 0.15994653609339454
[2m[36m(func pid=59162)[0m f1_weighted: 0.20735465454265423
[2m[36m(func pid=59162)[0m f1_per_class: [0.044, 0.016, 0.21, 0.538, 0.044, 0.199, 0.006, 0.433, 0.05, 0.059]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=64536)[0m top1: 0.30783582089552236
[2m[36m(func pid=64536)[0m top5: 0.8619402985074627
[2m[36m(func pid=64536)[0m f1_micro: 0.30783582089552236
[2m[36m(func pid=64536)[0m f1_macro: 0.19610556656432107
[2m[36m(func pid=64536)[0m f1_weighted: 0.2983974683443911
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.0, 0.084, 0.515, 0.112, 0.327, 0.301, 0.411, 0.0, 0.21]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.18423507462686567
[2m[36m(func pid=57543)[0m top5: 0.7672574626865671
[2m[36m(func pid=57543)[0m f1_micro: 0.1842350746268657
[2m[36m(func pid=57543)[0m f1_macro: 0.12047898641880028
[2m[36m(func pid=57543)[0m f1_weighted: 0.19463669026560004
[2m[36m(func pid=57543)[0m f1_per_class: [0.069, 0.332, 0.091, 0.038, 0.023, 0.0, 0.371, 0.217, 0.041, 0.023]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 74.4369 | Steps: 4 | Val loss: 106.7338 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 2.2481 | Steps: 4 | Val loss: 3.2682 | Batch size: 32 | lr: 0.01 | Duration: 2.88s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 2.4475 | Steps: 4 | Val loss: 1.9266 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=55167)[0m top1: 0.13526119402985073
[2m[36m(func pid=55167)[0m top5: 0.6847014925373134
[2m[36m(func pid=55167)[0m f1_micro: 0.13526119402985073
[2m[36m(func pid=55167)[0m f1_macro: 0.15324244192497635
[2m[36m(func pid=55167)[0m f1_weighted: 0.1444677132771059
[2m[36m(func pid=55167)[0m f1_per_class: [0.261, 0.0, 0.273, 0.229, 0.0, 0.0, 0.138, 0.491, 0.111, 0.029]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.6116 | Steps: 4 | Val loss: 2.2089 | Batch size: 32 | lr: 0.0001 | Duration: 2.80s
[2m[36m(func pid=64536)[0m top1: 0.10307835820895522
[2m[36m(func pid=64536)[0m top5: 0.48134328358208955
[2m[36m(func pid=64536)[0m f1_micro: 0.10307835820895522
[2m[36m(func pid=64536)[0m f1_macro: 0.10870879828186868
[2m[36m(func pid=64536)[0m f1_weighted: 0.07834339204692606
[2m[36m(func pid=64536)[0m f1_per_class: [0.125, 0.24, 0.0, 0.0, 0.027, 0.059, 0.0, 0.415, 0.044, 0.176]
[2m[36m(func pid=64536)[0m 
== Status ==
Current time: 2024-01-07 10:58:16 (running for 00:40:46.44)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  74.437 |      0.153 |                   69 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.689 |      0.12  |                   61 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.195 |      0.16  |                   52 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.248 |      0.109 |                   33 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.28591417910447764
[2m[36m(func pid=59162)[0m top5: 0.8736007462686567
[2m[36m(func pid=59162)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=59162)[0m f1_macro: 0.20550125781236622
[2m[36m(func pid=59162)[0m f1_weighted: 0.22655694410571287
[2m[36m(func pid=59162)[0m f1_per_class: [0.2, 0.0, 0.488, 0.536, 0.04, 0.104, 0.096, 0.483, 0.0, 0.108]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=57543)[0m top1: 0.1982276119402985
[2m[36m(func pid=57543)[0m top5: 0.7863805970149254
[2m[36m(func pid=57543)[0m f1_micro: 0.19822761194029853
[2m[36m(func pid=57543)[0m f1_macro: 0.12315088625820247
[2m[36m(func pid=57543)[0m f1_weighted: 0.18832483890721774
[2m[36m(func pid=57543)[0m f1_per_class: [0.113, 0.347, 0.113, 0.044, 0.015, 0.0, 0.335, 0.211, 0.026, 0.028]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 98.4520 | Steps: 4 | Val loss: 80.3815 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 2.4311 | Steps: 4 | Val loss: 2.8150 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 2.4338 | Steps: 4 | Val loss: 1.9915 | Batch size: 32 | lr: 0.001 | Duration: 2.95s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.6471 | Steps: 4 | Val loss: 2.2019 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=55167)[0m top1: 0.16184701492537312
[2m[36m(func pid=55167)[0m top5: 0.7472014925373134
[2m[36m(func pid=55167)[0m f1_micro: 0.16184701492537312
[2m[36m(func pid=55167)[0m f1_macro: 0.15468622892260522
[2m[36m(func pid=55167)[0m f1_weighted: 0.1976781982416544
[2m[36m(func pid=55167)[0m f1_per_class: [0.304, 0.133, 0.105, 0.319, 0.0, 0.031, 0.18, 0.312, 0.082, 0.081]
[2m[36m(func pid=55167)[0m 
== Status ==
Current time: 2024-01-07 10:58:22 (running for 00:40:51.83)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  98.452 |      0.155 |                   70 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.612 |      0.123 |                   62 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.448 |      0.206 |                   53 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.431 |      0.194 |                   34 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.12453358208955224
[2m[36m(func pid=64536)[0m top5: 0.7458022388059702
[2m[36m(func pid=64536)[0m f1_micro: 0.12453358208955224
[2m[36m(func pid=64536)[0m f1_macro: 0.19402085323637958
[2m[36m(func pid=64536)[0m f1_weighted: 0.1332708496644421
[2m[36m(func pid=64536)[0m f1_per_class: [0.345, 0.197, 0.375, 0.0, 0.038, 0.0, 0.184, 0.504, 0.11, 0.188]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.2150186567164179
[2m[36m(func pid=57543)[0m top5: 0.8027052238805971
[2m[36m(func pid=57543)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=57543)[0m f1_macro: 0.11886544845342843
[2m[36m(func pid=57543)[0m f1_weighted: 0.18170401087357674
[2m[36m(func pid=57543)[0m f1_per_class: [0.121, 0.359, 0.133, 0.044, 0.0, 0.0, 0.308, 0.207, 0.017, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.2733208955223881
[2m[36m(func pid=59162)[0m top5: 0.8726679104477612
[2m[36m(func pid=59162)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=59162)[0m f1_macro: 0.22236337665445594
[2m[36m(func pid=59162)[0m f1_weighted: 0.2763861151054131
[2m[36m(func pid=59162)[0m f1_per_class: [0.25, 0.0, 0.48, 0.509, 0.04, 0.143, 0.278, 0.452, 0.0, 0.071]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 74.8551 | Steps: 4 | Val loss: 79.3223 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 2.5428 | Steps: 4 | Val loss: 2.0634 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.6770 | Steps: 4 | Val loss: 2.1996 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.2206 | Steps: 4 | Val loss: 2.0436 | Batch size: 32 | lr: 0.001 | Duration: 3.05s
[2m[36m(func pid=55167)[0m top1: 0.19962686567164178
[2m[36m(func pid=55167)[0m top5: 0.7882462686567164
[2m[36m(func pid=55167)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=55167)[0m f1_macro: 0.1753808660573911
[2m[36m(func pid=55167)[0m f1_weighted: 0.24097106835159599
[2m[36m(func pid=55167)[0m f1_per_class: [0.157, 0.47, 0.06, 0.15, 0.0, 0.153, 0.283, 0.121, 0.079, 0.281]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=64536)[0m top1: 0.39365671641791045
[2m[36m(func pid=64536)[0m top5: 0.9095149253731343
[2m[36m(func pid=64536)[0m f1_micro: 0.3936567164179104
[2m[36m(func pid=64536)[0m f1_macro: 0.19496692617977357
[2m[36m(func pid=64536)[0m f1_weighted: 0.37634647852500797
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.336, 0.067, 0.513, 0.343, 0.0, 0.574, 0.0, 0.0, 0.116]
[2m[36m(func pid=64536)[0m 
== Status ==
Current time: 2024-01-07 10:58:27 (running for 00:40:57.28)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  74.855 |      0.175 |                   71 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.647 |      0.119 |                   63 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.434 |      0.222 |                   54 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.543 |      0.195 |                   35 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.208955223880597
[2m[36m(func pid=57543)[0m top5: 0.8222947761194029
[2m[36m(func pid=57543)[0m f1_micro: 0.208955223880597
[2m[36m(func pid=57543)[0m f1_macro: 0.10809727046481483
[2m[36m(func pid=57543)[0m f1_weighted: 0.1624483790624962
[2m[36m(func pid=57543)[0m f1_per_class: [0.129, 0.344, 0.144, 0.029, 0.0, 0.0, 0.279, 0.137, 0.019, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.18610074626865672
[2m[36m(func pid=59162)[0m top5: 0.8582089552238806
[2m[36m(func pid=59162)[0m f1_micro: 0.1861007462686567
[2m[36m(func pid=59162)[0m f1_macro: 0.16886147462542128
[2m[36m(func pid=59162)[0m f1_weighted: 0.2012449608374287
[2m[36m(func pid=59162)[0m f1_per_class: [0.184, 0.027, 0.412, 0.197, 0.061, 0.3, 0.305, 0.162, 0.0, 0.042]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 44.2300 | Steps: 4 | Val loss: 54.8820 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 2.9369 | Steps: 4 | Val loss: 2.0913 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.6888 | Steps: 4 | Val loss: 2.1897 | Batch size: 32 | lr: 0.0001 | Duration: 2.82s
[2m[36m(func pid=55167)[0m top1: 0.3582089552238806
[2m[36m(func pid=55167)[0m top5: 0.8106343283582089
[2m[36m(func pid=55167)[0m f1_micro: 0.35820895522388063
[2m[36m(func pid=55167)[0m f1_macro: 0.263540747874655
[2m[36m(func pid=55167)[0m f1_weighted: 0.3383584410352528
[2m[36m(func pid=55167)[0m f1_per_class: [0.087, 0.546, 0.084, 0.12, 0.261, 0.457, 0.424, 0.382, 0.126, 0.148]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 2.3460 | Steps: 4 | Val loss: 2.0561 | Batch size: 32 | lr: 0.001 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 10:58:32 (running for 00:41:02.48)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  44.23  |      0.264 |                   72 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.677 |      0.108 |                   64 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.221 |      0.169 |                   55 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.937 |      0.233 |                   36 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.28451492537313433
[2m[36m(func pid=64536)[0m top5: 0.7933768656716418
[2m[36m(func pid=64536)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=64536)[0m f1_macro: 0.23253802087653072
[2m[36m(func pid=64536)[0m f1_weighted: 0.22933602499586941
[2m[36m(func pid=64536)[0m f1_per_class: [0.257, 0.483, 0.24, 0.3, 0.318, 0.257, 0.0, 0.402, 0.0, 0.068]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.21921641791044777
[2m[36m(func pid=57543)[0m top5: 0.8381529850746269
[2m[36m(func pid=57543)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=57543)[0m f1_macro: 0.1084198218850839
[2m[36m(func pid=57543)[0m f1_weighted: 0.16660948593651168
[2m[36m(func pid=57543)[0m f1_per_class: [0.13, 0.347, 0.142, 0.035, 0.0, 0.016, 0.282, 0.132, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.15391791044776118
[2m[36m(func pid=59162)[0m top5: 0.8381529850746269
[2m[36m(func pid=59162)[0m f1_micro: 0.15391791044776118
[2m[36m(func pid=59162)[0m f1_macro: 0.11820270706041916
[2m[36m(func pid=59162)[0m f1_weighted: 0.1287322887626604
[2m[36m(func pid=59162)[0m f1_per_class: [0.199, 0.114, 0.186, 0.036, 0.1, 0.302, 0.193, 0.016, 0.0, 0.037]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 55.7257 | Steps: 4 | Val loss: 53.2823 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 1.7118 | Steps: 4 | Val loss: 2.6745 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.6131 | Steps: 4 | Val loss: 2.1745 | Batch size: 32 | lr: 0.0001 | Duration: 3.06s
[2m[36m(func pid=55167)[0m top1: 0.31669776119402987
[2m[36m(func pid=55167)[0m top5: 0.8572761194029851
[2m[36m(func pid=55167)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=55167)[0m f1_macro: 0.26658127866709286
[2m[36m(func pid=55167)[0m f1_weighted: 0.2808611820110182
[2m[36m(func pid=55167)[0m f1_per_class: [0.282, 0.54, 0.423, 0.316, 0.316, 0.453, 0.077, 0.232, 0.027, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 2.2576 | Steps: 4 | Val loss: 2.0315 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=64536)[0m top1: 0.125
[2m[36m(func pid=64536)[0m top5: 0.7392723880597015
[2m[36m(func pid=64536)[0m f1_micro: 0.125
[2m[36m(func pid=64536)[0m f1_macro: 0.22761801807415316
[2m[36m(func pid=64536)[0m f1_weighted: 0.11451117363645777
[2m[36m(func pid=64536)[0m f1_per_class: [0.352, 0.042, 0.571, 0.092, 0.053, 0.291, 0.0, 0.539, 0.101, 0.235]
[2m[36m(func pid=64536)[0m 
== Status ==
Current time: 2024-01-07 10:58:37 (running for 00:41:07.58)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  55.726 |      0.267 |                   73 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.689 |      0.108 |                   65 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.346 |      0.118 |                   56 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.712 |      0.228 |                   37 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.23227611940298507
[2m[36m(func pid=57543)[0m top5: 0.8558768656716418
[2m[36m(func pid=57543)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=57543)[0m f1_macro: 0.11853955005045384
[2m[36m(func pid=57543)[0m f1_weighted: 0.17139027632858497
[2m[36m(func pid=57543)[0m f1_per_class: [0.152, 0.349, 0.166, 0.016, 0.0, 0.0, 0.306, 0.197, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.18703358208955223
[2m[36m(func pid=59162)[0m top5: 0.8530783582089553
[2m[36m(func pid=59162)[0m f1_micro: 0.18703358208955223
[2m[36m(func pid=59162)[0m f1_macro: 0.1299282640547065
[2m[36m(func pid=59162)[0m f1_weighted: 0.19832541344934318
[2m[36m(func pid=59162)[0m f1_per_class: [0.119, 0.232, 0.124, 0.172, 0.051, 0.258, 0.251, 0.032, 0.0, 0.06]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 50.5453 | Steps: 4 | Val loss: 57.4958 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 1.8601 | Steps: 4 | Val loss: 2.2629 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.6526 | Steps: 4 | Val loss: 2.1665 | Batch size: 32 | lr: 0.0001 | Duration: 3.15s
[2m[36m(func pid=55167)[0m top1: 0.39552238805970147
[2m[36m(func pid=55167)[0m top5: 0.8460820895522388
[2m[36m(func pid=55167)[0m f1_micro: 0.39552238805970147
[2m[36m(func pid=55167)[0m f1_macro: 0.31289069759097143
[2m[36m(func pid=55167)[0m f1_weighted: 0.3702280743935039
[2m[36m(func pid=55167)[0m f1_per_class: [0.367, 0.472, 0.632, 0.566, 0.107, 0.361, 0.176, 0.421, 0.027, 0.0]
[2m[36m(func pid=55167)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 2.2235 | Steps: 4 | Val loss: 2.0315 | Batch size: 32 | lr: 0.001 | Duration: 3.02s
== Status ==
Current time: 2024-01-07 10:58:43 (running for 00:41:12.83)
Memory usage on this node: 25.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=19
Bracket: Iter 75.000: 0.242
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 PENDING, 4 RUNNING, 19 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00019 | RUNNING    | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  50.545 |      0.313 |                   74 |
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.613 |      0.119 |                   66 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.258 |      0.13  |                   57 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.86  |      0.263 |                   38 |
| train_952df_00023 | PENDING    |                     | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.20009328358208955
[2m[36m(func pid=64536)[0m top5: 0.7728544776119403
[2m[36m(func pid=64536)[0m f1_micro: 0.20009328358208955
[2m[36m(func pid=64536)[0m f1_macro: 0.26296672063611237
[2m[36m(func pid=64536)[0m f1_weighted: 0.19551219334794212
[2m[36m(func pid=64536)[0m f1_per_class: [0.284, 0.353, 0.545, 0.206, 0.04, 0.256, 0.003, 0.508, 0.153, 0.282]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.23973880597014927
[2m[36m(func pid=57543)[0m top5: 0.8656716417910447
[2m[36m(func pid=57543)[0m f1_micro: 0.23973880597014927
[2m[36m(func pid=57543)[0m f1_macro: 0.1243639367132944
[2m[36m(func pid=57543)[0m f1_weighted: 0.17901275517765236
[2m[36m(func pid=57543)[0m f1_per_class: [0.13, 0.352, 0.146, 0.026, 0.0, 0.0, 0.306, 0.283, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=55167)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 85.1509 | Steps: 4 | Val loss: 82.5712 | Batch size: 32 | lr: 0.1 | Duration: 2.84s
[2m[36m(func pid=59162)[0m top1: 0.14832089552238806
[2m[36m(func pid=59162)[0m top5: 0.8591417910447762
[2m[36m(func pid=59162)[0m f1_micro: 0.14832089552238806
[2m[36m(func pid=59162)[0m f1_macro: 0.1365914525009398
[2m[36m(func pid=59162)[0m f1_weighted: 0.13797498311176942
[2m[36m(func pid=59162)[0m f1_per_class: [0.075, 0.248, 0.163, 0.016, 0.043, 0.191, 0.138, 0.416, 0.0, 0.074]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 1.6524 | Steps: 4 | Val loss: 1.8301 | Batch size: 32 | lr: 0.01 | Duration: 2.91s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.6281 | Steps: 4 | Val loss: 2.1688 | Batch size: 32 | lr: 0.0001 | Duration: 2.88s
[2m[36m(func pid=55167)[0m top1: 0.27611940298507465
[2m[36m(func pid=55167)[0m top5: 0.742070895522388
[2m[36m(func pid=55167)[0m f1_micro: 0.27611940298507465
[2m[36m(func pid=55167)[0m f1_macro: 0.22722895070616408
[2m[36m(func pid=55167)[0m f1_weighted: 0.28581639746090387
[2m[36m(func pid=55167)[0m f1_per_class: [0.319, 0.394, 0.267, 0.451, 0.047, 0.232, 0.108, 0.404, 0.05, 0.0]
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 2.2573 | Steps: 4 | Val loss: 2.0112 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
[2m[36m(func pid=64536)[0m top1: 0.3162313432835821
[2m[36m(func pid=64536)[0m top5: 0.8502798507462687
[2m[36m(func pid=64536)[0m f1_micro: 0.3162313432835821
[2m[36m(func pid=64536)[0m f1_macro: 0.25664370884919785
[2m[36m(func pid=64536)[0m f1_weighted: 0.30884812014781765
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.452, 0.234, 0.224, 0.319, 0.34, 0.317, 0.448, 0.136, 0.097]
[2m[36m(func pid=57543)[0m top1: 0.21875
[2m[36m(func pid=57543)[0m top5: 0.8638059701492538
[2m[36m(func pid=57543)[0m f1_micro: 0.21875
[2m[36m(func pid=57543)[0m f1_macro: 0.1136531546339423
[2m[36m(func pid=57543)[0m f1_weighted: 0.1530883474738551
[2m[36m(func pid=57543)[0m f1_per_class: [0.136, 0.338, 0.138, 0.022, 0.0, 0.008, 0.231, 0.263, 0.0, 0.0]
[2m[36m(func pid=59162)[0m top1: 0.16417910447761194
[2m[36m(func pid=59162)[0m top5: 0.8488805970149254
[2m[36m(func pid=59162)[0m f1_micro: 0.16417910447761194
[2m[36m(func pid=59162)[0m f1_macro: 0.1689242971784151
[2m[36m(func pid=59162)[0m f1_weighted: 0.13346881663633992
[2m[36m(func pid=59162)[0m f1_per_class: [0.272, 0.251, 0.256, 0.003, 0.046, 0.234, 0.093, 0.457, 0.0, 0.077]
== Status ==
Current time: 2024-01-07 10:58:48 (running for 00:41:18.13)
Memory usage on this node: 22.6/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.653 |      0.124 |                   67 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.224 |      0.137 |                   58 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.86  |      0.263 |                   38 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m Dataloader to compute accuracy: val
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m Adjusting optimizer according to the provided configuration...
[2m[36m(func pid=74012)[0m Configuration completed!
[2m[36m(func pid=74012)[0m New optimizer parameters:
[2m[36m(func pid=74012)[0m SGD (
[2m[36m(func pid=74012)[0m Parameter Group 0
[2m[36m(func pid=74012)[0m     dampening: 0
[2m[36m(func pid=74012)[0m     differentiable: False
[2m[36m(func pid=74012)[0m     foreach: None
[2m[36m(func pid=74012)[0m     lr: 0.1
[2m[36m(func pid=74012)[0m     maximize: False
[2m[36m(func pid=74012)[0m     momentum: 0.9
[2m[36m(func pid=74012)[0m     nesterov: False
[2m[36m(func pid=74012)[0m     weight_decay: 1e-05
[2m[36m(func pid=74012)[0m )
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 10:58:57 (running for 00:41:27.17)
Memory usage on this node: 23.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.653 |      0.124 |                   67 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.257 |      0.169 |                   59 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.86  |      0.263 |                   38 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.6583 | Steps: 4 | Val loss: 2.1534 | Batch size: 32 | lr: 0.0001 | Duration: 3.11s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 2.2925 | Steps: 4 | Val loss: 1.9140 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 2.2828 | Steps: 4 | Val loss: 1.9981 | Batch size: 32 | lr: 0.001 | Duration: 3.25s
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 0] | Train loss: 7.2994 | Steps: 4 | Val loss: 3.5398 | Batch size: 32 | lr: 0.1 | Duration: 4.78s
== Status ==
Current time: 2024-01-07 10:59:02 (running for 00:41:32.20)
Memory usage on this node: 25.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.628 |      0.114 |                   68 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.257 |      0.169 |                   59 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.652 |      0.257 |                   39 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |         |            |                      |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.24207089552238806
[2m[36m(func pid=57543)[0m top5: 0.8782649253731343
[2m[36m(func pid=57543)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=57543)[0m f1_macro: 0.12242556588146344
[2m[36m(func pid=57543)[0m f1_weighted: 0.18222241082558246
[2m[36m(func pid=57543)[0m f1_per_class: [0.127, 0.355, 0.145, 0.052, 0.0, 0.03, 0.291, 0.223, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.28125
[2m[36m(func pid=64536)[0m top5: 0.8558768656716418
[2m[36m(func pid=64536)[0m f1_micro: 0.28125
[2m[36m(func pid=64536)[0m f1_macro: 0.23327561650556622
[2m[36m(func pid=64536)[0m f1_weighted: 0.2577188460627811
[2m[36m(func pid=64536)[0m f1_per_class: [0.241, 0.427, 0.106, 0.333, 0.226, 0.327, 0.078, 0.357, 0.0, 0.238]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=59162)[0m top1: 0.18097014925373134
[2m[36m(func pid=59162)[0m top5: 0.8381529850746269
[2m[36m(func pid=59162)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=59162)[0m f1_macro: 0.19983134854512527
[2m[36m(func pid=59162)[0m f1_weighted: 0.1375310099818874
[2m[36m(func pid=59162)[0m f1_per_class: [0.262, 0.264, 0.514, 0.041, 0.049, 0.265, 0.042, 0.484, 0.0, 0.077]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=74012)[0m top1: 0.2789179104477612
[2m[36m(func pid=74012)[0m top5: 0.5848880597014925
[2m[36m(func pid=74012)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=74012)[0m f1_macro: 0.04361779722830051
[2m[36m(func pid=74012)[0m f1_weighted: 0.12165784861251727
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.0, 0.436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 2.2616 | Steps: 4 | Val loss: 2.1066 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.6205 | Steps: 4 | Val loss: 2.1634 | Batch size: 32 | lr: 0.0001 | Duration: 2.89s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 2.1954 | Steps: 4 | Val loss: 1.9967 | Batch size: 32 | lr: 0.001 | Duration: 2.96s
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 1] | Train loss: 19.5365 | Steps: 4 | Val loss: 4.5279 | Batch size: 32 | lr: 0.1 | Duration: 2.90s
== Status ==
Current time: 2024-01-07 10:59:08 (running for 00:41:37.84)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.62  |      0.103 |                   70 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.283 |      0.2   |                   60 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.292 |      0.233 |                   40 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   7.299 |      0.044 |                    1 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=57543)[0m top1: 0.22061567164179105
[2m[36m(func pid=57543)[0m top5: 0.8680037313432836
[2m[36m(func pid=57543)[0m f1_micro: 0.22061567164179105
[2m[36m(func pid=57543)[0m f1_macro: 0.10326517613474853
[2m[36m(func pid=57543)[0m f1_weighted: 0.16483744448289733
[2m[36m(func pid=57543)[0m f1_per_class: [0.087, 0.344, 0.098, 0.078, 0.0, 0.015, 0.234, 0.177, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=64536)[0m top1: 0.23414179104477612
[2m[36m(func pid=64536)[0m top5: 0.8596082089552238
[2m[36m(func pid=64536)[0m f1_micro: 0.23414179104477612
[2m[36m(func pid=64536)[0m f1_macro: 0.27034696115487933
[2m[36m(func pid=64536)[0m f1_weighted: 0.20540674398993267
[2m[36m(func pid=64536)[0m f1_per_class: [0.392, 0.304, 0.556, 0.263, 0.22, 0.301, 0.0, 0.475, 0.118, 0.074]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=59162)[0m top1: 0.20988805970149255
[2m[36m(func pid=59162)[0m top5: 0.8572761194029851
[2m[36m(func pid=59162)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=59162)[0m f1_macro: 0.20308961935023034
[2m[36m(func pid=59162)[0m f1_weighted: 0.18509011071615428
[2m[36m(func pid=59162)[0m f1_per_class: [0.239, 0.304, 0.301, 0.143, 0.055, 0.277, 0.077, 0.47, 0.09, 0.074]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=74012)[0m top1: 0.11847014925373134
[2m[36m(func pid=74012)[0m top5: 0.652518656716418
[2m[36m(func pid=74012)[0m f1_micro: 0.11847014925373134
[2m[36m(func pid=74012)[0m f1_macro: 0.04520568138049748
[2m[36m(func pid=74012)[0m f1_weighted: 0.02549720489473262
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.242, 0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 1.6695 | Steps: 4 | Val loss: 2.1445 | Batch size: 32 | lr: 0.01 | Duration: 2.77s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.5713 | Steps: 4 | Val loss: 2.1577 | Batch size: 32 | lr: 0.0001 | Duration: 2.97s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 2.2404 | Steps: 4 | Val loss: 2.0562 | Batch size: 32 | lr: 0.001 | Duration: 3.09s
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 2] | Train loss: 18.9400 | Steps: 4 | Val loss: 5.6492 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
== Status ==
Current time: 2024-01-07 10:59:13 (running for 00:41:42.99)
Memory usage on this node: 25.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.62  |      0.103 |                   70 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.195 |      0.203 |                   61 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.67  |      0.199 |                   42 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  19.536 |      0.045 |                    2 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.21828358208955223
[2m[36m(func pid=64536)[0m top5: 0.8535447761194029
[2m[36m(func pid=64536)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=64536)[0m f1_macro: 0.19874279805897288
[2m[36m(func pid=64536)[0m f1_weighted: 0.24805517868790278
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.433, 0.545, 0.284, 0.068, 0.0, 0.228, 0.379, 0.0, 0.05]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.2355410447761194
[2m[36m(func pid=57543)[0m top5: 0.8703358208955224
[2m[36m(func pid=57543)[0m f1_micro: 0.2355410447761194
[2m[36m(func pid=57543)[0m f1_macro: 0.12310361923055209
[2m[36m(func pid=57543)[0m f1_weighted: 0.1909638809570654
[2m[36m(func pid=57543)[0m f1_per_class: [0.103, 0.354, 0.119, 0.139, 0.0, 0.085, 0.224, 0.207, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.15485074626865672
[2m[36m(func pid=59162)[0m top5: 0.8208955223880597
[2m[36m(func pid=59162)[0m f1_micro: 0.15485074626865672
[2m[36m(func pid=59162)[0m f1_macro: 0.17874036867560808
[2m[36m(func pid=59162)[0m f1_weighted: 0.13685079070607556
[2m[36m(func pid=59162)[0m f1_per_class: [0.264, 0.189, 0.27, 0.133, 0.041, 0.284, 0.003, 0.367, 0.119, 0.118]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=74012)[0m top1: 0.2789179104477612
[2m[36m(func pid=74012)[0m top5: 0.7770522388059702
[2m[36m(func pid=74012)[0m f1_micro: 0.2789179104477612
[2m[36m(func pid=74012)[0m f1_macro: 0.04361779722830051
[2m[36m(func pid=74012)[0m f1_weighted: 0.12165784861251727
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.0, 0.436, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 2.0474 | Steps: 4 | Val loss: 2.2999 | Batch size: 32 | lr: 0.01 | Duration: 2.96s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.6643 | Steps: 4 | Val loss: 2.1599 | Batch size: 32 | lr: 0.0001 | Duration: 3.01s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 2.0995 | Steps: 4 | Val loss: 2.0909 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 3] | Train loss: 23.1984 | Steps: 4 | Val loss: 14.9711 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 10:59:18 (running for 00:41:48.30)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.571 |      0.123 |                   71 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.24  |      0.179 |                   62 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.047 |      0.193 |                   43 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  18.94  |      0.044 |                    3 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.2896455223880597
[2m[36m(func pid=64536)[0m top5: 0.7952425373134329
[2m[36m(func pid=64536)[0m f1_micro: 0.2896455223880597
[2m[36m(func pid=64536)[0m f1_macro: 0.19301809560353642
[2m[36m(func pid=64536)[0m f1_weighted: 0.26851872024107637
[2m[36m(func pid=64536)[0m f1_per_class: [0.194, 0.484, 0.192, 0.053, 0.052, 0.0, 0.482, 0.346, 0.0, 0.128]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.23833955223880596
[2m[36m(func pid=57543)[0m top5: 0.8703358208955224
[2m[36m(func pid=57543)[0m f1_micro: 0.23833955223880596
[2m[36m(func pid=57543)[0m f1_macro: 0.13109772442771658
[2m[36m(func pid=57543)[0m f1_weighted: 0.19686770823377153
[2m[36m(func pid=57543)[0m f1_per_class: [0.119, 0.36, 0.14, 0.167, 0.0, 0.142, 0.195, 0.19, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.16138059701492538
[2m[36m(func pid=59162)[0m top5: 0.8134328358208955
[2m[36m(func pid=59162)[0m f1_micro: 0.16138059701492538
[2m[36m(func pid=59162)[0m f1_macro: 0.1517382747752533
[2m[36m(func pid=59162)[0m f1_weighted: 0.1411259139877806
[2m[36m(func pid=59162)[0m f1_per_class: [0.251, 0.011, 0.275, 0.377, 0.033, 0.0, 0.0, 0.381, 0.131, 0.058]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=74012)[0m top1: 0.29850746268656714
[2m[36m(func pid=74012)[0m top5: 0.4986007462686567
[2m[36m(func pid=74012)[0m f1_micro: 0.29850746268656714
[2m[36m(func pid=74012)[0m f1_macro: 0.04979097380828358
[2m[36m(func pid=74012)[0m f1_weighted: 0.1381190697751376
[2m[36m(func pid=74012)[0m f1_per_class: [0.037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.461, 0.0, 0.0, 0.0]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 2.3405 | Steps: 4 | Val loss: 2.0028 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.6213 | Steps: 4 | Val loss: 2.1587 | Batch size: 32 | lr: 0.0001 | Duration: 2.93s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 2.3081 | Steps: 4 | Val loss: 2.0729 | Batch size: 32 | lr: 0.001 | Duration: 2.91s
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 4] | Train loss: 26.2697 | Steps: 4 | Val loss: 26.2030 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 10:59:23 (running for 00:41:53.55)
Memory usage on this node: 25.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.664 |      0.131 |                   72 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.1   |      0.152 |                   63 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.34  |      0.273 |                   44 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  23.198 |      0.05  |                    4 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.37080223880597013
[2m[36m(func pid=64536)[0m top5: 0.8073694029850746
[2m[36m(func pid=64536)[0m f1_micro: 0.37080223880597013
[2m[36m(func pid=64536)[0m f1_macro: 0.2733542893277487
[2m[36m(func pid=64536)[0m f1_weighted: 0.3358857852019259
[2m[36m(func pid=64536)[0m f1_per_class: [0.339, 0.372, 0.455, 0.57, 0.164, 0.0, 0.265, 0.318, 0.073, 0.177]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.22621268656716417
[2m[36m(func pid=57543)[0m top5: 0.8600746268656716
[2m[36m(func pid=57543)[0m f1_micro: 0.22621268656716417
[2m[36m(func pid=57543)[0m f1_macro: 0.12883502653662296
[2m[36m(func pid=57543)[0m f1_weighted: 0.17235256159412246
[2m[36m(func pid=57543)[0m f1_per_class: [0.137, 0.351, 0.175, 0.146, 0.0, 0.136, 0.133, 0.211, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.18983208955223882
[2m[36m(func pid=59162)[0m top5: 0.8372201492537313
[2m[36m(func pid=59162)[0m f1_micro: 0.18983208955223882
[2m[36m(func pid=59162)[0m f1_macro: 0.16538304271121534
[2m[36m(func pid=59162)[0m f1_weighted: 0.16679443079195147
[2m[36m(func pid=59162)[0m f1_per_class: [0.176, 0.0, 0.471, 0.464, 0.031, 0.0, 0.019, 0.418, 0.0, 0.075]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=74012)[0m top1: 0.010727611940298507
[2m[36m(func pid=74012)[0m top5: 0.6086753731343284
[2m[36m(func pid=74012)[0m f1_micro: 0.010727611940298507
[2m[36m(func pid=74012)[0m f1_macro: 0.004946524064171123
[2m[36m(func pid=74012)[0m f1_weighted: 0.006018744845292255
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.034, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 2.4817 | Steps: 4 | Val loss: 1.7658 | Batch size: 32 | lr: 0.01 | Duration: 2.81s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.7385 | Steps: 4 | Val loss: 2.1580 | Batch size: 32 | lr: 0.0001 | Duration: 2.92s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 2.3978 | Steps: 4 | Val loss: 1.9839 | Batch size: 32 | lr: 0.001 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 10:59:28 (running for 00:41:58.61)
Memory usage on this node: 25.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.621 |      0.129 |                   73 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.308 |      0.165 |                   64 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.482 |      0.304 |                   45 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  26.27  |      0.005 |                    5 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.40345149253731344
[2m[36m(func pid=64536)[0m top5: 0.8880597014925373
[2m[36m(func pid=64536)[0m f1_micro: 0.40345149253731344
[2m[36m(func pid=64536)[0m f1_macro: 0.303517972335522
[2m[36m(func pid=64536)[0m f1_weighted: 0.38722710122404536
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.565, 0.25, 0.524, 0.261, 0.408, 0.2, 0.462, 0.159, 0.207]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 5] | Train loss: 24.8292 | Steps: 4 | Val loss: 16.9960 | Batch size: 32 | lr: 0.1 | Duration: 3.02s
[2m[36m(func pid=57543)[0m top1: 0.2103544776119403
[2m[36m(func pid=57543)[0m top5: 0.8647388059701493
[2m[36m(func pid=57543)[0m f1_micro: 0.2103544776119403
[2m[36m(func pid=57543)[0m f1_macro: 0.11514308383085506
[2m[36m(func pid=57543)[0m f1_weighted: 0.15353063659408242
[2m[36m(func pid=57543)[0m f1_per_class: [0.126, 0.343, 0.207, 0.116, 0.0, 0.132, 0.125, 0.102, 0.0, 0.0]
[2m[36m(func pid=57543)[0m 
[2m[36m(func pid=59162)[0m top1: 0.21921641791044777
[2m[36m(func pid=59162)[0m top5: 0.8638059701492538
[2m[36m(func pid=59162)[0m f1_micro: 0.21921641791044777
[2m[36m(func pid=59162)[0m f1_macro: 0.16964154714113072
[2m[36m(func pid=59162)[0m f1_weighted: 0.20461102429341155
[2m[36m(func pid=59162)[0m f1_per_class: [0.239, 0.047, 0.364, 0.485, 0.032, 0.0, 0.106, 0.377, 0.0, 0.047]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=74012)[0m top1: 0.02845149253731343
[2m[36m(func pid=74012)[0m top5: 0.46968283582089554
[2m[36m(func pid=74012)[0m f1_micro: 0.02845149253731343
[2m[36m(func pid=74012)[0m f1_macro: 0.013309098367136651
[2m[36m(func pid=74012)[0m f1_weighted: 0.005143219988091102
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.0, 0.0, 0.027, 0.018, 0.0, 0.0, 0.089, 0.0]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 2.0205 | Steps: 4 | Val loss: 2.5928 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=57543)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.5968 | Steps: 4 | Val loss: 2.1465 | Batch size: 32 | lr: 0.0001 | Duration: 2.87s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 2.0396 | Steps: 4 | Val loss: 1.9411 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=64536)[0m top1: 0.22527985074626866
[2m[36m(func pid=64536)[0m top5: 0.7308768656716418
[2m[36m(func pid=64536)[0m f1_micro: 0.22527985074626866
[2m[36m(func pid=64536)[0m f1_macro: 0.22355533497605534
[2m[36m(func pid=64536)[0m f1_weighted: 0.15349133273685828
[2m[36m(func pid=64536)[0m f1_per_class: [0.198, 0.454, 0.48, 0.0, 0.104, 0.28, 0.012, 0.514, 0.0, 0.194]
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 6] | Train loss: 24.7363 | Steps: 4 | Val loss: 15.2284 | Batch size: 32 | lr: 0.1 | Duration: 2.69s
== Status ==
Current time: 2024-01-07 10:59:34 (running for 00:42:03.76)
Memory usage on this node: 24.9/187.4 GiB 
Using AsyncHyperBand: num_stopped=20
Bracket: Iter 75.000: 0.23750000000000002
Resources requested: 16.0/72 CPUs, 4.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (4 RUNNING, 20 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00020 | RUNNING    | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.738 |      0.115 |                   74 |
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.398 |      0.17  |                   65 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.02  |      0.224 |                   46 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  24.829 |      0.013 |                    6 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=57543)[0m top1: 0.24067164179104478
[2m[36m(func pid=57543)[0m top5: 0.8805970149253731
[2m[36m(func pid=57543)[0m f1_micro: 0.24067164179104478
[2m[36m(func pid=57543)[0m f1_macro: 0.13792898613211055
[2m[36m(func pid=57543)[0m f1_weighted: 0.19497773551285105
[2m[36m(func pid=57543)[0m f1_per_class: [0.136, 0.359, 0.27, 0.128, 0.0, 0.118, 0.242, 0.126, 0.0, 0.0]
[2m[36m(func pid=74012)[0m top1: 0.2966417910447761
[2m[36m(func pid=74012)[0m top5: 0.7154850746268657
[2m[36m(func pid=74012)[0m f1_micro: 0.2966417910447761
[2m[36m(func pid=74012)[0m f1_macro: 0.09503947750831615
[2m[36m(func pid=74012)[0m f1_weighted: 0.19157999238579207
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.0, 0.568, 0.098, 0.285, 0.0, 0.0, 0.0, 0.0]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=59162)[0m top1: 0.28451492537313433
[2m[36m(func pid=59162)[0m top5: 0.898320895522388
[2m[36m(func pid=59162)[0m f1_micro: 0.28451492537313433
[2m[36m(func pid=59162)[0m f1_macro: 0.17543435221157455
[2m[36m(func pid=59162)[0m f1_weighted: 0.31030709231214426
[2m[36m(func pid=59162)[0m f1_per_class: [0.069, 0.091, 0.131, 0.494, 0.034, 0.0, 0.431, 0.438, 0.0, 0.067]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 2.0867 | Steps: 4 | Val loss: 2.6440 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 7] | Train loss: 25.4238 | Steps: 4 | Val loss: 17.5457 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 10:59:39 (running for 00:42:09.08)
Memory usage on this node: 22.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.233
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.04  |      0.175 |                   66 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.087 |      0.116 |                   47 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  24.736 |      0.095 |                    7 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.09981343283582089
[2m[36m(func pid=64536)[0m top5: 0.6450559701492538
[2m[36m(func pid=64536)[0m f1_micro: 0.0998134328358209
[2m[36m(func pid=64536)[0m f1_macro: 0.11573335384185937
[2m[36m(func pid=64536)[0m f1_weighted: 0.056800224899353324
[2m[36m(func pid=64536)[0m f1_per_class: [0.044, 0.011, 0.37, 0.0, 0.064, 0.255, 0.012, 0.258, 0.09, 0.053]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 2.1897 | Steps: 4 | Val loss: 1.9575 | Batch size: 32 | lr: 0.001 | Duration: 3.24s
[2m[36m(func pid=74012)[0m top1: 0.18516791044776118
[2m[36m(func pid=74012)[0m top5: 0.6021455223880597
[2m[36m(func pid=74012)[0m f1_micro: 0.18516791044776118
[2m[36m(func pid=74012)[0m f1_macro: 0.12034625204266496
[2m[36m(func pid=74012)[0m f1_weighted: 0.12856719465205932
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.412, 0.0, 0.037, 0.0, 0.342, 0.412, 0.0, 0.0]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=59162)[0m top1: 0.251865671641791
[2m[36m(func pid=59162)[0m top5: 0.8913246268656716
[2m[36m(func pid=59162)[0m f1_micro: 0.251865671641791
[2m[36m(func pid=59162)[0m f1_macro: 0.15670995806261315
[2m[36m(func pid=59162)[0m f1_weighted: 0.2779450614307764
[2m[36m(func pid=59162)[0m f1_per_class: [0.0, 0.162, 0.097, 0.457, 0.036, 0.0, 0.325, 0.416, 0.0, 0.074]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 2.3509 | Steps: 4 | Val loss: 2.1202 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 10:59:44 (running for 00:42:14.38)
Memory usage on this node: 22.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.233
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.19  |      0.157 |                   67 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.351 |      0.192 |                   48 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  25.424 |      0.12  |                    8 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.333955223880597
[2m[36m(func pid=64536)[0m top5: 0.8549440298507462
[2m[36m(func pid=64536)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=64536)[0m f1_macro: 0.19206039154004623
[2m[36m(func pid=64536)[0m f1_weighted: 0.3350684192096654
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.032, 0.101, 0.539, 0.128, 0.0, 0.489, 0.512, 0.05, 0.07]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 8] | Train loss: 33.0510 | Steps: 4 | Val loss: 25.4599 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 2.1835 | Steps: 4 | Val loss: 1.9470 | Batch size: 32 | lr: 0.001 | Duration: 3.06s
[2m[36m(func pid=74012)[0m top1: 0.08442164179104478
[2m[36m(func pid=74012)[0m top5: 0.4006529850746269
[2m[36m(func pid=74012)[0m f1_micro: 0.08442164179104478
[2m[36m(func pid=74012)[0m f1_macro: 0.10345227578503373
[2m[36m(func pid=74012)[0m f1_weighted: 0.039902976813421726
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.421, 0.0, 0.035, 0.11, 0.0, 0.364, 0.105, 0.0]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 1.6577 | Steps: 4 | Val loss: 1.9860 | Batch size: 32 | lr: 0.01 | Duration: 3.06s
[2m[36m(func pid=59162)[0m top1: 0.21735074626865672
[2m[36m(func pid=59162)[0m top5: 0.886660447761194
[2m[36m(func pid=59162)[0m f1_micro: 0.21735074626865672
[2m[36m(func pid=59162)[0m f1_macro: 0.14608979077948198
[2m[36m(func pid=59162)[0m f1_weighted: 0.22232834244094438
[2m[36m(func pid=59162)[0m f1_per_class: [0.042, 0.258, 0.116, 0.444, 0.033, 0.0, 0.096, 0.399, 0.0, 0.074]
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 10:59:50 (running for 00:42:19.84)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.233
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.184 |      0.146 |                   68 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.658 |      0.263 |                   49 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  33.051 |      0.103 |                    9 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.27238805970149255
[2m[36m(func pid=64536)[0m top5: 0.8815298507462687
[2m[36m(func pid=64536)[0m f1_micro: 0.27238805970149255
[2m[36m(func pid=64536)[0m f1_macro: 0.26315924205651303
[2m[36m(func pid=64536)[0m f1_weighted: 0.3071677367953844
[2m[36m(func pid=64536)[0m f1_per_class: [0.264, 0.43, 0.6, 0.302, 0.044, 0.263, 0.316, 0.213, 0.028, 0.171]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 9] | Train loss: 29.4355 | Steps: 4 | Val loss: 21.3443 | Batch size: 32 | lr: 0.1 | Duration: 3.05s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.1291 | Steps: 4 | Val loss: 1.9396 | Batch size: 32 | lr: 0.001 | Duration: 3.07s
[2m[36m(func pid=74012)[0m top1: 0.06949626865671642
[2m[36m(func pid=74012)[0m top5: 0.39972014925373134
[2m[36m(func pid=74012)[0m f1_micro: 0.06949626865671642
[2m[36m(func pid=74012)[0m f1_macro: 0.12810029859831856
[2m[36m(func pid=74012)[0m f1_weighted: 0.06227556350248712
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.175, 0.538, 0.0, 0.024, 0.0, 0.0, 0.431, 0.112, 0.0]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 1.4254 | Steps: 4 | Val loss: 2.3237 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=59162)[0m top1: 0.23227611940298507
[2m[36m(func pid=59162)[0m top5: 0.8894589552238806
[2m[36m(func pid=59162)[0m f1_micro: 0.23227611940298507
[2m[36m(func pid=59162)[0m f1_macro: 0.2243587756466452
[2m[36m(func pid=59162)[0m f1_weighted: 0.26278272963030275
[2m[36m(func pid=59162)[0m f1_per_class: [0.223, 0.324, 0.45, 0.307, 0.036, 0.129, 0.24, 0.457, 0.0, 0.077]
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 10:59:55 (running for 00:42:25.19)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.233
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.129 |      0.224 |                   69 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.425 |      0.246 |                   50 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  29.435 |      0.128 |                   10 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.3362873134328358
[2m[36m(func pid=64536)[0m top5: 0.7728544776119403
[2m[36m(func pid=64536)[0m f1_micro: 0.3362873134328358
[2m[36m(func pid=64536)[0m f1_macro: 0.24640851917143802
[2m[36m(func pid=64536)[0m f1_weighted: 0.2827753882425467
[2m[36m(func pid=64536)[0m f1_per_class: [0.331, 0.444, 0.435, 0.0, 0.083, 0.321, 0.507, 0.078, 0.053, 0.214]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 10] | Train loss: 23.5551 | Steps: 4 | Val loss: 14.3428 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.2363 | Steps: 4 | Val loss: 1.9598 | Batch size: 32 | lr: 0.001 | Duration: 2.99s
[2m[36m(func pid=74012)[0m top1: 0.1921641791044776
[2m[36m(func pid=74012)[0m top5: 0.6711753731343284
[2m[36m(func pid=74012)[0m f1_micro: 0.1921641791044776
[2m[36m(func pid=74012)[0m f1_macro: 0.11562952588383268
[2m[36m(func pid=74012)[0m f1_weighted: 0.16646511755759189
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.027, 0.421, 0.541, 0.0, 0.0, 0.0, 0.135, 0.0, 0.031]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 1.8802 | Steps: 4 | Val loss: 1.9683 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=59162)[0m top1: 0.23134328358208955
[2m[36m(func pid=59162)[0m top5: 0.9011194029850746
[2m[36m(func pid=59162)[0m f1_micro: 0.23134328358208955
[2m[36m(func pid=59162)[0m f1_macro: 0.21359422009980694
[2m[36m(func pid=59162)[0m f1_weighted: 0.2501980173949476
[2m[36m(func pid=59162)[0m f1_per_class: [0.176, 0.311, 0.353, 0.088, 0.046, 0.251, 0.367, 0.467, 0.0, 0.077]
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 11:00:00 (running for 00:42:30.59)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.233
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.236 |      0.214 |                   70 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.88  |      0.258 |                   51 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  23.555 |      0.116 |                   11 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.300839552238806
[2m[36m(func pid=64536)[0m top5: 0.8428171641791045
[2m[36m(func pid=64536)[0m f1_micro: 0.300839552238806
[2m[36m(func pid=64536)[0m f1_macro: 0.25847254673127523
[2m[36m(func pid=64536)[0m f1_weighted: 0.3195780312886146
[2m[36m(func pid=64536)[0m f1_per_class: [0.224, 0.364, 0.152, 0.335, 0.112, 0.367, 0.277, 0.462, 0.119, 0.173]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 11] | Train loss: 14.5386 | Steps: 4 | Val loss: 10.5235 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 2.2167 | Steps: 4 | Val loss: 1.9194 | Batch size: 32 | lr: 0.001 | Duration: 3.14s
[2m[36m(func pid=74012)[0m top1: 0.2691231343283582
[2m[36m(func pid=74012)[0m top5: 0.7877798507462687
[2m[36m(func pid=74012)[0m f1_micro: 0.2691231343283582
[2m[36m(func pid=74012)[0m f1_macro: 0.20645742820779156
[2m[36m(func pid=74012)[0m f1_weighted: 0.25841757819315825
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.516, 0.264, 0.111, 0.0, 0.507, 0.423, 0.109, 0.135]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 1.3881 | Steps: 4 | Val loss: 2.2504 | Batch size: 32 | lr: 0.01 | Duration: 2.79s
[2m[36m(func pid=59162)[0m top1: 0.27425373134328357
[2m[36m(func pid=59162)[0m top5: 0.8894589552238806
[2m[36m(func pid=59162)[0m f1_micro: 0.27425373134328357
[2m[36m(func pid=59162)[0m f1_macro: 0.2629365571387505
[2m[36m(func pid=59162)[0m f1_weighted: 0.2784491084352414
[2m[36m(func pid=59162)[0m f1_per_class: [0.256, 0.332, 0.522, 0.409, 0.053, 0.29, 0.113, 0.473, 0.105, 0.077]
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 11:00:06 (running for 00:42:35.72)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.233
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.217 |      0.263 |                   71 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.388 |      0.269 |                   52 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  14.539 |      0.206 |                   12 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.33675373134328357
[2m[36m(func pid=64536)[0m top5: 0.7518656716417911
[2m[36m(func pid=64536)[0m f1_micro: 0.33675373134328357
[2m[36m(func pid=64536)[0m f1_macro: 0.2688488219611647
[2m[36m(func pid=64536)[0m f1_weighted: 0.24023029813935212
[2m[36m(func pid=64536)[0m f1_per_class: [0.271, 0.0, 0.6, 0.571, 0.31, 0.368, 0.0, 0.436, 0.027, 0.106]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 12] | Train loss: 22.3014 | Steps: 4 | Val loss: 16.1788 | Batch size: 32 | lr: 0.1 | Duration: 2.92s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.1296 | Steps: 4 | Val loss: 1.8987 | Batch size: 32 | lr: 0.001 | Duration: 3.01s
[2m[36m(func pid=74012)[0m top1: 0.22434701492537312
[2m[36m(func pid=74012)[0m top5: 0.7546641791044776
[2m[36m(func pid=74012)[0m f1_micro: 0.22434701492537315
[2m[36m(func pid=74012)[0m f1_macro: 0.10578439155317076
[2m[36m(func pid=74012)[0m f1_weighted: 0.17116756790893817
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.31, 0.528, 0.027, 0.0, 0.045, 0.148, 0.0, 0.0]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 1.8019 | Steps: 4 | Val loss: 2.0263 | Batch size: 32 | lr: 0.01 | Duration: 2.78s
[2m[36m(func pid=59162)[0m top1: 0.3031716417910448
[2m[36m(func pid=59162)[0m top5: 0.8614738805970149
[2m[36m(func pid=59162)[0m f1_micro: 0.3031716417910448
[2m[36m(func pid=59162)[0m f1_macro: 0.18974916648966814
[2m[36m(func pid=59162)[0m f1_weighted: 0.24643197398059757
[2m[36m(func pid=59162)[0m f1_per_class: [0.043, 0.153, 0.19, 0.541, 0.061, 0.334, 0.0, 0.453, 0.051, 0.071]
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 11:00:11 (running for 00:42:40.95)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.233
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.13  |      0.19  |                   72 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.802 |      0.266 |                   53 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  22.301 |      0.106 |                   13 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.314365671641791
[2m[36m(func pid=64536)[0m top5: 0.8176305970149254
[2m[36m(func pid=64536)[0m f1_micro: 0.314365671641791
[2m[36m(func pid=64536)[0m f1_macro: 0.266344381119309
[2m[36m(func pid=64536)[0m f1_weighted: 0.33412338066613917
[2m[36m(func pid=64536)[0m f1_per_class: [0.2, 0.048, 0.296, 0.47, 0.157, 0.343, 0.375, 0.532, 0.146, 0.096]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 13] | Train loss: 17.5901 | Steps: 4 | Val loss: 17.2467 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.1884 | Steps: 4 | Val loss: 1.8776 | Batch size: 32 | lr: 0.001 | Duration: 3.12s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 1.7657 | Steps: 4 | Val loss: 1.7791 | Batch size: 32 | lr: 0.01 | Duration: 2.86s
[2m[36m(func pid=74012)[0m top1: 0.11520522388059702
[2m[36m(func pid=74012)[0m top5: 0.7098880597014925
[2m[36m(func pid=74012)[0m f1_micro: 0.11520522388059702
[2m[36m(func pid=74012)[0m f1_macro: 0.056985573428704896
[2m[36m(func pid=74012)[0m f1_weighted: 0.029783091390214567
[2m[36m(func pid=74012)[0m f1_per_class: [0.239, 0.0, 0.0, 0.0, 0.0, 0.205, 0.0, 0.0, 0.0, 0.126]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=59162)[0m top1: 0.333955223880597
[2m[36m(func pid=59162)[0m top5: 0.840018656716418
[2m[36m(func pid=59162)[0m f1_micro: 0.333955223880597
[2m[36m(func pid=59162)[0m f1_macro: 0.1964408958981867
[2m[36m(func pid=59162)[0m f1_weighted: 0.23054009543276288
[2m[36m(func pid=59162)[0m f1_per_class: [0.0, 0.0, 0.247, 0.558, 0.081, 0.373, 0.0, 0.476, 0.0, 0.229]
[2m[36m(func pid=59162)[0m 
== Status ==
Current time: 2024-01-07 11:00:16 (running for 00:42:46.08)
Memory usage on this node: 22.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.233
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.188 |      0.196 |                   73 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.766 |      0.248 |                   54 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  17.59  |      0.057 |                   14 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.3689365671641791
[2m[36m(func pid=64536)[0m top5: 0.8773320895522388
[2m[36m(func pid=64536)[0m f1_micro: 0.3689365671641791
[2m[36m(func pid=64536)[0m f1_macro: 0.24757334173015239
[2m[36m(func pid=64536)[0m f1_weighted: 0.3241889450871555
[2m[36m(func pid=64536)[0m f1_per_class: [0.217, 0.502, 0.258, 0.048, 0.114, 0.355, 0.545, 0.163, 0.093, 0.182]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 14] | Train loss: 15.6799 | Steps: 4 | Val loss: 11.6186 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.3403 | Steps: 4 | Val loss: 1.9235 | Batch size: 32 | lr: 0.001 | Duration: 3.04s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 2.0332 | Steps: 4 | Val loss: 1.9400 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=74012)[0m top1: 0.20615671641791045
[2m[36m(func pid=74012)[0m top5: 0.7728544776119403
[2m[36m(func pid=74012)[0m f1_micro: 0.20615671641791045
[2m[36m(func pid=74012)[0m f1_macro: 0.14467947202799653
[2m[36m(func pid=74012)[0m f1_weighted: 0.10262830412187554
[2m[36m(func pid=74012)[0m f1_per_class: [0.212, 0.323, 0.306, 0.0, 0.0, 0.0, 0.033, 0.482, 0.091, 0.0]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:00:21 (running for 00:42:51.12)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.233
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.34  |      0.182 |                   74 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.766 |      0.248 |                   54 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  15.68  |      0.145 |                   15 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=59162)[0m top1: 0.33348880597014924
[2m[36m(func pid=59162)[0m top5: 0.8264925373134329
[2m[36m(func pid=59162)[0m f1_micro: 0.33348880597014924
[2m[36m(func pid=59162)[0m f1_macro: 0.18246020861417117
[2m[36m(func pid=59162)[0m f1_weighted: 0.22477588049242814
[2m[36m(func pid=59162)[0m f1_per_class: [0.0, 0.0, 0.2, 0.546, 0.094, 0.356, 0.0, 0.489, 0.0, 0.14]
[2m[36m(func pid=59162)[0m 
[2m[36m(func pid=64536)[0m top1: 0.279384328358209
[2m[36m(func pid=64536)[0m top5: 0.8348880597014925
[2m[36m(func pid=64536)[0m f1_micro: 0.279384328358209
[2m[36m(func pid=64536)[0m f1_macro: 0.289227257123469
[2m[36m(func pid=64536)[0m f1_weighted: 0.2501375634369645
[2m[36m(func pid=64536)[0m f1_per_class: [0.186, 0.49, 0.632, 0.113, 0.081, 0.298, 0.186, 0.515, 0.1, 0.291]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 15] | Train loss: 10.9528 | Steps: 4 | Val loss: 24.7598 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
[2m[36m(func pid=59162)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 2.0724 | Steps: 4 | Val loss: 2.0155 | Batch size: 32 | lr: 0.001 | Duration: 3.08s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 1.3760 | Steps: 4 | Val loss: 2.6372 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=74012)[0m top1: 0.17257462686567165
[2m[36m(func pid=74012)[0m top5: 0.48600746268656714
[2m[36m(func pid=74012)[0m f1_micro: 0.17257462686567165
[2m[36m(func pid=74012)[0m f1_macro: 0.09393498968138328
[2m[36m(func pid=74012)[0m f1_weighted: 0.12357860632852379
[2m[36m(func pid=74012)[0m f1_per_class: [0.119, 0.0, 0.136, 0.424, 0.215, 0.0, 0.0, 0.0, 0.0, 0.045]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m top1: 0.18097014925373134
[2m[36m(func pid=64536)[0m top5: 0.7075559701492538
[2m[36m(func pid=64536)[0m f1_micro: 0.18097014925373134
[2m[36m(func pid=64536)[0m f1_macro: 0.18023455353361378
[2m[36m(func pid=64536)[0m f1_weighted: 0.17747839569323512
[2m[36m(func pid=64536)[0m f1_per_class: [0.0, 0.121, 0.581, 0.436, 0.052, 0.008, 0.0, 0.463, 0.089, 0.053]
== Status ==
Current time: 2024-01-07 11:00:26 (running for 00:42:56.52)
Memory usage on this node: 22.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=21
Bracket: Iter 75.000: 0.233
Resources requested: 12.0/72 CPUs, 3.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (3 RUNNING, 21 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00021 | RUNNING    | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.34  |      0.182 |                   74 |
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.376 |      0.18  |                   56 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  10.953 |      0.094 |                   16 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=59162)[0m top1: 0.2733208955223881
[2m[36m(func pid=59162)[0m top5: 0.8097014925373134
[2m[36m(func pid=59162)[0m f1_micro: 0.2733208955223881
[2m[36m(func pid=59162)[0m f1_macro: 0.20209741102424342
[2m[36m(func pid=59162)[0m f1_weighted: 0.21589411228399807
[2m[36m(func pid=59162)[0m f1_per_class: [0.273, 0.0, 0.37, 0.562, 0.066, 0.17, 0.0, 0.53, 0.0, 0.049]
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 16] | Train loss: 19.7842 | Steps: 4 | Val loss: 20.4193 | Batch size: 32 | lr: 0.1 | Duration: 2.86s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 1.7042 | Steps: 4 | Val loss: 2.1798 | Batch size: 32 | lr: 0.01 | Duration: 2.71s
[2m[36m(func pid=74012)[0m top1: 0.2150186567164179
[2m[36m(func pid=74012)[0m top5: 0.648320895522388
[2m[36m(func pid=74012)[0m f1_micro: 0.2150186567164179
[2m[36m(func pid=74012)[0m f1_macro: 0.2090387810015994
[2m[36m(func pid=74012)[0m f1_weighted: 0.1797751224467329
[2m[36m(func pid=74012)[0m f1_per_class: [0.12, 0.379, 0.556, 0.237, 0.182, 0.316, 0.0, 0.032, 0.0, 0.269]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:00:32 (running for 00:43:01.74)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.704 |      0.192 |                   57 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  19.784 |      0.209 |                   17 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.291044776119403
[2m[36m(func pid=64536)[0m top5: 0.7910447761194029
[2m[36m(func pid=64536)[0m f1_micro: 0.291044776119403
[2m[36m(func pid=64536)[0m f1_macro: 0.19249341314119806
[2m[36m(func pid=64536)[0m f1_weighted: 0.24066820694618968
[2m[36m(func pid=64536)[0m f1_per_class: [0.125, 0.236, 0.13, 0.544, 0.057, 0.156, 0.0, 0.376, 0.067, 0.233]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 17] | Train loss: 11.9547 | Steps: 4 | Val loss: 18.6302 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 1.2014 | Steps: 4 | Val loss: 1.7643 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=74012)[0m top1: 0.20988805970149255
[2m[36m(func pid=74012)[0m top5: 0.5625
[2m[36m(func pid=74012)[0m f1_micro: 0.20988805970149255
[2m[36m(func pid=74012)[0m f1_macro: 0.1120080789763714
[2m[36m(func pid=74012)[0m f1_weighted: 0.09758814155364323
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.39, 0.0, 0.0, 0.123, 0.0, 0.0, 0.453, 0.073, 0.081]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m top1: 0.3381529850746269
[2m[36m(func pid=64536)[0m top5: 0.8899253731343284
[2m[36m(func pid=64536)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=64536)[0m f1_macro: 0.29991412643085774
[2m[36m(func pid=64536)[0m f1_weighted: 0.37032499526875773
[2m[36m(func pid=64536)[0m f1_per_class: [0.224, 0.443, 0.562, 0.458, 0.054, 0.286, 0.323, 0.449, 0.027, 0.171]
[2m[36m(func pid=64536)[0m 
== Status ==
Current time: 2024-01-07 11:00:37 (running for 00:43:07.12)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.201 |      0.3   |                   58 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  11.955 |      0.112 |                   18 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 18] | Train loss: 18.7970 | Steps: 4 | Val loss: 31.0509 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 1.4808 | Steps: 4 | Val loss: 1.7099 | Batch size: 32 | lr: 0.01 | Duration: 2.85s
[2m[36m(func pid=74012)[0m top1: 0.07555970149253731
[2m[36m(func pid=74012)[0m top5: 0.5046641791044776
[2m[36m(func pid=74012)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=74012)[0m f1_macro: 0.07936198721913008
[2m[36m(func pid=74012)[0m f1_weighted: 0.09319043792324709
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.306, 0.0, 0.017, 0.0, 0.299, 0.0, 0.0, 0.171]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:00:42 (running for 00:43:12.42)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.481 |      0.311 |                   59 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  18.797 |      0.079 |                   19 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.36613805970149255
[2m[36m(func pid=64536)[0m top5: 0.8880597014925373
[2m[36m(func pid=64536)[0m f1_micro: 0.36613805970149255
[2m[36m(func pid=64536)[0m f1_macro: 0.3110530583467793
[2m[36m(func pid=64536)[0m f1_weighted: 0.3950501173450445
[2m[36m(func pid=64536)[0m f1_per_class: [0.286, 0.431, 0.308, 0.485, 0.071, 0.315, 0.363, 0.467, 0.072, 0.312]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 19] | Train loss: 28.6126 | Steps: 4 | Val loss: 31.5553 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 1.5599 | Steps: 4 | Val loss: 1.9894 | Batch size: 32 | lr: 0.01 | Duration: 3.04s
[2m[36m(func pid=74012)[0m top1: 0.06856343283582089
[2m[36m(func pid=74012)[0m top5: 0.6152052238805971
[2m[36m(func pid=74012)[0m f1_micro: 0.06856343283582089
[2m[36m(func pid=74012)[0m f1_macro: 0.06456184230881136
[2m[36m(func pid=74012)[0m f1_weighted: 0.01712723489601491
[2m[36m(func pid=74012)[0m f1_per_class: [0.082, 0.0, 0.301, 0.0, 0.0, 0.0, 0.0, 0.229, 0.0, 0.034]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:00:48 (running for 00:43:17.96)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.56  |      0.285 |                   60 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  28.613 |      0.065 |                   20 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.3003731343283582
[2m[36m(func pid=64536)[0m top5: 0.835820895522388
[2m[36m(func pid=64536)[0m f1_micro: 0.3003731343283582
[2m[36m(func pid=64536)[0m f1_macro: 0.28499046917857007
[2m[36m(func pid=64536)[0m f1_weighted: 0.2830508652293901
[2m[36m(func pid=64536)[0m f1_per_class: [0.24, 0.256, 0.632, 0.537, 0.112, 0.263, 0.054, 0.471, 0.147, 0.141]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 20] | Train loss: 33.5698 | Steps: 4 | Val loss: 24.4085 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 1.3301 | Steps: 4 | Val loss: 2.3867 | Batch size: 32 | lr: 0.01 | Duration: 2.82s
[2m[36m(func pid=74012)[0m top1: 0.2644589552238806
[2m[36m(func pid=74012)[0m top5: 0.683768656716418
[2m[36m(func pid=74012)[0m f1_micro: 0.2644589552238806
[2m[36m(func pid=74012)[0m f1_macro: 0.1844473757280251
[2m[36m(func pid=74012)[0m f1_weighted: 0.18814497361402885
[2m[36m(func pid=74012)[0m f1_per_class: [0.37, 0.0, 0.373, 0.547, 0.0, 0.0, 0.0, 0.383, 0.076, 0.095]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:00:53 (running for 00:43:23.14)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.33  |      0.176 |                   61 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  33.57  |      0.184 |                   21 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.23647388059701493
[2m[36m(func pid=64536)[0m top5: 0.7262126865671642
[2m[36m(func pid=64536)[0m f1_micro: 0.23647388059701493
[2m[36m(func pid=64536)[0m f1_macro: 0.1756208939367876
[2m[36m(func pid=64536)[0m f1_weighted: 0.22028156472638902
[2m[36m(func pid=64536)[0m f1_per_class: [0.261, 0.243, 0.324, 0.007, 0.038, 0.161, 0.493, 0.0, 0.043, 0.186]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 21] | Train loss: 24.6810 | Steps: 4 | Val loss: 25.9647 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 1.9373 | Steps: 4 | Val loss: 2.4141 | Batch size: 32 | lr: 0.01 | Duration: 2.93s
[2m[36m(func pid=74012)[0m top1: 0.022388059701492536
[2m[36m(func pid=74012)[0m top5: 0.6618470149253731
[2m[36m(func pid=74012)[0m f1_micro: 0.02238805970149254
[2m[36m(func pid=74012)[0m f1_macro: 0.02036598278488442
[2m[36m(func pid=74012)[0m f1_weighted: 0.011854717152320383
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.021, 0.134, 0.0, 0.0, 0.0, 0.024, 0.0, 0.0, 0.024]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:00:58 (running for 00:43:28.48)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.937 |      0.201 |                   62 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  24.681 |      0.02  |                   22 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.20475746268656717
[2m[36m(func pid=64536)[0m top5: 0.8097014925373134
[2m[36m(func pid=64536)[0m f1_micro: 0.20475746268656717
[2m[36m(func pid=64536)[0m f1_macro: 0.20088924574879186
[2m[36m(func pid=64536)[0m f1_weighted: 0.23613178338631688
[2m[36m(func pid=64536)[0m f1_per_class: [0.142, 0.43, 0.097, 0.023, 0.043, 0.093, 0.375, 0.371, 0.137, 0.298]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 22] | Train loss: 22.4836 | Steps: 4 | Val loss: 28.0012 | Batch size: 32 | lr: 0.1 | Duration: 2.83s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 1.8420 | Steps: 4 | Val loss: 2.3799 | Batch size: 32 | lr: 0.01 | Duration: 3.08s
[2m[36m(func pid=74012)[0m top1: 0.07929104477611941
[2m[36m(func pid=74012)[0m top5: 0.5746268656716418
[2m[36m(func pid=74012)[0m f1_micro: 0.07929104477611941
[2m[36m(func pid=74012)[0m f1_macro: 0.08346355544054232
[2m[36m(func pid=74012)[0m f1_weighted: 0.08136744867440589
[2m[36m(func pid=74012)[0m f1_per_class: [0.132, 0.0, 0.426, 0.0, 0.023, 0.0, 0.255, 0.0, 0.0, 0.0]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:01:04 (running for 00:43:34.03)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.842 |      0.263 |                   63 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  22.484 |      0.083 |                   23 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.3656716417910448
[2m[36m(func pid=64536)[0m top5: 0.7388059701492538
[2m[36m(func pid=64536)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=64536)[0m f1_macro: 0.26288321850099694
[2m[36m(func pid=64536)[0m f1_weighted: 0.30363144245227025
[2m[36m(func pid=64536)[0m f1_per_class: [0.216, 0.56, 0.169, 0.461, 0.192, 0.35, 0.0, 0.513, 0.0, 0.167]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 23] | Train loss: 22.5916 | Steps: 4 | Val loss: 25.2689 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 1.8728 | Steps: 4 | Val loss: 1.8763 | Batch size: 32 | lr: 0.01 | Duration: 3.00s
[2m[36m(func pid=74012)[0m top1: 0.3656716417910448
[2m[36m(func pid=74012)[0m top5: 0.7854477611940298
[2m[36m(func pid=74012)[0m f1_micro: 0.3656716417910448
[2m[36m(func pid=74012)[0m f1_macro: 0.1876697724248894
[2m[36m(func pid=74012)[0m f1_weighted: 0.22592669459401454
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.444, 0.521, 0.0, 0.449, 0.0, 0.463, 0.0, 0.0]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:01:09 (running for 00:43:39.37)
Memory usage on this node: 19.5/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.873 |      0.291 |                   64 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  22.592 |      0.188 |                   24 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.34888059701492535
[2m[36m(func pid=64536)[0m top5: 0.8470149253731343
[2m[36m(func pid=64536)[0m f1_micro: 0.34888059701492535
[2m[36m(func pid=64536)[0m f1_macro: 0.2908552462330283
[2m[36m(func pid=64536)[0m f1_weighted: 0.36450923524953344
[2m[36m(func pid=64536)[0m f1_per_class: [0.296, 0.482, 0.279, 0.431, 0.081, 0.323, 0.276, 0.511, 0.069, 0.16]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 24] | Train loss: 32.3766 | Steps: 4 | Val loss: 15.8901 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 1.2956 | Steps: 4 | Val loss: 2.4222 | Batch size: 32 | lr: 0.01 | Duration: 3.05s
[2m[36m(func pid=74012)[0m top1: 0.28404850746268656
[2m[36m(func pid=74012)[0m top5: 0.6702425373134329
[2m[36m(func pid=74012)[0m f1_micro: 0.28404850746268656
[2m[36m(func pid=74012)[0m f1_macro: 0.1868525061006277
[2m[36m(func pid=74012)[0m f1_weighted: 0.1822704141437035
[2m[36m(func pid=74012)[0m f1_per_class: [0.17, 0.097, 0.6, 0.0, 0.0, 0.0, 0.441, 0.429, 0.026, 0.105]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:01:15 (running for 00:43:45.12)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.296 |      0.186 |                   65 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  32.377 |      0.187 |                   25 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.310634328358209
[2m[36m(func pid=64536)[0m top5: 0.7588619402985075
[2m[36m(func pid=64536)[0m f1_micro: 0.310634328358209
[2m[36m(func pid=64536)[0m f1_macro: 0.18645528502827421
[2m[36m(func pid=64536)[0m f1_weighted: 0.2986462081210934
[2m[36m(func pid=64536)[0m f1_per_class: [0.218, 0.0, 0.353, 0.377, 0.069, 0.11, 0.571, 0.016, 0.043, 0.109]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 25] | Train loss: 14.0180 | Steps: 4 | Val loss: 31.9222 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 1.5494 | Steps: 4 | Val loss: 2.9435 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=74012)[0m top1: 0.20102611940298507
[2m[36m(func pid=74012)[0m top5: 0.44449626865671643
[2m[36m(func pid=74012)[0m f1_micro: 0.2010261194029851
[2m[36m(func pid=74012)[0m f1_macro: 0.08637420885117583
[2m[36m(func pid=74012)[0m f1_weighted: 0.10801658595900611
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.385, 0.061, 0.0, 0.0, 0.34, 0.0, 0.0, 0.078, 0.0]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:01:20 (running for 00:43:50.32)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.549 |      0.185 |                   66 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  14.018 |      0.086 |                   26 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.19263059701492538
[2m[36m(func pid=64536)[0m top5: 0.6865671641791045
[2m[36m(func pid=64536)[0m f1_micro: 0.19263059701492538
[2m[36m(func pid=64536)[0m f1_macro: 0.1850282274725869
[2m[36m(func pid=64536)[0m f1_weighted: 0.16882487876568328
[2m[36m(func pid=64536)[0m f1_per_class: [0.209, 0.0, 0.065, 0.262, 0.071, 0.306, 0.063, 0.514, 0.132, 0.227]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 26] | Train loss: 25.4605 | Steps: 4 | Val loss: 29.5954 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 1.6331 | Steps: 4 | Val loss: 2.6863 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=74012)[0m top1: 0.22667910447761194
[2m[36m(func pid=74012)[0m top5: 0.4748134328358209
[2m[36m(func pid=74012)[0m f1_micro: 0.22667910447761194
[2m[36m(func pid=74012)[0m f1_macro: 0.18103147482604826
[2m[36m(func pid=74012)[0m f1_weighted: 0.15242564476514667
[2m[36m(func pid=74012)[0m f1_per_class: [0.213, 0.529, 0.297, 0.0, 0.24, 0.441, 0.0, 0.0, 0.09, 0.0]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:01:25 (running for 00:43:55.55)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.633 |      0.227 |                   67 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  25.46  |      0.181 |                   27 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.27005597014925375
[2m[36m(func pid=64536)[0m top5: 0.6520522388059702
[2m[36m(func pid=64536)[0m f1_micro: 0.27005597014925375
[2m[36m(func pid=64536)[0m f1_macro: 0.22729879027406166
[2m[36m(func pid=64536)[0m f1_weighted: 0.22455482910374777
[2m[36m(func pid=64536)[0m f1_per_class: [0.292, 0.0, 0.224, 0.522, 0.052, 0.318, 0.0, 0.449, 0.187, 0.228]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 27] | Train loss: 25.4999 | Steps: 4 | Val loss: 19.7771 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 1.7046 | Steps: 4 | Val loss: 2.1480 | Batch size: 32 | lr: 0.01 | Duration: 2.90s
[2m[36m(func pid=74012)[0m top1: 0.08861940298507463
[2m[36m(func pid=74012)[0m top5: 0.632929104477612
[2m[36m(func pid=74012)[0m f1_micro: 0.08861940298507463
[2m[36m(func pid=74012)[0m f1_macro: 0.08458220298995889
[2m[36m(func pid=74012)[0m f1_weighted: 0.10219562155701736
[2m[36m(func pid=74012)[0m f1_per_class: [0.089, 0.248, 0.247, 0.183, 0.028, 0.035, 0.0, 0.016, 0.0, 0.0]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:01:31 (running for 00:44:00.93)
Memory usage on this node: 19.4/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.705 |      0.288 |                   68 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  25.5   |      0.085 |                   28 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.2593283582089552
[2m[36m(func pid=64536)[0m top5: 0.8083022388059702
[2m[36m(func pid=64536)[0m f1_micro: 0.2593283582089552
[2m[36m(func pid=64536)[0m f1_macro: 0.2878868180851167
[2m[36m(func pid=64536)[0m f1_weighted: 0.26198416929545926
[2m[36m(func pid=64536)[0m f1_per_class: [0.355, 0.46, 0.571, 0.397, 0.04, 0.107, 0.04, 0.49, 0.168, 0.25]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 28] | Train loss: 36.4046 | Steps: 4 | Val loss: 9.4167 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 2.2684 | Steps: 4 | Val loss: 1.9381 | Batch size: 32 | lr: 0.01 | Duration: 2.69s
[2m[36m(func pid=74012)[0m top1: 0.30830223880597013
[2m[36m(func pid=74012)[0m top5: 0.8708022388059702
[2m[36m(func pid=74012)[0m f1_micro: 0.30830223880597013
[2m[36m(func pid=74012)[0m f1_macro: 0.23739293242545884
[2m[36m(func pid=74012)[0m f1_weighted: 0.2551545134973038
[2m[36m(func pid=74012)[0m f1_per_class: [0.284, 0.053, 0.632, 0.557, 0.074, 0.406, 0.054, 0.314, 0.0, 0.0]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:01:36 (running for 00:44:06.04)
Memory usage on this node: 19.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.268 |      0.27  |                   69 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  36.405 |      0.237 |                   29 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.38572761194029853
[2m[36m(func pid=64536)[0m top5: 0.8675373134328358
[2m[36m(func pid=64536)[0m f1_micro: 0.3857276119402986
[2m[36m(func pid=64536)[0m f1_macro: 0.2700397926710257
[2m[36m(func pid=64536)[0m f1_weighted: 0.38641743529076333
[2m[36m(func pid=64536)[0m f1_per_class: [0.315, 0.53, 0.429, 0.512, 0.061, 0.119, 0.365, 0.344, 0.028, 0.0]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 29] | Train loss: 20.3494 | Steps: 4 | Val loss: 18.4854 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 2.1682 | Steps: 4 | Val loss: 2.2157 | Batch size: 32 | lr: 0.01 | Duration: 2.89s
[2m[36m(func pid=74012)[0m top1: 0.18050373134328357
[2m[36m(func pid=74012)[0m top5: 0.6930970149253731
[2m[36m(func pid=74012)[0m f1_micro: 0.18050373134328357
[2m[36m(func pid=74012)[0m f1_macro: 0.14287132261015287
[2m[36m(func pid=74012)[0m f1_weighted: 0.14369017881841553
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.381, 0.141, 0.041, 0.37, 0.129, 0.368, 0.0, 0.0]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m top1: 0.24253731343283583
[2m[36m(func pid=64536)[0m top5: 0.8628731343283582
[2m[36m(func pid=64536)[0m f1_micro: 0.24253731343283583
[2m[36m(func pid=64536)[0m f1_macro: 0.20890064558755878
[2m[36m(func pid=64536)[0m f1_weighted: 0.24478258926780858
[2m[36m(func pid=64536)[0m f1_per_class: [0.192, 0.073, 0.104, 0.493, 0.037, 0.215, 0.096, 0.492, 0.173, 0.214]
== Status ==
Current time: 2024-01-07 11:01:41 (running for 00:44:11.43)
Memory usage on this node: 19.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.168 |      0.209 |                   70 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  20.349 |      0.143 |                   30 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)

[2m[36m(func pid=64536)[0m 

[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 30] | Train loss: 16.0668 | Steps: 4 | Val loss: 14.2832 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 1.4835 | Steps: 4 | Val loss: 3.0443 | Batch size: 32 | lr: 0.01 | Duration: 2.95s
[2m[36m(func pid=74012)[0m top1: 0.166044776119403
[2m[36m(func pid=74012)[0m top5: 0.7672574626865671
[2m[36m(func pid=74012)[0m f1_micro: 0.166044776119403
[2m[36m(func pid=74012)[0m f1_macro: 0.1479039242197377
[2m[36m(func pid=74012)[0m f1_weighted: 0.17850715147062451
[2m[36m(func pid=74012)[0m f1_per_class: [0.166, 0.1, 0.113, 0.007, 0.033, 0.327, 0.351, 0.19, 0.0, 0.192]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:01:47 (running for 00:44:16.84)
Memory usage on this node: 19.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.484 |      0.231 |                   71 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  16.067 |      0.148 |                   31 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.21641791044776118
[2m[36m(func pid=64536)[0m top5: 0.5559701492537313
[2m[36m(func pid=64536)[0m f1_micro: 0.21641791044776118
[2m[36m(func pid=64536)[0m f1_macro: 0.2308391864142545
[2m[36m(func pid=64536)[0m f1_weighted: 0.13567420837890765
[2m[36m(func pid=64536)[0m f1_per_class: [0.386, 0.377, 0.526, 0.0, 0.162, 0.283, 0.006, 0.328, 0.118, 0.121]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 31] | Train loss: 11.5456 | Steps: 4 | Val loss: 17.6946 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 2.3588 | Steps: 4 | Val loss: 2.6617 | Batch size: 32 | lr: 0.01 | Duration: 2.99s
[2m[36m(func pid=74012)[0m top1: 0.3111007462686567
[2m[36m(func pid=74012)[0m top5: 0.8395522388059702
[2m[36m(func pid=74012)[0m f1_micro: 0.3111007462686567
[2m[36m(func pid=74012)[0m f1_macro: 0.21322869501060754
[2m[36m(func pid=74012)[0m f1_weighted: 0.18637054906091932
[2m[36m(func pid=74012)[0m f1_per_class: [0.286, 0.0, 0.6, 0.469, 0.267, 0.264, 0.0, 0.234, 0.013, 0.0]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:01:52 (running for 00:44:22.39)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.359 |      0.199 |                   72 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  11.546 |      0.213 |                   32 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.16744402985074627
[2m[36m(func pid=64536)[0m top5: 0.7145522388059702
[2m[36m(func pid=64536)[0m f1_micro: 0.16744402985074627
[2m[36m(func pid=64536)[0m f1_macro: 0.19894763088375095
[2m[36m(func pid=64536)[0m f1_weighted: 0.11389102632023766
[2m[36m(func pid=64536)[0m f1_per_class: [0.313, 0.156, 0.467, 0.064, 0.237, 0.298, 0.003, 0.377, 0.024, 0.051]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 32] | Train loss: 19.1590 | Steps: 4 | Val loss: 26.5426 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 2.0850 | Steps: 4 | Val loss: 1.7065 | Batch size: 32 | lr: 0.01 | Duration: 2.97s
[2m[36m(func pid=74012)[0m top1: 0.14598880597014927
[2m[36m(func pid=74012)[0m top5: 0.45615671641791045
[2m[36m(func pid=74012)[0m f1_micro: 0.14598880597014927
[2m[36m(func pid=74012)[0m f1_macro: 0.12005244900571395
[2m[36m(func pid=74012)[0m f1_weighted: 0.07285410543798698
[2m[36m(func pid=74012)[0m f1_per_class: [0.164, 0.0, 0.056, 0.0, 0.074, 0.318, 0.0, 0.518, 0.072, 0.0]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:01:58 (running for 00:44:27.79)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.085 |      0.317 |                   73 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  19.159 |      0.12  |                   33 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.44869402985074625
[2m[36m(func pid=64536)[0m top5: 0.8899253731343284
[2m[36m(func pid=64536)[0m f1_micro: 0.4486940298507463
[2m[36m(func pid=64536)[0m f1_macro: 0.31689751213496054
[2m[36m(func pid=64536)[0m f1_weighted: 0.4468562232927094
[2m[36m(func pid=64536)[0m f1_per_class: [0.351, 0.424, 0.381, 0.579, 0.102, 0.133, 0.538, 0.352, 0.111, 0.196]
[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 33] | Train loss: 19.8617 | Steps: 4 | Val loss: 19.7031 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 2.1045 | Steps: 4 | Val loss: 2.6673 | Batch size: 32 | lr: 0.01 | Duration: 2.87s
[2m[36m(func pid=74012)[0m top1: 0.1501865671641791
[2m[36m(func pid=74012)[0m top5: 0.6068097014925373
[2m[36m(func pid=74012)[0m f1_micro: 0.1501865671641791
[2m[36m(func pid=74012)[0m f1_macro: 0.1315148301496984
[2m[36m(func pid=74012)[0m f1_weighted: 0.12899694521469662
[2m[36m(func pid=74012)[0m f1_per_class: [0.234, 0.123, 0.123, 0.0, 0.052, 0.195, 0.215, 0.242, 0.0, 0.132]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=64536)[0m top1: 0.2873134328358209
[2m[36m(func pid=64536)[0m top5: 0.7262126865671642
[2m[36m(func pid=64536)[0m f1_micro: 0.2873134328358209
[2m[36m(func pid=64536)[0m f1_macro: 0.1977834545178676
[2m[36m(func pid=64536)[0m f1_weighted: 0.2743762208926729
[2m[36m(func pid=64536)[0m f1_per_class: [0.189, 0.506, 0.083, 0.0, 0.095, 0.091, 0.516, 0.214, 0.114, 0.169]
== Status ==
Current time: 2024-01-07 11:02:03 (running for 00:44:32.91)
Memory usage on this node: 19.1/187.4 GiB 
Using AsyncHyperBand: num_stopped=22
Bracket: Iter 75.000: 0.2315
Resources requested: 8.0/72 CPUs, 2.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (2 RUNNING, 22 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00022 | RUNNING    | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   2.105 |      0.198 |                   74 |
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  19.862 |      0.132 |                   34 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 34] | Train loss: 10.4154 | Steps: 4 | Val loss: 19.5179 | Batch size: 32 | lr: 0.1 | Duration: 2.80s
[2m[36m(func pid=64536)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 1.2644 | Steps: 4 | Val loss: 2.7606 | Batch size: 32 | lr: 0.01 | Duration: 2.83s
[2m[36m(func pid=74012)[0m top1: 0.15951492537313433
[2m[36m(func pid=74012)[0m top5: 0.5816231343283582
[2m[36m(func pid=74012)[0m f1_micro: 0.15951492537313433
[2m[36m(func pid=74012)[0m f1_macro: 0.13931922320388096
[2m[36m(func pid=74012)[0m f1_weighted: 0.120224501500508
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.396, 0.135, 0.0, 0.0, 0.102, 0.006, 0.558, 0.149, 0.048]
[2m[36m(func pid=74012)[0m 
== Status ==
Current time: 2024-01-07 11:02:08 (running for 00:44:38.16)
Memory usage on this node: 19.2/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  10.415 |      0.139 |                   35 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=64536)[0m top1: 0.2453358208955224
[2m[36m(func pid=64536)[0m top5: 0.6916977611940298
[2m[36m(func pid=64536)[0m f1_micro: 0.2453358208955224
[2m[36m(func pid=64536)[0m f1_macro: 0.20775093327895813
[2m[36m(func pid=64536)[0m f1_weighted: 0.20508899670680975
[2m[36m(func pid=64536)[0m f1_per_class: [0.144, 0.005, 0.343, 0.457, 0.12, 0.348, 0.0, 0.474, 0.071, 0.115]
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 35] | Train loss: 9.2860 | Steps: 4 | Val loss: 9.3172 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
[2m[36m(func pid=74012)[0m top1: 0.4118470149253731
[2m[36m(func pid=74012)[0m top5: 0.8530783582089553
[2m[36m(func pid=74012)[0m f1_micro: 0.4118470149253731
[2m[36m(func pid=74012)[0m f1_macro: 0.21674153113837794
[2m[36m(func pid=74012)[0m f1_weighted: 0.36014850932916387
[2m[36m(func pid=74012)[0m f1_per_class: [0.082, 0.126, 0.571, 0.581, 0.151, 0.0, 0.567, 0.0, 0.0, 0.088]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 36] | Train loss: 12.3602 | Steps: 4 | Val loss: 9.3409 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 11:02:18 (running for 00:44:47.91)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   9.286 |      0.217 |                   36 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.353544776119403
[2m[36m(func pid=74012)[0m top5: 0.8540111940298507
[2m[36m(func pid=74012)[0m f1_micro: 0.353544776119403
[2m[36m(func pid=74012)[0m f1_macro: 0.259545840805081
[2m[36m(func pid=74012)[0m f1_weighted: 0.36835684596256546
[2m[36m(func pid=74012)[0m f1_per_class: [0.323, 0.47, 0.387, 0.502, 0.0, 0.0, 0.365, 0.504, 0.0, 0.045]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 37] | Train loss: 17.1501 | Steps: 4 | Val loss: 13.8005 | Batch size: 32 | lr: 0.1 | Duration: 3.14s
== Status ==
Current time: 2024-01-07 11:02:23 (running for 00:44:53.34)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  12.36  |      0.26  |                   37 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.20149253731343283
[2m[36m(func pid=74012)[0m top5: 0.7672574626865671
[2m[36m(func pid=74012)[0m f1_micro: 0.20149253731343283
[2m[36m(func pid=74012)[0m f1_macro: 0.08913587149662773
[2m[36m(func pid=74012)[0m f1_weighted: 0.1008308327729247
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.496, 0.087, 0.003, 0.0, 0.0, 0.0, 0.218, 0.016, 0.07]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 38] | Train loss: 10.4106 | Steps: 4 | Val loss: 8.5769 | Batch size: 32 | lr: 0.1 | Duration: 2.88s
== Status ==
Current time: 2024-01-07 11:02:29 (running for 00:44:58.88)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  17.15  |      0.089 |                   38 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.32369402985074625
[2m[36m(func pid=74012)[0m top5: 0.8367537313432836
[2m[36m(func pid=74012)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=74012)[0m f1_macro: 0.24768271450370402
[2m[36m(func pid=74012)[0m f1_weighted: 0.34455051257444314
[2m[36m(func pid=74012)[0m f1_per_class: [0.126, 0.39, 0.383, 0.218, 0.1, 0.26, 0.566, 0.163, 0.0, 0.272]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 39] | Train loss: 9.4494 | Steps: 4 | Val loss: 7.5197 | Batch size: 32 | lr: 0.1 | Duration: 2.82s
== Status ==
Current time: 2024-01-07 11:02:34 (running for 00:45:04.19)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  10.411 |      0.248 |                   39 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.32369402985074625
[2m[36m(func pid=74012)[0m top5: 0.8423507462686567
[2m[36m(func pid=74012)[0m f1_micro: 0.32369402985074625
[2m[36m(func pid=74012)[0m f1_macro: 0.3081425888764597
[2m[36m(func pid=74012)[0m f1_weighted: 0.2876209959314768
[2m[36m(func pid=74012)[0m f1_per_class: [0.388, 0.529, 0.556, 0.381, 0.132, 0.273, 0.039, 0.49, 0.161, 0.133]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 40] | Train loss: 8.1323 | Steps: 4 | Val loss: 9.2352 | Batch size: 32 | lr: 0.1 | Duration: 2.91s
== Status ==
Current time: 2024-01-07 11:02:39 (running for 00:45:09.34)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   9.449 |      0.308 |                   40 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.3064365671641791
[2m[36m(func pid=74012)[0m top5: 0.824160447761194
[2m[36m(func pid=74012)[0m f1_micro: 0.3064365671641791
[2m[36m(func pid=74012)[0m f1_macro: 0.18353978456494377
[2m[36m(func pid=74012)[0m f1_weighted: 0.2393397548344915
[2m[36m(func pid=74012)[0m f1_per_class: [0.381, 0.152, 0.0, 0.588, 0.0, 0.074, 0.0, 0.524, 0.062, 0.054]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 41] | Train loss: 10.5775 | Steps: 4 | Val loss: 7.0011 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 11:02:44 (running for 00:45:14.55)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   8.132 |      0.184 |                   41 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.3460820895522388
[2m[36m(func pid=74012)[0m top5: 0.8652052238805971
[2m[36m(func pid=74012)[0m f1_micro: 0.3460820895522388
[2m[36m(func pid=74012)[0m f1_macro: 0.28316914164210066
[2m[36m(func pid=74012)[0m f1_weighted: 0.363378487494137
[2m[36m(func pid=74012)[0m f1_per_class: [0.179, 0.521, 0.043, 0.12, 0.254, 0.383, 0.525, 0.538, 0.0, 0.269]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 42] | Train loss: 6.6912 | Steps: 4 | Val loss: 6.3608 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 11:02:50 (running for 00:45:19.81)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  10.577 |      0.283 |                   42 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.3381529850746269
[2m[36m(func pid=74012)[0m top5: 0.8824626865671642
[2m[36m(func pid=74012)[0m f1_micro: 0.3381529850746269
[2m[36m(func pid=74012)[0m f1_macro: 0.3058076116737076
[2m[36m(func pid=74012)[0m f1_weighted: 0.379007861844047
[2m[36m(func pid=74012)[0m f1_per_class: [0.305, 0.243, 0.5, 0.404, 0.066, 0.284, 0.503, 0.469, 0.097, 0.188]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 43] | Train loss: 5.4038 | Steps: 4 | Val loss: 11.3057 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 11:02:55 (running for 00:45:25.17)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   6.691 |      0.306 |                   43 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.1767723880597015
[2m[36m(func pid=74012)[0m top5: 0.6464552238805971
[2m[36m(func pid=74012)[0m f1_micro: 0.1767723880597015
[2m[36m(func pid=74012)[0m f1_macro: 0.15900131739878912
[2m[36m(func pid=74012)[0m f1_weighted: 0.17081574613938133
[2m[36m(func pid=74012)[0m f1_per_class: [0.158, 0.027, 0.052, 0.013, 0.066, 0.277, 0.327, 0.421, 0.101, 0.148]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 44] | Train loss: 3.5616 | Steps: 4 | Val loss: 7.2220 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 11:03:00 (running for 00:45:30.60)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   5.404 |      0.159 |                   44 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.3041044776119403
[2m[36m(func pid=74012)[0m top5: 0.8460820895522388
[2m[36m(func pid=74012)[0m f1_micro: 0.3041044776119403
[2m[36m(func pid=74012)[0m f1_macro: 0.24048510293120176
[2m[36m(func pid=74012)[0m f1_weighted: 0.28513267740175763
[2m[36m(func pid=74012)[0m f1_per_class: [0.214, 0.464, 0.301, 0.209, 0.151, 0.183, 0.294, 0.522, 0.0, 0.066]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 45] | Train loss: 7.6885 | Steps: 4 | Val loss: 8.9532 | Batch size: 32 | lr: 0.1 | Duration: 2.94s
== Status ==
Current time: 2024-01-07 11:03:06 (running for 00:45:36.13)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   3.562 |      0.24  |                   45 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.19962686567164178
[2m[36m(func pid=74012)[0m top5: 0.8278917910447762
[2m[36m(func pid=74012)[0m f1_micro: 0.1996268656716418
[2m[36m(func pid=74012)[0m f1_macro: 0.2171552024833494
[2m[36m(func pid=74012)[0m f1_weighted: 0.21575431615055
[2m[36m(func pid=74012)[0m f1_per_class: [0.306, 0.241, 0.235, 0.317, 0.333, 0.244, 0.106, 0.188, 0.143, 0.059]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 46] | Train loss: 7.6935 | Steps: 4 | Val loss: 24.1878 | Batch size: 32 | lr: 0.1 | Duration: 2.98s
== Status ==
Current time: 2024-01-07 11:03:11 (running for 00:45:41.51)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   7.688 |      0.217 |                   46 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.07555970149253731
[2m[36m(func pid=74012)[0m top5: 0.5060634328358209
[2m[36m(func pid=74012)[0m f1_micro: 0.07555970149253731
[2m[36m(func pid=74012)[0m f1_macro: 0.17421564410285406
[2m[36m(func pid=74012)[0m f1_weighted: 0.04189306621919176
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.75, 0.0, 0.124, 0.0, 0.0, 0.534, 0.077, 0.257]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 47] | Train loss: 16.3341 | Steps: 4 | Val loss: 13.3705 | Batch size: 32 | lr: 0.1 | Duration: 3.00s
== Status ==
Current time: 2024-01-07 11:03:17 (running for 00:45:46.97)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   7.693 |      0.174 |                   47 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.31296641791044777
[2m[36m(func pid=74012)[0m top5: 0.7700559701492538
[2m[36m(func pid=74012)[0m f1_micro: 0.31296641791044777
[2m[36m(func pid=74012)[0m f1_macro: 0.17451435685277145
[2m[36m(func pid=74012)[0m f1_weighted: 0.3096567912865945
[2m[36m(func pid=74012)[0m f1_per_class: [0.133, 0.103, 0.178, 0.428, 0.07, 0.0, 0.554, 0.0, 0.0, 0.28]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 48] | Train loss: 13.1323 | Steps: 4 | Val loss: 17.0277 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 11:03:22 (running for 00:45:52.51)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  16.334 |      0.175 |                   48 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.23880597014925373
[2m[36m(func pid=74012)[0m top5: 0.6842350746268657
[2m[36m(func pid=74012)[0m f1_micro: 0.23880597014925373
[2m[36m(func pid=74012)[0m f1_macro: 0.18950732880965646
[2m[36m(func pid=74012)[0m f1_weighted: 0.1368621528634385
[2m[36m(func pid=74012)[0m f1_per_class: [0.323, 0.479, 0.727, 0.108, 0.0, 0.0, 0.0, 0.22, 0.0, 0.038]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 49] | Train loss: 17.7092 | Steps: 4 | Val loss: 10.7006 | Batch size: 32 | lr: 0.1 | Duration: 3.17s
== Status ==
Current time: 2024-01-07 11:03:28 (running for 00:45:58.17)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  13.132 |      0.19  |                   49 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.28824626865671643
[2m[36m(func pid=74012)[0m top5: 0.8647388059701493
[2m[36m(func pid=74012)[0m f1_micro: 0.28824626865671643
[2m[36m(func pid=74012)[0m f1_macro: 0.27649490645205155
[2m[36m(func pid=74012)[0m f1_weighted: 0.254658427801739
[2m[36m(func pid=74012)[0m f1_per_class: [0.387, 0.282, 0.72, 0.517, 0.218, 0.271, 0.003, 0.239, 0.028, 0.099]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 50] | Train loss: 7.8994 | Steps: 4 | Val loss: 22.1089 | Batch size: 32 | lr: 0.1 | Duration: 3.03s
== Status ==
Current time: 2024-01-07 11:03:34 (running for 00:46:03.77)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  17.709 |      0.276 |                   50 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.0960820895522388
[2m[36m(func pid=74012)[0m top5: 0.6669776119402985
[2m[36m(func pid=74012)[0m f1_micro: 0.0960820895522388
[2m[36m(func pid=74012)[0m f1_macro: 0.07509297218726758
[2m[36m(func pid=74012)[0m f1_weighted: 0.09497458666229029
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.025, 0.0, 0.077, 0.0, 0.293, 0.0, 0.127, 0.229]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 51] | Train loss: 18.8298 | Steps: 4 | Val loss: 8.3062 | Batch size: 32 | lr: 0.1 | Duration: 3.25s
== Status ==
Current time: 2024-01-07 11:03:39 (running for 00:46:09.23)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   7.899 |      0.075 |                   51 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.3773320895522388
[2m[36m(func pid=74012)[0m top5: 0.8857276119402985
[2m[36m(func pid=74012)[0m f1_micro: 0.3773320895522388
[2m[36m(func pid=74012)[0m f1_macro: 0.22002072402451098
[2m[36m(func pid=74012)[0m f1_weighted: 0.3333375563401634
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.189, 0.245, 0.573, 0.312, 0.377, 0.308, 0.0, 0.0, 0.196]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 52] | Train loss: 11.0954 | Steps: 4 | Val loss: 11.7777 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 11:03:45 (running for 00:46:14.86)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  18.83  |      0.22  |                   52 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.28591417910447764
[2m[36m(func pid=74012)[0m top5: 0.8512126865671642
[2m[36m(func pid=74012)[0m f1_micro: 0.28591417910447764
[2m[36m(func pid=74012)[0m f1_macro: 0.23855102341995207
[2m[36m(func pid=74012)[0m f1_weighted: 0.20147004582909783
[2m[36m(func pid=74012)[0m f1_per_class: [0.235, 0.469, 0.632, 0.066, 0.099, 0.306, 0.112, 0.411, 0.0, 0.054]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 53] | Train loss: 13.9336 | Steps: 4 | Val loss: 11.8776 | Batch size: 32 | lr: 0.1 | Duration: 3.07s
== Status ==
Current time: 2024-01-07 11:03:50 (running for 00:46:20.25)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  11.095 |      0.239 |                   53 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.2756529850746269
[2m[36m(func pid=74012)[0m top5: 0.8152985074626866
[2m[36m(func pid=74012)[0m f1_micro: 0.2756529850746269
[2m[36m(func pid=74012)[0m f1_macro: 0.21190024353210263
[2m[36m(func pid=74012)[0m f1_weighted: 0.2762302396506393
[2m[36m(func pid=74012)[0m f1_per_class: [0.222, 0.0, 0.177, 0.465, 0.059, 0.309, 0.254, 0.476, 0.0, 0.155]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 54] | Train loss: 10.8197 | Steps: 4 | Val loss: 13.6057 | Batch size: 32 | lr: 0.1 | Duration: 2.87s
== Status ==
Current time: 2024-01-07 11:03:56 (running for 00:46:25.95)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  13.934 |      0.212 |                   54 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.16511194029850745
[2m[36m(func pid=74012)[0m top5: 0.8269589552238806
[2m[36m(func pid=74012)[0m f1_micro: 0.16511194029850745
[2m[36m(func pid=74012)[0m f1_macro: 0.16224230305162168
[2m[36m(func pid=74012)[0m f1_weighted: 0.17035332482004684
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.0, 0.143, 0.223, 0.056, 0.284, 0.134, 0.5, 0.116, 0.167]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 55] | Train loss: 11.9380 | Steps: 4 | Val loss: 12.4393 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 11:04:01 (running for 00:46:31.18)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  10.82  |      0.162 |                   55 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.2294776119402985
[2m[36m(func pid=74012)[0m top5: 0.8260261194029851
[2m[36m(func pid=74012)[0m f1_micro: 0.2294776119402985
[2m[36m(func pid=74012)[0m f1_macro: 0.18377817283928458
[2m[36m(func pid=74012)[0m f1_weighted: 0.14262303560208625
[2m[36m(func pid=74012)[0m f1_per_class: [0.258, 0.43, 0.286, 0.0, 0.068, 0.251, 0.018, 0.45, 0.0, 0.077]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 56] | Train loss: 8.2514 | Steps: 4 | Val loss: 18.0297 | Batch size: 32 | lr: 0.1 | Duration: 2.95s
== Status ==
Current time: 2024-01-07 11:04:06 (running for 00:46:36.67)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  11.938 |      0.184 |                   56 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.24207089552238806
[2m[36m(func pid=74012)[0m top5: 0.6478544776119403
[2m[36m(func pid=74012)[0m f1_micro: 0.24207089552238806
[2m[36m(func pid=74012)[0m f1_macro: 0.1903517925975726
[2m[36m(func pid=74012)[0m f1_weighted: 0.23781607498326868
[2m[36m(func pid=74012)[0m f1_per_class: [0.211, 0.252, 0.076, 0.48, 0.104, 0.226, 0.0, 0.491, 0.0, 0.064]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 57] | Train loss: 15.6971 | Steps: 4 | Val loss: 13.1259 | Batch size: 32 | lr: 0.1 | Duration: 3.11s
== Status ==
Current time: 2024-01-07 11:04:12 (running for 00:46:42.29)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   8.251 |      0.19  |                   57 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.24113805970149255
[2m[36m(func pid=74012)[0m top5: 0.7019589552238806
[2m[36m(func pid=74012)[0m f1_micro: 0.24113805970149255
[2m[36m(func pid=74012)[0m f1_macro: 0.23607935061575785
[2m[36m(func pid=74012)[0m f1_weighted: 0.1612422902253733
[2m[36m(func pid=74012)[0m f1_per_class: [0.278, 0.464, 0.431, 0.026, 0.176, 0.284, 0.0, 0.477, 0.082, 0.141]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 58] | Train loss: 7.3239 | Steps: 4 | Val loss: 8.9398 | Batch size: 32 | lr: 0.1 | Duration: 2.85s
== Status ==
Current time: 2024-01-07 11:04:18 (running for 00:46:47.94)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  15.697 |      0.236 |                   58 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.34281716417910446
[2m[36m(func pid=74012)[0m top5: 0.8773320895522388
[2m[36m(func pid=74012)[0m f1_micro: 0.34281716417910446
[2m[36m(func pid=74012)[0m f1_macro: 0.23108152135099513
[2m[36m(func pid=74012)[0m f1_weighted: 0.3699067704145959
[2m[36m(func pid=74012)[0m f1_per_class: [0.305, 0.172, 0.0, 0.535, 0.036, 0.016, 0.505, 0.477, 0.071, 0.194]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 59] | Train loss: 13.3821 | Steps: 4 | Val loss: 11.8800 | Batch size: 32 | lr: 0.1 | Duration: 2.99s
== Status ==
Current time: 2024-01-07 11:04:23 (running for 00:46:53.49)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   7.324 |      0.231 |                   59 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.25093283582089554
[2m[36m(func pid=74012)[0m top5: 0.7084888059701493
[2m[36m(func pid=74012)[0m f1_micro: 0.25093283582089554
[2m[36m(func pid=74012)[0m f1_macro: 0.25262767317725304
[2m[36m(func pid=74012)[0m f1_weighted: 0.24026250716821118
[2m[36m(func pid=74012)[0m f1_per_class: [0.338, 0.294, 0.609, 0.442, 0.059, 0.317, 0.0, 0.281, 0.025, 0.161]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 60] | Train loss: 4.9990 | Steps: 4 | Val loss: 17.2824 | Batch size: 32 | lr: 0.1 | Duration: 3.10s
== Status ==
Current time: 2024-01-07 11:04:29 (running for 00:46:58.99)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  13.382 |      0.253 |                   60 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.22108208955223882
[2m[36m(func pid=74012)[0m top5: 0.5527052238805971
[2m[36m(func pid=74012)[0m f1_micro: 0.22108208955223882
[2m[36m(func pid=74012)[0m f1_macro: 0.1849156213341478
[2m[36m(func pid=74012)[0m f1_weighted: 0.20632339188827156
[2m[36m(func pid=74012)[0m f1_per_class: [0.172, 0.0, 0.11, 0.0, 0.274, 0.323, 0.47, 0.328, 0.095, 0.078]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 61] | Train loss: 14.2150 | Steps: 4 | Val loss: 11.1660 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:04:34 (running for 00:47:04.56)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   4.999 |      0.185 |                   61 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.375
[2m[36m(func pid=74012)[0m top5: 0.8101679104477612
[2m[36m(func pid=74012)[0m f1_micro: 0.375
[2m[36m(func pid=74012)[0m f1_macro: 0.225577204126914
[2m[36m(func pid=74012)[0m f1_weighted: 0.3745561091312252
[2m[36m(func pid=74012)[0m f1_per_class: [0.121, 0.338, 0.097, 0.456, 0.191, 0.0, 0.547, 0.341, 0.0, 0.164]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 62] | Train loss: 8.5343 | Steps: 4 | Val loss: 12.3675 | Batch size: 32 | lr: 0.1 | Duration: 2.97s
== Status ==
Current time: 2024-01-07 11:04:40 (running for 00:47:09.87)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  14.215 |      0.226 |                   62 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.2887126865671642
[2m[36m(func pid=74012)[0m top5: 0.7112873134328358
[2m[36m(func pid=74012)[0m f1_micro: 0.2887126865671642
[2m[36m(func pid=74012)[0m f1_macro: 0.21427200177503294
[2m[36m(func pid=74012)[0m f1_weighted: 0.23307220572664955
[2m[36m(func pid=74012)[0m f1_per_class: [0.043, 0.479, 0.667, 0.372, 0.0, 0.0, 0.057, 0.369, 0.091, 0.065]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 63] | Train loss: 8.3190 | Steps: 4 | Val loss: 10.6879 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:04:45 (running for 00:47:15.17)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   8.534 |      0.214 |                   63 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.31669776119402987
[2m[36m(func pid=74012)[0m top5: 0.8330223880597015
[2m[36m(func pid=74012)[0m f1_micro: 0.31669776119402987
[2m[36m(func pid=74012)[0m f1_macro: 0.2492231984463808
[2m[36m(func pid=74012)[0m f1_weighted: 0.313784173001475
[2m[36m(func pid=74012)[0m f1_per_class: [0.266, 0.301, 0.289, 0.506, 0.058, 0.271, 0.194, 0.374, 0.0, 0.233]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 64] | Train loss: 5.1348 | Steps: 4 | Val loss: 10.6251 | Batch size: 32 | lr: 0.1 | Duration: 3.04s
== Status ==
Current time: 2024-01-07 11:04:50 (running for 00:47:20.59)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   8.319 |      0.249 |                   64 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.36100746268656714
[2m[36m(func pid=74012)[0m top5: 0.8484141791044776
[2m[36m(func pid=74012)[0m f1_micro: 0.36100746268656714
[2m[36m(func pid=74012)[0m f1_macro: 0.21903480552116705
[2m[36m(func pid=74012)[0m f1_weighted: 0.3711750787967028
[2m[36m(func pid=74012)[0m f1_per_class: [0.133, 0.126, 0.126, 0.47, 0.289, 0.359, 0.564, 0.046, 0.0, 0.077]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 65] | Train loss: 11.1725 | Steps: 4 | Val loss: 11.5230 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 11:04:56 (running for 00:47:26.01)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   5.135 |      0.219 |                   65 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.30223880597014924
[2m[36m(func pid=74012)[0m top5: 0.7569962686567164
[2m[36m(func pid=74012)[0m f1_micro: 0.30223880597014924
[2m[36m(func pid=74012)[0m f1_macro: 0.23350853547956568
[2m[36m(func pid=74012)[0m f1_weighted: 0.2853196970687346
[2m[36m(func pid=74012)[0m f1_per_class: [0.209, 0.476, 0.185, 0.0, 0.06, 0.208, 0.517, 0.231, 0.043, 0.406]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 66] | Train loss: 9.7926 | Steps: 4 | Val loss: 21.1218 | Batch size: 32 | lr: 0.1 | Duration: 3.08s
== Status ==
Current time: 2024-01-07 11:05:01 (running for 00:47:31.61)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  11.173 |      0.234 |                   66 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.21828358208955223
[2m[36m(func pid=74012)[0m top5: 0.6595149253731343
[2m[36m(func pid=74012)[0m f1_micro: 0.21828358208955223
[2m[36m(func pid=74012)[0m f1_macro: 0.23328195820446643
[2m[36m(func pid=74012)[0m f1_weighted: 0.18506598137584906
[2m[36m(func pid=74012)[0m f1_per_class: [0.226, 0.442, 0.629, 0.0, 0.075, 0.244, 0.18, 0.188, 0.155, 0.194]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 67] | Train loss: 11.6794 | Steps: 4 | Val loss: 18.2932 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 11:05:07 (running for 00:47:37.12)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   9.793 |      0.233 |                   67 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.19636194029850745
[2m[36m(func pid=74012)[0m top5: 0.617070895522388
[2m[36m(func pid=74012)[0m f1_micro: 0.19636194029850748
[2m[36m(func pid=74012)[0m f1_macro: 0.18941489853089602
[2m[36m(func pid=74012)[0m f1_weighted: 0.11268439611371742
[2m[36m(func pid=74012)[0m f1_per_class: [0.259, 0.474, 0.625, 0.0, 0.043, 0.008, 0.003, 0.308, 0.0, 0.174]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 68] | Train loss: 9.6334 | Steps: 4 | Val loss: 11.8395 | Batch size: 32 | lr: 0.1 | Duration: 2.96s
== Status ==
Current time: 2024-01-07 11:05:12 (running for 00:47:42.69)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  11.679 |      0.189 |                   68 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.3302238805970149
[2m[36m(func pid=74012)[0m top5: 0.8763992537313433
[2m[36m(func pid=74012)[0m f1_micro: 0.3302238805970149
[2m[36m(func pid=74012)[0m f1_macro: 0.20541858630428694
[2m[36m(func pid=74012)[0m f1_weighted: 0.2303677678229601
[2m[36m(func pid=74012)[0m f1_per_class: [0.082, 0.277, 0.207, 0.459, 0.203, 0.098, 0.021, 0.524, 0.0, 0.182]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 69] | Train loss: 10.6974 | Steps: 4 | Val loss: 23.4370 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 11:05:18 (running for 00:47:48.04)
Memory usage on this node: 16.8/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   9.633 |      0.205 |                   69 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.2140858208955224
[2m[36m(func pid=74012)[0m top5: 0.5144589552238806
[2m[36m(func pid=74012)[0m f1_micro: 0.2140858208955224
[2m[36m(func pid=74012)[0m f1_macro: 0.18214830388691505
[2m[36m(func pid=74012)[0m f1_weighted: 0.14065341308860144
[2m[36m(func pid=74012)[0m f1_per_class: [0.142, 0.482, 0.082, 0.0, 0.308, 0.298, 0.0, 0.203, 0.126, 0.182]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 70] | Train loss: 8.4741 | Steps: 4 | Val loss: 22.0140 | Batch size: 32 | lr: 0.1 | Duration: 3.01s
== Status ==
Current time: 2024-01-07 11:05:24 (running for 00:47:53.76)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  10.697 |      0.182 |                   70 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.16977611940298507
[2m[36m(func pid=74012)[0m top5: 0.5853544776119403
[2m[36m(func pid=74012)[0m f1_micro: 0.16977611940298507
[2m[36m(func pid=74012)[0m f1_macro: 0.1690326607783754
[2m[36m(func pid=74012)[0m f1_weighted: 0.15173168397554068
[2m[36m(func pid=74012)[0m f1_per_class: [0.2, 0.376, 0.564, 0.0, 0.035, 0.0, 0.226, 0.188, 0.0, 0.102]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 71] | Train loss: 12.8780 | Steps: 4 | Val loss: 11.3004 | Batch size: 32 | lr: 0.1 | Duration: 3.06s
== Status ==
Current time: 2024-01-07 11:05:29 (running for 00:47:59.36)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   8.474 |      0.169 |                   71 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.32322761194029853
[2m[36m(func pid=74012)[0m top5: 0.7919776119402985
[2m[36m(func pid=74012)[0m f1_micro: 0.32322761194029853
[2m[36m(func pid=74012)[0m f1_macro: 0.24129864764885606
[2m[36m(func pid=74012)[0m f1_weighted: 0.3132031612239818
[2m[36m(func pid=74012)[0m f1_per_class: [0.252, 0.197, 0.632, 0.294, 0.062, 0.0, 0.595, 0.092, 0.077, 0.211]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 72] | Train loss: 5.1319 | Steps: 4 | Val loss: 8.8942 | Batch size: 32 | lr: 0.1 | Duration: 3.12s
== Status ==
Current time: 2024-01-07 11:05:35 (running for 00:48:04.98)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |  12.878 |      0.241 |                   72 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.3712686567164179
[2m[36m(func pid=74012)[0m top5: 0.882929104477612
[2m[36m(func pid=74012)[0m f1_micro: 0.3712686567164179
[2m[36m(func pid=74012)[0m f1_macro: 0.2319643701032914
[2m[36m(func pid=74012)[0m f1_weighted: 0.2988487042897931
[2m[36m(func pid=74012)[0m f1_per_class: [0.358, 0.136, 0.338, 0.555, 0.0, 0.048, 0.26, 0.421, 0.074, 0.13]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 73] | Train loss: 4.9200 | Steps: 4 | Val loss: 11.8599 | Batch size: 32 | lr: 0.1 | Duration: 2.93s
== Status ==
Current time: 2024-01-07 11:05:40 (running for 00:48:10.56)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   5.132 |      0.232 |                   73 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


[2m[36m(func pid=74012)[0m top1: 0.3316231343283582
[2m[36m(func pid=74012)[0m top5: 0.792910447761194
[2m[36m(func pid=74012)[0m f1_micro: 0.3316231343283582
[2m[36m(func pid=74012)[0m f1_macro: 0.1945581208675184
[2m[36m(func pid=74012)[0m f1_weighted: 0.30088994506023553
[2m[36m(func pid=74012)[0m f1_per_class: [0.0, 0.524, 0.048, 0.0, 0.0, 0.381, 0.513, 0.149, 0.07, 0.26]
[2m[36m(func pid=74012)[0m 
[2m[36m(func pid=74012)[0m [N0-GPU0] | [Epoch: 74] | Train loss: 5.2353 | Steps: 4 | Val loss: 11.1379 | Batch size: 32 | lr: 0.1 | Duration: 2.89s
== Status ==
Current time: 2024-01-07 11:05:46 (running for 00:48:16.09)
Memory usage on this node: 16.7/187.4 GiB 
Using AsyncHyperBand: num_stopped=23
Bracket: Iter 75.000: 0.23
Resources requested: 4.0/72 CPUs, 1.0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (1 RUNNING, 23 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00023 | RUNNING    | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   4.92  |      0.195 |                   74 |
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
... 4 more trials not shown (4 TERMINATED)


== Status ==
Current time: 2024-01-07 11:05:47 (running for 00:48:16.73)
Memory usage on this node: 16.3/187.4 GiB 
Using AsyncHyperBand: num_stopped=24
Bracket: Iter 75.000: 0.2285
Resources requested: 0/72 CPUs, 0/4 GPUs, 0.0/120.0 GiB heap, 0.0/55.42 GiB objects (0.0/1.0 accelerator_type:V100)
Result logdir: /home/ajsanchez/GitHub/ssl-bsu/output/finetuning/ray_tune/Supervised
Number of trials: 24/24 (24 TERMINATED)
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+
| Trial name        | status     | loc                 |     lr |   momentum |   weight_decay |    loss |   f1_macro |   training_iteration |
|-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------|
| train_952df_00000 | TERMINATED | 192.168.7.53:151704 | 0.0001 |       0.99 |         0      |   2.48  |      0.116 |                   75 |
| train_952df_00001 | TERMINATED | 192.168.7.53:152082 | 0.001  |       0.99 |         0      |   1.842 |      0.183 |                   75 |
| train_952df_00002 | TERMINATED | 192.168.7.53:152502 | 0.01   |       0.99 |         0      |   2.202 |      0.251 |                   75 |
| train_952df_00003 | TERMINATED | 192.168.7.53:152923 | 0.1    |       0.99 |         0      |  31.765 |      0.212 |                  100 |
| train_952df_00004 | TERMINATED | 192.168.7.53:170188 | 0.0001 |       0.9  |         0      |   2.602 |      0.158 |                   75 |
| train_952df_00005 | TERMINATED | 192.168.7.53:170672 | 0.001  |       0.9  |         0      |   2.155 |      0.191 |                   75 |
| train_952df_00006 | TERMINATED | 192.168.7.53:171228 | 0.01   |       0.9  |         0      |   1.971 |      0.155 |                   75 |
| train_952df_00007 | TERMINATED | 192.168.7.53:176697 | 0.1    |       0.9  |         0      |  11.224 |      0.195 |                  100 |
| train_952df_00008 | TERMINATED | 192.168.7.53:1142   | 0.0001 |       0.99 |         0.0001 |   2.245 |      0.168 |                   75 |
| train_952df_00009 | TERMINATED | 192.168.7.53:1670   | 0.001  |       0.99 |         0.0001 |   1.538 |      0.233 |                   75 |
| train_952df_00010 | TERMINATED | 192.168.7.53:2489   | 0.01   |       0.99 |         0.0001 |   0.97  |      0.142 |                   75 |
| train_952df_00011 | TERMINATED | 192.168.7.53:12671  | 0.1    |       0.99 |         0.0001 | 163.854 |      0.142 |                   75 |
| train_952df_00012 | TERMINATED | 192.168.7.53:20296  | 0.0001 |       0.9  |         0.0001 |   2.656 |      0.149 |                   75 |
| train_952df_00013 | TERMINATED | 192.168.7.53:20299  | 0.001  |       0.9  |         0.0001 |   2.438 |      0.134 |                   75 |
| train_952df_00014 | TERMINATED | 192.168.7.53:21639  | 0.01   |       0.9  |         0.0001 |   1.617 |      0.191 |                   75 |
| train_952df_00015 | TERMINATED | 192.168.7.53:31048  | 0.1    |       0.9  |         0.0001 |   5.802 |      0.162 |                  100 |
| train_952df_00016 | TERMINATED | 192.168.7.53:38780  | 0.0001 |       0.99 |         1e-05  |   2.381 |      0.135 |                   75 |
| train_952df_00017 | TERMINATED | 192.168.7.53:39363  | 0.001  |       0.99 |         1e-05  |   0.981 |      0.259 |                  100 |
| train_952df_00018 | TERMINATED | 192.168.7.53:40628  | 0.01   |       0.99 |         1e-05  |   6.719 |      0.172 |                   75 |
| train_952df_00019 | TERMINATED | 192.168.7.53:55167  | 0.1    |       0.99 |         1e-05  |  85.151 |      0.227 |                   75 |
| train_952df_00020 | TERMINATED | 192.168.7.53:57543  | 0.0001 |       0.9  |         1e-05  |   2.597 |      0.138 |                   75 |
| train_952df_00021 | TERMINATED | 192.168.7.53:59162  | 0.001  |       0.9  |         1e-05  |   2.072 |      0.202 |                   75 |
| train_952df_00022 | TERMINATED | 192.168.7.53:64536  | 0.01   |       0.9  |         1e-05  |   1.264 |      0.208 |                   75 |
| train_952df_00023 | TERMINATED | 192.168.7.53:74012  | 0.1    |       0.9  |         1e-05  |   5.235 |      0.193 |                   75 |
+-------------------+------------+---------------------+--------+------------+----------------+---------+------------+----------------------+


2024-01-07 11:05:47,038	INFO tune.py:798 -- Total run time: 2897.73 seconds (2896.71 seconds for the tuning loop).
[2m[36m(func pid=74012)[0m top1: 0.23787313432835822
[2m[36m(func pid=74012)[0m top5: 0.7910447761194029
[2m[36m(func pid=74012)[0m f1_micro: 0.23787313432835822
[2m[36m(func pid=74012)[0m f1_macro: 0.1930935408864617
[2m[36m(func pid=74012)[0m f1_weighted: 0.14607715686977243
[2m[36m(func pid=74012)[0m f1_per_class: [0.247, 0.462, 0.455, 0.073, 0.0, 0.008, 0.015, 0.483, 0.117, 0.069]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1341345.1 ON aap04 CANCELLED AT 2024-01-07T11:05:54 ***
srun: error: aap04: task 0: Exited with exit code 1
